diff --git a/Documentation/devicetree/bindings/net/cdns,macb.yaml b/Documentation/devicetree/bindings/net/cdns,macb.yaml
index 3c30dd23c..399483fed 100644
--- a/Documentation/devicetree/bindings/net/cdns,macb.yaml
+++ b/Documentation/devicetree/bindings/net/cdns,macb.yaml
@@ -27,6 +27,7 @@ properties:
 
       - items:
           - enum:
+              - amd,versal2-10gbe     # AMD Versal2 10gbe
               - xlnx,versal-gem       # Xilinx Versal
               - xlnx,zynq-gem         # Xilinx Zynq-7xxx SoC
               - xlnx,zynqmp-gem       # Xilinx Zynq Ultrascale+ MPSoC
@@ -111,6 +112,8 @@ properties:
   iommus:
     maxItems: 1
 
+  dma-coherent: true
+
   power-domains:
     maxItems: 1
 
diff --git a/Documentation/devicetree/bindings/net/xlnx,axi-ethernet.yaml b/Documentation/devicetree/bindings/net/xlnx,axi-ethernet.yaml
index fb02e5794..49bceb201 100644
--- a/Documentation/devicetree/bindings/net/xlnx,axi-ethernet.yaml
+++ b/Documentation/devicetree/bindings/net/xlnx,axi-ethernet.yaml
@@ -4,29 +4,69 @@
 $id: http://devicetree.org/schemas/net/xlnx,axi-ethernet.yaml#
 $schema: http://devicetree.org/meta-schemas/core.yaml#
 
-title: AXI 1G/2.5G Ethernet Subsystem
+title: XILINX AXI Ethernet Subsystem
 
 description: |
-  Also called  AXI 1G/2.5G Ethernet Subsystem, the xilinx axi ethernet IP core
-  provides connectivity to an external ethernet PHY supporting different
-  interfaces: MII, GMII, RGMII, SGMII, 1000BaseX. It also includes two
-  segments of memory for buffering TX and RX, as well as the capability of
-  offloading TX/RX checksum calculation off the processor.
+  Following MAC configurations are supported-
+  a) AXI 1G/2.5G Ethernet Subsystem.
+  b) 10G/25G High Speed Ethernet Subsystem.
+  c) 10 Gigabit Ethernet Subsystem.
+  d) USXGMII Ethernet Subsystem.
+  e) MRMAC Ethernet Subsystem.
+  f) 1G/10G/25G Switching Ethernet Subsystem.
+  g) DCMAC Ethernet Subsystem.
+
+  AXI 1G/2.5G Ethernet Subsystem- also called  AXI 1G/2.5G Ethernet Subsystem,
+  the xilinx axi ethernet IP core provides connectivity to an external ethernet
+  PHY supporting different interfaces: MII, GMII, RGMII, SGMII, 1000BaseX.
+  It also includes two segments of memory for buffering TX and RX, as well as
+  the capability of offloading TX/RX checksum calculation off the processor.
 
   Management configuration is done through the AXI interface, while payload is
   sent and received through means of an AXI DMA controller. This driver
   includes the DMA driver code, so this driver is incompatible with AXI DMA
   driver.
 
+  10G/25G High Speed Ethernet Subsystem implements the 25G Ethernet Media
+  Access Controller (MAC) with a Physical Coding Sublayer (PCS) as specified
+  by the 25G Ethernet Consortium. Legacy operation at 10 Gb/s is supported.
+
+  The 10G Ethernet subsystem provides 10 Gb/s Ethernet MAC, Physical Coding
+  Sublayer (PCS) and Physical Medium Attachment (PMA) transmit and receive
+  functionality over an AXI4-Stream interface.
+
+  The Universal Serial 10GE Media Independent Interface (USXGMII) ethernet
+  subsystem implements an Ethernet Media Access Controller (MAC) with a
+  mechanism to carry a single port of 10M, 100M, 1G, 2.5G, 5G or 10GE over an
+  IEEE 802.3 Clause 49 BASE-R physical coding sublayer/physical layer (PCS/PHY).
+
+  MRMAC is a hardened Ethernet IP on Versal supporting multiple rates from
+  10G to 100G which can be used with a soft DMA controller.
+
+  The Xilinx 1G/10G/25G Switching Ethernet Subsystem contains integrated MAC
+  and PCS and it supports runtime switchable speeds among 1G,10G & 25G
+
+  600G Channelized Multirate Ethernet Subsystem (DCMAC Subsystem) is hardened
+  Ethernet IP that can be configured for up to six ports with independent MAC
+  and PHY functions at the IEEE Standard MAC Rates from 100GE to 400GE
+  and an overall maximum bandwidth of 600 Gbps.
+
 maintainers:
   - Radhey Shyam Pandey <radhey.shyam.pandey@xilinx.com>
 
 properties:
   compatible:
     enum:
-      - xlnx,axi-ethernet-1.00.a
-      - xlnx,axi-ethernet-1.01.a
-      - xlnx,axi-ethernet-2.01.a
+      - xlnx,axi-ethernet-1.00.a # AXI 1G Ethernet Subsystem.
+      - xlnx,axi-ethernet-1.01.a # AXI 1G Ethernet Subsystem.
+      - xlnx,axi-ethernet-2.01.a # AXI 1G Ethernet Subsystem.
+      - xlnx,axi-2_5-gig-ethernet-1.0 # AXI 2.5G Ethernet Subsystem.
+      - xlnx,ten-gig-eth-mac # 10 Gigabit Ethernet Subsystem.
+      - xlnx,xxv-ethernet-1.0 # 10G/25G High Speed Ethernet Subsystem.
+      - xlnx,xxv-usxgmii-ethernet-1.0 # USXGMII Ethernet Subsystem.
+      - xlnx,mrmac-ethernet-1.0 # MRMAC Ethernet Subsystem.
+      - xlnx,ethernet-1-10-25g-2.7 # 1G/10G/25G Switching Ethernet Subsystem.
+      - xlnx,dcmac-2.4 # DCMAC Ethernet Subsystem.
 
   reg:
     description:
@@ -34,8 +74,9 @@ properties:
       and length of the AXI DMA controller IO space, unless
       axistream-connected is specified, in which case the reg
       attribute of the node referenced by it is used.
+      Address and length of Ethernet Offload Engine IO space.
     minItems: 1
-    maxItems: 2
+    maxItems: 3
 
   interrupts:
     items:
@@ -88,8 +129,9 @@ properties:
   xlnx,switch-x-sgmii:
     type: boolean
     description:
-      Indicate the Ethernet core is configured to support both 1000BaseX and
-      SGMII modes. If set, the phy-mode should be set to match the mode
+      Indicate that Ethernet core supports either 1000BaseX <-> SGMII modes
+      switching in 1G MAC or 2500BaseX <-> SGMII modes switching for
+      AXI 2.5G MAC. If set, the phy-mode should be set to match the mode
       selected on core reset (i.e. by the basex_or_sgmii core input line).
 
   clocks:
@@ -136,20 +178,264 @@ properties:
       Should be "rx_chan0", "rx_chan1" ... "rx_chan15" for DMA Rx channel
     minItems: 2
     maxItems: 32
+  dma-coherent: true
+
+  xlnx,eth-hasnobuf:
+    type: boolean
+    description: Used when 1G MAC is configured in non-processor mode.
+
+  xlnx,usxgmii-rate:
+    $ref: /schemas/types.yaml#/definitions/uint32
+    description: USXGMII PHY speed
+
+  xlnx,rxtsfifo:
+    $ref: /schemas/types.yaml#/definitions/phandle
+    description: Configures the axi fifo for receive timestamping.
+
+  axififo-connected:
+    $ref: /schemas/types.yaml#/definitions/phandle
+    description: Configures the axi fifo for transmit timestamping.
+
+  ptp-hardware-clock:
+    $ref: /schemas/types.yaml#/definitions/phandle
+    description: Indicate the xilinx PTP device node to read PTP clock
+      index(/dev/ptp0 etc). This property is for MRMAC and 10G/25G.
+
+  xlnx,mrmac-rate:
+    $ref: /schemas/types.yaml#/definitions/uint32
+    description: MRMAC rate in Mbps.
+    deprecated: true
+
+  xlnx,gtlane:
+    $ref: /schemas/types.yaml#/definitions/uint32
+    description: Indicate the GT reset and speed control lane for the
+      the current MRMAC lane.
+
+  xlnx,gtpll:
+    $ref: /schemas/types.yaml#/definitions/phandle
+    description: Handle to AXI GPIO instance for GT PLL mask control.
+      This is required to control the common PLL mask bits.
+
+  xlnx,gtctrl:
+    $ref: /schemas/types.yaml#/definitions/phandle
+    description: Handle to AXI GPIO instance for GT speed and reset
+      control for each MRMAC lane.
+
+  xlnx,phcindex:
+    $ref: /schemas/types.yaml#/definitions/uint32
+    description: Indicate the index of the physical hardware clock
+      to be used as per PTP clock connected to the given
+      MRMAC lane.
+    deprecated: true
+
+  xlnx,runtime-switch:
+    $ref: /schemas/types.yaml#/definitions/string
+    description: Indicates speed switching capability of 1G/10G/25G
+      ethernet IP. Can be either "1G / 10G", which is the default switching
+      capability if this property is not present or "1G / 10G / 25G".
+
+  xlnx,has-auto-neg:
+    type: boolean
+    description: Indicates whether IP supports Auto Negotiation
+      and Link Training.
+
+  max-speed: true
+
+  gt_ctrl-gpios:
+    description: Phandle and specifier for GT control GPIO used to
+      configure GT.
+
+  gt_rx_dpath-gpios:
+    description: Phandle and specifier for the GPIO used to
+      reset GT Rx datapath.
+
+  gt_tx_dpath-gpios:
+    description: Phandle and specifier for the GPIO used to
+      reset GT Tx datapath.
+
+  gt_tx_rst_done-gpios:
+    description: Phandle and specifier for the GPIO used to
+      get GT Tx reset status.
+
+  gt_rx_rst_done-gpios:
+    description: Phandle and specifier for the GPIO used to
+      get GT Rx reset status.
+
+  gt_rsts-gpios:
+    description: Phandle and specifier for the GPIO used to
+      reset Tx and Rx serdes.
+
+  xlnx,gt-mode:
+    description: Describes GT Quad interface operating mode.
+    $ref: /schemas/types.yaml#/definitions/string
+
+  xlnx,axistream-dwidth:
+    description: Describes AXI4-stream data width.
+    $ref: /schemas/types.yaml#/definitions/uint32
+
+  reg-names:
+    oneOf:
+      - items:
+          - const: mac
+          - const: dma
+          - const: eoe
+      - items:
+          - enum:
+              - mac
+          - enum:
+              - dma
+              - eoe
+      - items:
+          - const: mac
+
+  xlnx,has-hw-offload:
+    type: boolean
+    description: Used when Ethernet Offload is connected to existing Ethernet IP.
+
+  xlnx,tx-hw-offload:
+    description:
+      Segmentation offload. 0 for disabling segmentation offload,
+      1 for checksum offload and 2 for both checksum and UDP segmentation offload.
+    $ref: /schemas/types.yaml#/definitions/uint32
+    enum: [0, 1, 2]
+
+  xlnx,rx-hw-offload:
+    description:
+      Receive offload. 0 for disabling receive offload,
+      1 for checksum offload and 2 for both checksum and UDP receive offload.
+    $ref: /schemas/types.yaml#/definitions/uint32
+    enum: [0, 1, 2]
 
 required:
   - compatible
   - interrupts
   - reg
   - xlnx,rxmem
-  - phy-handle
 
 allOf:
   - $ref: /schemas/net/ethernet-controller.yaml#
 
+  - if:
+      properties:
+        compatible:
+          contains:
+            enum:
+              - xlnx,xxv-usxgmii-ethernet-1.0
+
+    then:
+      properties:
+        xlnx,usxgmii-rate:
+          enum: [10, 100, 1000, 2500, 5000, 10000]
+
+  - if:
+      properties:
+        compatible:
+          contains:
+            enum:
+              - xlnx,mrmac-ethernet-1.0
+
+    then:
+      properties:
+        max-speed:
+          enum: [10000, 25000]
+
+        xlnx,gtlane:
+          enum: [0, 1, 2, 3]
+
+        xlnx,phcindex:
+          enum: [0, 1, 2, 3]
+
+        xlnx,gt-mode:
+          enum:
+            - narrow  # 10G/25G
+            - wide  # 10G/25G
+            - xlaui4_narrow # 40G
+            - xlaui4_wide # 40G
+            - laui2_narrow # 50G
+            - laui2_wide  # 50G
+            - 50laui1_narrow  # 50G
+            - caui2_narrow  # 100G
+            - caui4_narrow  # 100G
+            - caui4_wide  # 100G
+
+        xlnx,axistream-dwidth:
+          enum: [32, 64, 128, 256, 384]
+
+      required:
+        - max-speed
+        - xlnx,gtlane
+        - xlnx,gtpll
+        - xlnx,gtctrl
+
+  - if:
+      properties:
+        compatible:
+          contains:
+            enum:
+              - xlnx,ethernet-1-10-25g-2.7
+
+    then:
+      properties:
+        xlnx,runtime-switch:
+          enum:
+            - 1G / 10G
+            - 1G / 10G / 25G
+
+  - if:
+      properties:
+        compatible:
+          contains:
+            enum:
+              - xlnx,axi-ethernet-1.00.a
+              - xlnx,axi-ethernet-1.01.a
+              - xlnx,axi-ethernet-2.01.a
+              - xlnx,axi-2_5-gig-ethernet-1.0
+    then:
+      required:
+        - phy-handle
+
+  - if:
+      properties:
+        compatible:
+          contains:
+            enum:
+              - xlnx,dcmac-2.4
+    then:
+      properties:
+        max-speed:
+          enum: [100000, 200000, 400000]
+
+        gt_ctrl-gpios:
+          maxItems: 32
+
+        gt_rx_dpath-gpios:
+          maxItems: 23
+
+        gt_tx_dpath-gpios:
+          maxItems: 23
+
+        gt_tx_rst_done-gpios:
+          maxItems: 24
+
+        gt_rx_rst_done-gpios:
+          maxItems: 24
+
+        gt_rsts-gpios:
+          maxItems: 14
+
+      required:
+        - max-speed
+        - gt_ctrl-gpios
+        - gt_rx_dpath-gpios
+        - gt_tx_dpath-gpios
+        - gt_tx_rst_done-gpios
+        - gt_rx_rst_done-gpios
+        - gt_rsts-gpios
+
 additionalProperties: false
 
 examples:
+# 1) AXI 1G/2.5G Ethernet Subsystem + AXIDMA without "axistream-connected" property
   - |
     axi_ethernet_eth: ethernet@40c00000 {
         compatible = "xlnx,axi-ethernet-1.00.a";
@@ -175,6 +461,7 @@ examples:
         };
     };
 
+# 2) AXI 1G/2.5G Ethernet Subsystem + AXIDMA with "axistream-connected" property
   - |
     axi_ethernet_eth1: ethernet@40000000 {
         compatible = "xlnx,axi-ethernet-1.00.a";
@@ -198,3 +485,119 @@ examples:
             };
         };
     };
+
+# 3) #Example for MRMAC Ethernet subsystem
+  - |
+    gt_pll: gpio@a4000000 {
+        reg = <0xa4000000 0x10000>;
+
+      };
+
+    gt_ctrl: gpio@a4010000 {
+        reg = <0xa4010000 0x40000>;
+
+    };
+
+    mrmac: ethernet@80000000 {
+        axistream-connected = <&dma>;
+        compatible = "xlnx,mrmac-ethernet-1.0";
+        reg = <0xa4090000 0x1000>;
+        interrupt-parent = <&gic>;
+        interrupts = <0 91 4>;
+        max-speed = <10000>;
+        xlnx,gtpll = <&gt_pll>;
+        xlnx,gtctrl = <&gt_ctrl>;
+        xlnx,gtlane = <0x0>;
+        xlnx,rxmem = <0x8000>;
+        ptp-hardware-clock = <&ptp_device>;
+
+    };
+
+# 4) #Example for DCMAC Ethernet subsystem
+  - |
+    gpio1: gpio@a4130000 {
+        compatible = "xlnx,xps-gpio-1.00.a";
+        reg = <0xa4130000 0x10000>;
+        #gpio-cells = <0x2>;
+        gpio-controller;
+      };
+
+    gpio2: gpio@a4150000 {
+        compatible = "xlnx,xps-gpio-1.00.a";
+        reg = <0xa4150000 0x10000>;
+        #gpio-cells = <0x2>;
+        gpio-controller;
+    };
+
+    gpio3: gpio@a4140000 {
+        compatible = "xlnx,xps-gpio-1.00.a";
+        reg = <0xa4140000 0x10000>;
+        #gpio-cells = <0x2>;
+        gpio-controller;
+    };
+
+    dcmac: ethernet@a4000000 {
+        axistream-connected = <&dma>;
+        compatible = "xlnx,dcmac-2.4";
+        reg = <0xa4000000 0x100000>;
+        interrupt-parent = <&gic>;
+        interrupts = <0x0 0x5c 0x4 0x0 0x5d 0x4>;
+        xlnx,rxmem = <0x8000>;
+        max-speed = <100000>;
+
+        gt_ctrl-gpios = <&gpio1 0 0>, <&gpio1 1 0>, <&gpio1 2 0>, <&gpio1 3 0>, <&gpio1 4 0>, <&gpio1 5 0>,
+        <&gpio1 6 0>, <&gpio1 7 0>, <&gpio1 8 0>, <&gpio1 9 0>, <&gpio1 10 0>, <&gpio1 11 0>, <&gpio1 12 0>,
+        <&gpio1 13 0>, <&gpio1 14 0>, <&gpio1 15 0>, <&gpio1 16 0>, <&gpio1 17 0>, <&gpio1 18 0>,
+        <&gpio1 19 0>, <&gpio1 20 0>, <&gpio1 21 0>, <&gpio1 22 0>, <&gpio1 23 0>, <&gpio1 24 0>,
+        <&gpio1 25 0>, <&gpio1 26 0>, <&gpio1 27 0>, <&gpio1 28 0>, <&gpio1 29 0>, <&gpio1 30 0>,
+        <&gpio1 31 0>;
+
+        gt_rx_dpath-gpios = <&gpio2 0 0>, <&gpio2 1 0>, <&gpio2 2 0>, <&gpio2 3 0>, <&gpio2 4 0>,
+        <&gpio2 5 0>, <&gpio2 6 0>, <&gpio2 7 0>, <&gpio2 8 0>, <&gpio2 9 0>, <&gpio2 10 0>, <&gpio2 11 0>,
+        <&gpio2 12 0>, <&gpio2 13 0>, <&gpio2 14 0>, <&gpio2 15 0>, <&gpio2 16 0>, <&gpio2 17 0>,
+        <&gpio2 18 0>, <&gpio2 19 0>, <&gpio2 20 0>, <&gpio2 21 0>, <&gpio2 22 0>;
+
+        gt_tx_dpath-gpios = <&gpio2 0 0>, <&gpio2 1 0>, <&gpio2 2 0>, <&gpio2 3 0>, <&gpio2 4 0>,
+        <&gpio2 5 0>, <&gpio2 6 0>, <&gpio2 7 0>, <&gpio2 8 0>, <&gpio2 9 0>, <&gpio2 10 0>, <&gpio2 11 0>,
+        <&gpio2 12 0>, <&gpio2 13 0>, <&gpio2 14 0>, <&gpio2 15 0>, <&gpio2 16 0>, <&gpio2 17 0>,
+        <&gpio2 18 0>, <&gpio2 19 0>, <&gpio2 20 0>, <&gpio2 21 0>, <&gpio2 22 0>;
+
+        gt_tx_rst_done-gpios =  <&gpio3 0 0>, <&gpio3 1 0>, <&gpio3 2 0>, <&gpio3 3 0>, <&gpio3 4 0>,
+        <&gpio3 5 0>, <&gpio3 6 0>, <&gpio3 7 0>, <&gpio3 8 0>, <&gpio3 9 0>, <&gpio3 10 0>,
+        <&gpio3 11 0>, <&gpio3 12 0>, <&gpio3 13 0>, <&gpio3 14 0>, <&gpio3 15 0>, <&gpio3 16 0>,
+        <&gpio3 17 0>, <&gpio3 18 0>, <&gpio3 19 0>, <&gpio3 20 0>, <&gpio3 21 0>, <&gpio3 22 0>,
+        <&gpio3 23 0>;
+
+        gt_rx_rst_done-gpios =  <&gpio3 24 0>, <&gpio3 25 0>, <&gpio3 26 0>, <&gpio3 27 0>,
+        <&gpio3 28 0>, <&gpio3 29 0>, <&gpio3 30 0>, <&gpio3 31 0>, <&gpio3 32 0>, <&gpio3 33 0>,
+        <&gpio3 34 0>, <&gpio3 35 0>, <&gpio3 36 0>, <&gpio3 37 0>, <&gpio3 38 0>, <&gpio3 39 0>,
+        <&gpio3 40 0>, <&gpio3 41 0>, <&gpio3 42 0>, <&gpio3 43 0>, <&gpio3 44 0>, <&gpio3 45 0>,
+        <&gpio3 46 0>, <&gpio3 47 0>;
+
+        gt_rsts-gpios = <&gpio3 0 0>, <&gpio3 1 0>, <&gpio3 2 0>, <&gpio3 3 0>, <&gpio3 4 0>, <&gpio3 5 0>,
+        <&gpio3 6 0>, <&gpio3 7 0>, <&gpio3 8 0>, <&gpio3 9 0>, <&gpio3 10 0>, <&gpio3 11 0>, <&gpio3 12 0>,
+        <&gpio3 13 0>;
+    };
+
+# 5) #Example for Ethernet Offload Engine
+  - |
+    axi_ethernet_eoe: ethernet@80020000 {
+        compatible = "xlnx,axi-ethernet-1.00.a";
+        interrupts = <0>;
+        clock-names = "s_axi_lite_clk", "axis_clk", "ref_clk", "mgt_clk";
+        clocks = <&axi_clk>, <&axi_clk>, <&pl_enet_ref_clk>, <&mgt_clk>;
+        phy-mode = "mii";
+        reg = <0x80020000 0x10000>,<0x80008000 0x8000>;
+        reg-names = "mac", "eoe";
+        phy-handle = <&phy2>;
+        xlnx,rxmem = <0x800>;
+
+        mdio {
+            #address-cells = <1>;
+            #size-cells = <0>;
+            phy2: ethernet-phy@1 {
+                device_type = "ethernet-phy";
+                reg = <1>;
+            };
+        };
+    };
diff --git a/Documentation/devicetree/bindings/net/xlnx,emaclite.yaml b/Documentation/devicetree/bindings/net/xlnx,emaclite.yaml
index 92d8ade98..e16384aff 100644
--- a/Documentation/devicetree/bindings/net/xlnx,emaclite.yaml
+++ b/Documentation/devicetree/bindings/net/xlnx,emaclite.yaml
@@ -29,6 +29,9 @@ properties:
   interrupts:
     maxItems: 1
 
+  clocks:
+    maxItems: 1
+
   phy-handle: true
 
   local-mac-address: true
@@ -45,6 +48,7 @@ required:
   - compatible
   - reg
   - interrupts
+  - clocks
   - phy-handle
 
 additionalProperties: false
@@ -56,6 +60,7 @@ examples:
         reg = <0x40e00000 0x10000>;
         interrupt-parent = <&axi_intc_1>;
         interrupts = <1>;
+        clocks = <&dummy>;
         local-mac-address = [00 00 00 00 00 00];
         phy-handle = <&phy0>;
         xlnx,rx-ping-pong;
diff --git a/drivers/net/ethernet/cadence/macb.h b/drivers/net/ethernet/cadence/macb.h
index 2847278d9..6a5da7823 100644
--- a/drivers/net/ethernet/cadence/macb.h
+++ b/drivers/net/ethernet/cadence/macb.h
@@ -563,11 +563,21 @@
 #define GEM_RX_SCR_BYPASS_SIZE			1
 #define GEM_TX_SCR_BYPASS_OFFSET		8
 #define GEM_TX_SCR_BYPASS_SIZE			1
+#define GEM_RX_SYNC_RESET_OFFSET		2
+#define GEM_RX_SYNC_RESET_SIZE			1
 #define GEM_TX_EN_OFFSET			1
 #define GEM_TX_EN_SIZE				1
 #define GEM_SIGNAL_OK_OFFSET			0
 #define GEM_SIGNAL_OK_SIZE			1
 
+/* Constants for USX_CONTROL */
+#define HS_SPEED_10000M				4
+#define HS_SPEED_5000M				3
+#define HS_SPEED_2500M				2
+#define HS_SPEED_1000M				1
+#define MACB_SERDES_RATE_10G			1
+#define MACB_SERDES_RATE_5G_2G5_1G		0
+
 /* Bitfields in USX_STATUS. */
 #define GEM_USX_BLOCK_LOCK_OFFSET		0
 #define GEM_USX_BLOCK_LOCK_SIZE			1
@@ -813,6 +823,7 @@
 	})
 
 #define MACB_READ_NSR(bp)	macb_readl(bp, NSR)
+#define MACB_READ_USX_STATUS(bp)	gem_readl(bp, USX_STATUS)
 
 /* struct macb_dma_desc - Hardware DMA descriptor
  * @addr: DMA address of data buffer
diff --git a/drivers/net/ethernet/cadence/macb_main.c b/drivers/net/ethernet/cadence/macb_main.c
index 3c2a7919b..8988d2ccb 100644
--- a/drivers/net/ethernet/cadence/macb_main.c
+++ b/drivers/net/ethernet/cadence/macb_main.c
@@ -29,6 +29,7 @@
 #include <linux/of_gpio.h>
 #include <linux/of_mdio.h>
 #include <linux/of_net.h>
+#include <linux/of_platform.h>
 #include <linux/ip.h>
 #include <linux/udp.h>
 #include <linux/tcp.h>
@@ -87,9 +88,6 @@ struct sifive_fu540_macb_mgmt {
 
 #define MACB_WOL_ENABLED		BIT(0)
 
-#define HS_SPEED_10000M			4
-#define MACB_SERDES_RATE_10G		1
-
 /* Graceful stop timeouts in us. We should allow up to
  * 1 frame time (10 Mbits/s, full-duplex, ignoring collisions)
  */
@@ -97,6 +95,7 @@ struct sifive_fu540_macb_mgmt {
 #define MACB_PM_TIMEOUT  100 /* ms */
 
 #define MACB_MDIO_TIMEOUT	1000000 /* in usecs */
+#define GEM_SYNC_TIMEOUT	2500000 /* in usecs */
 
 /* DMA buffer descriptor might be different size
  * depends on hardware configuration:
@@ -567,28 +566,74 @@ static void macb_usx_pcs_link_up(struct phylink_pcs *pcs, unsigned int neg_mode,
 				 int duplex)
 {
 	struct macb *bp = container_of(pcs, struct macb, phylink_usx_pcs);
-	u32 config;
+	u32 speed_val, serdes_rate, config;
+	bool hs_mac = false;
+
+	switch (speed) {
+	case SPEED_1000:
+		speed_val = HS_SPEED_1000M;
+		serdes_rate = MACB_SERDES_RATE_5G_2G5_1G;
+		break;
+	case SPEED_2500:
+		speed_val = HS_SPEED_2500M;
+		serdes_rate = MACB_SERDES_RATE_5G_2G5_1G;
+		break;
+	case SPEED_5000:
+		speed_val = HS_SPEED_5000M;
+		serdes_rate = MACB_SERDES_RATE_5G_2G5_1G;
+		hs_mac = true;
+		break;
+	case SPEED_10000:
+		speed_val = HS_SPEED_10000M;
+		serdes_rate = MACB_SERDES_RATE_10G;
+		hs_mac = true;
+		break;
+	default:
+		netdev_err(bp->dev, "Specified speed not supported\n");
+		return;
+	}
+
+	/* Configure HS MAC for specified speed */
+	config = gem_readl(bp, HS_MAC_CONFIG);
+	config = GEM_BFINS(HS_MAC_SPEED, speed_val, config);
+	gem_writel(bp, HS_MAC_CONFIG, config);
 
 	config = gem_readl(bp, USX_CONTROL);
-	config = GEM_BFINS(SERDES_RATE, MACB_SERDES_RATE_10G, config);
-	config = GEM_BFINS(USX_CTRL_SPEED, HS_SPEED_10000M, config);
+	config = GEM_BFINS(SERDES_RATE, serdes_rate, config);
+	config = GEM_BFINS(USX_CTRL_SPEED, speed_val, config);
 	config &= ~(GEM_BIT(TX_SCR_BYPASS) | GEM_BIT(RX_SCR_BYPASS));
+	config |= GEM_BIT(RX_SYNC_RESET);
+	gem_writel(bp, USX_CONTROL, config);
+	mdelay(250);
+	config &= ~GEM_BIT(RX_SYNC_RESET);
 	config |= GEM_BIT(TX_EN);
 	gem_writel(bp, USX_CONTROL, config);
+
+	if (hs_mac && readx_poll_timeout(MACB_READ_USX_STATUS, bp, config,
+					 config & GEM_BIT(USX_BLOCK_LOCK),
+					 1, GEM_SYNC_TIMEOUT))
+		netdev_err(bp->dev, "USX PCS block lock not achieved\n");
 }
 
 static void macb_usx_pcs_get_state(struct phylink_pcs *pcs,
 				   struct phylink_link_state *state)
 {
 	struct macb *bp = container_of(pcs, struct macb, phylink_usx_pcs);
+	u32 hs_mac_map[] = {SPEED_UNKNOWN, SPEED_1000, SPEED_2500,
+			    SPEED_5000, SPEED_10000};
 	u32 val;
 
-	state->speed = SPEED_10000;
 	state->duplex = 1;
 	state->an_complete = 1;
 
-	val = gem_readl(bp, USX_STATUS);
-	state->link = !!(val & GEM_BIT(USX_BLOCK_LOCK));
+	val = gem_readl(bp, HS_MAC_CONFIG);
+	val = GEM_BFEXT(HS_MAC_SPEED, val);
+	state->speed = hs_mac_map[val];
+
+	state->link = (state->speed < SPEED_5000) ?
+		!!(macb_readl(bp, NSR) & MACB_BIT(NSR_LINK)) :
+		!!(gem_readl(bp, USX_STATUS) & GEM_BIT(USX_BLOCK_LOCK));
+
 	val = gem_readl(bp, NCFGR);
 	if (val & GEM_BIT(PAE))
 		state->pause = MLO_PAUSE_RX;
@@ -663,6 +708,8 @@ static void macb_mac_config(struct phylink_config *config, unsigned int mode,
 
 		if (state->interface == PHY_INTERFACE_MODE_SGMII) {
 			ctrl |= GEM_BIT(SGMIIEN) | GEM_BIT(PCSSEL);
+		} else if (state->interface == PHY_INTERFACE_MODE_1000BASEX) {
+			ctrl |= GEM_BIT(PCSSEL);
 		} else if (state->interface == PHY_INTERFACE_MODE_10GBASER) {
 			ctrl |= GEM_BIT(PCSSEL);
 			ncr |= GEM_BIT(ENABLE_HS_MAC);
@@ -683,7 +730,8 @@ static void macb_mac_config(struct phylink_config *config, unsigned int mode,
 	 * Must be written after PCSSEL is set in NCFGR,
 	 * otherwise writes will not take effect.
 	 */
-	if (macb_is_gem(bp) && state->interface == PHY_INTERFACE_MODE_SGMII) {
+	if (macb_is_gem(bp) && (state->interface == PHY_INTERFACE_MODE_SGMII ||
+				state->interface == PHY_INTERFACE_MODE_1000BASEX)) {
 		u32 pcsctrl, old_pcsctrl;
 
 		old_pcsctrl = gem_readl(bp, PCSCNTRL);
@@ -769,10 +817,6 @@ static void macb_mac_link_up(struct phylink_config *config,
 
 	macb_or_gem_writel(bp, NCFGR, ctrl);
 
-	if (bp->phy_interface == PHY_INTERFACE_MODE_10GBASER)
-		gem_writel(bp, HS_MAC_CONFIG, GEM_BFINS(HS_MAC_SPEED, HS_SPEED_10000M,
-							gem_readl(bp, HS_MAC_CONFIG)));
-
 	spin_unlock_irqrestore(&bp->lock, flags);
 
 	if (!(bp->caps & MACB_CAPS_MACB_IS_EMAC))
@@ -794,7 +838,8 @@ static struct phylink_pcs *macb_mac_select_pcs(struct phylink_config *config,
 	struct net_device *ndev = to_net_dev(config->dev);
 	struct macb *bp = netdev_priv(ndev);
 
-	if (interface == PHY_INTERFACE_MODE_10GBASER)
+	if (interface == PHY_INTERFACE_MODE_10GBASER ||
+	    interface == PHY_INTERFACE_MODE_1000BASEX)
 		return &bp->phylink_usx_pcs;
 	else if (interface == PHY_INTERFACE_MODE_SGMII)
 		return &bp->phylink_sgmii_pcs;
@@ -842,6 +887,13 @@ static int macb_phylink_connect(struct macb *bp)
 		return ret;
 	}
 
+	/* Since this driver uses runtime handling of clocks, initiate a phy
+	 * reset if the attached phy requires it. Check return to see if phy
+	 * was reset and then do a phy initialization.
+	 */
+	if (phy_reset_after_clk_enable(dev->phydev) == 1)
+		phy_init_hw(dev->phydev);
+
 	phylink_start(bp->phylink);
 
 	return 0;
@@ -915,20 +967,16 @@ static int macb_mii_probe(struct net_device *dev)
 	return 0;
 }
 
-static int macb_mdiobus_register(struct macb *bp)
+static int macb_mdiobus_register(struct macb *bp, struct device_node *mdio_np)
 {
-	struct device_node *child, *np = bp->pdev->dev.of_node;
+	struct device_node *child, *np = bp->pdev->dev.of_node, *dev_np;
+	struct platform_device *mdio_pdev = NULL;
 
 	/* If we have a child named mdio, probe it instead of looking for PHYs
 	 * directly under the MAC node
 	 */
-	child = of_get_child_by_name(np, "mdio");
-	if (child) {
-		int ret = of_mdiobus_register(bp->mii_bus, child);
-
-		of_node_put(child);
-		return ret;
-	}
+	if (mdio_np)
+		return of_mdiobus_register(bp->mii_bus, mdio_np);
 
 	/* Only create the PHY from the device tree if at least one PHY is
 	 * described. Otherwise scan the entire MDIO bus. We do this to support
@@ -945,22 +993,43 @@ static int macb_mdiobus_register(struct macb *bp)
 			return of_mdiobus_register(bp->mii_bus, np);
 		}
 
+	/* For shared MDIO usecases find out MDIO producer platform
+	 * device node by traversing through phy-handle DT property.
+	 */
+	np = of_parse_phandle(np, "phy-handle", 0);
+	mdio_np = of_get_parent(np);
+	of_node_put(np);
+	dev_np = of_get_parent(mdio_np);
+	of_node_put(mdio_np);
+	/* Handle error where bus_find_device returns a match for NULL */
+	if (dev_np)
+		mdio_pdev = of_find_device_by_node(dev_np);
+
+	of_node_put(dev_np);
+
+	/* Check MDIO producer device driver data to see if it's probed */
+	if (mdio_pdev && !dev_get_drvdata(&mdio_pdev->dev)) {
+		platform_device_put(mdio_pdev);
+		netdev_info(bp->dev, "Defer probe as mdio producer %s is not probed\n",
+			    dev_name(&mdio_pdev->dev));
+		return -EPROBE_DEFER;
+	}
+
+	platform_device_put(mdio_pdev);
 	return mdiobus_register(bp->mii_bus);
 }
 
 static int macb_mii_init(struct macb *bp)
 {
-	struct device_node *child, *np = bp->pdev->dev.of_node;
+	struct device_node *mdio_np, *np = bp->pdev->dev.of_node;
 	int err = -ENXIO;
 
 	/* With fixed-link, we don't need to register the MDIO bus,
 	 * except if we have a child named "mdio" in the device tree.
 	 * In that case, some devices may be attached to the MACB's MDIO bus.
 	 */
-	child = of_get_child_by_name(np, "mdio");
-	if (child)
-		of_node_put(child);
-	else if (of_phy_is_fixed_link(np))
+	mdio_np = of_get_child_by_name(np, "mdio");
+	if (!mdio_np && of_phy_is_fixed_link(np))
 		return macb_mii_probe(bp->dev);
 
 	/* Enable management port */
@@ -984,7 +1053,7 @@ static int macb_mii_init(struct macb *bp)
 
 	dev_set_drvdata(&bp->dev->dev, bp->mii_bus);
 
-	err = macb_mdiobus_register(bp);
+	err = macb_mdiobus_register(bp, mdio_np);
 	if (err)
 		goto err_out_free_mdiobus;
 
@@ -999,6 +1068,8 @@ static int macb_mii_init(struct macb *bp)
 err_out_free_mdiobus:
 	mdiobus_free(bp->mii_bus);
 err_out:
+	of_node_put(mdio_np);
+
 	return err;
 }
 
@@ -4958,6 +5029,16 @@ static const struct macb_config versal_config = {
 	.usrio = &macb_default_usrio,
 };
 
+static const struct macb_config versal2_10gbe_config = {
+	.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE | MACB_CAPS_JUMBO |
+		MACB_CAPS_GEM_HAS_PTP | MACB_CAPS_BD_RD_PREFETCH | MACB_CAPS_QUEUE_DISABLE,
+	.dma_burst_length = 16,
+	.clk_init = macb_clk_init,
+	.init = init_reset_optional,
+	.jumbo_max_len = 10240,
+	.usrio = &macb_default_usrio,
+};
+
 static const struct of_device_id macb_dt_ids[] = {
 	{ .compatible = "cdns,at91sam9260-macb", .data = &at91sam9260_config },
 	{ .compatible = "cdns,macb" },
@@ -4981,6 +5062,7 @@ static const struct of_device_id macb_dt_ids[] = {
 	{ .compatible = "xlnx,zynqmp-gem", .data = &zynqmp_config},
 	{ .compatible = "xlnx,zynq-gem", .data = &zynq_config },
 	{ .compatible = "xlnx,versal-gem", .data = &versal_config},
+	{ .compatible = "amd,versal2-10gbe", .data = &versal2_10gbe_config},
 	{ /* sentinel */ }
 };
 MODULE_DEVICE_TABLE(of, macb_dt_ids);
diff --git a/drivers/net/ethernet/xilinx/Kconfig b/drivers/net/ethernet/xilinx/Kconfig
index 35d96c633..e741b5a17 100644
--- a/drivers/net/ethernet/xilinx/Kconfig
+++ b/drivers/net/ethernet/xilinx/Kconfig
@@ -32,6 +32,27 @@ config XILINX_AXI_EMAC
 	  This driver supports the 10/100/1000 Ethernet from Xilinx for the
 	  AXI bus interface used in Xilinx Virtex FPGAs and Soc's.
 
+config XILINX_AXI_EMAC_HWTSTAMP
+	bool "Generate hardware packet timestamps"
+	depends on XILINX_AXI_EMAC && PTP_1588_CLOCK
+	help
+	  Generate hardware packet timestamps. This is to facilitate IEEE 1588.
+
+config  AXIENET_HAS_MCDMA
+	bool "AXI Ethernet is configured with MCDMA"
+	depends on XILINX_AXI_EMAC
+	help
+	  When hardware is generated with AXI Ethernet with MCDMA select this option.
+
+config XILINX_AXI_EOE
+	bool "Xilinx Ethernet Offload Engine support"
+	depends on AXIENET_HAS_MCDMA
+	help
+	  When hardware is configured with Ethernet Offload Engine select this option.
+	  It supports hardware segmentation/receive offload and checksum offload,
+	  by offloading of the packet if its size is over MTU. It improves the network
+	  speed and overall CPU efficiency.
+
 config XILINX_LL_TEMAC
 	tristate "Xilinx LL TEMAC (LocalLink Tri-mode Ethernet MAC) driver"
 	depends on HAS_IOMEM
diff --git a/drivers/net/ethernet/xilinx/Makefile b/drivers/net/ethernet/xilinx/Makefile
index 7d7dc1771..12f86dcf8 100644
--- a/drivers/net/ethernet/xilinx/Makefile
+++ b/drivers/net/ethernet/xilinx/Makefile
@@ -6,5 +6,7 @@
 ll_temac-objs := ll_temac_main.o ll_temac_mdio.o
 obj-$(CONFIG_XILINX_LL_TEMAC) += ll_temac.o
 obj-$(CONFIG_XILINX_EMACLITE) += xilinx_emaclite.o
-xilinx_emac-objs := xilinx_axienet_main.o xilinx_axienet_mdio.o
+xilinx_emac-objs := xilinx_axienet_main.o xilinx_axienet_mdio.o xilinx_axienet_dma.o
 obj-$(CONFIG_XILINX_AXI_EMAC) += xilinx_emac.o
+obj-$(CONFIG_AXIENET_HAS_MCDMA) += xilinx_axienet_mcdma.o
+obj-$(CONFIG_XILINX_AXI_EOE) += xilinx_axienet_eoe.o
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet.h b/drivers/net/ethernet/xilinx/xilinx_axienet.h
index d64b8abcf..f9a7e7929 100644
--- a/drivers/net/ethernet/xilinx/xilinx_axienet.h
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet.h
@@ -9,12 +9,15 @@
 #ifndef XILINX_AXIENET_H
 #define XILINX_AXIENET_H
 
+#include <linux/clk.h>
 #include <linux/netdevice.h>
 #include <linux/spinlock.h>
 #include <linux/interrupt.h>
 #include <linux/if_vlan.h>
 #include <linux/phylink.h>
 #include <linux/skbuff.h>
+#include <linux/net_tstamp.h>
+#include <linux/of_platform.h>
 
 /* Packet size info */
 #define XAE_HDR_SIZE			14 /* Size of Ethernet header */
@@ -26,6 +29,16 @@
 #define XAE_MAX_VLAN_FRAME_SIZE  (XAE_MTU + VLAN_ETH_HLEN + XAE_TRL_SIZE)
 #define XAE_MAX_JUMBO_FRAME_SIZE (XAE_JUMBO_MTU + XAE_HDR_SIZE + XAE_TRL_SIZE)
 
+/* DMA address width min and max range */
+#define XAE_DMA_MASK_MIN	32
+#define XAE_DMA_MASK_MAX	64
+
+/* In AXI DMA Tx and Rx queue count is same */
+#define for_each_tx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_tx_queues; (var)++)
+
+#define for_each_rx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_rx_queues; (var)++)
 /* Configuration options */
 
 /* Accept all incoming packets. Default: disabled (cleared) */
@@ -142,6 +155,24 @@
 
 #define XAXIDMA_BD_MINIMUM_ALIGNMENT	0x40
 
+/* AXI Tx Timestamp Stream FIFO Register Definitions */
+#define XAXIFIFO_TXTS_ISR	0x00000000 /* Interrupt Status Register */
+#define XAXIFIFO_TXTS_TDFV	0x0000000C /* Transmit Data FIFO Vacancy */
+#define XAXIFIFO_TXTS_TXFD	0x00000010 /* Tx Data Write Port */
+#define XAXIFIFO_TXTS_TLR	0x00000014 /* Transmit Length Register */
+#define XAXIFIFO_TXTS_RFO	0x0000001C /* Rx Fifo Occupancy */
+#define XAXIFIFO_TXTS_RDFR	0x00000018 /* Rx Fifo reset */
+#define XAXIFIFO_TXTS_RXFD	0x00000020 /* Rx Data Read Port */
+#define XAXIFIFO_TXTS_RLR	0x00000024 /* Receive Length Register */
+#define XAXIFIFO_TXTS_SRR	0x00000028 /* AXI4-Stream Reset */
+
+#define XAXIFIFO_TXTS_INT_RC_MASK	0x04000000
+#define XAXIFIFO_TXTS_RXFD_MASK		0x7FFFFFFF
+#define XAXIFIFO_TXTS_RESET_MASK	0x000000A5
+#define XAXIFIFO_TXTS_TAG_MASK		0xFFFF0000
+#define XAXIFIFO_TXTS_TAG_SHIFT		16
+#define XAXIFIFO_TXTS_TAG_MAX		0xFFFE
+
 /* Axi Ethernet registers definition */
 #define XAE_RAF_OFFSET		0x00000000 /* Reset and Address filter */
 #define XAE_TPF_OFFSET		0x00000004 /* Tx Pause Frame */
@@ -235,6 +266,7 @@
 #define XAE_TPID_3_MASK		0xFFFF0000 /* TPID 1 */
 
 /* Bit masks for Axi Ethernet RCW1 register */
+#define XAE_RCW1_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
 #define XAE_RCW1_RST_MASK	0x80000000 /* Reset */
 #define XAE_RCW1_JUM_MASK	0x40000000 /* Jumbo frame enable */
 /* In-Band FCS enable (FCS not stripped) */
@@ -251,6 +283,7 @@
 #define XAE_RCW1_PAUSEADDR_MASK 0x0000FFFF
 
 /* Bit masks for Axi Ethernet TC register */
+#define XAE_TC_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
 #define XAE_TC_RST_MASK		0x80000000 /* Reset */
 #define XAE_TC_JUM_MASK		0x40000000 /* Jumbo frame enable */
 /* In-Band FCS enable (FCS not generated) */
@@ -275,6 +308,7 @@
 #define XAE_EMMC_LINKSPD_10	0x00000000 /* Link Speed mask for 10 Mbit */
 #define XAE_EMMC_LINKSPD_100	0x40000000 /* Link Speed mask for 100 Mbit */
 #define XAE_EMMC_LINKSPD_1000	0x80000000 /* Link Speed mask for 1000 Mbit */
+#define XAE_EMMC_LINKSPD_2500	0x80000000 /* Link Speed mask for 2500 Mbit */
 
 /* Bit masks for Axi Ethernet PHYC register */
 #define XAE_PHYC_SGMIILINKSPEED_MASK	0xC0000000 /* SGMII link speed mask*/
@@ -336,6 +370,8 @@
 #define XAE_PHY_TYPE_RGMII_2_0		3
 #define XAE_PHY_TYPE_SGMII		4
 #define XAE_PHY_TYPE_1000BASE_X		5
+#define XAE_PHY_TYPE_2500		6
+#define XXE_PHY_TYPE_USXGMII		7
 
  /* Total number of entries in the hardware multicast table. */
 #define XAE_MULTICAST_CAM_TABLE_NUM	4
@@ -415,6 +451,348 @@ enum temac_stat {
 	STAT_COUNT,
 };
 
+/* Definition of 1588 PTP in Axi Ethernet IP */
+#define TX_TS_OP_NOOP           0x0
+#define TX_TS_OP_ONESTEP        0x1
+#define TX_TS_OP_TWOSTEP        0x2
+#define TX_TS_CSUM_UPDATE       0x1
+#define TX_TS_CSUM_UPDATE_MRMAC		0x4
+#define TX_TS_PDELAY_UPDATE_MRMAC	0x8
+#define TX_PTP_CSUM_OFFSET      0x28
+#define TX_PTP_TS_OFFSET        0x4C
+#define TX_PTP_CF_OFFSET        0x32
+
+/* XXV MAC Register Definitions */
+#define XXV_GT_RESET_OFFSET		0x00000000
+#define XXV_TC_OFFSET			0x0000000C
+#define XXV_RCW1_OFFSET			0x00000014
+#define XXV_JUM_OFFSET			0x00000018
+#define XXV_TICKREG_OFFSET		0x00000020
+#define XXV_STATRX_BLKLCK_OFFSET	0x0000040C
+#define XXV_STAT_AN_STS_OFFSET	0x00000458
+#define XXV_STAT_CORE_SPEED_OFFSET	0x00000498
+#define XXV_STAT_GTWIZ_OFFSET		0x000004A0
+#define XXV_CONFIG_REVISION		0x00000024
+#define XXV_AN_CTL1_OFFSET		0x000000e0
+#define XXV_USXGMII_AN_OFFSET		0x000000C8
+#define XXV_USXGMII_AN_STS_OFFSET	0x00000458
+/* Switchable 1/10/25G MAC Register Definitions */
+#define XXVS_RESET_OFFSET		0x00000004
+#define XXVS_AN_CTL1_OFFSET		0x000000e0
+#define XXVS_AN_ABILITY_OFFSET		0x000000f8
+#define XXVS_LT_CTL_OFFSET		0x00000100
+#define XXVS_LT_TRAINED_OFFSET		0x00000104
+#define XXVS_LT_SEED_OFFSET		0x00000110
+#define XXVS_LT_COEF_OFFSET		0x00000130
+#define XXVS_SPEED_OFFSET		0x00000180
+
+#define XXVS_AN_STATUS_OFFSET		0x0000458
+#define XXVS_AN_LP_STATUS_OFFSET	0x000045C
+#define XXVS_LT_STATUS_OFFSET		0x000046C
+#define XXVS_RX_STATUS_REG1		0x00000404
+#define XXVS_TC_OFFSET			0x0000000C
+
+/* Switchable 1/10/25G MAC Register Mask Definitions */
+#define XXVS_RX_SERDES_RESET		BIT(28)
+#define XXVS_AN_ENABLE_MASK		BIT(0)
+#define XXVS_AN_BYPASS			BIT(1)
+#define XXVS_AN_1G_ABILITY_MASK		BIT(0)
+#define XXVS_AN_10G_ABILITY_MASK	BIT(1)
+#define XXVS_AN_25G_ABILITY_MASK	BIT(10)
+#define XXVS_LT_ENABLE_MASK		BIT(0)
+#define XXVS_LT_TRAINED_MASK		BIT(0)
+#define XXVS_AN_COMPLETE_MASK		BIT(2)
+#define XXVS_LT_DETECT_MASK		BIT(0)
+#define XXVS_SPEED_1G			BIT(0)
+#define	XXVS_SPEED_10G			BIT(1)
+#define XXVS_SPEED_25G			~(BIT(0) | BIT(1))
+#define XXVS_RX_STATUS_MASK		BIT(0)
+#define XXVS_RX_RESET			BIT(30)
+#define XXVS_TX_RESET			BIT(31)
+#define XXVS_CTRL_CORE_SPEED_SEL_CLEAR		~(BIT(6) | BIT(7))
+#define XXVS_CTRL_CORE_SPEED_SEL_1G		BIT(6)
+#define XXVS_CTRL_CORE_SPEED_SEL_10G	BIT(7)
+
+/* XXV MAC Register Mask Definitions */
+#define XXV_GT_RESET_MASK	BIT(0)
+#define XXV_TC_TX_MASK		BIT(0)
+#define XXV_RCW1_RX_MASK	BIT(0)
+#define XXV_RCW1_FCS_MASK	BIT(1)
+#define XXV_TC_FCS_MASK		BIT(1)
+#define XXV_MIN_JUM_MASK	GENMASK(7, 0)
+#define XXV_MAX_JUM_MASK	GENMASK(10, 8)
+#define XXV_RX_BLKLCK_MASK	BIT(0)
+#define XXV_TICKREG_STATEN_MASK BIT(0)
+#define XXV_MAC_MIN_PKT_LEN	64
+#define XXV_GTWIZ_RESET_DONE	(BIT(0) | BIT(1))
+#define XXV_MAJ_MASK		GENMASK(7, 0)
+#define XXV_MIN_MASK		GENMASK(15, 8)
+#define XXV_AN_10G_ABILITY_MASK	(BIT(1) | BIT(2))
+#define XXV_AN_25G_ABILITY_MASK	(BIT(9) | BIT(10) | BIT(16) | BIT(17))
+#define XXV_AN_RESTART_MASK	BIT(11)
+#define XXV_AN_COMPLETE_MASK		BIT(2)
+#define XXV_TX_PAUSE_MASK	BIT(4)
+#define XXV_RX_PAUSE_MASK	BIT(5)
+#define XXV_STAT_CORE_SPEED_RTSW_MASK	BIT(1)
+#define XXV_STAT_CORE_SPEED_10G_MASK	BIT(0)
+
+/* USXGMII Register Mask Definitions  */
+#define USXGMII_AN_EN		BIT(5)
+#define USXGMII_AN_RESET	BIT(6)
+#define USXGMII_AN_RESTART	BIT(7)
+#define USXGMII_EN		BIT(16)
+#define USXGMII_RATE_MASK	0x0E000700
+#define USXGMII_RATE_1G		0x04000200
+#define USXGMII_RATE_2G5	0x08000400
+#define USXGMII_RATE_10M	0x0
+#define USXGMII_RATE_100M	0x02000100
+#define USXGMII_RATE_5G		0x0A000500
+#define USXGMII_RATE_10G	0x06000300
+#define USXGMII_FD		BIT(28)
+#define USXGMII_LINK_STS	BIT(31)
+
+/* USXGMII AN STS register mask definitions */
+#define USXGMII_AN_STS_COMP_MASK	BIT(16)
+
+/* MCDMA Register Definitions */
+#define XMCDMA_CR_OFFSET	0x00
+#define XMCDMA_SR_OFFSET	0x04
+#define XMCDMA_CHEN_OFFSET	0x08
+#define XMCDMA_CHSER_OFFSET	0x0C
+#define XMCDMA_ERR_OFFSET	0x10
+#define XMCDMA_PKTDROP_OFFSET	0x14
+#define XMCDMA_TXWEIGHT0_OFFSET 0x18
+#define XMCDMA_TXWEIGHT1_OFFSET 0x1C
+#define XMCDMA_RXINT_SER_OFFSET 0x20
+#define XMCDMA_TXINT_SER_OFFSET 0x28
+
+#define XMCDMA_CHOBS1_OFFSET	0x440
+#define XMCDMA_CHOBS2_OFFSET	0x444
+#define XMCDMA_CHOBS3_OFFSET	0x448
+#define XMCDMA_CHOBS4_OFFSET	0x44C
+#define XMCDMA_CHOBS5_OFFSET	0x450
+#define XMCDMA_CHOBS6_OFFSET	0x454
+
+#define XMCDMA_CHAN_RX_OFFSET  0x500
+
+/* Per Channel Registers */
+#define XMCDMA_CHAN_CR_OFFSET(chan_id)		(0x40 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_SR_OFFSET(chan_id)		(0x44 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_CURDESC_OFFSET(chan_id)	(0x48 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_TAILDESC_OFFSET(chan_id)	(0x50 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_PKTDROP_OFFSET(chan_id)	(0x58 + ((chan_id) - 1) * 0x40)
+
+#define XMCDMA_RX_OFFSET	0x500
+
+/* MCDMA Mask registers */
+#define XMCDMA_CR_RUNSTOP_MASK		BIT(0) /* Start/stop DMA channel */
+#define XMCDMA_CR_RESET_MASK		BIT(2) /* Reset DMA engine */
+
+#define XMCDMA_SR_HALTED_MASK		BIT(0)
+#define XMCDMA_SR_IDLE_MASK		BIT(1)
+
+#define XMCDMA_IRQ_ERRON_OTHERQ_MASK	BIT(3)
+#define XMCDMA_IRQ_PKTDROP_MASK		BIT(4)
+#define XMCDMA_IRQ_IOC_MASK		BIT(5)
+#define XMCDMA_IRQ_DELAY_MASK		BIT(6)
+#define XMCDMA_IRQ_ERR_MASK		BIT(7)
+#define XMCDMA_IRQ_ALL_MASK		GENMASK(7, 5)
+#define XMCDMA_PKTDROP_COALESCE_MASK	GENMASK(15, 8)
+#define XMCDMA_COALESCE_MASK		GENMASK(23, 16)
+#define XMCDMA_DELAY_MASK		GENMASK(31, 24)
+
+#define XMCDMA_CHEN_MASK		GENMASK(7, 0)
+#define XMCDMA_CHID_MASK		GENMASK(7, 0)
+
+#define XMCDMA_ERR_INTERNAL_MASK	BIT(0)
+#define XMCDMA_ERR_SLAVE_MASK		BIT(1)
+#define XMCDMA_ERR_DECODE_MASK		BIT(2)
+#define XMCDMA_ERR_SG_INT_MASK		BIT(4)
+#define XMCDMA_ERR_SG_SLV_MASK		BIT(5)
+#define XMCDMA_ERR_SG_DEC_MASK		BIT(6)
+
+#define XMCDMA_PKTDROP_CNT_MASK		GENMASK(31, 0)
+
+#define XMCDMA_BD_CTRL_TXSOF_MASK	0x80000000 /* First tx packet */
+#define XMCDMA_BD_CTRL_TXEOF_MASK	0x40000000 /* Last tx packet */
+#define XMCDMA_BD_CTRL_ALL_MASK		0xC0000000 /* All control bits */
+#define XMCDMA_BD_STS_ALL_MASK		0xF0000000 /* All status bits */
+
+#define XMCDMA_COALESCE_SHIFT		16
+#define XMCDMA_DELAY_SHIFT		24
+#define XMCDMA_DFT_TX_THRESHOLD		1
+
+#define XMCDMA_TXWEIGHT_CH_MASK(chan_id)	GENMASK(((chan_id) * 4 + 3), \
+							(chan_id) * 4)
+#define XMCDMA_TXWEIGHT_CH_SHIFT(chan_id)	((chan_id) * 4)
+
+/* PTP Packet length */
+#define XAE_TX_PTP_LEN		16
+#define XXV_TX_PTP_LEN		12
+
+/* Default number of Tx descriptors */
+#define TX_BD_NUM_DEFAULT               128
+/* Switching 1/10/25G MAC AN & LT seed values */
+#define XXVS_AN_NONCE_SEED		0x16C
+#define XXVS_AN_NONCE_SEED1		0x10
+#define XXVS_LT_SEED			0x605
+#define XXVS_LT_COEF_P1			0x1
+#define XXVS_LT_COEF_P1_SHIFT		6
+#define XXVS_LT_COEF_STATE0		0x1
+#define XXVS_LT_COEF_STATE0_SHIFT	8
+#define XXVS_LT_COEF_M1			0x1
+#define XXVS_LT_COEF_M1_SHIFT		10
+/* Switching 1/10/25G MAC "xlnx,runtime-switch" DT property value */
+#define XXVS_RT_SWITCH_1G_10G_25G		"1G / 10G / 25G"
+
+/* Macros used when AXI DMA h/w is configured without DRE */
+#define XAE_MAX_PKT_LEN		8192
+
+/* MRMAC Register Definitions */
+/* Configuration Registers */
+#define MRMAC_REV_OFFSET		0x00000000
+#define MRMAC_RESET_OFFSET		0x00000004
+#define MRMAC_MODE_OFFSET		0x00000008
+#define MRMAC_CONFIG_TX_OFFSET		0x0000000C
+#define MRMAC_CONFIG_RX_OFFSET		0x00000010
+#define MRMAC_TICK_OFFSET		0x0000002C
+#define MRMAC_CFG1588_OFFSET	0x00000040
+
+/* Status Registers */
+#define MRMAC_TX_STS_OFFSET		0x00000740
+#define MRMAC_RX_STS_OFFSET		0x00000744
+#define MRMAC_TX_RT_STS_OFFSET		0x00000748
+#define MRMAC_RX_RT_STS_OFFSET		0x0000074C
+#define MRMAC_STATRX_BLKLCK_OFFSET	0x00000754
+#define MRMAC_STATRX_VALID_CTRL_OFFSET	0x000007B8
+
+/* Register bit masks */
+#define MRMAC_RX_SERDES_RST_MASK	(BIT(3) | BIT(2) | BIT(1) | BIT(0))
+#define MRMAC_TX_SERDES_RST_MASK	BIT(4)
+#define MRMAC_RX_RST_MASK		BIT(5)
+#define MRMAC_TX_RST_MASK		BIT(6)
+#define MRMAC_RX_AXI_RST_MASK		BIT(8)
+#define MRMAC_TX_AXI_RST_MASK		BIT(9)
+#define MRMAC_STS_ALL_MASK		0xFFFFFFFF
+
+#define MRMAC_RX_EN_MASK		BIT(0)
+#define MRMAC_RX_DEL_FCS_MASK		BIT(1)
+
+#define MRMAC_TX_EN_MASK		BIT(0)
+#define MRMAC_TX_INS_FCS_MASK		BIT(1)
+
+#define MRMAC_RX_BLKLCK_MASK		BIT(0)
+#define MRMAC_RX_STATUS_MASK		BIT(0)
+#define MRMAC_RX_VALID_MASK		BIT(0)
+
+#define MRMAC_CTL_DATA_RATE_MASK	GENMASK(2, 0)
+#define MRMAC_CTL_DATA_RATE_10G		0
+#define MRMAC_CTL_DATA_RATE_25G		1
+#define MRMAC_CTL_DATA_RATE_40G		2
+#define MRMAC_CTL_DATA_RATE_50G		3
+#define MRMAC_CTL_DATA_RATE_100G	4
+
+#define MRMAC_CTL_AXIS_CFG_MASK		GENMASK(11, 9)
+#define MRMAC_CTL_AXIS_CFG_SHIFT	9
+#define MRMAC_CTL_AXIS_CFG_10G_IND	1
+#define MRMAC_CTL_AXIS_CFG_25G_IND_64	1
+#define MRMAC_CTL_AXIS_CFG_25G_IND_128	5
+
+#define MRMAC_STREAM_DWIDTH_32		0x20
+#define MRMAC_STREAM_DWIDTH_64		0x40
+#define MRMAC_STREAM_DWIDTH_128	0x80
+
+#define MRMAC_CTL_SERDES_WIDTH_MASK	GENMASK(6, 4)
+#define MRMAC_CTL_SERDES_WIDTH_SHIFT	4
+#define MRMAC_CTL_SERDES_WIDTH_10G_NRW		0
+#define MRMAC_CTL_SERDES_WIDTH_10G_WIDE	4
+#define MRMAC_CTL_SERDES_WIDTH_25G_NRW		2
+#define MRMAC_CTL_SERDES_WIDTH_25G_WIDE	6
+
+#define MRMAC_CTL_RATE_CFG_MASK		(MRMAC_CTL_DATA_RATE_MASK |	\
+					 MRMAC_CTL_AXIS_CFG_MASK |	\
+					 MRMAC_CTL_SERDES_WIDTH_MASK)
+
+#define MRMAC_CTL_PM_TICK_MASK		BIT(30)
+#define MRMAC_TICK_TRIGGER		BIT(0)
+#define MRMAC_ONE_STEP_EN		BIT(0)
+
+/* MRMAC GT wrapper registers */
+#define MRMAC_GT_PLL_OFFSET		0x0
+#define MRMAC_GT_PLL_STS_OFFSET		0x8
+#define MRMAC_GT_RATE_OFFSET		0x0
+#define MRMAC_GT_CTRL_OFFSET		0x8
+
+#define MRMAC_GT_PLL_RST_MASK		0x00030003
+#define MRMAC_GT_PLL_DONE_MASK		0xFF
+#define MRMAC_GT_RST_ALL_MASK		BIT(0)
+#define MRMAC_GT_RST_RX_MASK		BIT(1)
+#define MRMAC_GT_RST_TX_MASK		BIT(2)
+#define MRMAC_GT_10G_MASK		0x00000001
+#define MRMAC_GT_25G_MASK		0x00000002
+
+#define MRMAC_GT_LANE_OFFSET		BIT(16)
+#define MRMAC_MAX_GT_LANES		4
+#define GT_MODE_NARROW			"Narrow"
+
+/* DCMAC Register Definitions */
+/* Global registers */
+#define DCMAC_G_MODE_OFFSET		0x00000004
+#define DCMAC_G_CTRL_RX_OFFSET		0x000000F0
+#define DCMAC_G_CTRL_TX_OFFSET		0x000000F8
+/* Port registers */
+#define DCMAC_P_CTRL_RX_OFFSET		0x000010F0
+#define DCMAC_P_CTRL_TX_OFFSET		0x000010F8
+#define DCMAC_STS_RX_PHY_OFFSET		0x00001C00
+/* Port channel registers */
+#define DCMAC_CH_CFG_TX_OFFSET		0x00001000
+#define DCMAC_CH_CFG_RX_OFFSET		0x00001004
+#define DCMAC_CH_CTRL_RX_OFFSET		0x00001030
+#define DCMAC_CH_CTRL_TX_OFFSET		0x00001038
+#define DCMAC_CH_MODE_TX_OFFSET		0x00001040
+#define DCMAC_CH_MODE_RX_OFFSET		0x00001044
+/* Status Registers */
+#define DCMAC_TX_STS_OFFSET		0X00001100
+#define DCMAC_RX_STS_OFFSET		0X00001140
+
+/* Register bit masks */
+#define DCMAC_TX_ACTV_PRT_ALL_MASK	(BIT(16) | BIT(18))
+#define DCMAC_RX_ACTV_PRT_ALL_MASK	(BIT(20) | BIT(22))
+#define DCMAC_RX_ERR_IND_STD_MASK	BIT(24)	/* FEC error indication mode as IEEE Standard */
+#define DCMAC_TX_FEC_UNIQUE_FLIP_MASK	BIT(25)
+#define DCMAC_RX_FEC_UNIQUE_FLIP_MASK	BIT(26)
+#define DCMAC_CH_RX_FCS_MASK		BIT(1)
+#define DCMAC_CH_RX_PREAMBLE_MASK	BIT(5)
+#define DCMAC_RX_IGNR_INRANGE_MASK	BIT(6)
+#define DCMAC_RX_MAX_PKT_LEN_MASK	(BIT(23) | BIT(24) | BIT(26) | BIT(29))
+#define DCMAC_CH_TX_FCS_MASK		BIT(0)
+#define DCMAC_CH_TX_IPG_MASK		(BIT(10) | BIT(11))
+#define DCMAC_P_SPEED_100G_MASK		~(BIT(0) | BIT(1))
+#define DCMAC_P_SPEED_200G_MASK	BIT(1)
+#define DCMAC_P_SPEED_400G_MASK	BIT(2)
+#define DCMAC_CH_TXMD_PM_TICK_INTERNAL_MASK	BIT(4)
+#define DCMAC_CH_RXMD_PM_TICK_INTERNAL_MASK	BIT(11)
+#define DCMAC_CH_MD_FEC_KR4		(BIT(16) | BIT(18))
+#define DCMAC_CH_MD_FEC_200G		BIT(19)
+#define DCMAC_CH_MD_FEC_400G		BIT(20)
+#define DCMAC_P_CTRL_CLR_SERDES		BIT(1)
+#define DCMAC_G_CTRL_RESET_ALL		GENMASK(2, 0)
+#define DCMAC_P_CTRL_CLEAR_ALL		(BIT(0) | BIT(1))
+#define DCMAC_CH_CTRL_CLEAR_STATE	BIT(0)
+#define DCMAC_RXPHY_RX_STS_MASK		BIT(0)
+#define DCMAC_RXPHY_RX_ALIGN_MASK	BIT(2)
+#define DCMAC_RELEASE_RESET		0x0
+#define DCMAC_GT_RESET_DONE_MASK	GENMASK(3, 0)
+#define DCMAC_STS_ALL_MASK		GENMASK(31, 0)
+
+/* DCMAC GT wrapper bitmasks */
+#define DCMAC_GT_RESET_ALL	BIT(0)
+#define DCMAC_GT_TX_PRECURSOR	(BIT(12) | BIT(13))	/* gt_txprecursor */
+#define DCMAC_GT_TX_POSTCURSOR	(BIT(18) | BIT(21))	/* gt_txpostcursor */
+#define DCMAC_GT_MAINCURSOR	(BIT(24) | BIT(25) | BIT(27) | BIT(30))	/* gt maincursor */
+
+#define DCMAC_GT_RXDPATH_RST	GENMASK(23, 0)
+
 /**
  * struct axidma_bd - Axi Dma buffer descriptor layout
  * @next:         MM2S/S2MM Next Descriptor Pointer
@@ -430,7 +808,13 @@ enum temac_stat {
  * @app2:         MM2S/S2MM User Application Field 2.
  * @app3:         MM2S/S2MM User Application Field 3.
  * @app4:         MM2S/S2MM User Application Field 4.
- * @skb:          Pointer to SKB transferred using DMA
+ * @sw_id_offset: MM2S/S2MM Sw ID
+ * @ptp_tx_skb:   If timestamping is enabled used for timestamping skb
+ *		  Otherwise reserved.
+ * @ptp_tx_ts_tag: Tag value of 2 step timestamping if timestamping is enabled
+ *		   Otherwise reserved.
+ * @tx_skb:	  Transmit skb address
+ * @tx_desc_mapping: Tx Descriptor DMA mapping type.
  */
 struct axidma_bd {
 	u32 next;	/* Physical address of next buffer descriptor */
@@ -445,11 +829,74 @@ struct axidma_bd {
 	u32 app1;	/* TX start << 16 | insert */
 	u32 app2;	/* TX csum seed */
 	u32 app3;
-	u32 app4;   /* Last field used by HW */
-	struct sk_buff *skb;
+	u32 app4;
+	struct sk_buff *sw_id_offset; /* first unused field by h/w */
+	struct sk_buff *ptp_tx_skb;
+	u32 ptp_tx_ts_tag;
+	struct sk_buff *tx_skb;
+	u32 tx_desc_mapping;
+} __aligned(XAXIDMA_BD_MINIMUM_ALIGNMENT);
+/**
+ * struct aximcdma_bd - Axi MCDMA buffer descriptor layout
+ * @next:         MM2S/S2MM Next Descriptor Pointer
+ * @next_msb:     MM2S/S2MM Next Descriptor Pointer (high 32 bits)
+ * @phys:         MM2S/S2MM Buffer Address
+ * @phys_msb:     MM2S/S2MM Buffer Address (high 32 bits)
+ * @reserved3:    Reserved and not used
+ * @cntrl:        MM2S/S2MM Control value
+ * @status:       S2MM Status value
+ * @sband_stats:  S2MM Sideband Status value
+ *		  MM2S Status value
+ * @app0:         MM2S/S2MM User Application Field 0.
+ * @app1:         MM2S/S2MM User Application Field 1.
+ * @app2:         MM2S/S2MM User Application Field 2.
+ * @app3:         MM2S/S2MM User Application Field 3.
+ * @app4:         MM2S/S2MM User Application Field 4.
+ * @sw_id_offset: MM2S/S2MM Sw ID
+ * @ptp_tx_skb:   If timestamping is enabled used for timestamping skb
+ *		  Otherwise reserved.
+ * @ptp_tx_ts_tag: Tag value of 2 step timestamping if timestamping is enabled
+ *		   Otherwise reserved.
+ * @tx_skb:	  Transmit skb address
+ * @tx_desc_mapping: Tx Descriptor DMA mapping type.
+ * @page:	page buffer to access the data passed by GRO packet
+ */
+struct aximcdma_bd {
+	u32 next;	/* Physical address of next buffer descriptor */
+	u32 next_msb;
+	u32 phys;
+	u32 phys_msb;
+	u32 reserved3;
+	u32 cntrl;
+	u32 status;
+	u32 sband_stats;
+	u32 app0;
+	u32 app1;	/* TX start << 16 | insert */
+	u32 app2;	/* TX csum seed */
+	u32 app3;
+	u32 app4;
+	struct sk_buff *sw_id_offset; /* first unused field by h/w */
+	struct sk_buff *ptp_tx_skb;
+	u32 ptp_tx_ts_tag;
+	struct sk_buff *tx_skb;
+	u32 tx_desc_mapping;
+	struct page *page;
 } __aligned(XAXIDMA_BD_MINIMUM_ALIGNMENT);
 
 #define XAE_NUM_MISC_CLOCKS 3
+#define DESC_DMA_MAP_SINGLE 0
+#define DESC_DMA_MAP_PAGE 1
+
+#if defined(CONFIG_AXIENET_HAS_MCDMA)
+#define XAE_MAX_QUEUES		16
+#else
+#define XAE_MAX_QUEUES		1
+#endif
+
+struct ethtool_rx_fs_list {
+	struct list_head list;
+	unsigned int count;
+};
 
 /**
  * struct skbuf_dma_descriptor - skb for each dma descriptor
@@ -482,26 +929,9 @@ struct skbuf_dma_descriptor {
  * @mii_clk_div: MII bus clock divider value
  * @regs_start: Resource start for axienet device addresses
  * @regs:	Base address for the axienet_local device address space
- * @dma_regs:	Base address for the axidma device address space
- * @napi_rx:	NAPI RX control structure
- * @rx_dma_cr:  Nominal content of RX DMA control register
- * @rx_bd_v:	Virtual address of the RX buffer descriptor ring
- * @rx_bd_p:	Physical address(start address) of the RX buffer descr. ring
- * @rx_bd_num:	Size of RX buffer descriptor ring
- * @rx_bd_ci:	Stores the index of the Rx buffer descriptor in the ring being
- *		accessed currently.
  * @rx_packets: RX packet count for statistics
  * @rx_bytes:	RX byte count for statistics
  * @rx_stat_sync: Synchronization object for RX stats
- * @napi_tx:	NAPI TX control structure
- * @tx_dma_cr:  Nominal content of TX DMA control register
- * @tx_bd_v:	Virtual address of the TX buffer descriptor ring
- * @tx_bd_p:	Physical address(start address) of the TX buffer descr. ring
- * @tx_bd_num:	Size of TX buffer descriptor ring
- * @tx_bd_ci:	Stores the next Tx buffer descriptor in the ring that may be
- *		complete. Only updated at runtime by TX NAPI poll.
- * @tx_bd_tail:	Stores the index of the next Tx buffer descriptor in the ring
- *              to be populated.
  * @tx_packets: TX packet count for statistics
  * @tx_bytes:	TX byte count for statistics
  * @tx_stat_sync: Synchronization object for TX stats
@@ -515,15 +945,23 @@ struct skbuf_dma_descriptor {
  * @stats_lock: Lock for @hw_stats_seqcount
  * @stats_work: Work for reading the hardware statistics counters often enough
  *              to catch overflows.
- * @dma_err_task: Work structure to process Axi DMA errors
  * @stopping:   Set when @dma_err_task shouldn't do anything because we are
  *              about to stop the device.
- * @tx_irq:	Axidma TX IRQ number
- * @rx_irq:	Axidma RX IRQ number
  * @eth_irq:	Ethernet core IRQ number
  * @phy_mode:	Phy type to identify between MII/GMII/RGMII/SGMII/1000 Base-X
  * @options:	AxiEthernet option word
+ * @dma_err_tasklet: Tasklet structure to process Axi DMA errors
+ * @mcdma_regs:	Base address for the aximcdma device address space
+ * @num_tx_queues: Total number of Tx DMA queues
+ * @num_rx_queues: Total number of Rx DMA queues
+ * @dq:		DMA queues data
+ * @phy_mode:  Phy type to identify between MII/GMII/RGMII/SGMII/1000 Base-X
+ * @ptp_tx_lock: PTP Tx lock
+ * @eth_irq:	Axi Ethernet IRQ number
+ * @options:	AxiEthernet option word
  * @features:	Stores the extended features supported by the axienet hw
+ * @tx_bd_num:	Size of TX buffer descriptor ring
+ * @rx_bd_num:	Size of RX buffer descriptor ring
  * @max_frm_size: Stores the maximum size of the frame that can be that
  *		  Txed/Rxed in the existing hardware. If jumbo option is
  *		  supported, the maximum frame size would be 9k. Else it is
@@ -542,6 +980,49 @@ struct skbuf_dma_descriptor {
  * @tx_ring_tail: TX skb ring buffer tail index.
  * @rx_ring_head: RX skb ring buffer head index.
  * @rx_ring_tail: RX skb ring buffer tail index.
+ * @eth_hasnobuf: Ethernet is configured in Non buf mode.
+ * @eth_hasptp: Ethernet is configured for ptp.
+ * @axienet_config: Ethernet config structure
+ * @ptp_os_cf: CF TS of PTP PDelay req for one step usage.
+ * @xxv_ip_version: XXV IP version
+ * @tx_ts_regs:	  Base address for the axififo device address space.
+ * @rx_ts_regs:	  Base address for the rx axififo device address space.
+ * @tstamp_config: Hardware timestamp config structure.
+ * @tx_ptpheader: Stores the tx ptp header.
+ * @aclk: AXI4-Lite clock for ethernet and dma.
+ * @eth_sclk: AXI4-Stream interface clock.
+ * @eth_refclk: Stable clock used by signal delay primitives and transceivers.
+ * @eth_dclk: Dynamic Reconfiguration Port(DRP) clock.
+ * @dma_sg_clk: DMA Scatter Gather Clock.
+ * @dma_rx_clk: DMA S2MM Primary Clock.
+ * @dma_tx_clk: DMA MM2S Primary Clock.
+ * @qnum:     Axi Ethernet queue number to be operate on.
+ * @chan_num: MCDMA Channel number to be operate on.
+ * @chan_id:  MCMDA Channel id used in conjunction with weight parameter.
+ * @weight:   MCDMA Channel weight value to be configured for.
+ * @dma_mask: Specify the width of the DMA address space.
+ * @usxgmii_rate: USXGMII PHY speed.
+ * @max_speed: Maximum possible MAC speed.
+ * @gt_pll: Common GT PLL mask control register space.
+ * @gt_ctrl: GT speed and reset control register space.
+ * @gds_gt_ctrl:	GPIO descriptor array for GT control.
+ * @gds_gt_rx_dpath: GPIO descriptor array for GT Rx datapath reset.
+ * @gds_gt_tx_dpath: GPIO descriptor array for GT Tx datapath reset.
+ * @gds_gt_rsts: GPIO descriptor array for GT serdes and core reset.
+ * @gds_gt_tx_reset_done: GPIO descriptor array to get Tx reset status.
+ * @gds_gt_rx_reset_done: GPIO descriptor array to get Rx reset status.
+ * @phc_index: Index to corresponding PTP clock used.
+ * @gt_lane: MRMAC GT lane index used.
+ * @gt_mode_narrow: true if GT is configured to operate in Narrow mode, false for Wide mode.
+ * @mrmac_stream_dwidth: MRMAC AXI4-Stream data width (bits).
+ * @switch_lock: Spinlock for switchable IP.
+ * @auto_neg: true if auto neg property is enabled in the IP.
+ * @eoe_regs: Ethernet offload IP base address.
+ * @eoe_connected: Tells whether ethernet offload IP is connected to Ethernet IP.
+ * @eoe_features: EOE IP supported configuration.
+ * @inetaddr_notifier: Notifier callback function for specific event.
+ * @rx_fs_list: RX queue filter rule set.
+ * @assigned_rx_port: Ports assigned to GRO Queue.
  */
 struct axienet_local {
 	struct net_device *ndev;
@@ -563,29 +1044,16 @@ struct axienet_local {
 
 	resource_size_t regs_start;
 	void __iomem *regs;
-	void __iomem *dma_regs;
-
-	struct napi_struct napi_rx;
-	u32 rx_dma_cr;
-	struct axidma_bd *rx_bd_v;
-	dma_addr_t rx_bd_p;
-	u32 rx_bd_num;
-	u32 rx_bd_ci;
-	u64_stats_t rx_packets;
-	u64_stats_t rx_bytes;
-	struct u64_stats_sync rx_stat_sync;
+	void __iomem *mcdma_regs;
 
-	struct napi_struct napi_tx;
-	u32 tx_dma_cr;
-	struct axidma_bd *tx_bd_v;
-	dma_addr_t tx_bd_p;
-	u32 tx_bd_num;
-	u32 tx_bd_ci;
-	u32 tx_bd_tail;
 	u64_stats_t tx_packets;
 	u64_stats_t tx_bytes;
 	struct u64_stats_sync tx_stat_sync;
 
+	u64_stats_t rx_packets;
+	u64_stats_t rx_bytes;
+	struct u64_stats_sync rx_stat_sync;
+
 	u64 hw_stat_base[STAT_COUNT];
 	u32 hw_last_counter[STAT_COUNT];
 	seqcount_mutex_t hw_stats_seqcount;
@@ -593,16 +1061,20 @@ struct axienet_local {
 	struct delayed_work stats_work;
 	bool reset_in_progress;
 
-	struct work_struct dma_err_task;
 	bool stopping;
+	struct tasklet_struct dma_err_tasklet[XAE_MAX_QUEUES];
+	u16    num_tx_queues;	/* Number of TX DMA queues */
+	u16    num_rx_queues;	/* Number of RX DMA queues */
+	struct axienet_dma_q *dq[XAE_MAX_QUEUES];	/* DMA queue data*/
 
-	int tx_irq;
-	int rx_irq;
-	int eth_irq;
 	phy_interface_t phy_mode;
+	spinlock_t ptp_tx_lock;		/* PTP tx lock*/
+	int eth_irq;
 
 	u32 options;
 	u32 features;
+	u32 tx_bd_num;
+	u32 rx_bd_num;
 
 	u32 max_frm_size;
 	u32 rxmem;
@@ -620,6 +1092,171 @@ struct axienet_local {
 	int tx_ring_tail;
 	int rx_ring_head;
 	int rx_ring_tail;
+	bool eth_hasnobuf;
+	bool eth_hasptp;
+	const struct axienet_config *axienet_config;
+	u64 ptp_os_cf;		/* CF TS of PTP PDelay req for one step usage */
+	u32 xxv_ip_version;
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	void __iomem *tx_ts_regs;
+	void __iomem *rx_ts_regs;
+	struct hwtstamp_config tstamp_config;
+	u8 *tx_ptpheader;
+#endif
+	struct clk *aclk;
+	struct clk *eth_sclk;
+	struct clk *eth_refclk;
+	struct clk *eth_dclk;
+	struct clk *dma_sg_clk;
+	struct clk *dma_rx_clk;
+	struct clk *dma_tx_clk;
+
+	/* MCDMA Fields */
+	int qnum[XAE_MAX_QUEUES];
+	int chan_num[XAE_MAX_QUEUES];
+	/* WRR Fields */
+	u16 chan_id;
+	u16 weight;
+
+	u32 dma_mask;
+	u32 usxgmii_rate;
+
+	u32 max_speed;		/* Max MAC speed */
+	void __iomem *gt_pll;	/* Common GT PLL mask control register space */
+	void __iomem *gt_ctrl;	/* GT speed and reset control register space */
+	struct gpio_descs *gds_gt_ctrl;
+	struct gpio_descs *gds_gt_rx_dpath;
+	struct gpio_descs *gds_gt_tx_dpath;
+	struct gpio_descs *gds_gt_rsts;
+	struct gpio_descs *gds_gt_tx_reset_done;
+	struct gpio_descs *gds_gt_rx_reset_done;
+	u32 phc_index;		/* Index to corresponding PTP clock used  */
+	u32 gt_lane;		/* MRMAC GT lane index used */
+	bool gt_mode_narrow;
+	int mrmac_stream_dwidth;
+	spinlock_t switch_lock;	/* To protect Link training programming from multiple context */
+	bool auto_neg;
+	void __iomem *eoe_regs;
+	bool eoe_connected;
+	u32 eoe_features;
+	struct notifier_block inetaddr_notifier;
+	struct ethtool_rx_fs_list rx_fs_list;
+	u16 assigned_rx_port[XAE_MAX_QUEUES];
+};
+
+/**
+ * struct axienet_dma_q - axienet private per dma queue data
+ * @lp:		Parent pointer
+ * @dma_regs:	Base address for the axidma device address space
+ * @tx_irq:	Axidma TX IRQ number
+ * @rx_irq:	Axidma RX IRQ number
+ * @tx_lock:	Spin lock for tx path
+ * @napi_tx:	NAPI TX control structure
+ * @tx_dma_cr:  Nominal content of TX DMA control register
+ * @tx_bd_v:	Virtual address of the TX buffer descriptor ring
+ * @tx_bd_p:	Physical address(start address) of the TX buffer descr. ring
+ * @tx_bd_ci:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while alloc. BDs before a TX starts
+ * @tx_bd_tail:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while processing BDs after the TX
+ *		completed.
+ * @napi_rx:	NAPI RX control structure
+ * @rx_dma_cr:  Nominal content of RX DMA control register
+ * @rx_bd_v:	Virtual address of the RX buffer descriptor ring
+ * @rx_bd_p:	Physical address(start address) of the RX buffer descr. ring
+ * @rx_bd_ci:	Stores the index of the Rx buffer descriptor in the ring being
+ * @tx_buf:	Virtual address of the Tx buffer pool used by the driver when
+ *		DMA h/w is configured without DRE.
+ * @tx_bufs:	Virutal address of the Tx buffer address.
+ * @tx_bufs_dma: Physical address of the Tx buffer address used by the driver
+ *		 when DMA h/w is configured without DRE.
+ * @eth_hasdre: Tells whether DMA h/w is configured with dre or not.
+ * @chan_id:    MCDMA channel to operate on.
+ * @rx_offset:	MCDMA S2MM channel starting offset.
+ * @txq_bd_v:	Virtual address of the MCDMA TX buffer descriptor ring
+ * @rxq_bd_v:	Virtual address of the MCDMA RX buffer descriptor ring
+ * @txq_packets: Number of transmit packets processed by the dma queue.
+ * @txq_bytes:   Number of transmit bytes processed by the dma queue.
+ * @rxq_packets: Number of receive packets processed by the dma queue.
+ * @rxq_bytes:	Number of receive bytes processed by the dma queue.
+ * @skb:	Socket buffer for GRO.
+ * @rx_data:	stores the length of GRO skb fragments.
+ */
+struct axienet_dma_q {
+	struct axienet_local	*lp; /* parent */
+	void __iomem *dma_regs;
+
+	int tx_irq;
+	int rx_irq;
+
+	spinlock_t tx_lock;		/* tx lock */
+
+	struct napi_struct napi_tx;
+	u32 tx_dma_cr;
+	struct axidma_bd *tx_bd_v;
+	dma_addr_t tx_bd_p;
+	u32 tx_bd_ci;
+	u32 tx_bd_tail;
+
+	struct napi_struct napi_rx;
+	u32 rx_dma_cr;
+	struct axidma_bd *rx_bd_v;
+	dma_addr_t rx_bd_p;
+	u32 rx_bd_ci;
+
+	unsigned char *tx_buf[TX_BD_NUM_DEFAULT];
+	unsigned char *tx_bufs;
+	dma_addr_t tx_bufs_dma;
+	bool eth_hasdre;
+
+	/* MCDMA fields */
+	u16 chan_id;
+	u32 rx_offset;
+	struct aximcdma_bd *txq_bd_v;
+	struct aximcdma_bd *rxq_bd_v;
+
+	unsigned long txq_packets;
+	unsigned long txq_bytes;
+	unsigned long rxq_packets;
+	unsigned long rxq_bytes;
+	struct sk_buff *skb;
+	u32 rx_data;
+};
+
+#define AXIENET_ETHTOOLS_SSTATS_LEN 6
+#define AXIENET_TX_SSTATS_LEN(lp) ((lp)->num_tx_queues * 2)
+#define AXIENET_RX_SSTATS_LEN(lp) ((lp)->num_rx_queues * 2)
+
+/**
+ * enum axienet_ip_type - AXIENET IP/MAC type.
+ *
+ * @XAXIENET_1_2p5G:	 IP is 1G/2.5G
+ * @XAXIENET_LEGACY_10G: IP type is legacy 10G MAC.
+ * @XAXIENET_10G_25G:	 IP type is 10G/25G MAC(XXV MAC).
+ * @XAXIENET_MRMAC:	 IP type is hardened Multi Rate MAC (MRMAC).
+ * @XAXIENET_1G_10G_25G: IP type is 1G/10G/25G MAC.
+ * @XAXIENET_DCMAC: IP type is 600G Channelized Multirate Ethernet (DCMAC)
+ *
+ */
+enum axienet_ip_type {
+	XAXIENET_1_2p5G = 0,
+	XAXIENET_LEGACY_10G,
+	XAXIENET_10G_25G,
+	XAXIENET_MRMAC,
+	XAXIENET_1G_10G_25G,
+	XAXIENET_DCMAC,
+};
+
+struct axienet_config {
+	enum axienet_ip_type mactype;
+	void (*setoptions)(struct net_device *ndev, u32 options);
+	int (*clk_init)(struct platform_device *pdev, struct clk **axi_aclk,
+			struct clk **axis_clk, struct clk **ref_clk,
+			struct clk **dclk);
+	u32 tx_ptplen;
+	u8 ts_header_len;
+	int (*gt_reset)(struct net_device *ndev);
 };
 
 /**
@@ -634,6 +1271,17 @@ struct axienet_option {
 	u32 m_or;
 };
 
+struct xxvenet_option {
+	u32 opt;
+	u32 reg;
+	u32 m_or;
+};
+
+extern void __iomem *mrmac_gt_pll;
+extern void __iomem *mrmac_gt_ctrl;
+extern int mrmac_pll_reg;
+extern int mrmac_pll_rst;
+
 /**
  * axienet_ior - Memory mapped Axi Ethernet register read
  * @lp:         Pointer to axienet local structure
@@ -681,58 +1329,228 @@ static inline void axienet_iow(struct axienet_local *lp, off_t offset,
 }
 
 /**
- * axienet_dma_out32 - Memory mapped Axi DMA register write.
- * @lp:		Pointer to axienet local structure
- * @reg:	Address offset from the base address of the Axi DMA core
- * @value:	Value to be written into the Axi DMA register
+ * axienet_get_mrmac_blocklock - Write to Clear MRMAC RX block lock status register
+ * and read the latest status
+ * @lp:         Pointer to axienet local structure
  *
- * This function writes the desired value into the corresponding Axi DMA
- * register.
+ * Return: The contents of the Contents of MRMAC RX block lock status register
  */
 
-static inline void axienet_dma_out32(struct axienet_local *lp,
-				     off_t reg, u32 value)
+static inline u32 axienet_get_mrmac_blocklock(struct axienet_local *lp)
 {
-	iowrite32(value, lp->dma_regs + reg);
+	axienet_iow(lp, MRMAC_STATRX_BLKLCK_OFFSET, MRMAC_STS_ALL_MASK);
+	return axienet_ior(lp, MRMAC_STATRX_BLKLCK_OFFSET);
 }
 
-#if defined(CONFIG_64BIT) && defined(iowrite64)
 /**
- * axienet_dma_out64 - Memory mapped Axi DMA register write.
+ * axienet_get_mrmac_rx_status - Write to Clear MRMAC RX status register
+ * and read the latest status
  * @lp:		Pointer to axienet local structure
- * @reg:	Address offset from the base address of the Axi DMA core
- * @value:	Value to be written into the Axi DMA register
  *
- * This function writes the desired value into the corresponding Axi DMA
- * register.
+ * Return: The contents of the Contents of MRMAC RX status register
  */
-static inline void axienet_dma_out64(struct axienet_local *lp,
-				     off_t reg, u64 value)
+
+static inline u32 axienet_get_mrmac_rx_status(struct axienet_local *lp)
 {
-	iowrite64(value, lp->dma_regs + reg);
+	axienet_iow(lp, MRMAC_RX_STS_OFFSET, MRMAC_STS_ALL_MASK);
+	return axienet_ior(lp, MRMAC_RX_STS_OFFSET);
 }
 
-static inline void axienet_dma_out_addr(struct axienet_local *lp, off_t reg,
-					dma_addr_t addr)
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+/**
+ * axienet_txts_ior - Memory mapped AXI FIFO MM S register read
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core
+ *
+ * Return: the contents of the AXI FIFO MM S register
+ */
+
+static inline u32 axienet_txts_ior(struct axienet_local *lp, off_t reg)
+{
+	return ioread32(lp->tx_ts_regs + reg);
+}
+
+/**
+ * axienet_txts_iow - Memory mapper AXI FIFO MM S register write
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core.
+ * @value:      Value to be written into the AXI FIFO MM S register
+ */
+static inline void axienet_txts_iow(struct  axienet_local *lp, off_t reg,
+				    u32 value)
 {
+	iowrite32(value, (lp->tx_ts_regs + reg));
+}
+
+/**
+ * axienet_rxts_ior - Memory mapped AXI FIFO MM S register read
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core
+ *
+ * Return: the contents of the AXI FIFO MM S register
+ */
+
+static inline u32 axienet_rxts_ior(struct axienet_local *lp, off_t reg)
+{
+	return ioread32(lp->rx_ts_regs + reg);
+}
+
+/**
+ * axienet_rxts_iow - Memory mapper AXI FIFO MM S register write
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core.
+ * @value:      Value to be written into the AXI FIFO MM S register
+ */
+static inline void axienet_rxts_iow(struct  axienet_local *lp, off_t reg,
+				    u32 value)
+{
+	iowrite32(value, (lp->rx_ts_regs + reg));
+}
+#endif
+
+static inline void desc_set_phys_addr(struct axienet_local *lp, dma_addr_t addr,
+				      struct axidma_bd *desc)
+{
+	desc->phys = lower_32_bits(addr);
 	if (lp->features & XAE_FEATURE_DMA_64BIT)
-		axienet_dma_out64(lp, reg, addr);
-	else
-		axienet_dma_out32(lp, reg, lower_32_bits(addr));
+		desc->phys_msb = upper_32_bits(addr);
+}
+
+static inline dma_addr_t desc_get_phys_addr(struct axienet_local *lp,
+					    struct axidma_bd *desc)
+{
+	dma_addr_t ret = desc->phys;
+
+	if (lp->features & XAE_FEATURE_DMA_64BIT)
+		ret |= ((dma_addr_t)desc->phys_msb << 16) << 16;
+
+	return ret;
+}
+
+static inline void mcdma_desc_set_phys_addr(struct axienet_local *lp, dma_addr_t addr,
+					    struct aximcdma_bd *desc)
+{
+	desc->phys = lower_32_bits(addr);
+	if (lp->features & XAE_FEATURE_DMA_64BIT)
+		desc->phys_msb = upper_32_bits(addr);
+}
+
+static inline dma_addr_t mcdma_desc_get_phys_addr(struct axienet_local *lp,
+						  struct aximcdma_bd *desc)
+{
+	dma_addr_t ret = desc->phys;
+
+	if (lp->features & XAE_FEATURE_DMA_64BIT)
+		ret |= ((dma_addr_t)desc->phys_msb << 16) << 16;
+
+	return ret;
 }
 
-#else /* CONFIG_64BIT */
+/**
+ * axienet_dma_in32 - Memory mapped Axi DMA register read
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ *
+ * Return: The contents of the Axi DMA register
+ *
+ * This function returns the contents of the corresponding Axi DMA register.
+ */
+static inline u32 axienet_dma_in32(struct axienet_dma_q *q, off_t reg)
+{
+	return ioread32(q->dma_regs + reg);
+}
 
-static inline void axienet_dma_out_addr(struct axienet_local *lp, off_t reg,
-					dma_addr_t addr)
+/**
+ * axienet_dma_out32 - Memory mapped Axi DMA register write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_out32(struct axienet_dma_q *q,
+				     off_t reg, u32 value)
 {
-	axienet_dma_out32(lp, reg, lower_32_bits(addr));
+	iowrite32(value, q->dma_regs + reg);
 }
 
-#endif /* CONFIG_64BIT */
+/**
+ * axienet_dma_bdout - Memory mapped Axi DMA register Buffer Descriptor write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_bdout(struct axienet_dma_q *q,
+				     off_t reg, dma_addr_t value)
+{
+#if defined(CONFIG_PHYS_ADDR_T_64BIT) && defined(iowrite64)
+	iowrite64(value, (q->dma_regs + reg));
+#else
+	writel(value, (q->dma_regs + reg));
+#endif
+}
 
 /* Function prototypes visible in xilinx_axienet_mdio.c for other files */
 int axienet_mdio_setup(struct axienet_local *lp);
 void axienet_mdio_teardown(struct axienet_local *lp);
+void __maybe_unused axienet_bd_free(struct net_device *ndev,
+				    struct axienet_dma_q *q);
+int __maybe_unused axienet_dma_q_init(struct net_device *ndev,
+				      struct axienet_dma_q *q);
+void axienet_dma_err_handler(unsigned long data);
+void axienet_dma_start(struct axienet_dma_q *dq);
+void axienet_dma_stop(struct axienet_dma_q *dq);
+irqreturn_t __maybe_unused axienet_tx_irq(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_rx_irq(int irq, void *_ndev);
+void axienet_dma_bd_release(struct net_device *ndev);
+int __axienet_device_reset(struct axienet_dma_q *q);
+void axienet_set_mac_address(struct net_device *ndev, const void *address);
+void axienet_set_multicast_list(struct net_device *ndev);
+int xaxienet_rx_poll(struct napi_struct *napi, int quota);
+int axienet_tx_poll(struct napi_struct *napi, int budget);
+
+#if defined(CONFIG_AXIENET_HAS_MCDMA)
+int __maybe_unused axienet_mcdma_rx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q);
+int __maybe_unused axienet_mcdma_tx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_tx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_rx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q);
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq(int irq, void *_ndev);
+void __maybe_unused axienet_mcdma_err_handler(unsigned long data);
+void axienet_strings(struct net_device *ndev, u32 sset, u8 *data);
+int axienet_sset_count(struct net_device *ndev, int sset);
+void axienet_get_stats(struct net_device *ndev,
+		       struct ethtool_stats *stats,
+		       u64 *data);
+int axeinet_mcdma_create_sysfs(struct kobject *kobj);
+void axeinet_mcdma_remove_sysfs(struct kobject *kobj);
+int __maybe_unused axienet_mcdma_tx_probe(struct platform_device *pdev,
+					  struct device_node *np,
+					  struct axienet_local *lp);
+int __maybe_unused axienet_mcdma_rx_probe(struct platform_device *pdev,
+					  struct axienet_local *lp,
+					  struct net_device *ndev);
+#endif
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct aximcdma_bd *cur_p);
+#else
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct axidma_bd *cur_p);
+#endif
+u32 axienet_usec_to_timer(struct axienet_local *lp, u32 coalesce_usec);
 
 #endif /* XILINX_AXI_ENET_H */
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_dma.c b/drivers/net/ethernet/xilinx/xilinx_axienet_dma.c
new file mode 100644
index 000000000..282297d6a
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_dma.c
@@ -0,0 +1,441 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx AXI Ethernet (DMA programming)
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2010 - 2012 Xilinx, Inc.
+ * Copyright (C) 2018 - 2022 Xilinx, Inc. All rights reserved.
+ * Copyright (c) 2022 Advanced Micro Devices, Inc.
+ *
+ * This file contains helper functions for AXI DMA TX and RX programming.
+ */
+
+#include "xilinx_axienet.h"
+
+/**
+ * axienet_bd_free - Release buffer descriptor rings for individual dma queue
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is helper function to axienet_dma_bd_release.
+ */
+
+void __maybe_unused axienet_bd_free(struct net_device *ndev,
+				    struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t phys;
+	int i;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		phys = desc_get_phys_addr(lp, &q->rx_bd_v[i]);
+		if (phys)
+			dma_unmap_single(ndev->dev.parent, phys,
+					 lp->max_frm_size, DMA_FROM_DEVICE);
+		dev_kfree_skb((struct sk_buff *)
+			      (q->rx_bd_v[i].sw_id_offset));
+	}
+
+	if (q->rx_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->rx_bd_v) * lp->rx_bd_num,
+				  q->rx_bd_v,
+				  q->rx_bd_p);
+	}
+	if (q->tx_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->tx_bd_v) * lp->tx_bd_num,
+				  q->tx_bd_v,
+				  q->tx_bd_p);
+	}
+	if (q->tx_bufs) {
+		dma_free_coherent(ndev->dev.parent,
+				  XAE_MAX_PKT_LEN * lp->tx_bd_num,
+				  q->tx_bufs,
+				  q->tx_bufs_dma);
+	}
+}
+
+/**
+ * __dma_txq_init - Setup buffer descriptor rings for individual Axi DMA-Tx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_q_init
+ */
+static int __dma_txq_init(struct net_device *ndev, struct axienet_dma_q *q)
+{
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+
+	q->tx_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					sizeof(*q->tx_bd_v) * lp->tx_bd_num,
+					&q->tx_bd_p, GFP_KERNEL);
+	if (!q->tx_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		dma_addr_t addr = q->tx_bd_p +
+				  sizeof(*q->tx_bd_v) *
+				  ((i + 1) % lp->tx_bd_num);
+
+		q->tx_bd_v[i].next = lower_32_bits(addr);
+		if (lp->features & XAE_FEATURE_DMA_64BIT)
+			q->tx_bd_v[i].next_msb = upper_32_bits(addr);
+	}
+
+	if (!q->eth_hasdre) {
+		q->tx_bufs = dma_alloc_coherent(ndev->dev.parent,
+						XAE_MAX_PKT_LEN * lp->tx_bd_num,
+						&q->tx_bufs_dma,
+						GFP_KERNEL);
+		if (!q->tx_bufs)
+			goto out;
+
+		for (i = 0; i < lp->tx_bd_num; i++)
+			q->tx_buf[i] = &q->tx_bufs[i * XAE_MAX_PKT_LEN];
+	}
+	return 0;
+out:
+	return -ENOMEM;
+}
+
+/**
+ * __dma_rxq_init - Setup buffer descriptor rings for individual Axi DMA-Rx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_q_init
+ */
+static int __dma_rxq_init(struct net_device *ndev,
+			  struct axienet_dma_q *q)
+{
+	int i;
+	struct sk_buff *skb;
+	struct axienet_local *lp = netdev_priv(ndev);
+	/* Reset the indexes which are used for accessing the BDs */
+	q->rx_bd_ci = 0;
+
+	/* Allocate the Rx buffer descriptors. */
+	q->rx_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					sizeof(*q->rx_bd_v) * lp->rx_bd_num,
+					&q->rx_bd_p, GFP_KERNEL);
+	if (!q->rx_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		dma_addr_t addr;
+
+		addr = q->rx_bd_p + sizeof(*q->rx_bd_v) *
+			((i + 1) % lp->rx_bd_num);
+		q->rx_bd_v[i].next = lower_32_bits(addr);
+		if (lp->features & XAE_FEATURE_DMA_64BIT)
+			q->rx_bd_v[i].next_msb = upper_32_bits(addr);
+
+		skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!skb)
+			goto out;
+
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		q->rx_bd_v[i].sw_id_offset = skb;
+
+		addr = dma_map_single(lp->dev, skb->data,
+				      lp->max_frm_size, DMA_FROM_DEVICE);
+		if (unlikely(dma_mapping_error(lp->dev, addr))) {
+			netdev_err(ndev, "AXI DMA mapping error\n");
+			goto out;
+		}
+		desc_set_phys_addr(lp, addr, &q->rx_bd_v[i]);
+
+		q->rx_bd_v[i].cntrl = lp->max_frm_size;
+	}
+
+	return 0;
+out:
+	return -ENOMEM;
+}
+
+/**
+ * axienet_dma_q_init - Setup buffer descriptor rings for individual Axi DMA
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_dma_q_init(struct net_device *ndev,
+				      struct axienet_dma_q *q)
+{
+	if (__dma_txq_init(ndev, q))
+		goto out;
+
+	if (__dma_rxq_init(ndev, q))
+		goto out;
+
+	axienet_dma_start(q);
+	return 0;
+out:
+	axienet_dma_bd_release(ndev);
+	return -ENOMEM;
+}
+
+/**
+ * map_dma_q_irq - Map dma q based on interrupt number.
+ * @irq:	irq number
+ * @lp:		axienet local structure
+ *
+ * Return: DMA queue.
+ *
+ * This returns the DMA number on which interrupt has occurred.
+ */
+static int map_dma_q_irq(int irq, struct axienet_local *lp)
+{
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		if (irq == lp->dq[i]->tx_irq || irq == lp->dq[i]->rx_irq)
+			return i;
+	}
+	pr_err("Error mapping DMA irq\n");
+	return -ENODEV;
+}
+
+/**
+ * axienet_tx_irq - Tx Done Isr.
+ * @irq:	irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return: IRQ_HANDLED if device generated a TX interrupt, IRQ_NONE otherwise.
+ *
+ * This is the Axi DMA Tx done Isr.
+ */
+irqreturn_t __maybe_unused axienet_tx_irq(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i = map_dma_q_irq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (i < 0)
+		return IRQ_NONE;
+
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XAXIDMA_TX_SR_OFFSET);
+	if (status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) {
+		/* Disable further TX completion interrupts and schedule
+		 * NAPI to handle the completions.
+		 */
+		axienet_dma_out32(q, XAXIDMA_TX_SR_OFFSET, status);
+		cr = q->tx_dma_cr;
+
+		cr &= ~(XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);
+		if (napi_schedule_prep(&q->napi_tx)) {
+			axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
+			__napi_schedule(&q->napi_tx);
+		}
+		goto out;
+	}
+
+	if (!(status & XAXIDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+	if (status & XAXIDMA_IRQ_ERROR_MASK) {
+		dev_err(&ndev->dev, "DMA Tx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: 0x%x%08x\n",
+			q->tx_bd_v[q->tx_bd_ci].phys_msb,
+			q->tx_bd_v[q->tx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+		/* Write to the Tx channel control register */
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
+
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+		/* Write to the Rx channel control register */
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XAXIDMA_TX_SR_OFFSET, status);
+	}
+out:
+	return IRQ_HANDLED;
+}
+
+/**
+ * axienet_rx_irq - Rx Isr.
+ * @irq:	irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return: IRQ_HANDLED if device generated a RX interrupt, IRQ_NONE otherwise.
+ *
+ * This is the Axi DMA Rx Isr. It invokes "axienet_recv" to complete the BD
+ * processing.
+ */
+irqreturn_t __maybe_unused axienet_rx_irq(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i = map_dma_q_irq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (i < 0)
+		return IRQ_NONE;
+
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
+	if (status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) {
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		cr &= ~(XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);
+		if (napi_schedule_prep(&q->napi_rx)) {
+			axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+			__napi_schedule(&q->napi_rx);
+		}
+	}
+
+	if (!(status & XAXIDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	if (status & XAXIDMA_IRQ_ERROR_MASK) {
+		dev_err(&ndev->dev, "DMA Rx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: 0x%x%08x\n",
+			q->rx_bd_v[q->rx_bd_ci].phys_msb,
+			q->rx_bd_v[q->rx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
+
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+			/* write to the Rx channel control register */
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XAXIDMA_RX_SR_OFFSET, status);
+	}
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * axienet_dma_err_handler - Tasklet handler for Axi DMA Error
+ * @data:	Data passed
+ *
+ * Resets the Axi DMA and Axi Ethernet devices, and reconfigures the
+ * Tx/Rx BDs.
+ */
+void __maybe_unused axienet_dma_err_handler(unsigned long data)
+{
+	u32 axienet_status;
+	u32 i;
+	struct axienet_dma_q *q = (struct axienet_dma_q *)data;
+	struct axienet_local *lp = q->lp;
+	struct net_device *ndev = lp->ndev;
+	struct axidma_bd *cur_p;
+
+	/* Don't bother if we are going to stop anyway */
+	if (READ_ONCE(lp->stopping))
+		return;
+
+	napi_disable(&q->napi_tx);
+	napi_disable(&q->napi_rx);
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	/* When we do an Axi Ethernet reset, it resets the complete core
+	 * including the MDIO. MDIO must be disabled before resetting.
+	 * Hold MDIO bus lock to avoid MDIO accesses during the reset.
+	 */
+	axienet_dma_stop(q);
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		cur_p = &q->tx_bd_v[i];
+		if (cur_p->cntrl) {
+			dma_addr_t addr = desc_get_phys_addr(lp, cur_p);
+
+			dma_unmap_single(ndev->dev.parent, addr,
+					 (cur_p->cntrl &
+					  XAXIDMA_BD_CTRL_LENGTH_MASK),
+					 DMA_TO_DEVICE);
+		}
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+		cur_p->phys = 0;
+		cur_p->phys_msb = 0;
+		cur_p->cntrl = 0;
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+		cur_p->sw_id_offset = 0;
+		cur_p->tx_skb = 0;
+	}
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		cur_p = &q->rx_bd_v[i];
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+	}
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+	q->rx_bd_ci = 0;
+
+	axienet_dma_start(q);
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G &&
+	    !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G)
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	axienet_set_mac_address(ndev, NULL);
+	axienet_set_multicast_list(ndev);
+	napi_enable(&q->napi_rx);
+	napi_enable(&q->napi_tx);
+	lp->axienet_config->setoptions(ndev, lp->options);
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_eoe.c b/drivers/net/ethernet/xilinx/xilinx_axienet_eoe.c
new file mode 100644
index 000000000..8b10e2092
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_eoe.c
@@ -0,0 +1,533 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx AXI EOE (EOE programming)
+ *
+ * Copyright (c) 2024 Advanced Micro Devices, Inc.
+ *
+ * This file contains probe function for EOE TX and RX programming.
+ */
+
+#include <linux/of_address.h>
+#include <linux/platform_device.h>
+#include <linux/skbuff.h>
+#include <linux/udp.h>
+#include <linux/tcp.h>
+#include <linux/ip.h>
+
+#include "xilinx_axienet_eoe.h"
+
+/**
+ * axienet_eoe_probe - Axi EOE probe function
+ * @pdev:       Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *         Non-zero error value on failure.
+ *
+ * This is the probe routine for Ethernet Offload Engine and called when
+ * EOE is connected to Ethernet IP. It allocates the address space
+ * for EOE. Parses through device tree and updates Tx and RX offload features
+ * in netdev and axiethernet private structure respectively.
+ */
+int axienet_eoe_probe(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct resource eoe_res;
+	int index, ret = 0;
+	int value;
+
+	index = of_property_match_string(pdev->dev.of_node, "reg-names", "eoe");
+
+	if (index < 0)
+		return dev_err_probe(&pdev->dev, -EINVAL, "failed to find EOE registers\n");
+
+	ret = of_address_to_resource(pdev->dev.of_node, index, &eoe_res);
+	if (ret)
+		return dev_err_probe(&pdev->dev, ret, "unable to get EOE resource\n");
+
+	lp->eoe_regs = devm_ioremap_resource(&pdev->dev, &eoe_res);
+
+	if (IS_ERR(lp->eoe_regs))
+		return dev_err_probe(&pdev->dev, PTR_ERR(lp->eoe_regs), "couldn't map EOE regs\n");
+
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,tx-hw-offload", &value);
+	if (!ret) {
+		dev_dbg(&pdev->dev, "xlnx,tx-hw-offload %d\n", value);
+
+		switch (value) {
+		case 0:
+			break;
+
+		case 1:
+			/* Can checksum Tx UDP over IPv4. */
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+			ndev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+			break;
+
+		case 2:
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM | NETIF_F_GSO_UDP_L4;
+			ndev->hw_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM
+					| NETIF_F_GSO_UDP_L4;
+			break;
+
+		default:
+			dev_warn(&pdev->dev, "xlnx,tx-hw-offload: %d is an invalid value\n", value);
+			return -EINVAL;
+		}
+	}
+
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,rx-hw-offload", &value);
+	if (!ret) {
+		dev_dbg(&pdev->dev, "xlnx,rx-hw-offload %d\n", value);
+
+		switch (value) {
+		case 0:
+			lp->eoe_features |= RX_HW_NO_OFFLOAD;
+			break;
+		case 1:
+			lp->eoe_features |= RX_HW_CSO;
+			break;
+		case 2:
+			lp->eoe_features |= RX_HW_UDP_GRO;
+			break;
+		default:
+			dev_warn(&pdev->dev, "xlnx,rx-hw-offload: %d is an invalid value\n", value);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static inline int axienet_eoe_packet_header_length(struct sk_buff *skb)
+{
+	u32 hdr_len = skb_mac_header_len(skb) + skb_network_header_len(skb);
+
+	if (skb->sk->sk_protocol == IPPROTO_UDP)
+		hdr_len += sizeof(struct udphdr);
+	else if (skb->sk->sk_protocol == IPPROTO_TCP)
+		hdr_len += tcp_hdrlen(skb);
+
+	return hdr_len;
+}
+
+void axienet_eoe_config_hwcso(struct net_device *ndev,
+			      struct aximcdma_bd *cur_p)
+{
+	/* 1) When total length < MSS, APP0 can be made all 0's and no need to program
+	 * valid values on other fields except bits 8 to 11 in APP1
+	 * 2) When APP0 is all 0's, the total length is assumed to be less than the MSS
+	 * size
+	 * 3) Bit 9(checksum offload) must be 0 to calculate checksum on segmented
+	 * packets.
+	 */
+	cur_p->app1 |= (ndev->mtu << XMCDMA_APP1_MSS_SIZE_SHIFT) &
+			XMCDMA_APP1_MSS_SIZE_MASK;
+
+	cur_p->app1 |= (XMCDMA_APP1_GSO_PKT_MASK |
+			XMCDMA_APP1_UDP_SO_MASK |
+			XMCDMA_APP1_TCP_SO_MASK);
+}
+
+void axienet_eoe_config_hwgso(struct net_device *ndev,
+			      struct sk_buff *skb,
+			      struct aximcdma_bd *cur_p)
+{
+	/* 1) Total length, MSS, Header length has to be filled out correctly. There is
+	 * no error checking mechanism in the code. Code blindly believes in this
+	 * information for segmentation.
+	 * 2) When total length < MSS, APP0 can be made all 0's and no need to program
+	 * valid values on other fields except bits 8 to 11 in APP1
+	 * 3) When APP0 is all 0's, the total length is assumed to be less than the MSS
+	 * size and no segmentation will be performed
+	 * 4) TCP segmentation is performed when bit 10 (TCP segmentation offload) and
+	 * bit 8(is GSO packet) are 0's in APP1. Otherwise the packets are bypassed.
+	 * 5) UDP segmentation is performed when bit 11 (UDP segmentation offload) and
+	 * bit 8(is GSO packet) are 0's in APP1.Otherwise the packets are bypassed.
+	 * 6) Bit 9(checksum offload) must be 0 to calculate checksum on segmented
+	 * packets.
+	 */
+	cur_p->app1 = (ndev->mtu << XMCDMA_APP1_MSS_SIZE_SHIFT) &
+		       XMCDMA_APP1_MSS_SIZE_MASK;
+
+	if (skb_shinfo(skb)->gso_size) {
+		cur_p->app0 = (skb->len - XAE_HDR_SIZE) & XMCDMA_APP0_TOTAL_PKT_LEN_MASK;
+		cur_p->app0 |= (axienet_eoe_packet_header_length(skb) <<
+				XMCDMA_APP0_PKT_HEAD_LEN_SHIFT) &
+				XMCDMA_APP0_PKT_HEAD_LEN_MASK;
+
+		if (skb_shinfo(skb)->gso_type == SKB_GSO_UDP_L4)
+			cur_p->app1 |= XMCDMA_APP1_TCP_SO_MASK;
+		else if (skb_shinfo(skb)->gso_type == SKB_GSO_TCPV4)
+			cur_p->app1 |= XMCDMA_APP1_UDP_SO_MASK;
+
+	} else {
+		cur_p->app1 |= (XMCDMA_APP1_GSO_PKT_MASK | XMCDMA_APP1_UDP_SO_MASK |
+				XMCDMA_APP1_TCP_SO_MASK);
+	}
+}
+
+int __maybe_unused axienet_eoe_mcdma_gro_q_init(struct net_device *ndev,
+						struct axienet_dma_q *q,
+						int i)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t mapping;
+	struct page *page;
+
+	page = alloc_pages(GFP_KERNEL, 0);
+	if (!page) {
+		netdev_err(ndev, "page allocation failed\n");
+		goto out;
+	}
+	q->rxq_bd_v[i].page = page;
+	mapping = dma_map_page(ndev->dev.parent, page, 0,
+			       PAGE_SIZE, DMA_FROM_DEVICE);
+	if (unlikely(dma_mapping_error(ndev->dev.parent, mapping))) {
+		netdev_err(ndev, "dma mapping error\n");
+		goto free_page;
+	}
+	mcdma_desc_set_phys_addr(lp, mapping, &q->rxq_bd_v[i]);
+	q->rxq_bd_v[i].cntrl = PAGE_SIZE;
+
+	return 0;
+
+free_page:
+	__free_pages(q->rxq_bd_v[i].page, 0);
+out:
+	return -ENOMEM;
+}
+
+void __maybe_unused axienet_eoe_mcdma_gro_bd_free(struct net_device *ndev,
+						  struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t phys;
+	int i;
+
+	if (!q->rxq_bd_v)
+		return;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		phys = mcdma_desc_get_phys_addr(lp, &q->rxq_bd_v[i]);
+		if (phys) {
+			dma_unmap_page(ndev->dev.parent, phys, PAGE_SIZE,
+				       DMA_FROM_DEVICE);
+			__free_pages(q->rxq_bd_v[i].page, 0);
+		}
+	}
+
+	dma_free_coherent(ndev->dev.parent,
+			  sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+			  q->rxq_bd_v,
+			  q->rx_bd_p);
+
+	q->rxq_bd_v = NULL;
+}
+
+int axienet_eoe_recv_gro(struct net_device *ndev, int budget,
+			 struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 length, packets = 0, size = 0;
+	dma_addr_t phys, tail_p = 0;
+	unsigned int numbdfree = 0;
+	struct aximcdma_bd *cur_p;
+	struct iphdr *iphdr;
+	struct page *page;
+	struct udphdr *uh;
+	void *page_addr;
+
+	/* Get relevat BD status value */
+	rmb();
+	cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+
+	while ((numbdfree < budget) &&
+	       (cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
+		tail_p = q->rx_bd_p + sizeof(*q->rxq_bd_v) * q->rx_bd_ci;
+		phys = mcdma_desc_get_phys_addr(lp, cur_p);
+		dma_unmap_page(ndev->dev.parent, phys, PAGE_SIZE,
+			       DMA_FROM_DEVICE);
+
+		length = cur_p->status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+
+		page = (struct page *)cur_p->page;
+		if (!page) {
+			netdev_err(ndev, "Page is Not Defined\n");
+			break;
+		}
+
+		page_addr = page_address(page);
+
+		q->rx_data += length;
+
+		if (q->skb) {
+			skb_add_rx_frag(q->skb,
+					skb_shinfo(q->skb)->nr_frags,
+					page, 0, length, q->rx_data);
+		}
+
+		if (((cur_p->app0 & XEOE_UDP_GRO_RXSOP_MASK) >> XEOE_UDP_GRO_RXSOP_SHIFT)) {
+			/* Allocate new skb and update in BD */
+			q->skb = netdev_alloc_skb(ndev, length);
+			memcpy(q->skb->data, page_addr, length);
+			skb_put(q->skb, length);
+			put_page(page);
+		} else if (((cur_p->app0 & XEOE_UDP_GRO_RXEOP_MASK) >> XEOE_UDP_GRO_RXEOP_SHIFT)) {
+			skb_set_network_header(q->skb, XEOE_MAC_HEADER_LENGTH);
+			iphdr = (struct iphdr *)skb_network_header(q->skb);
+			skb_set_transport_header(q->skb,
+						 iphdr->ihl * 4 + XEOE_MAC_HEADER_LENGTH);
+			uh = (struct udphdr *)skb_transport_header(q->skb);
+
+			/* App Fields are in Little Endian Byte Order */
+			iphdr->tot_len = htons(cur_p->app1 & XEOE_UDP_GRO_PKT_LEN_MASK);
+			iphdr->check = (__force __sum16)htons((cur_p->app1 &
+					XEOE_UDP_GRO_RX_CSUM_MASK) >> XEOE_UDP_GRO_RX_CSUM_SHIFT);
+			uh->len = htons((cur_p->app1 & XEOE_UDP_GRO_PKT_LEN_MASK) - iphdr->ihl * 4);
+			q->skb->protocol = eth_type_trans(q->skb, ndev);
+			q->skb->ip_summed = CHECKSUM_UNNECESSARY;
+			q->rx_data = 0;
+			/* This will give SKB to n/w Stack */
+			if (skb_shinfo(q->skb)->nr_frags <= XEOE_UDP_GRO_MAX_FRAG) {
+				netif_receive_skb(q->skb);
+				q->skb = NULL;
+			}
+		}
+
+		size += length;
+		packets++;
+		/* Ensure that the skb is completely updated
+		 * prior to mapping the MCDMA
+		 */
+		wmb();
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		page = alloc_pages(GFP_KERNEL, 0);
+		if (!page) {
+			netdev_err(ndev, "Page allocation failed\n");
+			break;
+		}
+		cur_p->page = page;
+		phys = dma_map_page(ndev->dev.parent, page, 0,
+				    PAGE_SIZE, DMA_FROM_DEVICE);
+
+		if (unlikely(dma_mapping_error(ndev->dev.parent, phys))) {
+			phys = 0;
+			mcdma_desc_set_phys_addr(lp, phys, cur_p);
+			__free_pages(cur_p->page, 0);
+			netdev_err(ndev, "dma mapping failed\n");
+			break;
+		}
+		mcdma_desc_set_phys_addr(lp, phys, cur_p);
+		cur_p->cntrl = PAGE_SIZE;
+
+		if (++q->rx_bd_ci >= lp->rx_bd_num)
+			q->rx_bd_ci = 0;
+
+		/* Get relevat BD status value */
+		rmb();
+		cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+		numbdfree++;
+	}
+
+	ndev->stats.rx_packets += packets;
+	ndev->stats.rx_bytes += size;
+	q->rxq_packets += packets;
+	q->rxq_bytes += size;
+
+	if (tail_p) {
+		axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+				q->rx_offset, tail_p);
+	}
+
+	return numbdfree;
+}
+
+int axienet_eoe_add_udp_port_register(struct net_device *ndev, struct ethtool_rx_flow_spec *fs,
+				      int chan_id, struct axienet_local *lp)
+{
+	int udp_port = lp->assigned_rx_port[chan_id - 1];
+	int ret = 0;
+	u32 val;
+
+	/* Configure Control Register to Disable GRO */
+	val = axienet_eoe_ior(lp, XEOE_UDP_GRO_CR_OFFSET(chan_id));
+	axienet_eoe_iow(lp, XEOE_UDP_GRO_CR_OFFSET(chan_id), val & (~XEOE_UDP_GRO_ENABLE));
+
+	/* Set 16 Fragments to stitch other than header and add 3 Tuple and Checksum */
+	axienet_eoe_iow(lp, XEOE_UDP_GRO_RX_COMMON_CR_OFFSET,
+			(XEOE_UDP_GRO_FRAG | XEOE_UDP_GRO_4K_FRAG_SIZE
+			| XEOE_UDP_GRO_TUPLE | XEOE_UDP_GRO_CHKSUM));
+
+	/* Configure Port Number */
+	axienet_eoe_iow(lp, XEOE_UDP_GRO_PORT_OFFSET(chan_id),
+			((udp_port << XEOE_UDP_GRO_DSTPORT_SHIFT) & XEOE_UDP_GRO_DST_PORT_MASK));
+
+	/* Check Status whether GRO Channel is busy */
+	/* Wait for GRO Channel busy with timeout */
+	ret = readl_poll_timeout(lp->eoe_regs + XEOE_UDP_GRO_SR_OFFSET(chan_id),
+				 val, !(val & XEOE_UDP_GRO_BUSY_MASK),
+				 10, DELAY_OF_ONE_MILLISEC);
+	if (ret) {
+		netdev_err(ndev, "GRO Channel %d is busy and can't be configured\n", chan_id);
+		return ret;
+	}
+
+	/* Configure Control Register to Enable GRO */
+	axienet_eoe_iow(lp, XEOE_UDP_GRO_CR_OFFSET(chan_id),
+			(((XEOE_UDP_CR_PROTOCOL << XEOE_UDP_GRO_PROTOCOL_SHIFT) &
+			XEOE_UDP_GRO_PROTOCOL_MASK) | XEOE_UDP_GRO_ENABLE));
+
+	lp->rx_fs_list.count++;
+	return 0;
+}
+
+int axienet_eoe_add_flow_filter(struct net_device *ndev, struct ethtool_rxnfc *cmd)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethtool_rx_flow_spec *fs = &cmd->fs;
+	struct ethtool_rx_fs_item *item, *newfs;
+	int ret = -EINVAL, chan_id = 0;
+	bool added = false;
+
+	newfs = kmalloc(sizeof(*newfs), GFP_KERNEL);
+	if (!newfs)
+		return -ENOMEM;
+	memcpy(&newfs->fs, fs, sizeof(newfs->fs));
+
+	netdev_dbg(ndev,
+		   "Adding flow filter entry,type=%u,queue=%u,loc=%u,src=%08X,dst=%08X,ps=%u,pd=%u\n",
+		   fs->flow_type, (int)fs->ring_cookie, fs->location,
+		   fs->h_u.tcp_ip4_spec.ip4src,
+		   fs->h_u.tcp_ip4_spec.ip4dst,
+		   be16_to_cpu(fs->h_u.udp_ip4_spec.psrc),
+		   be16_to_cpu(fs->h_u.udp_ip4_spec.pdst));
+
+	/* check for Repeated Port Number */
+	for (int i = 0; i < XAE_MAX_QUEUES; i++) {
+		if (lp->assigned_rx_port[i] == be16_to_cpu(fs->h_u.udp_ip4_spec.pdst)) {
+			netdev_err(ndev, "GRO Port %d is Repeated\n", lp->assigned_rx_port[i]);
+			ret = -EBUSY;
+			goto err_kfree;
+		}
+	}
+	/* find correct place to add in list */
+	list_for_each_entry(item, &lp->rx_fs_list.list, list) {
+		if (item->fs.location > newfs->fs.location) {
+			chan_id = lp->dq[newfs->fs.location]->chan_id;
+			lp->assigned_rx_port[newfs->fs.location] =
+				be16_to_cpu(fs->h_u.udp_ip4_spec.pdst);
+			list_add_tail(&newfs->list, &item->list);
+			added = true;
+			break;
+		} else if (item->fs.location == fs->location) {
+			netdev_err(ndev, "Rule not added: location %d not free!\n",
+				   fs->location);
+			ret = -EBUSY;
+			goto err_kfree;
+		}
+	}
+	if (!added) {
+		chan_id = lp->dq[newfs->fs.location]->chan_id;
+		lp->assigned_rx_port[newfs->fs.location] = be16_to_cpu(fs->h_u.udp_ip4_spec.pdst);
+		list_add_tail(&newfs->list, &lp->rx_fs_list.list);
+	}
+
+	switch (fs->flow_type) {
+	case UDP_V4_FLOW:
+		ret = axienet_eoe_add_udp_port_register(ndev, fs, chan_id, lp);
+		if (ret)
+			goto err_del_list;
+		break;
+	default:
+		netdev_err(ndev, "Invalid flow type\n");
+		ret = -EINVAL;
+		goto err_del_list;
+	}
+
+	return ret;
+
+err_del_list:
+	lp->assigned_rx_port[cmd->fs.location] = 0;
+	list_del(&newfs->list);
+err_kfree:
+	kfree(newfs);
+	return ret;
+}
+
+int axienet_eoe_del_flow_filter(struct net_device *ndev,
+				struct ethtool_rxnfc *cmd)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethtool_rx_fs_item *item;
+	struct ethtool_rx_flow_spec *fs;
+	int chan_id;
+	u32 val;
+
+	list_for_each_entry(item, &lp->rx_fs_list.list, list) {
+		if (item->fs.location == cmd->fs.location) {
+			/* disable screener regs for the flow entry */
+			fs = &item->fs;
+			netdev_dbg(ndev,
+				   "Deleting flow filter entry,type=%u,queue=%u,loc=%u,src=%08X,dst=%08X,ps=%u,pd=%u\n",
+				   fs->flow_type, (int)fs->ring_cookie, fs->location,
+				   fs->h_u.udp_ip4_spec.ip4src,
+				   fs->h_u.udp_ip4_spec.ip4dst,
+				   be16_to_cpu(fs->h_u.tcp_ip4_spec.psrc),
+				   be16_to_cpu(fs->h_u.tcp_ip4_spec.pdst));
+
+			chan_id = lp->dq[cmd->fs.location]->chan_id;
+			/* Configure Control Register to Disable GRO */
+			val = axienet_eoe_ior(lp, XEOE_UDP_GRO_CR_OFFSET(chan_id));
+			axienet_eoe_iow(lp, XEOE_UDP_GRO_CR_OFFSET(chan_id),
+					val & (~XEOE_UDP_GRO_ENABLE));
+			/* Disable Port Number */
+			axienet_eoe_iow(lp, XEOE_UDP_GRO_PORT_OFFSET(chan_id), 0);
+
+			lp->assigned_rx_port[cmd->fs.location] = 0;
+			list_del(&item->list);
+			lp->rx_fs_list.count--;
+			kfree(item);
+			return 0;
+		}
+	}
+
+	return -EINVAL;
+}
+
+int axienet_eoe_get_flow_entry(struct net_device *ndev,
+			       struct ethtool_rxnfc *cmd)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethtool_rx_fs_item *item;
+
+	list_for_each_entry(item, &lp->rx_fs_list.list, list) {
+		if (item->fs.location == cmd->fs.location) {
+			memcpy(&cmd->fs, &item->fs, sizeof(cmd->fs));
+			cmd->fs.ring_cookie = item->fs.location;
+			return 0;
+		}
+	}
+	return -EINVAL;
+}
+
+int axienet_eoe_get_all_flow_entries(struct net_device *ndev,
+				     struct ethtool_rxnfc *cmd, u32 *rule_locs)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethtool_rx_fs_item *item;
+	u32 cnt = 0;
+
+	list_for_each_entry(item, &lp->rx_fs_list.list, list) {
+		if (cnt == cmd->rule_cnt)
+			return -EMSGSIZE;
+		rule_locs[cnt] = item->fs.location;
+		cnt++;
+	}
+	cmd->data = lp->num_rx_queues;
+	cmd->rule_cnt = cnt;
+
+	return 0;
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_eoe.h b/drivers/net/ethernet/xilinx/xilinx_axienet_eoe.h
new file mode 100644
index 000000000..fcc6bf43c
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_eoe.h
@@ -0,0 +1,167 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Definitions for Xilinx Ethernet Offload Engine.
+ *
+ * Copyright (c) 2024 Advanced Micro Devices, Inc.
+ */
+
+#ifndef XILINX_AXIENET_EOE_H
+#define XILINX_AXIENET_EOE_H
+
+#include "xilinx_axienet.h"
+
+#define XMCDMA_DFT_RX_THRESHOLD		16
+
+/* UDP Tx : GSO- Generic Segmentation offload APP0/APP1 HW offset */
+#define XMCDMA_APP0_TOTAL_PKT_LEN_MASK	GENMASK(23, 0)
+#define XMCDMA_APP0_PKT_HEAD_LEN_MASK	GENMASK(31, 24)
+#define XMCDMA_APP0_PKT_HEAD_LEN_SHIFT	24
+
+#define XMCDMA_APP1_MSS_SIZE_MASK	GENMASK(29, 16)
+#define XMCDMA_APP1_UDP_SO_MASK		BIT(11)
+#define XMCDMA_APP1_TCP_SO_MASK		BIT(10)
+#define XMCDMA_APP1_CSO_MASK		BIT(9)
+#define XMCDMA_APP1_GSO_PKT_MASK	BIT(8)
+
+#define XMCDMA_APP1_MSS_SIZE_SHIFT	16
+
+#define XEOE_UDP_NON_GRO_CHAN_ID	1
+#define XEOE_MAC_HEADER_LENGTH		0xe
+
+/* UDP Rx : GRO- Generic Receive Offload HW offset */
+#define XEOE_UDP_GRO_RX_COMMON_CR_OFFSET	0x10
+#define XEOE_UDP_CR_PROTOCOL			0x11
+#define XEOE_UDP_GRO_CR_OFFSET(chan_id)		(0x00 + ((chan_id) - 1) * 0x40)
+#define XEOE_UDP_GRO_SR_OFFSET(chan_id)		(0x04 + ((chan_id) - 1) * 0x40)
+#define XEOE_UDP_GRO_SRC_IP_OFFSET(chan_id)	(0x08 + ((chan_id) - 1) * 0x40)
+#define XEOE_UDP_GRO_DST_IP_OFFSET(chan_id)	(0x0C + ((chan_id) - 1) * 0x40)
+#define XEOE_UDP_GRO_PORT_OFFSET(chan_id)	(0x10 + ((chan_id) - 1) * 0x40)
+#define XEOE_UDP_GRO_FRAG		0x10000000
+#define XEOE_UDP_GRO_TUPLE		BIT(3)
+#define XEOE_UDP_GRO_CHKSUM		BIT(1)
+#define XEOE_UDP_GRO_BUSY_MASK		BIT(0)
+#define XEOE_UDP_GRO_4K_FRAG_SIZE	BIT(20)
+#define XEOE_UDP_GRO_ENABLE		BIT(0)
+
+#define XEOE_UDP_GRO_RXSOP_SHIFT	30 /* First GRO Packet */
+#define XEOE_UDP_GRO_RXEOP_SHIFT	29 /* Last GRO Packet */
+
+#define XEOE_UDP_GRO_RXSOP_MASK		BIT(30)
+#define XEOE_UDP_GRO_RXEOP_MASK		BIT(29)
+
+#define XEOE_UDP_GRO_MAX_FRAG		16
+
+#define XEOE_UDP_GRO_PKT_LEN_MASK	GENMASK(15, 0)
+#define XEOE_UDP_GRO_RX_CSUM_MASK	GENMASK(31, 16)
+#define XEOE_UDP_GRO_RX_CSUM_SHIFT	16
+
+#define XEOE_UDP_GRO_DSTPORT_SHIFT	16
+#define XEOE_UDP_GRO_PROTOCOL_SHIFT	24
+
+#define XEOE_UDP_GRO_DST_PORT_MASK	GENMASK(31, 16)
+#define XEOE_UDP_GRO_PROTOCOL_MASK	GENMASK(31, 24)
+
+/* EOE Features */
+#define RX_HW_NO_OFFLOAD		BIT(0)
+#define RX_HW_CSO			BIT(1)
+#define RX_HW_UDP_GRO			BIT(2)
+
+struct ethtool_rx_fs_item {
+	struct ethtool_rx_flow_spec fs;
+	struct list_head list;
+};
+
+#ifdef CONFIG_XILINX_AXI_EOE
+int axienet_eoe_probe(struct platform_device *pdev);
+void axienet_eoe_config_hwcso(struct net_device *ndev,
+			      struct aximcdma_bd *cur_p);
+void axienet_eoe_config_hwgso(struct net_device *ndev,
+			      struct sk_buff *skb,
+			      struct aximcdma_bd *cur_p);
+int __maybe_unused axienet_eoe_mcdma_gro_q_init(struct net_device *ndev,
+						struct axienet_dma_q *q,
+						int i);
+void __maybe_unused axienet_eoe_mcdma_gro_bd_free(struct net_device *ndev,
+						  struct axienet_dma_q *q);
+int axienet_eoe_recv_gro(struct net_device *ndev,
+			 int budget,
+			 struct axienet_dma_q *q);
+int axienet_eoe_add_udp_port_register(struct net_device *ndev,
+				      struct ethtool_rx_flow_spec *fs,
+				      int chan_id, struct axienet_local *lp);
+int axienet_eoe_add_flow_filter(struct net_device *ndev, struct ethtool_rxnfc *cmd);
+int axienet_eoe_del_flow_filter(struct net_device *ndev, struct ethtool_rxnfc *cmd);
+int axienet_eoe_get_flow_entry(struct net_device *ndev, struct ethtool_rxnfc *cmd);
+int axienet_eoe_get_all_flow_entries(struct net_device *ndev,
+				     struct ethtool_rxnfc *cmd,
+				     u32 *rule_locs);
+#else
+static inline int axienet_eoe_probe(struct platform_device *pdev)
+{
+	return -ENODEV;
+}
+
+static inline void axienet_eoe_config_hwcso(struct net_device *ndev,
+					    struct aximcdma_bd *cur_p)
+{ }
+
+static inline void axienet_eoe_config_hwgso(struct net_device *ndev,
+					    struct sk_buff *skb,
+					    struct aximcdma_bd *cur_p)
+{ }
+
+static inline int __maybe_unused axienet_eoe_mcdma_gro_q_init(struct net_device *ndev,
+							      struct axienet_dma_q *q,
+							      int i)
+{
+	return 0;
+}
+
+static inline void __maybe_unused axienet_eoe_mcdma_gro_bd_free(struct net_device *ndev,
+								struct axienet_dma_q *q)
+{ }
+
+static inline int axienet_eoe_recv_gro(struct net_device *ndev,
+				       int budget,
+				       struct axienet_dma_q *q)
+{
+	return 0;
+}
+#endif
+
+static inline bool axienet_eoe_is_channel_gro(struct axienet_local *lp,
+					      struct axienet_dma_q *q)
+{
+	return ((lp->eoe_features & RX_HW_UDP_GRO) && q->chan_id != XEOE_UDP_NON_GRO_CHAN_ID);
+}
+
+/**
+ * axienet_eoe_ior - Memory mapped EOE register read
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Address offset from the base address of EOE
+ *
+ * Return: The contents of the EOE register
+ *
+ * This function returns the contents of the corresponding register.
+ */
+static inline u32 axienet_eoe_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->eoe_regs + offset);
+}
+
+/**
+ * axienet_eoe_iow - Memory mapped EOE register write
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Address offset from the base address of EOE
+ * @value:	Value to be written into the EOE register
+ *
+ * This function writes the desired value into the corresponding EOE
+ * register.
+ */
+static inline void axienet_eoe_iow(struct axienet_local *lp, off_t offset,
+				   u32 value)
+{
+	iowrite32(value, lp->eoe_regs + offset);
+}
+
+#endif /* XILINX_AXIENET_EOE_H */
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_main.c b/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
index 2d47b3544..b96dcc77e 100644
--- a/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
@@ -28,6 +28,7 @@
 #include <linux/module.h>
 #include <linux/netdevice.h>
 #include <linux/of.h>
+#include <linux/of_platform.h>
 #include <linux/of_mdio.h>
 #include <linux/of_net.h>
 #include <linux/of_irq.h>
@@ -43,12 +44,20 @@
 #include <linux/dma/xilinx_dma.h>
 #include <linux/circ_buf.h>
 #include <net/netdev_queues.h>
+#include <linux/iopoll.h>
+#include <linux/ptp_classify.h>
+#include <linux/net_tstamp.h>
+#include <linux/random.h>
+#include <net/sock.h>
+#include <linux/ptp/ptp_xilinx.h>
+#include <linux/gpio/consumer.h>
+#include <linux/inetdevice.h>
 
 #include "xilinx_axienet.h"
+#include "xilinx_axienet_eoe.h"
 
 /* Descriptors defines for Tx and Rx DMA */
-#define TX_BD_NUM_DEFAULT		128
-#define RX_BD_NUM_DEFAULT		1024
+#define RX_BD_NUM_DEFAULT		128
 #define TX_BD_NUM_MIN			(MAX_SKB_FRAGS + 1)
 #define TX_BD_NUM_MAX			4096
 #define RX_BD_NUM_MAX			4096
@@ -62,18 +71,38 @@
 #define DRIVER_VERSION		"1.00a"
 
 #define AXIENET_REGS_N		40
+#define AXIENET_TS_HEADER_LEN	8
+#define XXVENET_TS_HEADER_LEN	4
+#define MRMAC_TS_HEADER_LEN	16
+#define MRMAC_TS_HEADER_WORDS	(MRMAC_TS_HEADER_LEN / 4)
+#define NS_PER_SEC              1000000000ULL /* Nanoseconds per second */
 
-static void axienet_rx_submit_desc(struct net_device *ndev);
+#define	DELAY_1MS	1	/* 1 msecs delay*/
 
-/* Match table for of_platform binding */
-static const struct of_device_id axienet_of_match[] = {
-	{ .compatible = "xlnx,axi-ethernet-1.00.a", },
-	{ .compatible = "xlnx,axi-ethernet-1.01.a", },
-	{ .compatible = "xlnx,axi-ethernet-2.01.a", },
-	{},
-};
+/* IEEE1588 Message Type field values  */
+#define PTP_TYPE_SYNC		0
+#define PTP_TYPE_PDELAY_REQ	2
+#define PTP_TYPE_PDELAY_RESP	3
+#define PTP_TYPE_OFFSET		42
+/* SW flags used to convey message type for command FIFO handling */
+#define MSG_TYPE_SHIFT			4
+#define MSG_TYPE_SYNC_FLAG		((PTP_TYPE_SYNC + 1) << MSG_TYPE_SHIFT)
+#define MSG_TYPE_PDELAY_RESP_FLAG	((PTP_TYPE_PDELAY_RESP + 1) << \
+									 MSG_TYPE_SHIFT)
 
-MODULE_DEVICE_TABLE(of, axienet_of_match);
+void __iomem *mrmac_gt_pll;
+EXPORT_SYMBOL(mrmac_gt_pll);
+
+void __iomem *mrmac_gt_ctrl;
+EXPORT_SYMBOL(mrmac_gt_ctrl);
+
+int mrmac_pll_reg;
+EXPORT_SYMBOL(mrmac_pll_reg);
+
+int mrmac_pll_rst;
+EXPORT_SYMBOL(mrmac_pll_rst);
+
+static void axienet_rx_submit_desc(struct net_device *ndev);
 
 /* Option table for setting up Axi Ethernet hardware options */
 static struct axienet_option axienet_options[] = {
@@ -140,38 +169,62 @@ static struct skbuf_dma_descriptor *axienet_get_tx_desc(struct axienet_local *lp
 	return lp->tx_skb_ring[i & (TX_BD_NUM_MAX - 1)];
 }
 
-/**
- * axienet_dma_in32 - Memory mapped Axi DMA register read
- * @lp:		Pointer to axienet local structure
- * @reg:	Address offset from the base address of the Axi DMA core
- *
- * Return: The contents of the Axi DMA register
- *
- * This function returns the contents of the corresponding Axi DMA register.
- */
-static inline u32 axienet_dma_in32(struct axienet_local *lp, off_t reg)
-{
-	return ioread32(lp->dma_regs + reg);
-}
-
-static void desc_set_phys_addr(struct axienet_local *lp, dma_addr_t addr,
-			       struct axidma_bd *desc)
-{
-	desc->phys = lower_32_bits(addr);
-	if (lp->features & XAE_FEATURE_DMA_64BIT)
-		desc->phys_msb = upper_32_bits(addr);
-}
+/* Option table for setting up Axi Ethernet hardware options */
+static struct xxvenet_option xxvenet_options[] = {
+	{ /* Turn on FCS stripping on receive packets */
+		.opt = XAE_OPTION_FCS_STRIP,
+		.reg = XXV_RCW1_OFFSET,
+		.m_or = XXV_RCW1_FCS_MASK,
+	}, { /* Turn on FCS insertion on transmit packets */
+		.opt = XAE_OPTION_FCS_INSERT,
+		.reg = XXV_TC_OFFSET,
+		.m_or = XXV_TC_FCS_MASK,
+	}, { /* Enable transmitter */
+		.opt = XAE_OPTION_TXEN,
+		.reg = XXV_TC_OFFSET,
+		.m_or = XXV_TC_TX_MASK,
+	}, { /* Enable receiver */
+		.opt = XAE_OPTION_RXEN,
+		.reg = XXV_RCW1_OFFSET,
+		.m_or = XXV_RCW1_RX_MASK,
+	},
+	{}
+};
 
-static dma_addr_t desc_get_phys_addr(struct axienet_local *lp,
-				     struct axidma_bd *desc)
-{
-	dma_addr_t ret = desc->phys;
+/* Option table for setting up MRMAC hardware options */
+static struct xxvenet_option mrmacenet_options[] = {
+	{ /* Turn on FCS stripping on receive packets */
+		.opt = XAE_OPTION_FCS_STRIP,
+		.reg = MRMAC_CONFIG_RX_OFFSET,
+		.m_or = MRMAC_RX_DEL_FCS_MASK,
+	}, { /* Turn on FCS insertion on transmit packets */
+		.opt = XAE_OPTION_FCS_INSERT,
+		.reg = MRMAC_CONFIG_TX_OFFSET,
+		.m_or = MRMAC_TX_INS_FCS_MASK,
+	}, { /* Enable transmitter */
+		.opt = XAE_OPTION_TXEN,
+		.reg = MRMAC_CONFIG_TX_OFFSET,
+		.m_or = MRMAC_TX_EN_MASK,
+	}, { /* Enable receiver */
+		.opt = XAE_OPTION_RXEN,
+		.reg = MRMAC_CONFIG_RX_OFFSET,
+		.m_or = MRMAC_RX_EN_MASK,
+	},
+	{}
+};
 
-	if (lp->features & XAE_FEATURE_DMA_64BIT)
-		ret |= ((dma_addr_t)desc->phys_msb << 16) << 16;
+struct axienet_ethtools_stat {
+	const char *name;
+};
 
-	return ret;
-}
+static struct axienet_ethtools_stat axienet_get_ethtools_strings_stats[] = {
+	{ "tx_packets" },
+	{ "rx_packets" },
+	{ "tx_bytes" },
+	{ "rx_bytes" },
+	{ "tx_errors" },
+	{ "rx_errors" },
+};
 
 /**
  * axienet_dma_bd_release - Release buffer descriptor rings
@@ -181,46 +234,26 @@ static dma_addr_t desc_get_phys_addr(struct axienet_local *lp,
  * axienet_dma_bd_init. axienet_dma_bd_release is called when Axi Ethernet
  * driver stop api is called.
  */
-static void axienet_dma_bd_release(struct net_device *ndev)
+void axienet_dma_bd_release(struct net_device *ndev)
 {
 	int i;
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	/* If we end up here, tx_bd_v must have been DMA allocated. */
-	dma_free_coherent(lp->dev,
-			  sizeof(*lp->tx_bd_v) * lp->tx_bd_num,
-			  lp->tx_bd_v,
-			  lp->tx_bd_p);
-
-	if (!lp->rx_bd_v)
-		return;
-
-	for (i = 0; i < lp->rx_bd_num; i++) {
-		dma_addr_t phys;
-
-		/* A NULL skb means this descriptor has not been initialised
-		 * at all.
-		 */
-		if (!lp->rx_bd_v[i].skb)
-			break;
-
-		dev_kfree_skb(lp->rx_bd_v[i].skb);
-
-		/* For each descriptor, we programmed cntrl with the (non-zero)
-		 * descriptor size, after it had been successfully allocated.
-		 * So a non-zero value in there means we need to unmap it.
-		 */
-		if (lp->rx_bd_v[i].cntrl) {
-			phys = desc_get_phys_addr(lp, &lp->rx_bd_v[i]);
-			dma_unmap_single(lp->dev, phys,
-					 lp->max_frm_size, DMA_FROM_DEVICE);
-		}
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free(ndev, lp->dq[i]);
+	}
+#endif
+	for_each_rx_dma_queue(lp, i) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		if (axienet_eoe_is_channel_gro(lp, lp->dq[i]))
+			axienet_eoe_mcdma_gro_bd_free(ndev, lp->dq[i]);
+		else
+			axienet_mcdma_rx_bd_free(ndev, lp->dq[i]);
+#else
+		axienet_bd_free(ndev, lp->dq[i]);
+#endif
 	}
-
-	dma_free_coherent(lp->dev,
-			  sizeof(*lp->rx_bd_v) * lp->rx_bd_num,
-			  lp->rx_bd_v,
-			  lp->rx_bd_p);
 }
 
 /**
@@ -228,7 +261,7 @@ static void axienet_dma_bd_release(struct net_device *ndev)
  * @lp:		Pointer to the axienet_local structure
  * @coalesce_usec: Microseconds to convert into timer value
  */
-static u32 axienet_usec_to_timer(struct axienet_local *lp, u32 coalesce_usec)
+u32 axienet_usec_to_timer(struct axienet_local *lp, u32 coalesce_usec)
 {
 	u32 result;
 	u64 clk_rate = 125000000; /* arbitrary guess if no clock rate set */
@@ -247,57 +280,59 @@ static u32 axienet_usec_to_timer(struct axienet_local *lp, u32 coalesce_usec)
 
 /**
  * axienet_dma_start - Set up DMA registers and start DMA operation
- * @lp:		Pointer to the axienet_local structure
+ * @dq:		Pointer to the axienet_dma_q structure
  */
-static void axienet_dma_start(struct axienet_local *lp)
+void axienet_dma_start(struct axienet_dma_q *dq)
 {
+	struct axienet_local *lp = dq->lp;
+
 	/* Start updating the Rx channel control register */
-	lp->rx_dma_cr = (lp->coalesce_count_rx << XAXIDMA_COALESCE_SHIFT) |
+	dq->rx_dma_cr = (lp->coalesce_count_rx << XAXIDMA_COALESCE_SHIFT) |
 			XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_ERROR_MASK;
 	/* Only set interrupt delay timer if not generating an interrupt on
 	 * the first RX packet. Otherwise leave at 0 to disable delay interrupt.
 	 */
 	if (lp->coalesce_count_rx > 1)
-		lp->rx_dma_cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_rx)
+		dq->rx_dma_cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_rx)
 					<< XAXIDMA_DELAY_SHIFT) |
 				 XAXIDMA_IRQ_DELAY_MASK;
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, lp->rx_dma_cr);
+	axienet_dma_out32(dq, XAXIDMA_RX_CR_OFFSET, dq->rx_dma_cr);
 
 	/* Start updating the Tx channel control register */
-	lp->tx_dma_cr = (lp->coalesce_count_tx << XAXIDMA_COALESCE_SHIFT) |
+	dq->tx_dma_cr = (lp->coalesce_count_tx << XAXIDMA_COALESCE_SHIFT) |
 			XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_ERROR_MASK;
 	/* Only set interrupt delay timer if not generating an interrupt on
 	 * the first TX packet. Otherwise leave at 0 to disable delay interrupt.
 	 */
 	if (lp->coalesce_count_tx > 1)
-		lp->tx_dma_cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_tx)
+		dq->tx_dma_cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_tx)
 					<< XAXIDMA_DELAY_SHIFT) |
 				 XAXIDMA_IRQ_DELAY_MASK;
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, lp->tx_dma_cr);
+	axienet_dma_out32(dq, XAXIDMA_TX_CR_OFFSET, dq->tx_dma_cr);
 
 	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
 	 * halted state. This will make the Rx side ready for reception.
 	 */
-	axienet_dma_out_addr(lp, XAXIDMA_RX_CDESC_OFFSET, lp->rx_bd_p);
-	lp->rx_dma_cr |= XAXIDMA_CR_RUNSTOP_MASK;
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, lp->rx_dma_cr);
-	axienet_dma_out_addr(lp, XAXIDMA_RX_TDESC_OFFSET, lp->rx_bd_p +
-			     (sizeof(*lp->rx_bd_v) * (lp->rx_bd_num - 1)));
+	axienet_dma_bdout(dq, XAXIDMA_RX_CDESC_OFFSET, dq->rx_bd_p);
+	dq->rx_dma_cr |= XAXIDMA_CR_RUNSTOP_MASK;
+	axienet_dma_out32(dq, XAXIDMA_RX_CR_OFFSET, dq->rx_dma_cr);
+	axienet_dma_bdout(dq, XAXIDMA_RX_TDESC_OFFSET, dq->rx_bd_p +
+			  (sizeof(*dq->rx_bd_v) * (lp->rx_bd_num - 1)));
 
 	/* Write to the RS (Run-stop) bit in the Tx channel control register.
 	 * Tx channel is now ready to run. But only after we write to the
 	 * tail pointer register that the Tx channel will start transmitting.
 	 */
-	axienet_dma_out_addr(lp, XAXIDMA_TX_CDESC_OFFSET, lp->tx_bd_p);
-	lp->tx_dma_cr |= XAXIDMA_CR_RUNSTOP_MASK;
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, lp->tx_dma_cr);
+	axienet_dma_bdout(dq, XAXIDMA_TX_CDESC_OFFSET, dq->tx_bd_p);
+	dq->tx_dma_cr |= XAXIDMA_CR_RUNSTOP_MASK;
+	axienet_dma_out32(dq, XAXIDMA_TX_CR_OFFSET, dq->tx_dma_cr);
 }
 
 /**
  * axienet_dma_bd_init - Setup buffer descriptor rings for Axi DMA
  * @ndev:	Pointer to the net_device structure
  *
- * Return: 0, on success -ENOMEM, on failure
+ * Return: 0, on success -ENOMEM, on failure -EINVAL, on default return
  *
  * This function is called to initialize the Rx and Tx DMA descriptor
  * rings. This initializes the descriptors with required default values
@@ -305,69 +340,28 @@ static void axienet_dma_start(struct axienet_local *lp)
  */
 static int axienet_dma_bd_init(struct net_device *ndev)
 {
-	int i;
-	struct sk_buff *skb;
+	int i, ret = -EINVAL;
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	/* Reset the indexes which are used for accessing the BDs */
-	lp->tx_bd_ci = 0;
-	lp->tx_bd_tail = 0;
-	lp->rx_bd_ci = 0;
-
-	/* Allocate the Tx and Rx buffer descriptors. */
-	lp->tx_bd_v = dma_alloc_coherent(lp->dev,
-					 sizeof(*lp->tx_bd_v) * lp->tx_bd_num,
-					 &lp->tx_bd_p, GFP_KERNEL);
-	if (!lp->tx_bd_v)
-		return -ENOMEM;
-
-	lp->rx_bd_v = dma_alloc_coherent(lp->dev,
-					 sizeof(*lp->rx_bd_v) * lp->rx_bd_num,
-					 &lp->rx_bd_p, GFP_KERNEL);
-	if (!lp->rx_bd_v)
-		goto out;
-
-	for (i = 0; i < lp->tx_bd_num; i++) {
-		dma_addr_t addr = lp->tx_bd_p +
-				  sizeof(*lp->tx_bd_v) *
-				  ((i + 1) % lp->tx_bd_num);
-
-		lp->tx_bd_v[i].next = lower_32_bits(addr);
-		if (lp->features & XAE_FEATURE_DMA_64BIT)
-			lp->tx_bd_v[i].next_msb = upper_32_bits(addr);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	for_each_tx_dma_queue(lp, i) {
+		ret = axienet_mcdma_tx_q_init(ndev, lp->dq[i]);
+		if (ret != 0)
+			break;
 	}
-
-	for (i = 0; i < lp->rx_bd_num; i++) {
-		dma_addr_t addr;
-
-		addr = lp->rx_bd_p + sizeof(*lp->rx_bd_v) *
-			((i + 1) % lp->rx_bd_num);
-		lp->rx_bd_v[i].next = lower_32_bits(addr);
-		if (lp->features & XAE_FEATURE_DMA_64BIT)
-			lp->rx_bd_v[i].next_msb = upper_32_bits(addr);
-
-		skb = netdev_alloc_skb_ip_align(ndev, lp->max_frm_size);
-		if (!skb)
-			goto out;
-
-		lp->rx_bd_v[i].skb = skb;
-		addr = dma_map_single(lp->dev, skb->data,
-				      lp->max_frm_size, DMA_FROM_DEVICE);
-		if (dma_mapping_error(lp->dev, addr)) {
-			netdev_err(ndev, "DMA mapping error\n");
-			goto out;
+#endif
+	for_each_rx_dma_queue(lp, i) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		ret = axienet_mcdma_rx_q_init(ndev, lp->dq[i]);
+#else
+		ret = axienet_dma_q_init(ndev, lp->dq[i]);
+#endif
+		if (ret != 0) {
+			netdev_err(ndev, "%s: Failed to init DMA buf %d\n", __func__, ret);
+			break;
 		}
-		desc_set_phys_addr(lp, addr, &lp->rx_bd_v[i]);
-
-		lp->rx_bd_v[i].cntrl = lp->max_frm_size;
 	}
-
-	axienet_dma_start(lp);
-
-	return 0;
-out:
-	axienet_dma_bd_release(ndev);
-	return -ENOMEM;
+	return ret;
 }
 
 /**
@@ -378,8 +372,8 @@ static int axienet_dma_bd_init(struct net_device *ndev)
  * This function is called to initialize the MAC address of the Axi Ethernet
  * core. It writes to the UAW0 and UAW1 registers of the core.
  */
-static void axienet_set_mac_address(struct net_device *ndev,
-				    const void *address)
+void axienet_set_mac_address(struct net_device *ndev,
+			     const void *address)
 {
 	struct axienet_local *lp = netdev_priv(ndev);
 
@@ -388,6 +382,9 @@ static void axienet_set_mac_address(struct net_device *ndev,
 	if (!is_valid_ether_addr(ndev->dev_addr))
 		eth_hw_addr_random(ndev);
 
+	if (lp->axienet_config->mactype != XAXIENET_1_2p5G)
+		return;
+
 	/* Set up unicast MAC address filter set its mac address */
 	axienet_iow(lp, XAE_UAW0_OFFSET,
 		    (ndev->dev_addr[0]) |
@@ -431,12 +428,15 @@ static int netdev_set_mac_address(struct net_device *ndev, void *p)
  * means whenever the multicast table entries need to be updated this
  * function gets called.
  */
-static void axienet_set_multicast_list(struct net_device *ndev)
+void axienet_set_multicast_list(struct net_device *ndev)
 {
 	int i = 0;
 	u32 reg, af0reg, af1reg;
 	struct axienet_local *lp = netdev_priv(ndev);
 
+	if (lp->axienet_config->mactype != XAXIENET_1_2p5G ||
+	    lp->eth_hasnobuf)
+		return;
 	reg = axienet_ior(lp, XAE_FMI_OFFSET);
 	reg &= ~XAE_FMI_PM_MASK;
 	if (ndev->flags & IFF_PROMISC)
@@ -558,8 +558,350 @@ static void axienet_refresh_stats(struct work_struct *work)
 	schedule_delayed_work(&lp->stats_work, 13 * HZ);
 }
 
-static int __axienet_device_reset(struct axienet_local *lp)
+static void xxvenet_setoptions(struct net_device *ndev, u32 options)
+{
+	int reg;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct xxvenet_option *tp;
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+		tp = &mrmacenet_options[0];
+	else
+		tp = &xxvenet_options[0];
+
+	while (tp->opt) {
+		reg = ((axienet_ior(lp, tp->reg)) & ~(tp->m_or));
+		if (options & tp->opt)
+			reg |= tp->m_or;
+		axienet_iow(lp, tp->reg, reg);
+		tp++;
+	}
+
+	lp->options |= options;
+}
+
+static inline void axienet_mrmac_reset(struct axienet_local *lp)
+{
+	u32 val, reg, serdes_width, axis_cfg;
+
+	val = axienet_ior(lp, MRMAC_RESET_OFFSET);
+	val |= (MRMAC_RX_SERDES_RST_MASK | MRMAC_TX_SERDES_RST_MASK |
+		MRMAC_RX_RST_MASK | MRMAC_TX_RST_MASK);
+	axienet_iow(lp, MRMAC_RESET_OFFSET, val);
+	mdelay(DELAY_1MS);
+
+	reg = axienet_ior(lp, MRMAC_MODE_OFFSET);
+	reg &= ~MRMAC_CTL_RATE_CFG_MASK;
+
+	if (lp->max_speed == SPEED_25000) {
+		axis_cfg = (lp->mrmac_stream_dwidth == MRMAC_STREAM_DWIDTH_128 ?
+			    MRMAC_CTL_AXIS_CFG_25G_IND_128 :
+			    MRMAC_CTL_AXIS_CFG_25G_IND_64);
+		serdes_width = (lp->gt_mode_narrow ?
+				MRMAC_CTL_SERDES_WIDTH_25G_NRW :
+				MRMAC_CTL_SERDES_WIDTH_25G_WIDE);
+		reg |= MRMAC_CTL_DATA_RATE_25G;
+	} else {
+		axis_cfg = MRMAC_CTL_AXIS_CFG_10G_IND;
+		serdes_width = (lp->gt_mode_narrow ?
+				MRMAC_CTL_SERDES_WIDTH_10G_NRW :
+				MRMAC_CTL_SERDES_WIDTH_10G_WIDE);
+		reg |= MRMAC_CTL_DATA_RATE_10G;
+	}
+	reg |= (axis_cfg << MRMAC_CTL_AXIS_CFG_SHIFT);
+	reg |= (serdes_width <<	MRMAC_CTL_SERDES_WIDTH_SHIFT);
+
+	/* For tick reg */
+	reg |= MRMAC_CTL_PM_TICK_MASK;
+	axienet_iow(lp, MRMAC_MODE_OFFSET, reg);
+
+	val = axienet_ior(lp, MRMAC_RESET_OFFSET);
+	val &= ~(MRMAC_RX_SERDES_RST_MASK | MRMAC_TX_SERDES_RST_MASK |
+		MRMAC_RX_RST_MASK | MRMAC_TX_RST_MASK);
+	axienet_iow(lp, MRMAC_RESET_OFFSET, val);
+}
+
+static ulong dcmac_gt_tx_reset_status(struct axienet_local *lp)
+{
+	ulong val;
+
+	gpiod_get_array_value_cansleep(lp->gds_gt_tx_reset_done->ndescs,
+				       lp->gds_gt_tx_reset_done->desc,
+				       lp->gds_gt_tx_reset_done->info, &val);
+	return val;
+}
+
+static ulong dcmac_gt_rx_reset_status(struct axienet_local *lp)
+{
+	ulong val;
+
+	gpiod_get_array_value_cansleep(lp->gds_gt_rx_reset_done->ndescs,
+				       lp->gds_gt_rx_reset_done->desc,
+				       lp->gds_gt_rx_reset_done->info, &val);
+	return val;
+}
+
+static void dcmac_init(struct axienet_local *lp)
+{
+	u32 val, val_tx, val_rx;
+
+	val = (DCMAC_TX_ACTV_PRT_ALL_MASK | DCMAC_RX_ACTV_PRT_ALL_MASK |
+		DCMAC_RX_ERR_IND_STD_MASK | DCMAC_TX_FEC_UNIQUE_FLIP_MASK |
+		DCMAC_RX_FEC_UNIQUE_FLIP_MASK);
+	axienet_iow(lp, DCMAC_G_MODE_OFFSET, val);
+
+	val = (DCMAC_CH_RX_FCS_MASK | DCMAC_CH_RX_PREAMBLE_MASK |
+		DCMAC_RX_IGNR_INRANGE_MASK | DCMAC_RX_MAX_PKT_LEN_MASK);
+	axienet_iow(lp, DCMAC_CH_CFG_RX_OFFSET, val);
+
+	val = (DCMAC_CH_TX_FCS_MASK | DCMAC_CH_TX_IPG_MASK);
+	axienet_iow(lp, DCMAC_CH_CFG_TX_OFFSET, val);
+
+	/* Set data rate and FEC mode */
+	val_tx = 0x0;
+	val_rx = 0x0;
+
+	switch (lp->max_speed) {
+	case SPEED_100000:
+		val_tx &= DCMAC_P_SPEED_100G_MASK;
+		val_rx &= DCMAC_P_SPEED_100G_MASK;
+		/* 100G KR4 FEC operating mode */
+		val_tx |= DCMAC_CH_MD_FEC_KR4;
+		val_rx |= DCMAC_CH_MD_FEC_KR4;
+		break;
+	case SPEED_200000:
+		val_tx |= DCMAC_P_SPEED_200G_MASK;
+		val_rx |= DCMAC_P_SPEED_200G_MASK;
+		/* 200G FEC operating mode */
+		val_tx |= DCMAC_CH_MD_FEC_200G;
+		val_rx |= DCMAC_CH_MD_FEC_200G;
+		break;
+	case SPEED_400000:
+		val_tx |= DCMAC_P_SPEED_400G_MASK;
+		val_rx |= DCMAC_P_SPEED_400G_MASK;
+		/* 400G FEC operating mode */
+		val_tx |= DCMAC_CH_MD_FEC_400G;
+		val_rx |= DCMAC_CH_MD_FEC_400G;
+
+		break;
+	default:
+		break;
+	}
+	/* pm_tick triggered by internal registers for channel statistics */
+	val_tx |= DCMAC_CH_TXMD_PM_TICK_INTERNAL_MASK;
+	val_rx |= DCMAC_CH_RXMD_PM_TICK_INTERNAL_MASK;
+
+	axienet_iow(lp, DCMAC_CH_MODE_TX_OFFSET, val_tx);
+	axienet_iow(lp, DCMAC_CH_MODE_RX_OFFSET, val_rx);
+}
+
+static ulong dcmac_rx_phy_status(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	ulong val;
+	int ret;
+
+	/* Reset GT Rx datapath */
+	val = DCMAC_GT_RXDPATH_RST;
+	gpiod_set_array_value_cansleep(lp->gds_gt_rx_dpath->ndescs,
+				       lp->gds_gt_rx_dpath->desc,
+				       lp->gds_gt_rx_dpath->info, &val);
+	mdelay(DELAY_1MS);
+	val = 0;
+	gpiod_set_array_value_cansleep(lp->gds_gt_rx_dpath->ndescs,
+				       lp->gds_gt_rx_dpath->desc,
+				       lp->gds_gt_rx_dpath->info, &val);
+
+	/* Tx and Rx serdes reset */
+	gpiod_set_array_value_cansleep(lp->gds_gt_rsts->ndescs,
+				       lp->gds_gt_rsts->desc,
+				       lp->gds_gt_rsts->info, &val);
+	mdelay(DELAY_1MS);
+
+	ret = readx_poll_timeout(dcmac_gt_rx_reset_status, lp, val,
+				 val == (ulong)DCMAC_GT_RESET_DONE_MASK, 10,
+				 100 * DELAY_OF_ONE_MILLISEC);
+	if (ret) {
+		netdev_err(ndev,
+			   "GT RX reset done not achieved (Status = 0x%lx)\n",
+			   val);
+		return ret;
+	}
+
+	mdelay(DELAY_1MS);
+	/* Assert and deassert DCMAC Rx port reset */
+	axienet_iow(lp, DCMAC_P_CTRL_RX_OFFSET,
+		    DCMAC_P_CTRL_CLR_SERDES);
+	mdelay(DELAY_1MS);
+	axienet_iow(lp, DCMAC_P_CTRL_RX_OFFSET, 0);
+
+	/* Delay of 2ms is needed */
+	mdelay(2 * DELAY_1MS);
+
+	/* Clear previous status */
+	axienet_iow(lp, DCMAC_STS_RX_PHY_OFFSET, 0xFFFFFFFF);
+	mdelay(DELAY_1MS);
+
+	/* Read phy status for PCS alignment, Rx status and Block lock */
+	val = axienet_ior(lp, DCMAC_STS_RX_PHY_OFFSET);
+	return val;
+}
+
+static void dcmac_assert_reset(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 val;
+
+	val = DCMAC_G_CTRL_RESET_ALL;
+	axienet_iow(lp, DCMAC_G_CTRL_RX_OFFSET, val);
+	axienet_iow(lp, DCMAC_G_CTRL_TX_OFFSET, val);
+	val = DCMAC_P_CTRL_CLEAR_ALL;
+	axienet_iow(lp, DCMAC_P_CTRL_RX_OFFSET, val);
+	axienet_iow(lp, DCMAC_P_CTRL_TX_OFFSET, val);
+
+	/* Assert channel resets */
+	val = DCMAC_CH_CTRL_CLEAR_STATE;
+	axienet_iow(lp, DCMAC_CH_CTRL_RX_OFFSET, val);
+	axienet_iow(lp, DCMAC_CH_CTRL_TX_OFFSET, val);
+	mdelay(DELAY_1MS);
+	val = DCMAC_P_CTRL_CLEAR_ALL;
+	axienet_iow(lp, DCMAC_P_CTRL_RX_OFFSET, val);
+	axienet_iow(lp, DCMAC_P_CTRL_TX_OFFSET, val);
+}
+
+static void dcmac_release_reset(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	/* Release DCMAC global port and channel reset */
+	axienet_iow(lp, DCMAC_G_CTRL_TX_OFFSET, DCMAC_RELEASE_RESET);
+	axienet_iow(lp, DCMAC_G_CTRL_RX_OFFSET, DCMAC_RELEASE_RESET);
+	axienet_iow(lp, DCMAC_P_CTRL_TX_OFFSET, DCMAC_RELEASE_RESET);
+	axienet_iow(lp, DCMAC_P_CTRL_RX_OFFSET, DCMAC_RELEASE_RESET);
+	mdelay(DELAY_1MS);
+	axienet_iow(lp, DCMAC_CH_CTRL_TX_OFFSET, DCMAC_RELEASE_RESET);
+	axienet_iow(lp, DCMAC_CH_CTRL_RX_OFFSET, DCMAC_RELEASE_RESET);
+	mdelay(DELAY_1MS);
+}
+
+static int dcmac_gt_reset(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	ulong val_gpio;
+	u32 ret;
+
+	val_gpio = (DCMAC_GT_RESET_ALL | DCMAC_GT_TX_PRECURSOR |
+	       DCMAC_GT_TX_POSTCURSOR | DCMAC_GT_MAINCURSOR);
+	gpiod_set_array_value_cansleep(lp->gds_gt_ctrl->ndescs,
+				       lp->gds_gt_ctrl->desc,
+				       lp->gds_gt_ctrl->info, &val_gpio);
+	mdelay(DELAY_1MS);
+
+	val_gpio &= ~DCMAC_GT_RESET_ALL;
+	gpiod_set_array_value_cansleep(lp->gds_gt_ctrl->ndescs,
+				       lp->gds_gt_ctrl->desc,
+				       lp->gds_gt_ctrl->info, &val_gpio);
+
+	/* Ensure the GT TX Datapath Reset is not asserted */
+	val_gpio = 0;
+	gpiod_set_array_value_cansleep(lp->gds_gt_tx_dpath->ndescs,
+				       lp->gds_gt_tx_dpath->desc,
+				       lp->gds_gt_tx_dpath->info, &val_gpio);
+
+	mdelay(DELAY_1MS);
+
+	/* Check for GT TX RESET DONE */
+	ret = readx_poll_timeout(dcmac_gt_tx_reset_status, lp, val_gpio,
+				 val_gpio == (ulong)DCMAC_GT_RESET_DONE_MASK,
+				 10, 100 * DELAY_OF_ONE_MILLISEC);
+	if (ret) {
+		netdev_err(ndev,
+			   "GT TX Reset Done not achieved (Status = 0x%lx)\n",
+			   val_gpio);
+		return ret;
+	}
+
+	/* Check for GT RX RESET DONE */
+	ret = readx_poll_timeout(dcmac_gt_rx_reset_status, lp, val_gpio,
+				 val_gpio == (ulong)DCMAC_GT_RESET_DONE_MASK, 10,
+				 100 * DELAY_OF_ONE_MILLISEC);
+	if (ret) {
+		netdev_err(ndev,
+			   "GT RX Reset Done not achieved (Status = 0x%lx)\n",
+			   val_gpio);
+		return ret;
+	}
+
+	return ret;
+}
+
+static inline int axienet_mrmac_gt_reset(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 err, val;
+	int i;
+
+	if (mrmac_pll_rst == 0) {
+		for (i = 0; i < MRMAC_MAX_GT_LANES; i++) {
+			iowrite32(MRMAC_GT_RST_ALL_MASK, (lp->gt_ctrl +
+				  (MRMAC_GT_LANE_OFFSET * i) +
+				  MRMAC_GT_CTRL_OFFSET));
+			mdelay(DELAY_1MS);
+			iowrite32(0, (lp->gt_ctrl + (MRMAC_GT_LANE_OFFSET * i) +
+				      MRMAC_GT_CTRL_OFFSET));
+		}
+
+		/* Wait for PLL lock with timeout */
+		err = readl_poll_timeout(lp->gt_pll + MRMAC_GT_PLL_STS_OFFSET,
+					 val, (val & MRMAC_GT_PLL_DONE_MASK),
+					 10, DELAY_OF_ONE_MILLISEC * 100);
+		if (err) {
+			netdev_err(ndev, "MRMAC PLL lock not complete! Cross-check the MAC ref clock configuration\n");
+			return -ENODEV;
+		}
+		mrmac_pll_rst = 1;
+	}
+
+	if (lp->max_speed == SPEED_25000)
+		iowrite32(MRMAC_GT_25G_MASK, (lp->gt_ctrl +
+			  MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+			  MRMAC_GT_RATE_OFFSET));
+	else
+		iowrite32(MRMAC_GT_10G_MASK, (lp->gt_ctrl +
+			  MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+			  MRMAC_GT_RATE_OFFSET));
+
+	iowrite32(MRMAC_GT_RST_RX_MASK | MRMAC_GT_RST_TX_MASK,
+		  (lp->gt_ctrl + MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+		  MRMAC_GT_CTRL_OFFSET));
+	mdelay(DELAY_1MS);
+	iowrite32(0, (lp->gt_ctrl + MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+		  MRMAC_GT_CTRL_OFFSET));
+	mdelay(DELAY_1MS);
+
+	return 0;
+}
+
+static inline int xxv_gt_reset(struct net_device *ndev)
 {
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 val;
+
+	val = axienet_ior(lp, XXV_GT_RESET_OFFSET);
+	val |= XXV_GT_RESET_MASK;
+	axienet_iow(lp, XXV_GT_RESET_OFFSET, val);
+	/* Wait for 1ms for GT reset to complete as per spec */
+	mdelay(DELAY_1MS);
+	val = axienet_ior(lp, XXV_GT_RESET_OFFSET);
+	val &= ~XXV_GT_RESET_MASK;
+	axienet_iow(lp, XXV_GT_RESET_OFFSET, val);
+
+	return 0;
+}
+
+int __axienet_device_reset(struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = q->lp;
 	u32 value;
 	int ret;
 
@@ -575,24 +917,26 @@ static int __axienet_device_reset(struct axienet_local *lp)
 	 * Note that even though both TX and RX have their own reset register,
 	 * they both reset the entire DMA core, so only one needs to be used.
 	 */
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, XAXIDMA_CR_RESET_MASK);
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, XAXIDMA_CR_RESET_MASK);
 	ret = read_poll_timeout(axienet_dma_in32, value,
 				!(value & XAXIDMA_CR_RESET_MASK),
-				DELAY_OF_ONE_MILLISEC, 50000, false, lp,
+				DELAY_OF_ONE_MILLISEC, 50000, false, q,
 				XAXIDMA_TX_CR_OFFSET);
 	if (ret) {
 		dev_err(lp->dev, "%s: DMA reset timeout!\n", __func__);
 		goto out;
 	}
 
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G) {
 	/* Wait for PhyRstCmplt bit to be set, indicating the PHY reset has finished */
-	ret = read_poll_timeout(axienet_ior, value,
-				value & XAE_INT_PHYRSTCMPLT_MASK,
-				DELAY_OF_ONE_MILLISEC, 50000, false, lp,
-				XAE_IS_OFFSET);
-	if (ret) {
-		dev_err(lp->dev, "%s: timeout waiting for PhyRstCmplt\n", __func__);
-		goto out;
+		ret = read_poll_timeout(axienet_ior, value,
+					value & XAE_INT_PHYRSTCMPLT_MASK,
+					DELAY_OF_ONE_MILLISEC, 50000, false, lp,
+					XAE_IS_OFFSET);
+		if (ret) {
+			dev_err(lp->dev, "%s: timeout waiting for PhyRstCmplt\n", __func__);
+			goto out;
+		}
 	}
 
 	/* Update statistics counters with new values */
@@ -619,39 +963,40 @@ static int __axienet_device_reset(struct axienet_local *lp)
 
 /**
  * axienet_dma_stop - Stop DMA operation
- * @lp:		Pointer to the axienet_local structure
+ * @dq:		Pointer to the axienet_dma_q structure
  */
-static void axienet_dma_stop(struct axienet_local *lp)
+void axienet_dma_stop(struct axienet_dma_q *dq)
 {
 	int count;
 	u32 cr, sr;
+	struct axienet_local *lp = dq->lp;
 
-	cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
+	cr = axienet_dma_in32(dq, XAXIDMA_RX_CR_OFFSET);
 	cr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);
-	synchronize_irq(lp->rx_irq);
+	axienet_dma_out32(dq, XAXIDMA_RX_CR_OFFSET, cr);
+	synchronize_irq(dq->rx_irq);
 
-	cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
+	cr = axienet_dma_in32(dq, XAXIDMA_TX_CR_OFFSET);
 	cr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);
-	synchronize_irq(lp->tx_irq);
+	axienet_dma_out32(dq, XAXIDMA_TX_CR_OFFSET, cr);
+	synchronize_irq(dq->tx_irq);
 
 	/* Give DMAs a chance to halt gracefully */
-	sr = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
+	sr = axienet_dma_in32(dq, XAXIDMA_RX_SR_OFFSET);
 	for (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {
 		msleep(20);
-		sr = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
+		sr = axienet_dma_in32(dq, XAXIDMA_RX_SR_OFFSET);
 	}
 
-	sr = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
+	sr = axienet_dma_in32(dq, XAXIDMA_TX_SR_OFFSET);
 	for (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {
 		msleep(20);
-		sr = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
+		sr = axienet_dma_in32(dq, XAXIDMA_TX_SR_OFFSET);
 	}
 
 	/* Do a reset to ensure DMA is really stopped */
 	axienet_lock_mii(lp);
-	__axienet_device_reset(lp);
+	__axienet_device_reset(dq);
 	axienet_unlock_mii(lp);
 }
 
@@ -659,6 +1004,8 @@ static void axienet_dma_stop(struct axienet_local *lp)
  * axienet_device_reset - Reset and initialize the Axi Ethernet hardware.
  * @ndev:	Pointer to the net_device structure
  *
+ * Return: 0 on success, Negative value on errors
+ *
  * This function is called to reset and initialize the Axi Ethernet core. This
  * is typically called during initialization. It does a reset of the Axi DMA
  * Rx/Tx channels and initializes the Axi DMA BDs. Since Axi DMA reset lines
@@ -672,23 +1019,46 @@ static int axienet_device_reset(struct net_device *ndev)
 	u32 axienet_status;
 	struct axienet_local *lp = netdev_priv(ndev);
 	int ret;
+	u32 err, val;
+	u8 maj, minor;
+	struct axienet_dma_q *q;
+	u32 i;
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		/* Reset MRMAC */
+		axienet_mrmac_reset(lp);
+	}
+
+	if (lp->axienet_config->gt_reset) {
+		ret = lp->axienet_config->gt_reset(ndev);
+		if (ret)
+			return ret;
+	}
 
 	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
-	lp->options |= XAE_OPTION_VLAN;
-	lp->options &= (~XAE_OPTION_JUMBO);
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_1G_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		lp->options |= XAE_OPTION_VLAN;
+		lp->options &= (~XAE_OPTION_JUMBO);
+	}
 
 	if (ndev->mtu > XAE_MTU && ndev->mtu <= XAE_JUMBO_MTU) {
 		lp->max_frm_size = ndev->mtu + VLAN_ETH_HLEN +
 					XAE_TRL_SIZE;
 
-		if (lp->max_frm_size <= lp->rxmem)
+		if (lp->max_frm_size <= lp->rxmem &&
+		    lp->axienet_config->mactype != XAXIENET_10G_25G &&
+		    lp->axienet_config->mactype != XAXIENET_1G_10G_25G &&
+		    lp->axienet_config->mactype != XAXIENET_MRMAC)
 			lp->options |= XAE_OPTION_JUMBO;
 	}
 
 	if (!lp->use_dmaengine) {
-		ret = __axienet_device_reset(lp);
-		if (ret)
-			return ret;
+		for_each_rx_dma_queue(lp, i) {
+			q = lp->dq[i];
+			__axienet_device_reset(q);
+		}
 
 		ret = axienet_dma_bd_init(ndev);
 		if (ret) {
@@ -698,73 +1068,344 @@ static int axienet_device_reset(struct net_device *ndev)
 		}
 	}
 
-	axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
-	axienet_status &= ~XAE_RCW1_RX_MASK;
-	axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_1G_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC &&
+	    lp->axienet_config->mactype != XAXIENET_DCMAC) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_1G_10G_25G) {
+		/* Check for block lock bit got set or not
+		 * This ensures that 10G ethernet IP
+		 * is functioning normally or not.
+		 * IP version 3.2 and above, check GT status
+		 * before reading any register
+		 */
+		maj = lp->xxv_ip_version & XXV_MAJ_MASK;
+		minor = (lp->xxv_ip_version & XXV_MIN_MASK) >> 8;
+
+		if (maj == 3 ? minor >= 2 : maj > 3) {
+			err = readl_poll_timeout(lp->regs + XXV_STAT_GTWIZ_OFFSET,
+						 val, (val & XXV_GTWIZ_RESET_DONE),
+						 10, DELAY_OF_ONE_MILLISEC);
+			if (err) {
+				netdev_err(ndev, "XXV MAC GT reset not complete! Cross-check the MAC ref clock configuration\n");
+				axienet_dma_bd_release(ndev);
+				return err;
+			}
+		}
+		err = readl_poll_timeout(lp->regs + XXV_STATRX_BLKLCK_OFFSET,
+					 val, (val & XXV_RX_BLKLCK_MASK),
+					 10, DELAY_OF_ONE_MILLISEC);
+		if (err)
+			netdev_err(ndev, "XXV MAC block lock not complete! Cross-check the MAC ref clock configuration\n");
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+#endif
+	}
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+	}
+#endif
 
-	axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
-	if (axienet_status & XAE_INT_RXRJECT_MASK)
-		axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
-	axienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?
-		    XAE_INT_RECV_ERROR_MASK : 0);
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G &&
+	    !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+		/* Enable receive erros */
+		axienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?
+			    XAE_INT_RECV_ERROR_MASK : 0);
+	}
 
-	axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_1G_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		lp->options |= XAE_OPTION_FCS_STRIP;
+		lp->options |= XAE_OPTION_FCS_INSERT;
+	} else {
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	}
+	if (lp->axienet_config->setoptions)
+		lp->axienet_config->setoptions(ndev, lp->options &
+					~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
 
-	/* Sync default options with HW but leave receiver and
-	 * transmitter disabled.
-	 */
-	axienet_setoptions(ndev, lp->options &
-			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
 	axienet_set_mac_address(ndev, NULL);
 	axienet_set_multicast_list(ndev);
-	axienet_setoptions(ndev, lp->options);
+	if (lp->axienet_config->mactype == XAXIENET_DCMAC) {
+		dcmac_assert_reset(ndev);
+		dcmac_init(lp);
+		dcmac_release_reset(ndev);
+
+		/* Check for alignment */
+		ret = readx_poll_timeout(dcmac_rx_phy_status, ndev, val,
+					 (val > 0) &&
+					 (val & DCMAC_RXPHY_RX_STS_MASK) &&
+					 (val & DCMAC_RXPHY_RX_ALIGN_MASK),
+					 10, 100 * DELAY_OF_ONE_MILLISEC);
+
+		if (ret) {
+			netdev_err(ndev, "Alignment not achieved. Failed to reset DCMAC\n");
+			return -ENODEV;
+		}
+	}
+
+	if (lp->axienet_config->setoptions)
+		lp->axienet_config->setoptions(ndev, lp->options);
 
 	netif_trans_update(ndev);
 
 	return 0;
 }
 
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
 /**
- * axienet_free_tx_chain - Clean up a series of linked TX descriptors.
- * @lp:		Pointer to the axienet_local structure
- * @first_bd:	Index of first descriptor to clean up
- * @nr_bds:	Max number of descriptors to clean up
- * @force:	Whether to clean descriptors even if not complete
- * @sizep:	Pointer to a u32 filled with the total sum of all bytes
- *		in all cleaned-up descriptors. Ignored if NULL.
- * @budget:	NAPI budget (use 0 when not called from NAPI poll)
+ * axienet_tx_hwtstamp - Read tx timestamp from hw and update it to the skbuff
+ * @lp:		Pointer to axienet local structure
+ * @cur_p:	Pointer to the axi_dma/axi_mcdma current bd
  *
- * Would either be called after a successful transmit operation, or after
- * there was an error when setting up the chain.
- * Returns the number of packets handled.
+ * Return:	None.
  */
-static int axienet_free_tx_chain(struct axienet_local *lp, u32 first_bd,
-				 int nr_bds, bool force, u32 *sizep, int budget)
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct aximcdma_bd *cur_p)
+#else
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct axidma_bd *cur_p)
+#endif
 {
-	struct axidma_bd *cur_p;
-	unsigned int status;
-	int i, packets = 0;
-	dma_addr_t phys;
+	u32 sec = 0, nsec = 0, val;
+	u64 time64;
+	int err = 0;
+	u32 count, len = lp->axienet_config->tx_ptplen;
+	struct skb_shared_hwtstamps *shhwtstamps =
+		skb_hwtstamps((struct sk_buff *)cur_p->ptp_tx_skb);
+
+	val = axienet_txts_ior(lp, XAXIFIFO_TXTS_ISR);
+	if (unlikely(!(val & XAXIFIFO_TXTS_INT_RC_MASK)))
+		dev_info(lp->dev, "Did't get FIFO tx interrupt %d\n", val);
+
+	/* Ensure to read Occupany register before accessing Length register */
+	if (!axienet_txts_ior(lp, XAXIFIFO_TXTS_RFO)) {
+		netdev_err(lp->ndev, "%s: TX Timestamp FIFO is empty", __func__);
+		goto skb_exit;
+	}
 
-	for (i = 0; i < nr_bds; i++) {
-		cur_p = &lp->tx_bd_v[(first_bd + i) % lp->tx_bd_num];
-		status = cur_p->status;
+	/* If FIFO is configured in cut through Mode we will get Rx complete
+	 * interrupt even one byte is there in the fifo wait for the full packet
+	 */
+	err = readl_poll_timeout_atomic(lp->tx_ts_regs + XAXIFIFO_TXTS_RLR, val,
+					((val & XAXIFIFO_TXTS_RXFD_MASK) >=
+					len), 0, 1000000);
+	if (err) {
+		netdev_err(lp->ndev, "%s: Didn't get the full timestamp packet",
+			   __func__);
+		goto skb_exit;
+	}
 
-		/* If force is not specified, clean up only descriptors
-		 * that have been completed by the MAC.
-		 */
-		if (!force && !(status & XAXIDMA_BD_STS_COMPLETE_MASK))
-			break;
+	nsec = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	sec  = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	val = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	val = ((val & XAXIFIFO_TXTS_TAG_MASK) >> XAXIFIFO_TXTS_TAG_SHIFT);
+	dev_dbg(lp->dev, "tx_stamp:[%04x] %04x %u %9u\n",
+		cur_p->ptp_tx_ts_tag, val, sec, nsec);
+
+	if (val != cur_p->ptp_tx_ts_tag) {
+		count = axienet_txts_ior(lp, XAXIFIFO_TXTS_RFO);
+		while (count) {
+			nsec = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+			sec  = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+			val = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+			val = ((val & XAXIFIFO_TXTS_TAG_MASK) >>
+				XAXIFIFO_TXTS_TAG_SHIFT);
+
+			dev_dbg(lp->dev, "tx_stamp:[%04x] %04x %u %9u\n",
+				cur_p->ptp_tx_ts_tag, val, sec, nsec);
+			if (val == cur_p->ptp_tx_ts_tag)
+				break;
+			count = axienet_txts_ior(lp, XAXIFIFO_TXTS_RFO);
+		}
+		if (val != cur_p->ptp_tx_ts_tag) {
+			dev_info(lp->dev, "Mismatching 2-step tag. Got %x",
+				 val);
+			dev_info(lp->dev, "Expected %x\n",
+				 cur_p->ptp_tx_ts_tag);
+		}
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		val = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+
+skb_exit:
+	time64 = sec * NS_PER_SEC + nsec;
+	memset(shhwtstamps, 0, sizeof(struct skb_shared_hwtstamps));
+	shhwtstamps->hwtstamp = ns_to_ktime(time64);
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		skb_pull((struct sk_buff *)cur_p->ptp_tx_skb,
+			 AXIENET_TS_HEADER_LEN);
+
+	skb_tstamp_tx((struct sk_buff *)cur_p->ptp_tx_skb, shhwtstamps);
+	dev_kfree_skb_any((struct sk_buff *)cur_p->ptp_tx_skb);
+	cur_p->ptp_tx_skb = 0;
+}
+
+static inline bool is_ptp_os_pdelay_req(struct sk_buff *skb,
+					struct axienet_local *lp)
+{
+	u8 *msg_type;
+
+	msg_type = (u8 *)skb->data + PTP_TYPE_OFFSET;
+	return (((*msg_type & 0xF) == PTP_TYPE_PDELAY_REQ) &&
+		(lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_P2P));
+}
+
+/**
+ * axienet_rx_hwtstamp - Read rx timestamp from hw and update it to the skbuff
+ * @lp:		Pointer to axienet local structure
+ * @skb:	Pointer to the sk_buff structure
+ *
+ * Return:	None.
+ */
+static void axienet_rx_hwtstamp(struct axienet_local *lp,
+				struct sk_buff *skb)
+{
+	u32 sec = 0, nsec = 0, val;
+	u64 time64;
+	int err = 0;
+	struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
+
+	val = axienet_rxts_ior(lp, XAXIFIFO_TXTS_ISR);
+	if (unlikely(!(val & XAXIFIFO_TXTS_INT_RC_MASK))) {
+		dev_info(lp->dev, "Did't get FIFO rx interrupt %d\n", val);
+		return;
+	}
+
+	val = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RFO);
+	if (!val)
+		return;
+
+	/* If FIFO is configured in cut through Mode we will get Rx complete
+	 * interrupt even one byte is there in the fifo wait for the full packet
+	 */
+	err = readl_poll_timeout_atomic(lp->rx_ts_regs + XAXIFIFO_TXTS_RLR, val,
+					((val & XAXIFIFO_TXTS_RXFD_MASK) >= 12),
+					0, 1000000);
+	if (err) {
+		netdev_err(lp->ndev, "%s: Didn't get the full timestamp packet",
+			   __func__);
+		return;
+	}
+
+	nsec = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	sec  = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	val = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RXFD);
+
+	if (is_ptp_os_pdelay_req(skb, lp)) {
+		/* Need to save PDelay resp RX time for HW 1 step
+		 * timestamping on PDelay Response.
+		 */
+		lp->ptp_os_cf = mul_u32_u32(sec, NSEC_PER_SEC);
+		lp->ptp_os_cf += nsec;
+		lp->ptp_os_cf = (lp->ptp_os_cf << 16);
+	}
+
+	if (lp->tstamp_config.rx_filter == HWTSTAMP_FILTER_ALL) {
+		time64 = sec * NS_PER_SEC + nsec;
+		shhwtstamps->hwtstamp = ns_to_ktime(time64);
+	}
+}
+#endif
 
+/**
+ * axienet_free_tx_chain - Clean up a series of linked TX descriptors.
+ * @q:		Pointer to the axienet_dma_q structure
+ * @first_bd:	Index of first descriptor to clean up
+ * @nr_bds:	Max number of descriptors to clean up
+ * @force:	Whether to clean descriptors even if not complete
+ * @sizep:	Pointer to a u32 filled with the total sum of all bytes
+ *		in all cleaned-up descriptors. Ignored if NULL.
+ * @budget:	NAPI budget (use 0 when not called from NAPI poll)
+ *
+ * Would either be called after a successful transmit operation, or after
+ * there was an error when setting up the chain.
+ * Returns the number of packets handled.
+ */
+
+static int axienet_free_tx_chain(struct axienet_dma_q *q, u32 first_bd,
+				 int nr_bds, bool force, u32 *sizep, int budget)
+{
+	struct axienet_local *lp = q->lp;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
+	struct axidma_bd *cur_p;
+#endif
+	unsigned int status;
+	int i, packets = 0;
+	dma_addr_t phys;
+
+	for (i = 0; i < nr_bds; i++) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p = &q->txq_bd_v[(first_bd + i) % lp->tx_bd_num];
+		status = cur_p->sband_stats;
+#else
+		cur_p = &q->tx_bd_v[(first_bd + i) % lp->tx_bd_num];
+		status = cur_p->status;
+#endif
+
+		/* If force is not specified, clean up only descriptors
+		 * that have been completed by the MAC.
+		 */
+		if (!force && !(status & XAXIDMA_BD_STS_COMPLETE_MASK))
+			break;
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+		if (cur_p->ptp_tx_skb)
+			axienet_tx_hwtstamp(lp, cur_p);
+#endif
 		/* Ensure we see complete descriptor update */
 		dma_rmb();
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		phys = mcdma_desc_get_phys_addr(lp, cur_p);
+#else
 		phys = desc_get_phys_addr(lp, cur_p);
-		dma_unmap_single(lp->dev, phys,
-				 (cur_p->cntrl & XAXIDMA_BD_CTRL_LENGTH_MASK),
-				 DMA_TO_DEVICE);
+#endif
+
+		if (cur_p->tx_desc_mapping == DESC_DMA_MAP_PAGE)
+			dma_unmap_page(lp->dev, phys,
+				       cur_p->cntrl &
+				       XAXIDMA_BD_CTRL_LENGTH_MASK,
+				       DMA_TO_DEVICE);
+		else
+			dma_unmap_single(lp->dev, phys,
+					 cur_p->cntrl &
+					 XAXIDMA_BD_CTRL_LENGTH_MASK,
+					 DMA_TO_DEVICE);
 
-		if (cur_p->skb && (status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
-			napi_consume_skb(cur_p->skb, budget);
+		if (cur_p->tx_skb && (status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
+			napi_consume_skb((struct sk_buff *)cur_p->tx_skb, budget);
 			packets++;
 		}
 
@@ -772,20 +1413,23 @@ static int axienet_free_tx_chain(struct axienet_local *lp, u32 first_bd,
 		cur_p->app1 = 0;
 		cur_p->app2 = 0;
 		cur_p->app4 = 0;
-		cur_p->skb = NULL;
+		cur_p->tx_skb = 0;
+
 		/* ensure our transmit path and device don't prematurely see status cleared */
 		wmb();
 		cur_p->cntrl = 0;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p->sband_stats = 0;
+#endif
 		cur_p->status = 0;
-
 		if (sizep)
 			*sizep += status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
 	}
 
 	if (!force) {
-		lp->tx_bd_ci += i;
-		if (lp->tx_bd_ci >= lp->tx_bd_num)
-			lp->tx_bd_ci %= lp->tx_bd_num;
+		q->tx_bd_ci += i;
+		if (q->tx_bd_ci >= lp->tx_bd_num)
+			q->tx_bd_ci %= lp->tx_bd_num;
 	}
 
 	return packets;
@@ -793,7 +1437,7 @@ static int axienet_free_tx_chain(struct axienet_local *lp, u32 first_bd,
 
 /**
  * axienet_check_tx_bd_space - Checks if a BD/group of BDs are currently busy
- * @lp:		Pointer to the axienet_local structure
+ * @q:		Pointer to DMA queue structure
  * @num_frag:	The number of BDs to check for
  *
  * Return: 0, on success
@@ -804,17 +1448,29 @@ static int axienet_free_tx_chain(struct axienet_local *lp, u32 first_bd,
  * transmission. If the BD or any of the BDs are not free the function
  * returns a busy status.
  */
-static inline int axienet_check_tx_bd_space(struct axienet_local *lp,
+static inline int axienet_check_tx_bd_space(struct axienet_dma_q *q,
 					    int num_frag)
 {
+	struct axienet_local *lp = q->lp;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+
+	if (CIRC_SPACE(q->tx_bd_tail, q->tx_bd_ci, lp->tx_bd_num) < (num_frag + 1))
+		return NETDEV_TX_BUSY;
+
+	cur_p = &q->txq_bd_v[(q->tx_bd_tail + num_frag) % lp->tx_bd_num];
+	if (cur_p->sband_stats & XMCDMA_BD_STS_ALL_MASK)
+		return NETDEV_TX_BUSY;
+#else
 	struct axidma_bd *cur_p;
 
-	/* Ensure we see all descriptor updates from device or TX polling */
-	rmb();
-	cur_p = &lp->tx_bd_v[(READ_ONCE(lp->tx_bd_tail) + num_frag) %
-			     lp->tx_bd_num];
-	if (cur_p->cntrl)
+	if (CIRC_SPACE(q->tx_bd_tail, q->tx_bd_ci, lp->tx_bd_num) < (num_frag + 1))
+		return NETDEV_TX_BUSY;
+
+	cur_p = &q->tx_bd_v[(q->tx_bd_tail + num_frag) % lp->tx_bd_num];
+	if (cur_p->status & XAXIDMA_BD_STS_ALL_MASK)
 		return NETDEV_TX_BUSY;
+#endif
 	return 0;
 }
 
@@ -954,26 +1610,30 @@ axienet_start_xmit_dmaengine(struct sk_buff *skb, struct net_device *ndev)
  * buffer. It finally invokes "netif_wake_queue" to restart transmission if
  * required.
  */
-static int axienet_tx_poll(struct napi_struct *napi, int budget)
+int axienet_tx_poll(struct napi_struct *napi, int budget)
 {
-	struct axienet_local *lp = container_of(napi, struct axienet_local, napi_tx);
+	struct axienet_dma_q *q = container_of(napi, struct axienet_dma_q, napi_tx);
+	struct axienet_local *lp = q->lp;
 	struct net_device *ndev = lp->ndev;
 	u32 size = 0;
 	int packets;
 
-	packets = axienet_free_tx_chain(lp, lp->tx_bd_ci, lp->tx_bd_num, false,
+	packets = axienet_free_tx_chain(q, q->tx_bd_ci, lp->tx_bd_num, false,
 					&size, budget);
-
 	if (packets) {
 		u64_stats_update_begin(&lp->tx_stat_sync);
 		u64_stats_add(&lp->tx_packets, packets);
 		u64_stats_add(&lp->tx_bytes, size);
 		u64_stats_update_end(&lp->tx_stat_sync);
+		ndev->stats.tx_packets += packets;
+		ndev->stats.tx_bytes += size;
+		q->txq_packets += packets;
+		q->txq_bytes += size;
 
 		/* Matches barrier in axienet_start_xmit */
 		smp_mb();
 
-		if (!axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1))
+		if (!axienet_check_tx_bd_space(q, MAX_SKB_FRAGS + 1))
 			netif_wake_queue(ndev);
 	}
 
@@ -982,55 +1642,312 @@ static int axienet_tx_poll(struct napi_struct *napi, int budget)
 		 * cause an immediate interrupt if any TX packets are
 		 * already pending.
 		 */
-		axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, lp->tx_dma_cr);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		u32 cr;
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		cr |= (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+#else
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, q->tx_dma_cr);
+
+#endif
 	}
 	return packets;
 }
 
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
 /**
- * axienet_start_xmit - Starts the transmission.
- * @skb:	sk_buff pointer that contains data to be Txed.
- * @ndev:	Pointer to net_device structure.
- *
- * Return: NETDEV_TX_OK, on success
- *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ * axienet_create_tsheader - Create timestamp header for tx
+ * @q:		Pointer to DMA queue structure
+ * @buf:	Pointer to the buf to copy timestamp header
+ * @msg_type:	PTP message type
  *
- * This function is invoked from upper layers to initiate transmission. The
- * function uses the next available free BDs and populates their fields to
- * start the transmission. Additionally if checksum offloading is supported,
- * it populates AXI Stream Control fields with appropriate values.
+ * Return: 0, on success
+ *	    NETDEV_TX_BUSY, if timestamp FIFO has no vacancy
  */
-static netdev_tx_t
-axienet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
+static int axienet_create_tsheader(u8 *buf, u8 msg_type,
+				   struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = q->lp;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
+	struct axidma_bd *cur_p;
+#endif
+	u64 val;
+	u32 tmp[MRMAC_TS_HEADER_WORDS];
+	int i;
+	unsigned long flags;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+	cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
+
+	if ((msg_type & 0xF) == TX_TS_OP_NOOP) {
+		buf[0] = TX_TS_OP_NOOP;
+	} else if ((msg_type & 0xF) == TX_TS_OP_ONESTEP) {
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			/* For Sync Packet */
+			if ((msg_type & 0xF0) == MSG_TYPE_SYNC_FLAG) {
+				buf[0] = TX_TS_OP_ONESTEP | TX_TS_CSUM_UPDATE_MRMAC;
+				buf[2] = cur_p->ptp_tx_ts_tag & 0xFF;
+				buf[3] = (cur_p->ptp_tx_ts_tag >> 8) & 0xFF;
+				buf[4] = TX_PTP_CF_OFFSET;
+				buf[6] = TX_PTP_CSUM_OFFSET;
+			}
+			/* For PDelay Response packet */
+			if ((msg_type & 0xF0) == MSG_TYPE_PDELAY_RESP_FLAG) {
+				buf[0] = TX_TS_OP_ONESTEP | TX_TS_CSUM_UPDATE_MRMAC |
+					TX_TS_PDELAY_UPDATE_MRMAC;
+				buf[2] = cur_p->ptp_tx_ts_tag & 0xFF;
+				buf[3] = (cur_p->ptp_tx_ts_tag >> 8) & 0xFF;
+				buf[4] = TX_PTP_CF_OFFSET;
+				buf[6] = TX_PTP_CSUM_OFFSET;
+				/* Prev saved TS */
+				memcpy(&buf[8], &lp->ptp_os_cf, 8);
+			}
+		} else {
+			/* Legacy */
+			buf[0] = TX_TS_OP_ONESTEP;
+			buf[1] = TX_TS_CSUM_UPDATE;
+			buf[4] = TX_PTP_TS_OFFSET;
+			buf[6] = TX_PTP_CSUM_OFFSET;
+		}
+	} else {
+		buf[0] = TX_TS_OP_TWOSTEP;
+		buf[2] = cur_p->ptp_tx_ts_tag & 0xFF;
+		buf[3] = (cur_p->ptp_tx_ts_tag >> 8) & 0xFF;
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G) {
+		memcpy(&val, buf, AXIENET_TS_HEADER_LEN);
+		swab64s(&val);
+		memcpy(buf, &val, AXIENET_TS_HEADER_LEN);
+	} else if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		   lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		memcpy(&tmp[0], buf, lp->axienet_config->ts_header_len);
+		/* Check for Transmit Data FIFO Vacancy */
+		spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+		if (!axienet_txts_ior(lp, XAXIFIFO_TXTS_TDFV)) {
+			spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+		for (i = 0; i < lp->axienet_config->ts_header_len / 4; i++)
+			axienet_txts_iow(lp, XAXIFIFO_TXTS_TXFD, tmp[i]);
+
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_TLR, lp->axienet_config->ts_header_len);
+		spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+	}
+
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+static inline u8 ptp_os(struct sk_buff *skb, struct axienet_local *lp)
+{
+	u8 *msg_type;
+	int packet_flags = 0;
+
+	/* Identify and return packets requiring PTP one step TS */
+	msg_type = (u8 *)skb->data + PTP_TYPE_OFFSET;
+	if ((*msg_type & 0xF) == PTP_TYPE_SYNC)
+		packet_flags = MSG_TYPE_SYNC_FLAG;
+	else if (((*msg_type & 0xF) == PTP_TYPE_PDELAY_RESP) &&
+		 (lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_P2P))
+		packet_flags = MSG_TYPE_PDELAY_RESP_FLAG;
+
+	return packet_flags;
+}
+
+static int axienet_skb_tstsmp(struct sk_buff **__skb, struct axienet_dma_q *q,
+			      struct net_device *ndev)
+{
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
+	struct axidma_bd *cur_p;
+#endif
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct sk_buff *old_skb = *__skb;
+	struct sk_buff *skb = *__skb;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+	cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
+
+	if (((lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_SYNC ||
+	      lp->tstamp_config.tx_type == HWTSTAMP_TX_ON) ||
+	     lp->eth_hasptp) && lp->axienet_config->mactype !=
+	    XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		u8 *tmp;
+		struct sk_buff *new_skb;
+
+		if (skb_headroom(old_skb) < AXIENET_TS_HEADER_LEN) {
+			new_skb =
+			skb_realloc_headroom(old_skb,
+					     AXIENET_TS_HEADER_LEN);
+			if (!new_skb) {
+				dev_err(&ndev->dev, "failed to allocate new socket buffer\n");
+				dev_kfree_skb_any(old_skb);
+				return NETDEV_TX_BUSY;
+			}
+
+			/*  Transfer the ownership to the
+			 *  new socket buffer if required
+			 */
+			if (old_skb->sk)
+				skb_set_owner_w(new_skb, old_skb->sk);
+			dev_kfree_skb_any(old_skb);
+			*__skb = new_skb;
+			skb = new_skb;
+		}
+
+		tmp = skb_push(skb, AXIENET_TS_HEADER_LEN);
+		memset(tmp, 0, AXIENET_TS_HEADER_LEN);
+		cur_p->ptp_tx_ts_tag++;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {
+			if (lp->tstamp_config.tx_type ==
+				HWTSTAMP_TX_ONESTEP_SYNC) {
+				axienet_create_tsheader(tmp,
+							TX_TS_OP_ONESTEP
+							, q);
+			} else {
+				axienet_create_tsheader(tmp,
+							TX_TS_OP_TWOSTEP, q);
+				skb_shinfo(skb)->tx_flags |=
+						SKBTX_IN_PROGRESS;
+				cur_p->ptp_tx_skb = skb_get(skb);
+			}
+		}
+	} else if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+		   (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		   lp->axienet_config->mactype == XAXIENET_MRMAC)) {
+		cur_p->ptp_tx_ts_tag = get_random_u32_below(XAXIFIFO_TXTS_TAG_MAX) + 1;
+			dev_dbg(lp->dev, "tx_tag:[%04x]\n",
+				cur_p->ptp_tx_ts_tag);
+			if (lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_SYNC ||
+			    lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_P2P) {
+				u8 packet_flags = ptp_os(skb, lp);
+
+				/* Pass one step flag with packet type (sync/pdelay resp)
+				 * to command FIFO helper only when one step TS is required.
+				 * Pass the default two step flag for other PTP events.
+				 */
+				if (!packet_flags)
+					packet_flags = TX_TS_OP_TWOSTEP;
+				else
+					packet_flags |= TX_TS_OP_ONESTEP;
+
+				if (axienet_create_tsheader(lp->tx_ptpheader,
+							    packet_flags,
+							    q))
+					return NETDEV_TX_BUSY;
+
+				/* skb TS passing is required for non one step TS packets */
+				if (packet_flags == TX_TS_OP_TWOSTEP) {
+					skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+					cur_p->ptp_tx_skb = skb_get(skb);
+				}
+			} else {
+				if (axienet_create_tsheader(lp->tx_ptpheader,
+							    TX_TS_OP_TWOSTEP,
+							    q))
+					return NETDEV_TX_BUSY;
+				skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+				cur_p->ptp_tx_skb = skb_get(skb);
+			}
+	} else if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		   lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		dev_dbg(lp->dev, "tx_tag:NOOP\n");
+			if (axienet_create_tsheader(lp->tx_ptpheader,
+						    TX_TS_OP_NOOP, q))
+				return NETDEV_TX_BUSY;
+	}
+
+	return NETDEV_TX_OK;
+}
+#endif
+
+static int axienet_queue_xmit(struct sk_buff *skb,
+			      struct net_device *ndev, u16 map)
 {
 	u32 ii;
 	u32 num_frag;
 	u32 csum_start_off;
 	u32 csum_index_off;
-	skb_frag_t *frag;
 	dma_addr_t tail_p, phys;
-	u32 orig_tail_ptr, new_tail_ptr;
 	struct axienet_local *lp = netdev_priv(ndev);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
 	struct axidma_bd *cur_p;
-
-	orig_tail_ptr = lp->tx_bd_tail;
-	new_tail_ptr = orig_tail_ptr;
-
+#endif
+	unsigned long flags;
+	struct axienet_dma_q *q;
+
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_1G_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_MRMAC ||
+	    lp->axienet_config->mactype == XAXIENET_DCMAC) {
+		/* Need to manually pad the small frames in case of XXV MAC
+		 * because the pad field is not added by the IP. We must present
+		 * a packet that meets the minimum length to the IP core.
+		 * When the IP core is configured to calculate and add the FCS
+		 * to the packet the minimum packet length is 60 bytes.
+		 */
+		if (eth_skb_pad(skb)) {
+			ndev->stats.tx_dropped++;
+			ndev->stats.tx_errors++;
+			return NETDEV_TX_OK;
+		}
+	}
 	num_frag = skb_shinfo(skb)->nr_frags;
-	cur_p = &lp->tx_bd_v[orig_tail_ptr];
 
-	if (axienet_check_tx_bd_space(lp, num_frag + 1)) {
-		/* Should not happen as last start_xmit call should have
-		 * checked for sufficient space and queue should only be
-		 * woken when sufficient space is available.
-		 */
+	q = lp->dq[map];
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+	cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
+	spin_lock_irqsave(&q->tx_lock, flags);
+	if (axienet_check_tx_bd_space(q, num_frag)) {
+		if (netif_queue_stopped(ndev)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
 		netif_stop_queue(ndev);
-		if (net_ratelimit())
-			netdev_warn(ndev, "TX ring unexpectedly full\n");
+
+		/* Matches barrier in axienet_free_tx_chain */
+		smp_mb();
+
+		/* Space might have just been freed - check again */
+		if (axienet_check_tx_bd_space(q, num_frag)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
+		netif_wake_queue(ndev);
+	}
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (axienet_skb_tstsmp(&skb, q, ndev)) {
+		spin_unlock_irqrestore(&q->tx_lock, flags);
 		return NETDEV_TX_BUSY;
 	}
+#endif
 
-	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+	if (skb->ip_summed == CHECKSUM_PARTIAL && !lp->eth_hasnobuf &&
+	    lp->axienet_config->mactype == XAXIENET_1_2p5G &&
+	    !(lp->eoe_connected)) {
 		if (lp->features & XAE_FEATURE_FULL_TX_CSUM) {
 			/* Tx Full Checksum Offload Enabled */
 			cur_p->app0 |= 2;
@@ -1041,68 +1958,140 @@ axienet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
 			cur_p->app0 |= 1;
 			cur_p->app1 = (csum_start_off << 16) | csum_index_off;
 		}
-	} else if (skb->ip_summed == CHECKSUM_UNNECESSARY) {
+	} else if (skb->ip_summed == CHECKSUM_UNNECESSARY &&
+		   !lp->eth_hasnobuf &&
+		   (lp->axienet_config->mactype == XAXIENET_1_2p5G) &&
+		   !(lp->eoe_connected)) {
 		cur_p->app0 |= 2; /* Tx Full Checksum Offload Enabled */
 	}
 
-	phys = dma_map_single(lp->dev, skb->data,
-			      skb_headlen(skb), DMA_TO_DEVICE);
-	if (unlikely(dma_mapping_error(lp->dev, phys))) {
-		if (net_ratelimit())
-			netdev_err(ndev, "TX DMA mapping error\n");
-		ndev->stats.tx_dropped++;
-		dev_kfree_skb_any(skb);
-		return NETDEV_TX_OK;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p->cntrl = (skb_headlen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK);
+#else
+	cur_p->cntrl = (skb_headlen(skb) | XAXIDMA_BD_CTRL_TXSOF_MASK);
+#endif
+
+	if (!q->eth_hasdre &&
+	    (((uintptr_t)skb->data & 0x3) || num_frag > 0)) {
+		skb_copy_and_csum_dev(skb, q->tx_buf[q->tx_bd_tail]);
+
+		phys = q->tx_bufs_dma + (q->tx_buf[q->tx_bd_tail] - q->tx_bufs);
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p->cntrl = skb_pagelen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK;
+		mcdma_desc_set_phys_addr(lp, phys, cur_p);
+#else
+		desc_set_phys_addr(lp, phys, cur_p);
+		cur_p->cntrl = skb_pagelen(skb) | XAXIDMA_BD_CTRL_TXSOF_MASK;
+#endif
+		goto out;
+	} else {
+		phys = dma_map_single(ndev->dev.parent, skb->data,
+				      skb_headlen(skb), DMA_TO_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, phys))) {
+			phys = 0;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			mcdma_desc_set_phys_addr(lp, phys, cur_p);
+#else
+			desc_set_phys_addr(lp, phys, cur_p);
+#endif
+
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			dev_err(&ndev->dev, "TX buffer map failed\n");
+			return NETDEV_TX_BUSY;
+		}
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			mcdma_desc_set_phys_addr(lp, phys, cur_p);
+#else
+			desc_set_phys_addr(lp, phys, cur_p);
+#endif
 	}
-	desc_set_phys_addr(lp, phys, cur_p);
-	cur_p->cntrl = skb_headlen(skb) | XAXIDMA_BD_CTRL_TXSOF_MASK;
+
+	cur_p->tx_desc_mapping = DESC_DMA_MAP_SINGLE;
+
+	/* Update the APP fields for UDP segmentation by HW, if it is enabled.
+	 * This automatically enables the checksum calculation by HW.
+	 * If UDP segmentation by HW is not supported, then update the APP fields for
+	 * checksum calculation by HW, if it is enabled.
+	 */
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	if (ndev->hw_features & NETIF_F_GSO_UDP_L4)
+		axienet_eoe_config_hwgso(ndev, skb, cur_p);
+	else if (ndev->hw_features & NETIF_F_IP_CSUM)
+		axienet_eoe_config_hwcso(ndev, cur_p);
+#endif
 
 	for (ii = 0; ii < num_frag; ii++) {
-		if (++new_tail_ptr >= lp->tx_bd_num)
-			new_tail_ptr = 0;
-		cur_p = &lp->tx_bd_v[new_tail_ptr];
+		u32 len;
+		skb_frag_t *frag;
+
+		if (++q->tx_bd_tail >= lp->tx_bd_num)
+			q->tx_bd_tail = 0;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+		cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
 		frag = &skb_shinfo(skb)->frags[ii];
-		phys = dma_map_single(lp->dev,
-				      skb_frag_address(frag),
-				      skb_frag_size(frag),
-				      DMA_TO_DEVICE);
-		if (unlikely(dma_mapping_error(lp->dev, phys))) {
-			if (net_ratelimit())
-				netdev_err(ndev, "TX DMA mapping error\n");
-			ndev->stats.tx_dropped++;
-			axienet_free_tx_chain(lp, orig_tail_ptr, ii + 1,
-					      true, NULL, 0);
-			dev_kfree_skb_any(skb);
-			return NETDEV_TX_OK;
-		}
-		desc_set_phys_addr(lp, phys, cur_p);
-		cur_p->cntrl = skb_frag_size(frag);
+		len = skb_frag_size(frag);
+		phys = skb_frag_dma_map(ndev->dev.parent, frag, 0, len,
+					DMA_TO_DEVICE);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			mcdma_desc_set_phys_addr(lp, phys, cur_p);
+#else
+			desc_set_phys_addr(lp, phys, cur_p);
+#endif
+		cur_p->cntrl = len;
+		cur_p->tx_desc_mapping = DESC_DMA_MAP_PAGE;
 	}
 
+out:
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p->cntrl |= XMCDMA_BD_CTRL_TXEOF_MASK;
+	tail_p = q->tx_bd_p + sizeof(*q->txq_bd_v) * q->tx_bd_tail;
+#else
 	cur_p->cntrl |= XAXIDMA_BD_CTRL_TXEOF_MASK;
-	cur_p->skb = skb;
+	tail_p = q->tx_bd_p + sizeof(*q->tx_bd_v) * q->tx_bd_tail;
+#endif
+	cur_p->tx_skb = skb;
 
-	tail_p = lp->tx_bd_p + sizeof(*lp->tx_bd_v) * new_tail_ptr;
-	if (++new_tail_ptr >= lp->tx_bd_num)
-		new_tail_ptr = 0;
-	WRITE_ONCE(lp->tx_bd_tail, new_tail_ptr);
+	tail_p = q->tx_bd_p + sizeof(*q->tx_bd_v) * q->tx_bd_tail;
+	/* Ensure BD write before starting transfer */
+	wmb();
 
 	/* Start the transfer */
-	axienet_dma_out_addr(lp, XAXIDMA_TX_TDESC_OFFSET, tail_p);
-
-	/* Stop queue if next transmit may not have space */
-	if (axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1)) {
-		netif_stop_queue(ndev);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id),
+			  tail_p);
+#else
+	axienet_dma_bdout(q, XAXIDMA_TX_TDESC_OFFSET, tail_p);
+#endif
+	if (++q->tx_bd_tail >= lp->tx_bd_num)
+		q->tx_bd_tail = 0;
 
-		/* Matches barrier in axienet_tx_poll */
-		smp_mb();
+	spin_unlock_irqrestore(&q->tx_lock, flags);
+	return NETDEV_TX_OK;
+}
 
-		/* Space might have just been freed - check again */
-		if (!axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1))
-			netif_wake_queue(ndev);
-	}
+/**
+ * axienet_start_xmit - Starts the transmission.
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ *
+ * This function is invoked from upper layers to initiate transmission. The
+ * function uses the next available free BDs and populates their fields to
+ * start the transmission. Additionally if checksum offloading is supported,
+ * it populates AXI Stream Control fields with appropriate values.
+ */
+static int axienet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	u16 map = skb_get_queue_mapping(skb); /* Single dma queue default*/
 
-	return NETDEV_TX_OK;
+	return axienet_queue_xmit(skb, ndev, map);
 }
 
 /**
@@ -1142,33 +2131,64 @@ static void axienet_dma_rx_cb(void *data, const struct dmaengine_result *result)
 }
 
 /**
- * axienet_rx_poll - Triggered by RX ISR to complete the BD processing.
- * @napi:	Pointer to NAPI structure.
- * @budget:	Max number of RX packets to process.
+ * axienet_recv - Is called from Axi DMA Rx Isr to complete the received
+ *		  BD processing.
+ * @ndev:	Pointer to net_device structure.
+ * @budget:	NAPI budget
+ * @q:		Pointer to axienet DMA queue structure
  *
- * Return: Number of RX packets processed.
+ * This function is invoked from the Axi DMA Rx isr(poll) to process the Rx BDs
+ * It does minimal processing and invokes "netif_receive_skb" to complete
+ * further processing.
+ * Return: Number of BD's processed.
  */
-static int axienet_rx_poll(struct napi_struct *napi, int budget)
+
+static int axienet_recv(struct net_device *ndev, int budget,
+			struct axienet_dma_q *q)
 {
 	u32 length;
 	u32 csumstatus;
 	u32 size = 0;
-	int packets = 0;
-	dma_addr_t tail_p = 0;
-	struct axidma_bd *cur_p;
+	u32 packets = 0;
+	dma_addr_t phys, tail_p = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
 	struct sk_buff *skb, *new_skb;
-	struct axienet_local *lp = container_of(napi, struct axienet_local, napi_rx);
+	struct napi_struct *napi = &q->napi_rx;
 
-	cur_p = &lp->rx_bd_v[lp->rx_bd_ci];
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
+	struct axidma_bd *cur_p;
+#endif
+	unsigned int numbdfree = 0;
 
-	while (packets < budget && (cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
-		dma_addr_t phys;
+	/* Get relevat BD status value */
+	rmb();
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+#else
+	cur_p = &q->rx_bd_v[q->rx_bd_ci];
+#endif
 
-		/* Ensure we see complete descriptor update */
-		dma_rmb();
+	while ((numbdfree < budget) &&
+	       (cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
+		new_skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!new_skb)
+			break;
 
-		skb = cur_p->skb;
-		cur_p->skb = NULL;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		tail_p = q->rx_bd_p + sizeof(*q->rxq_bd_v) * q->rx_bd_ci;
+		phys = mcdma_desc_get_phys_addr(lp, cur_p);
+#else
+		tail_p = q->rx_bd_p + sizeof(*q->rx_bd_v) * q->rx_bd_ci;
+		phys = desc_get_phys_addr(lp, cur_p);
+#endif
+
+		dma_unmap_single(ndev->dev.parent, phys,
+				 lp->max_frm_size,
+				 DMA_FROM_DEVICE);
+
+		skb = (struct sk_buff *)(cur_p->sw_id_offset);
 
 		/* skb could be NULL if a previous pass already received the
 		 * packet for this slot in the ring, but failed to refill it
@@ -1176,171 +2196,207 @@ static int axienet_rx_poll(struct napi_struct *napi, int budget)
 		 * receive it again.
 		 */
 		if (likely(skb)) {
-			length = cur_p->app4 & 0x0000FFFF;
-
-			phys = desc_get_phys_addr(lp, cur_p);
-			dma_unmap_single(lp->dev, phys, lp->max_frm_size,
-					 DMA_FROM_DEVICE);
+			if (lp->eth_hasnobuf ||
+			    lp->axienet_config->mactype != XAXIENET_1_2p5G)
+				length = cur_p->status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+			else
+				length = cur_p->app4 & 0x0000FFFF;
 
 			skb_put(skb, length);
-			skb->protocol = eth_type_trans(skb, lp->ndev);
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+			if ((lp->tstamp_config.rx_filter == HWTSTAMP_FILTER_ALL ||
+			     lp->eth_hasptp) &&
+			    lp->axienet_config->mactype != XAXIENET_10G_25G &&
+			    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+				u32 sec, nsec;
+				u64 time64;
+				struct skb_shared_hwtstamps *shhwtstamps;
+
+				if (lp->axienet_config->mactype == XAXIENET_1_2p5G) {
+					/* The first 8 bytes will be the timestamp */
+					memcpy(&sec, &skb->data[0], 4);
+					memcpy(&nsec, &skb->data[4], 4);
+
+					sec = cpu_to_be32(sec);
+					nsec = cpu_to_be32(nsec);
+				} else {
+					/* The first 8 bytes will be the timestamp */
+					memcpy(&nsec, &skb->data[0], 4);
+					memcpy(&sec, &skb->data[4], 4);
+				}
+
+				/* Remove these 8 bytes from the buffer */
+				skb_pull(skb, 8);
+				time64 = sec * NS_PER_SEC + nsec;
+				shhwtstamps = skb_hwtstamps(skb);
+				shhwtstamps->hwtstamp = ns_to_ktime(time64);
+			} else if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+				   lp->axienet_config->mactype == XAXIENET_MRMAC) {
+				axienet_rx_hwtstamp(lp, skb);
+			}
+#endif
+			skb->protocol = eth_type_trans(skb, ndev);
 			/*skb_checksum_none_assert(skb);*/
 			skb->ip_summed = CHECKSUM_NONE;
 
 			/* if we're doing Rx csum offload, set it up */
-			if (lp->features & XAE_FEATURE_FULL_RX_CSUM) {
+			if (lp->features & XAE_FEATURE_FULL_RX_CSUM &&
+			    lp->axienet_config->mactype == XAXIENET_1_2p5G &&
+			    !lp->eth_hasnobuf) {
 				csumstatus = (cur_p->app2 &
 					      XAE_FULL_CSUM_STATUS_MASK) >> 3;
 				if (csumstatus == XAE_IP_TCP_CSUM_VALIDATED ||
 				    csumstatus == XAE_IP_UDP_CSUM_VALIDATED) {
 					skb->ip_summed = CHECKSUM_UNNECESSARY;
 				}
-			} else if (lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) {
+			} else if ((lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) != 0 &&
+				   skb->protocol == htons(ETH_P_IP) &&
+				   skb->len > 64 && !lp->eth_hasnobuf &&
+				   lp->axienet_config->mactype == XAXIENET_1_2p5G) {
 				skb->csum = be32_to_cpu(cur_p->app3 & 0xFFFF);
 				skb->ip_summed = CHECKSUM_COMPLETE;
 			}
 
-			napi_gro_receive(napi, skb);
+		napi_gro_receive(napi, skb);
 
 			size += length;
 			packets++;
 		}
 
-		new_skb = napi_alloc_skb(napi, lp->max_frm_size);
-		if (!new_skb)
-			break;
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
 
-		phys = dma_map_single(lp->dev, new_skb->data,
+		phys = dma_map_single(ndev->dev.parent, new_skb->data,
 				      lp->max_frm_size,
 				      DMA_FROM_DEVICE);
-		if (unlikely(dma_mapping_error(lp->dev, phys))) {
-			if (net_ratelimit())
-				netdev_err(lp->ndev, "RX DMA mapping error\n");
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			mcdma_desc_set_phys_addr(lp, phys, cur_p);
+#else
+			desc_set_phys_addr(lp, phys, cur_p);
+#endif
+		if (unlikely(dma_mapping_error(ndev->dev.parent, phys))) {
+			phys = 0;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			mcdma_desc_set_phys_addr(lp, phys, cur_p);
+#else
+			desc_set_phys_addr(lp, phys, cur_p);
+#endif
 			dev_kfree_skb(new_skb);
+			dev_err(lp->dev, "RX buffer map failed\n");
 			break;
 		}
-		desc_set_phys_addr(lp, phys, cur_p);
-
 		cur_p->cntrl = lp->max_frm_size;
 		cur_p->status = 0;
-		cur_p->skb = new_skb;
+		cur_p->sw_id_offset = new_skb;
 
-		/* Only update tail_p to mark this slot as usable after it has
-		 * been successfully refilled.
-		 */
-		tail_p = lp->rx_bd_p + sizeof(*lp->rx_bd_v) * lp->rx_bd_ci;
+		if (++q->rx_bd_ci >= lp->rx_bd_num)
+			q->rx_bd_ci = 0;
 
-		if (++lp->rx_bd_ci >= lp->rx_bd_num)
-			lp->rx_bd_ci = 0;
-		cur_p = &lp->rx_bd_v[lp->rx_bd_ci];
+		/* Get relevat BD status value */
+		rmb();
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+#else
+		cur_p = &q->rx_bd_v[q->rx_bd_ci];
+#endif
+		numbdfree++;
 	}
 
 	u64_stats_update_begin(&lp->rx_stat_sync);
 	u64_stats_add(&lp->rx_packets, packets);
 	u64_stats_add(&lp->rx_bytes, size);
 	u64_stats_update_end(&lp->rx_stat_sync);
+	ndev->stats.rx_packets += packets;
+	ndev->stats.rx_bytes += size;
+	q->rxq_packets += packets;
+	q->rxq_bytes += size;
+
+	if (tail_p) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+				  q->rx_offset, tail_p);
+#else
+		axienet_dma_bdout(q, XAXIDMA_RX_TDESC_OFFSET, tail_p);
+#endif
+	}
 
-	if (tail_p)
-		axienet_dma_out_addr(lp, XAXIDMA_RX_TDESC_OFFSET, tail_p);
-
-	if (packets < budget && napi_complete_done(napi, packets)) {
-		/* Re-enable RX completion interrupts. This should
-		 * cause an immediate interrupt if any RX packets are
-		 * already pending.
-		 */
-		axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, lp->rx_dma_cr);
-	}
-	return packets;
-}
+	return numbdfree;
+}
 
 /**
- * axienet_tx_irq - Tx Done Isr.
- * @irq:	irq number
- * @_ndev:	net_device pointer
+ * xaxienet_rx_poll - Poll routine for rx packets (NAPI)
+ * @napi:	napi structure pointer
+ * @quota:	Max number of rx packets to be processed.
  *
- * Return: IRQ_HANDLED if device generated a TX interrupt, IRQ_NONE otherwise.
+ * This is the poll routine for rx part.
+ * It will process the packets maximux quota value.
  *
- * This is the Axi DMA Tx done Isr. It invokes NAPI polling to complete the
- * TX BD processing.
+ * Return: number of packets received
  */
-static irqreturn_t axienet_tx_irq(int irq, void *_ndev)
+int xaxienet_rx_poll(struct napi_struct *napi, int quota)
 {
-	unsigned int status;
-	struct net_device *ndev = _ndev;
+	struct net_device *ndev = napi->dev;
 	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = container_of(napi, struct axienet_dma_q, napi_rx);
+	int work_done = 0;
+	unsigned int status, cr;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	while ((status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) &&
+	       (work_done < quota)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+		if (status & XMCDMA_IRQ_ERR_MASK) {
+			dev_err(lp->dev, "Rx error 0x%x\n\r", status);
+			break;
+		}
 
-	status = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
-
-	if (!(status & XAXIDMA_IRQ_ALL_MASK))
-		return IRQ_NONE;
-
-	axienet_dma_out32(lp, XAXIDMA_TX_SR_OFFSET, status);
-
-	if (unlikely(status & XAXIDMA_IRQ_ERROR_MASK)) {
-		netdev_err(ndev, "DMA Tx error 0x%x\n", status);
-		netdev_err(ndev, "Current BD is at: 0x%x%08x\n",
-			   (lp->tx_bd_v[lp->tx_bd_ci]).phys_msb,
-			   (lp->tx_bd_v[lp->tx_bd_ci]).phys);
-		schedule_work(&lp->dma_err_task);
-	} else {
-		/* Disable further TX completion interrupts and schedule
-		 * NAPI to handle the completions.
-		 */
-		u32 cr = lp->tx_dma_cr;
+		if (axienet_eoe_is_channel_gro(lp, q))
+			work_done += axienet_eoe_recv_gro(lp->ndev, quota - work_done, q);
+		else
+			work_done += axienet_recv(lp->ndev, quota - work_done, q);
 
-		cr &= ~(XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);
-		if (napi_schedule_prep(&lp->napi_tx)) {
-			axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);
-			__napi_schedule(&lp->napi_tx);
+		status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+					  q->rx_offset);
+	}
+#else
+
+	status = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
+	while ((status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) &&
+	       (work_done < quota)) {
+		axienet_dma_out32(q, XAXIDMA_RX_SR_OFFSET, status);
+		if (status & XAXIDMA_IRQ_ERROR_MASK) {
+			dev_err(lp->dev, "Rx error 0x%x\n\r", status);
+			break;
 		}
+		work_done += axienet_recv(lp->ndev, quota - work_done, q);
+		status = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
 	}
 
-	return IRQ_HANDLED;
-}
-
-/**
- * axienet_rx_irq - Rx Isr.
- * @irq:	irq number
- * @_ndev:	net_device pointer
- *
- * Return: IRQ_HANDLED if device generated a RX interrupt, IRQ_NONE otherwise.
- *
- * This is the Axi DMA Rx Isr. It invokes NAPI polling to complete the RX BD
- * processing.
- */
-static irqreturn_t axienet_rx_irq(int irq, void *_ndev)
-{
-	unsigned int status;
-	struct net_device *ndev = _ndev;
-	struct axienet_local *lp = netdev_priv(ndev);
-
-	status = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
-
-	if (!(status & XAXIDMA_IRQ_ALL_MASK))
-		return IRQ_NONE;
-
-	axienet_dma_out32(lp, XAXIDMA_RX_SR_OFFSET, status);
-
-	if (unlikely(status & XAXIDMA_IRQ_ERROR_MASK)) {
-		netdev_err(ndev, "DMA Rx error 0x%x\n", status);
-		netdev_err(ndev, "Current BD is at: 0x%x%08x\n",
-			   (lp->rx_bd_v[lp->rx_bd_ci]).phys_msb,
-			   (lp->rx_bd_v[lp->rx_bd_ci]).phys);
-		schedule_work(&lp->dma_err_task);
-	} else {
-		/* Disable further RX completion interrupts and schedule
-		 * NAPI receive.
-		 */
-		u32 cr = lp->rx_dma_cr;
-
-		cr &= ~(XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);
-		if (napi_schedule_prep(&lp->napi_rx)) {
-			axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);
-			__napi_schedule(&lp->napi_rx);
-		}
+#endif
+	if (work_done < quota) {
+		napi_complete(napi);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		/* Enable the interrupts again */
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      XMCDMA_RX_OFFSET);
+		cr |= (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  XMCDMA_RX_OFFSET, cr);
+#else
+		/* Enable the interrupts again */
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		cr |= (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+#endif
 	}
 
-	return IRQ_HANDLED;
+	return work_done;
 }
 
 /**
@@ -1372,7 +2428,55 @@ static irqreturn_t axienet_eth_irq(int irq, void *_ndev)
 	return IRQ_HANDLED;
 }
 
-static void axienet_dma_err_handler(struct work_struct *work);
+static void
+axienet_config_autoneg_link_training(struct axienet_local *lp, unsigned int speed_config)
+{
+	struct net_device *ndev = lp->ndev;
+	unsigned int ret, val;
+
+	spin_lock(&lp->switch_lock);
+
+	axienet_iow(lp, XXVS_LT_CTL_OFFSET, 0);
+	axienet_iow(lp, XXVS_LT_TRAINED_OFFSET, 0);
+	axienet_iow(lp, XXVS_LT_COEF_OFFSET, 0);
+	axienet_iow(lp, XXVS_AN_ABILITY_OFFSET, speed_config);
+	if (speed_config != XXVS_SPEED_1G) {
+		axienet_iow(lp, XXVS_AN_CTL1_OFFSET,
+			    (XXVS_AN_ENABLE_MASK | XXVS_AN_NONCE_SEED));
+		axienet_iow(lp, XXVS_LT_CTL_OFFSET, XXVS_LT_ENABLE_MASK);
+		axienet_iow(lp, XXVS_LT_TRAINED_OFFSET, XXVS_LT_TRAINED_MASK);
+		axienet_iow(lp, XXVS_LT_SEED_OFFSET, XXVS_LT_SEED);
+		axienet_iow(lp, XXVS_LT_COEF_OFFSET,
+			    XXVS_LT_COEF_P1 << XXVS_LT_COEF_P1_SHIFT |
+			    XXVS_LT_COEF_STATE0 << XXVS_LT_COEF_STATE0_SHIFT |
+			    XXVS_LT_COEF_M1 << XXVS_LT_COEF_M1_SHIFT);
+
+	} else {
+		axienet_iow(lp, XXVS_AN_CTL1_OFFSET,
+			    (XXVS_AN_ENABLE_MASK | XXVS_AN_NONCE_SEED1));
+	}
+	axienet_iow(lp, XXVS_RESET_OFFSET, XXVS_RX_SERDES_RESET);
+
+	spin_unlock(&lp->switch_lock);
+
+	ret = readl_poll_timeout(lp->regs + XXVS_AN_STATUS_OFFSET,
+				 val, (val & XXVS_AN_COMPLETE_MASK),
+				 100, DELAY_OF_ONE_MILLISEC * 15000);
+
+	if (speed_config == XXVS_SPEED_1G) {
+		if (ret) {
+			netdev_err(ndev, "Autoneg failed");
+			return;
+		}
+	}
+	if (!ret) {
+		ret = readl_poll_timeout(lp->regs + XXVS_LT_STATUS_OFFSET,
+					 val, (val & XXVS_LT_DETECT_MASK),
+					 100, DELAY_OF_ONE_MILLISEC * 15000);
+		if (ret)
+			netdev_err(ndev, "Link Training failed\n");
+	}
+}
 
 /**
  * axienet_rx_submit_desc - Submit the rx descriptors to dmaengine.
@@ -1455,7 +2559,7 @@ static int axienet_init_dmaengine(struct net_device *ndev)
 		ret = PTR_ERR(lp->rx_chan);
 		dev_err(lp->dev, "No Ethernet DMA (RX) channel found\n");
 		goto err_dma_release_tx;
-	}
+}
 
 	lp->tx_ring_tail = 0;
 	lp->tx_ring_head = 0;
@@ -1525,28 +2629,68 @@ static int axienet_init_dmaengine(struct net_device *ndev)
  */
 static int axienet_init_legacy_dma(struct net_device *ndev)
 {
-	int ret;
+	int ret, i;
 	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
 
-	/* Enable worker thread for Axi DMA error handling */
 	lp->stopping = false;
-	INIT_WORK(&lp->dma_err_task, axienet_dma_err_handler);
+	/* Enable tasklets for Axi DMA error handling */
+	for_each_rx_dma_queue(lp, i) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		tasklet_init(&lp->dma_err_tasklet[i],
+			     axienet_mcdma_err_handler,
+			     (unsigned long)lp->dq[i]);
+#else
+		tasklet_init(&lp->dma_err_tasklet[i],
+			     axienet_dma_err_handler,
+			     (unsigned long)lp->dq[i]);
+#endif
 
-	napi_enable(&lp->napi_rx);
-	napi_enable(&lp->napi_tx);
+		/* Enable NAPI scheduling before enabling Axi DMA Rx IRQ, or you
+		 * might run into a race condition; the RX ISR disables IRQ processing
+		 * before scheduling the NAPI function to complete the processing.
+		 * If NAPI scheduling is (still) disabled at that time, no more RX IRQs
+		 * will be processed as only the NAPI function re-enables them!
+		 */
+		napi_enable(&lp->dq[i]->napi_rx);
+		napi_enable(&lp->dq[i]->napi_tx);
+	}
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		/* Enable interrupts for Axi MCDMA Tx */
+		ret = request_irq(q->tx_irq, axienet_mcdma_tx_irq,
+				  IRQF_SHARED, ndev->name, ndev);
+		if (ret)
+			goto err_tx_irq;
+#else
+		/* Enable interrupts for Axi DMA Tx */
+		ret = request_irq(q->tx_irq, axienet_tx_irq,
+				  0, ndev->name, ndev);
+		if (ret)
+			goto err_tx_irq;
+#endif
+		}
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		/* Enable interrupts for Axi MCDMA Rx */
+		ret = request_irq(q->rx_irq, axienet_mcdma_rx_irq,
+				  IRQF_SHARED, ndev->name, ndev);
+		if (ret)
+			goto err_rx_irq;
+#else
+		/* Enable interrupts for Axi DMA Rx */
+		ret = request_irq(q->rx_irq, axienet_rx_irq,
+				  0, ndev->name, ndev);
+		if (ret)
+			goto err_rx_irq;
+#endif
+	}
 
-	/* Enable interrupts for Axi DMA Tx */
-	ret = request_irq(lp->tx_irq, axienet_tx_irq, IRQF_SHARED,
-			  ndev->name, ndev);
-	if (ret)
-		goto err_tx_irq;
-	/* Enable interrupts for Axi DMA Rx */
-	ret = request_irq(lp->rx_irq, axienet_rx_irq, IRQF_SHARED,
-			  ndev->name, ndev);
-	if (ret)
-		goto err_rx_irq;
 	/* Enable interrupts for Axi Ethernet core (if defined) */
-	if (lp->eth_irq > 0) {
+	if (!lp->eth_hasnobuf && lp->axienet_config->mactype == XAXIENET_1_2p5G) {
 		ret = request_irq(lp->eth_irq, axienet_eth_irq, IRQF_SHARED,
 				  ndev->name, ndev);
 		if (ret)
@@ -1556,17 +2700,52 @@ static int axienet_init_legacy_dma(struct net_device *ndev)
 	return 0;
 
 err_eth_irq:
-	free_irq(lp->rx_irq, ndev);
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		free_irq(q->rx_irq, ndev);
+	}
+	i = lp->num_tx_queues;
 err_rx_irq:
-	free_irq(lp->tx_irq, ndev);
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		free_irq(q->tx_irq, ndev);
+	}
 err_tx_irq:
-	napi_disable(&lp->napi_tx);
-	napi_disable(&lp->napi_rx);
-	cancel_work_sync(&lp->dma_err_task);
+	for_each_tx_dma_queue(lp, i) {
+		napi_disable(&lp->dq[i]->napi_tx);
+		napi_disable(&lp->dq[i]->napi_rx);
+		tasklet_kill(&lp->dma_err_tasklet[i]);
+	}
+
 	dev_err(lp->dev, "request_irq() failed\n");
 	return ret;
 }
 
+static void axienet_eoe_set_gro_address(struct axienet_local *lp)
+{
+	struct in_ifaddr *ifa = NULL;
+	struct in_device *in_dev;
+	struct axienet_dma_q *q;
+	int i;
+
+	in_dev = __in_dev_get_rcu(lp->ndev);
+	if (in_dev)
+		ifa = rcu_dereference((in_dev)->ifa_list);
+
+	if (!ifa) {
+		netdev_dbg(lp->ndev, "IP address not assigned\n");
+		return;
+	}
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		if (axienet_eoe_is_channel_gro(lp, q))
+			axienet_eoe_iow(lp,
+					XEOE_UDP_GRO_DST_IP_OFFSET(q->chan_id),
+					ntohl(ifa->ifa_address));
+	}
+}
+
 /**
  * axienet_open - Driver open routine.
  * @ndev:	Pointer to net_device structure
@@ -1582,7 +2761,8 @@ static int axienet_init_legacy_dma(struct net_device *ndev)
  */
 static int axienet_open(struct net_device *ndev)
 {
-	int ret;
+	int ret = 0;
+	u32 reg;
 	struct axienet_local *lp = netdev_priv(ndev);
 
 	/* When we do an Axi Ethernet reset, it resets the complete core
@@ -1593,16 +2773,144 @@ static int axienet_open(struct net_device *ndev)
 	ret = axienet_device_reset(ndev);
 	axienet_unlock_mii(lp);
 
-	ret = phylink_of_phy_connect(lp->phylink, lp->dev->of_node, 0);
-	if (ret) {
-		dev_err(lp->dev, "phylink_of_phy_connect() failed: %d\n", ret);
+	if (ret < 0) {
+		dev_err(lp->dev, "axienet_device_reset failed\n");
 		return ret;
 	}
 
-	phylink_start(lp->phylink);
+	if (lp->phylink) {
+		ret = phylink_of_phy_connect(lp->phylink, lp->dev->of_node, 0);
+		if (ret) {
+			dev_err(lp->dev, "phylink_of_phy_connect() failed: %d\n", ret);
+			return ret;
+		}
+
+		phylink_start(lp->phylink);
+	}
+
+	if (lp->features & XAE_FEATURE_STATS) {
+		/* Start the statistics refresh work */
+		schedule_delayed_work(&lp->stats_work, 0);
+	}
+
+	if (lp->phy_mode == PHY_INTERFACE_MODE_USXGMII) {
+		netdev_dbg(ndev, "RX reg: 0x%x\n",
+			   axienet_ior(lp, XXV_RCW1_OFFSET));
+		/* USXGMII setup at selected speed */
+		reg = axienet_ior(lp, XXV_USXGMII_AN_OFFSET);
+		reg &= ~USXGMII_RATE_MASK;
+		netdev_dbg(ndev, "usxgmii_rate %d\n", lp->usxgmii_rate);
+		switch (lp->usxgmii_rate) {
+		case SPEED_1000:
+			reg |= USXGMII_RATE_1G;
+			break;
+		case SPEED_2500:
+			reg |= USXGMII_RATE_2G5;
+			break;
+		case SPEED_10:
+			reg |= USXGMII_RATE_10M;
+			break;
+		case SPEED_100:
+			reg |= USXGMII_RATE_100M;
+			break;
+		case SPEED_5000:
+			reg |= USXGMII_RATE_5G;
+			break;
+		case SPEED_10000:
+			reg |= USXGMII_RATE_10G;
+			break;
+		default:
+			reg |= USXGMII_RATE_1G;
+		}
+		reg |= USXGMII_FD;
+		reg |= (USXGMII_EN | USXGMII_LINK_STS);
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET, reg);
+		reg |= USXGMII_AN_EN;
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET, reg);
+		/* AN Restart bit should be reset, set and then reset as per
+		 * spec with a 1 ms delay for a raising edge trigger
+		 */
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET,
+			    reg & ~USXGMII_AN_RESTART);
+		mdelay(1);
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET,
+			    reg | USXGMII_AN_RESTART);
+		mdelay(1);
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET,
+			    reg & ~USXGMII_AN_RESTART);
+
+		/* Check block lock bit to make sure RX path is ok with
+		 * USXGMII initialization.
+		 */
+		ret = readl_poll_timeout(lp->regs + XXV_STATRX_BLKLCK_OFFSET,
+					 reg, (reg & XXV_RX_BLKLCK_MASK),
+					 100, DELAY_OF_ONE_MILLISEC);
+		if (ret) {
+			netdev_err(ndev, "%s: USXGMII Block lock bit not set",
+				   __func__);
+			ret = -ENODEV;
+			goto err_phy;
+		}
+
+		ret = readl_poll_timeout(lp->regs + XXV_USXGMII_AN_STS_OFFSET,
+					 reg, (reg & USXGMII_AN_STS_COMP_MASK),
+					 1000000, DELAY_OF_ONE_MILLISEC);
+		if (ret) {
+			netdev_err(ndev, "%s: USXGMII AN not complete",
+				   __func__);
+			ret = -ENODEV;
+			goto err_phy;
+		}
+
+		netdev_info(ndev, "USXGMII setup at %d\n", lp->usxgmii_rate);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		u32 val;
+
+		/* Reset MRMAC */
+		axienet_mrmac_reset(lp);
+
+		mdelay(DELAY_1MS * 100);
+		/* Check for block lock bit to be set. This ensures that
+		 * MRMAC ethernet IP is functioning normally.
+		 */
+		axienet_iow(lp, MRMAC_TX_STS_OFFSET, MRMAC_STS_ALL_MASK);
+		axienet_iow(lp, MRMAC_RX_STS_OFFSET, MRMAC_STS_ALL_MASK);
+		ret = readx_poll_timeout(axienet_get_mrmac_blocklock, lp, val,
+					 (val & MRMAC_RX_BLKLCK_MASK), 10, DELAY_OF_ONE_MILLISEC);
+		if (ret)
+			netdev_err(ndev, "MRMAC block lock not complete! Cross-check the MAC ref clock configuration\n");
+
+		ret = readx_poll_timeout(axienet_get_mrmac_rx_status, lp, val,
+					 (val & MRMAC_RX_STATUS_MASK), 10, DELAY_OF_ONE_MILLISEC);
+		if (ret) {
+			netdev_err(ndev, "MRMAC Link is down!\n");
+			ret = -ENODEV;
+			goto err_phy;
+		}
+
+		axienet_iow(lp, MRMAC_STATRX_VALID_CTRL_OFFSET, MRMAC_STS_ALL_MASK);
+		val = axienet_ior(lp, MRMAC_STATRX_VALID_CTRL_OFFSET);
+
+		if (!(val & MRMAC_RX_VALID_MASK)) {
+			netdev_err(ndev, "MRMAC Link is down! No recent RX Valid Control Code\n");
+			ret = -ENODEV;
+			goto err_phy;
+		}
+		netdev_info(ndev, "MRMAC setup at %d\n", lp->max_speed);
+		axienet_iow(lp, MRMAC_TICK_OFFSET, MRMAC_TICK_TRIGGER);
+	}
 
-	/* Start the statistics refresh work */
-	schedule_delayed_work(&lp->stats_work, 0);
+	/* If Runtime speed switching supported */
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G &&
+	    (axienet_ior(lp, XXV_STAT_CORE_SPEED_OFFSET) &
+	     XXV_STAT_CORE_SPEED_RTSW_MASK)) {
+		axienet_iow(lp, XXVS_AN_ABILITY_OFFSET,
+			    XXV_AN_10G_ABILITY_MASK | XXV_AN_25G_ABILITY_MASK);
+		axienet_iow(lp, XXVS_AN_CTL1_OFFSET,
+			    (XXVS_AN_ENABLE_MASK | XXVS_AN_NONCE_SEED));
+	}
 
 	if (lp->use_dmaengine) {
 		/* Enable interrupts for Axi Ethernet core (if defined) */
@@ -1622,6 +2930,10 @@ static int axienet_open(struct net_device *ndev)
 			goto err_phy;
 	}
 
+	if (lp->eoe_features & RX_HW_UDP_GRO)
+		axienet_eoe_set_gro_address(lp);
+
+	netif_tx_start_all_queues(ndev);
 	return 0;
 
 err_free_eth_irq:
@@ -1629,8 +2941,10 @@ static int axienet_open(struct net_device *ndev)
 		free_irq(lp->eth_irq, ndev);
 err_phy:
 	cancel_delayed_work_sync(&lp->stats_work);
-	phylink_stop(lp->phylink);
-	phylink_disconnect_phy(lp->phylink);
+	if (lp->phylink) {
+		phylink_stop(lp->phylink);
+		phylink_disconnect_phy(lp->phylink);
+	}
 	return ret;
 }
 
@@ -1649,27 +2963,31 @@ static int axienet_stop(struct net_device *ndev)
 	struct axienet_local *lp = netdev_priv(ndev);
 	int i;
 
-	if (!lp->use_dmaengine) {
+	if (!lp->use_dmaengine)
 		WRITE_ONCE(lp->stopping, true);
-		flush_work(&lp->dma_err_task);
-
-		napi_disable(&lp->napi_tx);
-		napi_disable(&lp->napi_rx);
-	}
 
 	cancel_delayed_work_sync(&lp->stats_work);
 
-	phylink_stop(lp->phylink);
-	phylink_disconnect_phy(lp->phylink);
+	if (lp->phylink) {
+		phylink_stop(lp->phylink);
+		phylink_disconnect_phy(lp->phylink);
+	}
 
-	axienet_setoptions(ndev, lp->options &
-			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	if (lp->axienet_config->setoptions)
+		lp->axienet_config->setoptions(ndev, lp->options &
+				~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
 
 	if (!lp->use_dmaengine) {
-		axienet_dma_stop(lp);
-		cancel_work_sync(&lp->dma_err_task);
-		free_irq(lp->tx_irq, ndev);
-		free_irq(lp->rx_irq, ndev);
+		for_each_tx_dma_queue(lp, i)  {
+			axienet_dma_stop(lp->dq[i]);
+			netif_stop_queue(ndev);
+			napi_disable(&lp->dq[i]->napi_tx);
+			napi_disable(&lp->dq[i]->napi_rx);
+			tasklet_kill(&lp->dma_err_tasklet[i]);
+			free_irq(lp->dq[i]->tx_irq, ndev);
+			free_irq(lp->dq[i]->rx_irq, ndev);
+}
+
 		axienet_dma_bd_release(ndev);
 	} else {
 		dmaengine_terminate_sync(lp->tx_chan);
@@ -1687,11 +3005,24 @@ static int axienet_stop(struct net_device *ndev)
 		dma_release_channel(lp->rx_chan);
 		dma_release_channel(lp->tx_chan);
 	}
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G)
+		axienet_iow(lp, XAE_IE_OFFSET, 0);
 
-	axienet_iow(lp, XAE_IE_OFFSET, 0);
-
-	if (lp->eth_irq > 0)
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G && !lp->eth_hasnobuf)
 		free_irq(lp->eth_irq, ndev);
+
+	/* Delete the GRO Filter Rules when Reset is done */
+	if (lp->eoe_features & RX_HW_UDP_GRO && lp->rx_fs_list.count > 0) {
+		struct ethtool_rx_fs_item *item, *tmp;
+
+		list_for_each_entry_safe(item, tmp, &lp->rx_fs_list.list, list) {
+			lp->assigned_rx_port[item->fs.location] = 0;
+			list_del(&item->list);
+			lp->rx_fs_list.count--;
+			kfree(item);
+		}
+	}
+
 	return 0;
 }
 
@@ -1722,6 +3053,18 @@ static int axienet_change_mtu(struct net_device *ndev, int new_mtu)
 	return 0;
 }
 
+static netdev_features_t axienet_features_check(struct sk_buff *skb,
+						struct net_device *dev,
+						netdev_features_t features)
+{
+	struct axienet_local *lp = netdev_priv(dev);
+
+	if (lp->eoe_connected && (ip_hdr(skb)->version != 4))
+		features &= ~(NETIF_F_CSUM_MASK | NETIF_F_GSO_MASK);
+
+	return features;
+}
+
 #ifdef CONFIG_NET_POLL_CONTROLLER
 /**
  * axienet_poll_controller - Axi Ethernet poll mechanism.
@@ -1733,16 +3076,158 @@ static int axienet_change_mtu(struct net_device *ndev, int new_mtu)
 static void axienet_poll_controller(struct net_device *ndev)
 {
 	struct axienet_local *lp = netdev_priv(ndev);
+	int i;
+
+	for_each_tx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->rx_irq);
+
+	for_each_rx_dma_queue(lp, i) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_mcdma_rx_irq(lp->dq[i]->rx_irq, ndev);
+		axienet_mcdma_tx_irq(lp->dq[i]->tx_irq, ndev);
+#else
+		axienet_rx_irq(lp->dq[i]->rx_irq, ndev);
+		axienet_tx_irq(lp->dq[i]->tx_irq, ndev);
+#endif
+	}
+	for_each_tx_dma_queue(lp, i)
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_mcdma_tx_irq(lp->dq[i]->tx_irq, ndev);
+#else
+		axienet_tx_irq(lp->dq[i]->tx_irq, ndev);
+#endif
+	for_each_tx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->rx_irq);
+}
+#endif
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+/**
+ *  axienet_set_timestamp_mode - sets up the hardware for the requested mode
+ *  @lp: Pointer to axienet local structure
+ *  @config: the hwtstamp configuration requested
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_timestamp_mode(struct axienet_local *lp,
+				      struct hwtstamp_config *config)
+{
+	u32 regval;
+
+	/* reserved for future extensions */
+	if (config->flags)
+		return -EINVAL;
+
+	/* Read the current value in the MAC TX CTRL register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_TC_OFFSET);
+
+	switch (config->tx_type) {
+	case HWTSTAMP_TX_OFF:
+		regval &= ~XAE_TC_INBAND1588_MASK;
+		break;
+	case HWTSTAMP_TX_ON:
+		config->tx_type = HWTSTAMP_TX_ON;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, 0x0);
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+		config->tx_type = HWTSTAMP_TX_ONESTEP_SYNC;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		break;
+	case HWTSTAMP_TX_ONESTEP_P2P:
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			config->tx_type = HWTSTAMP_TX_ONESTEP_P2P;
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		} else {
+			return -ERANGE;
+		}
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_TC_OFFSET, regval);
+
+	/* Read the current value in the MAC RX RCW1 register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_RCW1_OFFSET);
+
+	/* On RX always timestamp everything */
+	switch (config->rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		regval &= ~XAE_RCW1_INBAND1588_MASK;
+		break;
+	default:
+		config->rx_filter = HWTSTAMP_FILTER_ALL;
+		regval |= XAE_RCW1_INBAND1588_MASK;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_RCW1_OFFSET, regval);
+
+	return 0;
+}
+
+/**
+ * axienet_set_ts_config - user entry point for timestamp mode
+ * @lp: Pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Set hardware to the requested more. If unsupported return an error
+ * with no changes. Otherwise, store the mode for future reference
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config config;
+	int err;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	err = axienet_set_timestamp_mode(lp, &config);
+	if (err)
+		return err;
+
+	/* save these settings for future reference */
+	memcpy(&lp->tstamp_config, &config, sizeof(lp->tstamp_config));
+
+	return copy_to_user(ifr->ifr_data, &config,
+			    sizeof(config)) ? -EFAULT : 0;
+}
+
+/**
+ * axienet_get_ts_config - return the current timestamp configuration
+ * to the user
+ * @lp: pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_get_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config *config = &lp->tstamp_config;
 
-	disable_irq(lp->tx_irq);
-	disable_irq(lp->rx_irq);
-	axienet_rx_irq(lp->tx_irq, ndev);
-	axienet_tx_irq(lp->rx_irq, ndev);
-	enable_irq(lp->tx_irq);
-	enable_irq(lp->rx_irq);
+	return copy_to_user(ifr->ifr_data, config,
+			    sizeof(*config)) ? -EFAULT : 0;
 }
 #endif
 
+/* Ioctl MII Interface */
 static int axienet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
 {
 	struct axienet_local *lp = netdev_priv(dev);
@@ -1750,7 +3235,22 @@ static int axienet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
 	if (!netif_running(dev))
 		return -EINVAL;
 
-	return phylink_mii_ioctl(lp->phylink, rq, cmd);
+	switch (cmd) {
+	case SIOCGMIIPHY:
+	case SIOCGMIIREG:
+	case SIOCSMIIREG:
+		if (!lp->phylink)
+			return -EOPNOTSUPP;
+		return phylink_mii_ioctl(lp->phylink, rq, cmd);
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	case SIOCSHWTSTAMP:
+		return axienet_set_ts_config(lp, rq);
+	case SIOCGHWTSTAMP:
+		return axienet_get_ts_config(lp, rq);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
 }
 
 static void
@@ -1761,6 +3261,14 @@ axienet_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *stats)
 
 	netdev_stats_to_stats64(stats, &dev->stats);
 
+	if (lp->axienet_config->mactype != XAXIENET_1_2p5G) {
+		stats->rx_packets = dev->stats.rx_packets;
+		stats->rx_bytes = dev->stats.rx_bytes;
+		stats->tx_packets = dev->stats.tx_packets;
+		stats->tx_bytes = dev->stats.tx_bytes;
+		return;
+	}
+
 	do {
 		start = u64_stats_fetch_begin(&lp->rx_stat_sync);
 		stats->rx_packets = u64_stats_read(&lp->rx_packets);
@@ -1813,9 +3321,11 @@ static const struct net_device_ops axienet_netdev_ops = {
 	.ndo_validate_addr = eth_validate_addr,
 	.ndo_eth_ioctl = axienet_ioctl,
 	.ndo_set_rx_mode = axienet_set_multicast_list,
+	.ndo_do_ioctl = axienet_ioctl,
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller = axienet_poll_controller,
 #endif
+	.ndo_features_check = axienet_features_check,
 };
 
 static const struct net_device_ops axienet_netdev_dmaengine_ops = {
@@ -1910,14 +3420,15 @@ static void axienet_ethtools_get_regs(struct net_device *ndev,
 	data[30] = axienet_ior(lp, XAE_AF0_OFFSET);
 	data[31] = axienet_ior(lp, XAE_AF1_OFFSET);
 	if (!lp->use_dmaengine) {
-		data[32] = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-		data[33] = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
-		data[34] = axienet_dma_in32(lp, XAXIDMA_TX_CDESC_OFFSET);
-		data[35] = axienet_dma_in32(lp, XAXIDMA_TX_TDESC_OFFSET);
-		data[36] = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-		data[37] = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
-		data[38] = axienet_dma_in32(lp, XAXIDMA_RX_CDESC_OFFSET);
-		data[39] = axienet_dma_in32(lp, XAXIDMA_RX_TDESC_OFFSET);
+		/* Support only single DMA queue */
+		data[32] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CR_OFFSET);
+		data[33] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_SR_OFFSET);
+		data[34] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CDESC_OFFSET);
+		data[35] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_TDESC_OFFSET);
+		data[36] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CR_OFFSET);
+		data[37] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_SR_OFFSET);
+		data[38] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CDESC_OFFSET);
+		data[39] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_TDESC_OFFSET);
 	}
 }
 
@@ -1977,14 +3488,15 @@ axienet_ethtools_get_pauseparam(struct net_device *ndev,
 {
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	phylink_ethtool_get_pauseparam(lp->phylink, epauseparm);
+	if (lp->phylink)
+		phylink_ethtool_get_pauseparam(lp->phylink, epauseparm);
 }
 
 /**
  * axienet_ethtools_set_pauseparam - Set device pause parameter(flow control)
  *				     settings.
  * @ndev:	Pointer to net_device structure
- * @epauseparm:Pointer to ethtool_pauseparam structure
+ * @epauseparm:	Pointer to ethtool_pauseparam structure
  *
  * This implements ethtool command for enabling flow control on Rx and Tx
  * paths. Issue "ethtool -A ethX tx on|off" under linux prompt to execute this
@@ -1998,6 +3510,9 @@ axienet_ethtools_set_pauseparam(struct net_device *ndev,
 {
 	struct axienet_local *lp = netdev_priv(ndev);
 
+	if (!lp->phylink)
+		return -EOPNOTSUPP;
+
 	return phylink_ethtool_set_pauseparam(lp->phylink, epauseparm);
 }
 
@@ -2020,12 +3535,28 @@ axienet_ethtools_get_coalesce(struct net_device *ndev,
 			      struct kernel_ethtool_coalesce *kernel_coal,
 			      struct netlink_ext_ack *extack)
 {
+	u32 regval = 0;
 	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
 
-	ecoalesce->rx_max_coalesced_frames = lp->coalesce_count_rx;
-	ecoalesce->rx_coalesce_usecs = lp->coalesce_usec_rx;
-	ecoalesce->tx_max_coalesced_frames = lp->coalesce_count_tx;
-	ecoalesce->tx_coalesce_usecs = lp->coalesce_usec_tx;
+		regval = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		ecoalesce->rx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+		ecoalesce->rx_coalesce_usecs = lp->coalesce_usec_rx;
+	}
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		regval = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		ecoalesce->tx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+		ecoalesce->tx_coalesce_usecs = lp->coalesce_usec_tx;
+	}
 	return 0;
 }
 
@@ -2080,6 +3611,9 @@ axienet_ethtools_get_link_ksettings(struct net_device *ndev,
 {
 	struct axienet_local *lp = netdev_priv(ndev);
 
+	if (!lp->phylink)
+		return -EOPNOTSUPP;
+
 	return phylink_ethtool_ksettings_get(lp->phylink, cmd);
 }
 
@@ -2089,6 +3623,9 @@ axienet_ethtools_set_link_ksettings(struct net_device *ndev,
 {
 	struct axienet_local *lp = netdev_priv(ndev);
 
+	if (!lp->phylink)
+		return -EOPNOTSUPP;
+
 	return phylink_ethtool_ksettings_set(lp->phylink, cmd);
 }
 
@@ -2104,7 +3641,21 @@ static void axienet_ethtools_get_ethtool_stats(struct net_device *dev,
 					       u64 *data)
 {
 	struct axienet_local *lp = netdev_priv(dev);
-	unsigned int start;
+	unsigned int start, i = 0;
+
+	if (!(lp->features & XAE_FEATURE_STATS)) {
+		data[i++] = dev->stats.tx_packets;
+		data[i++] = dev->stats.rx_packets;
+		data[i++] = dev->stats.tx_bytes;
+		data[i++] = dev->stats.rx_bytes;
+		data[i++] = dev->stats.tx_errors;
+		data[i++] = dev->stats.rx_missed_errors + dev->stats.rx_frame_errors;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_get_stats(dev, stats, data);
+#endif
+		return;
+	}
 
 	do {
 		start = read_seqcount_begin(&lp->hw_stats_seqcount);
@@ -2134,10 +3685,24 @@ static const char axienet_ethtool_stats_strings[][ETH_GSTRING_LEN] = {
 
 static void axienet_ethtools_get_strings(struct net_device *dev, u32 stringset, u8 *data)
 {
+	struct axienet_local *lp = netdev_priv(dev);
+	int i;
+
 	switch (stringset) {
 	case ETH_SS_STATS:
-		memcpy(data, axienet_ethtool_stats_strings,
-		       sizeof(axienet_ethtool_stats_strings));
+		if (lp->features & XAE_FEATURE_STATS) {
+			memcpy(data, axienet_ethtool_stats_strings,
+			       sizeof(axienet_ethtool_stats_strings));
+		} else {
+			for (i = 0; i < AXIENET_ETHTOOLS_SSTATS_LEN; i++) {
+				memcpy(data + i * ETH_GSTRING_LEN,
+				       axienet_get_ethtools_strings_stats[i].name,
+				       ETH_GSTRING_LEN);
+			}
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			axienet_strings(dev, stringset, data);
+#endif
+		}
 		break;
 	}
 }
@@ -2150,6 +3715,11 @@ static int axienet_ethtools_get_sset_count(struct net_device *dev, int sset)
 	case ETH_SS_STATS:
 		if (lp->features & XAE_FEATURE_STATS)
 			return ARRAY_SIZE(axienet_ethtool_stats_strings);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		return axienet_sset_count(dev, sset);
+#else
+		return AXIENET_ETHTOOLS_SSTATS_LEN;
+#endif
 		fallthrough;
 	default:
 		return -EOPNOTSUPP;
@@ -2306,22 +3876,133 @@ axienet_ethtool_get_rmon_stats(struct net_device *dev,
 	*ranges = axienet_rmon_ranges;
 }
 
-static const struct ethtool_ops axienet_ethtool_ops = {
-	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES |
-				     ETHTOOL_COALESCE_USECS,
-	.get_drvinfo    = axienet_ethtools_get_drvinfo,
-	.get_regs_len   = axienet_ethtools_get_regs_len,
-	.get_regs       = axienet_ethtools_get_regs,
-	.get_link       = ethtool_op_get_link,
-	.get_ringparam	= axienet_ethtools_get_ringparam,
-	.set_ringparam	= axienet_ethtools_set_ringparam,
-	.get_pauseparam = axienet_ethtools_get_pauseparam,
-	.set_pauseparam = axienet_ethtools_set_pauseparam,
-	.get_coalesce   = axienet_ethtools_get_coalesce,
-	.set_coalesce   = axienet_ethtools_set_coalesce,
-	.get_link_ksettings = axienet_ethtools_get_link_ksettings,
-	.set_link_ksettings = axienet_ethtools_set_link_ksettings,
-	.nway_reset	= axienet_ethtools_nway_reset,
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+/**
+ * axienet_ethtools_get_ts_info - Get h/w timestamping capabilities.
+ * @ndev:	Pointer to net_device structure
+ * @info:	Pointer to ethtool_ts_info structure
+ *
+ * Return: 0, on success, Non-zero error value on failure.
+ */
+static int axienet_ethtools_get_ts_info(struct net_device *ndev,
+					struct kernel_ethtool_ts_info *info)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
+				SOF_TIMESTAMPING_RX_HARDWARE |
+				SOF_TIMESTAMPING_RAW_HARDWARE;
+	info->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON) |
+			(1 << HWTSTAMP_TX_ONESTEP_SYNC) |
+			(1 << HWTSTAMP_TX_ONESTEP_P2P);
+	info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
+			   (1 << HWTSTAMP_FILTER_ALL);
+	info->phc_index = lp->phc_index;
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC ||
+	    lp->axienet_config->mactype == XAXIENET_10G_25G) {
+		struct device_node *np;
+		struct xlnx_ptp_timer *timer = NULL;
+		struct platform_device *ptpnode;
+
+		np = of_parse_phandle(lp->dev->of_node, "ptp-hardware-clock", 0);
+
+		ptpnode = of_find_device_by_node(np);
+
+		if (ptpnode)
+			timer = platform_get_drvdata(ptpnode);
+
+		if (timer)
+			info->phc_index = timer->phc_index;
+		else if (!timer)
+			netdev_warn(ndev, "PTP timer node not found\n");
+
+		of_node_put(np);
+		platform_device_put(ptpnode);
+	}
+
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_XILINX_AXI_EOE
+static int axienet_eoe_get_rxnfc(struct net_device *ndev, struct ethtool_rxnfc *cmd, u32 *rule_locs)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int ret = 0;
+
+	switch (cmd->cmd) {
+	case ETHTOOL_GRXRINGS:
+		cmd->data = lp->num_rx_queues;
+		break;
+	case ETHTOOL_GRXCLSRLCNT:
+		cmd->rule_cnt = lp->rx_fs_list.count;
+		break;
+	case ETHTOOL_GRXCLSRULE:
+		ret = axienet_eoe_get_flow_entry(ndev, cmd);
+		break;
+	case ETHTOOL_GRXCLSRLALL:
+		ret = axienet_eoe_get_all_flow_entries(ndev, cmd, rule_locs);
+		break;
+	default:
+		netdev_err(ndev, "Command parameter %d is not supported\n", cmd->cmd);
+		ret = -EOPNOTSUPP;
+	}
+
+	return ret;
+}
+
+static int axienet_eoe_set_rxnfc(struct net_device *ndev, struct ethtool_rxnfc *cmd)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int ret = -EOPNOTSUPP;
+
+	if (!(lp->eoe_features & RX_HW_UDP_GRO)) {
+		netdev_err(ndev, "HW GRO is not supported\n");
+		ret = -EINVAL;
+		return ret;
+	}
+
+	switch (cmd->cmd) {
+	case ETHTOOL_SRXCLSRLINS:
+		if (cmd->fs.location >= lp->num_rx_queues || cmd->fs.location == 0) {
+			netdev_err(ndev, "Invalid Location, 1 to 15 are valid GRO locations.");
+			ret = -EINVAL;
+			break;
+		}
+		ret = axienet_eoe_add_flow_filter(ndev, cmd);
+		break;
+	case ETHTOOL_SRXCLSRLDEL:
+		ret = axienet_eoe_del_flow_filter(ndev, cmd);
+		break;
+	default:
+		netdev_err(ndev, "Command parameter %d is not supported\n", cmd->cmd);
+		ret = -EOPNOTSUPP;
+	}
+
+	return ret;
+}
+#endif
+
+static const struct ethtool_ops axienet_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES |
+				     ETHTOOL_COALESCE_USECS,
+	.get_drvinfo    = axienet_ethtools_get_drvinfo,
+	.get_regs_len   = axienet_ethtools_get_regs_len,
+	.get_regs       = axienet_ethtools_get_regs,
+	.get_link       = ethtool_op_get_link,
+	.get_ringparam	= axienet_ethtools_get_ringparam,
+	.set_ringparam	= axienet_ethtools_set_ringparam,
+	.get_pauseparam = axienet_ethtools_get_pauseparam,
+	.set_pauseparam = axienet_ethtools_set_pauseparam,
+	.get_coalesce   = axienet_ethtools_get_coalesce,
+	.set_coalesce   = axienet_ethtools_set_coalesce,
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	.get_ts_info    = axienet_ethtools_get_ts_info,
+#endif
+	.get_link_ksettings = axienet_ethtools_get_link_ksettings,
+	.set_link_ksettings = axienet_ethtools_set_link_ksettings,
+	.nway_reset	= axienet_ethtools_nway_reset,
 	.get_ethtool_stats = axienet_ethtools_get_ethtool_stats,
 	.get_strings    = axienet_ethtools_get_strings,
 	.get_sset_count = axienet_ethtools_get_sset_count,
@@ -2329,26 +4010,256 @@ static const struct ethtool_ops axienet_ethtool_ops = {
 	.get_eth_mac_stats = axienet_ethtool_get_eth_mac_stats,
 	.get_eth_ctrl_stats = axienet_ethtool_get_eth_ctrl_stats,
 	.get_rmon_stats = axienet_ethtool_get_rmon_stats,
+#ifdef CONFIG_XILINX_AXI_EOE
+	.get_rxnfc = axienet_eoe_get_rxnfc,
+	.set_rxnfc = axienet_eoe_set_rxnfc,
+#endif
 };
 
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+static int __maybe_unused axienet_mcdma_probe(struct platform_device *pdev,
+					      struct axienet_local *lp,
+					      struct net_device *ndev)
+{
+	int i, ret = 0;
+	struct axienet_dma_q *q;
+	struct device_node *np;
+	struct resource dmares;
+	const char *str;
+
+	ret = of_property_count_strings(pdev->dev.of_node, "xlnx,channel-ids");
+	if (ret < 0)
+		return -EINVAL;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = kzalloc(sizeof(*q), GFP_KERNEL);
+
+		/* parent */
+		q->lp = lp;
+		lp->dq[i] = q;
+		ret = of_property_read_string_index(pdev->dev.of_node,
+						    "xlnx,channel-ids", i,
+						    &str);
+		ret = kstrtou16(str, 16, &q->chan_id);
+		lp->qnum[i] = i;
+		lp->chan_num[i] = q->chan_id;
+	}
+
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected",
+			      0);
+	if (IS_ERR(np)) {
+		dev_err(&pdev->dev, "could not find DMA node\n");
+		return ret;
+	}
+
+	ret = of_address_to_resource(np, 0, &dmares);
+	if (ret) {
+		dev_err(&pdev->dev, "unable to get DMA resource\n");
+		return ret;
+	}
+
+	ret = of_property_read_u32(np, "xlnx,addrwidth", &lp->dma_mask);
+	if (ret < 0 || lp->dma_mask < XAE_DMA_MASK_MIN ||
+	    lp->dma_mask > XAE_DMA_MASK_MAX) {
+		dev_info(&pdev->dev, "missing/invalid xlnx,addrwidth property, using default\n");
+		lp->dma_mask = XAE_DMA_MASK_MIN;
+	}
+
+	lp->mcdma_regs = devm_ioremap_resource(&pdev->dev, &dmares);
+	if (IS_ERR(lp->mcdma_regs)) {
+		dev_err(&pdev->dev, "iormeap failed for the dma\n");
+		ret = PTR_ERR(lp->mcdma_regs);
+		return ret;
+	}
+
+	axienet_mcdma_tx_probe(pdev, np, lp);
+	axienet_mcdma_rx_probe(pdev, lp, ndev);
+
+	return 0;
+}
+#endif
+
+static int __maybe_unused axienet_dma_probe(struct platform_device *pdev,
+					    struct net_device *ndev)
+{
+	int i, ret;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	struct device_node *np = NULL;
+	struct resource dmares;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = devm_kzalloc(&pdev->dev, sizeof(*q), GFP_KERNEL);
+		if (!q)
+			return -ENOMEM;
+
+		/* parent */
+		q->lp = lp;
+
+		lp->dq[i] = q;
+	}
+
+	/* Find the DMA node, map the DMA registers, and decode the DMA IRQs */
+	/* TODO handle error ret */
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		np = of_parse_phandle(pdev->dev.of_node, "axistream-connected",
+				      i);
+		if (np) {
+			ret = of_address_to_resource(np, 0, &dmares);
+			if (ret >= 0) {
+				q->dma_regs = devm_ioremap_resource(&pdev->dev,
+								    &dmares);
+			} else {
+				dev_err(&pdev->dev, "unable to get DMA resource for %pOF\n",
+					np);
+				return -ENODEV;
+			}
+
+			lp->dq[i]->tx_irq = irq_of_parse_and_map(np, 0);
+			lp->dq[i]->rx_irq = irq_of_parse_and_map(np, 1);
+
+			q->eth_hasdre = of_property_read_bool(np,
+							      "xlnx,include-dre");
+			ret = of_property_read_u32(np, "xlnx,addrwidth",
+						   &lp->dma_mask);
+			if (ret <  0 || lp->dma_mask < XAE_DMA_MASK_MIN ||
+			    lp->dma_mask > XAE_DMA_MASK_MAX) {
+				dev_info(&pdev->dev, "missing/invalid xlnx,addrwidth property, using default\n");
+				lp->dma_mask = XAE_DMA_MASK_MIN;
+			}
+		} else {
+			/* Check for these resources directly on the Ethernet node. */
+			q->dma_regs = devm_platform_get_and_ioremap_resource(pdev, 1, NULL);
+			q->rx_irq = platform_get_irq(pdev, 1);
+			q->tx_irq = platform_get_irq(pdev, 0);
+			if (IS_ERR(q->dma_regs)) {
+				dev_err(&pdev->dev, "unable to get DMA resource for %pOF\n",
+					np);
+				return -ENODEV;
+			}
+			if (q->rx_irq <= 0 || q->tx_irq <= 0) {
+				dev_err(&pdev->dev, "could not determine irqs\n");
+				return -ENOMEM;
+			}
+		}
+		netif_napi_add(ndev, &q->napi_tx, axienet_tx_poll);
+		netif_napi_add(ndev, &q->napi_rx, xaxienet_rx_poll);
+
+		spin_lock_init(&q->tx_lock);
+	}
+
+	of_node_put(np);
+
+	return 0;
+}
+
 static struct axienet_local *pcs_to_axienet_local(struct phylink_pcs *pcs)
 {
 	return container_of(pcs, struct axienet_local, pcs);
 }
 
+static void axienet_dcmac_get_fixed_state(struct phylink_config *config,
+					  struct phylink_link_state *state)
+{
+	struct net_device *ndev = to_net_dev(config->dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 rx_phy_stat;
+
+	state->duplex = DUPLEX_FULL;
+	state->speed = lp->max_speed;
+	state->an_complete = PHYLINK_PCS_NEG_NONE;
+
+	/* Clear previous status */
+	axienet_iow(lp, DCMAC_STS_RX_PHY_OFFSET, DCMAC_STS_ALL_MASK);
+	rx_phy_stat = axienet_ior(lp, DCMAC_STS_RX_PHY_OFFSET);
+
+	state->link = (rx_phy_stat & DCMAC_RXPHY_RX_STS_MASK &&
+			rx_phy_stat & DCMAC_RXPHY_RX_ALIGN_MASK);
+	phylink_clear(state->advertising, Autoneg);
+}
+
 static void axienet_pcs_get_state(struct phylink_pcs *pcs,
 				  struct phylink_link_state *state)
 {
-	struct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;
+	struct axienet_local *lp = pcs_to_axienet_local(pcs);
+	u32 speed, an_status, val;
+	bool tx_pause, rx_pause;
+
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G) {
+		int gt_rst, blk_lock;
+
+		speed = axienet_ior(lp, XXV_STAT_CORE_SPEED_OFFSET);
+		if (speed & XXV_STAT_CORE_SPEED_10G_MASK)
+			state->speed = SPEED_10000;
+		else
+			state->speed = SPEED_25000;
+
+		state->duplex = DUPLEX_FULL;
+		if (lp->auto_neg) {
+			an_status = axienet_ior(lp, XXV_STAT_AN_STS_OFFSET);
+			tx_pause = an_status & XXV_TX_PAUSE_MASK;
+			rx_pause = an_status & XXV_RX_PAUSE_MASK;
+
+			state->pause = (tx_pause & MLO_PAUSE_TX) | (rx_pause & MLO_PAUSE_RX);
+			state->an_complete = an_status & XXV_AN_COMPLETE_MASK;
+		}
+		state->link = 0;
+
+		gt_rst = readl_poll_timeout(lp->regs + XXV_STAT_GTWIZ_OFFSET,
+					    val, (val & XXV_GTWIZ_RESET_DONE),
+					    10, DELAY_OF_ONE_MILLISEC);
+
+		if (!gt_rst) {
+			blk_lock = readl_poll_timeout(lp->regs + XXV_STATRX_BLKLCK_OFFSET,
+						      val, (val & XXV_RX_BLKLCK_MASK),
+						      10, DELAY_OF_ONE_MILLISEC);
+			if (!blk_lock)
+				state->link = 1;
+		}
+	} else if (lp->axienet_config->mactype == XAXIENET_1G_10G_25G) {
+		speed = axienet_ior(lp, XXVS_SPEED_OFFSET);
+		if (speed & XXVS_SPEED_1G)
+			state->speed = SPEED_1000;
+		else if (speed & XXVS_SPEED_10G)
+			state->speed = SPEED_10000;
+		else if (!(speed & ~XXVS_SPEED_25G))
+			state->speed = SPEED_25000;
+		else
+			state->speed = SPEED_UNKNOWN;
+
+		state->duplex = DUPLEX_FULL;
+		an_status = axienet_ior(lp, XXV_STAT_AN_STS_OFFSET);
+		tx_pause = an_status & XXV_TX_PAUSE_MASK;
+		rx_pause = an_status & XXV_RX_PAUSE_MASK;
+
+		state->pause = (tx_pause & MLO_PAUSE_TX) | (rx_pause & MLO_PAUSE_RX);
+		state->an_complete = an_status & XXV_AN_COMPLETE_MASK;
+
+		/* rx status bit indicates current status of link */
+		state->link = axienet_ior(lp, XXVS_RX_STATUS_REG1) & XXVS_RX_STATUS_MASK;
+	} else {
+		struct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;
 
-	phylink_mii_c22_pcs_get_state(pcs_phy, state);
+		phylink_mii_c22_pcs_get_state(pcs_phy, state);
+	}
 }
 
 static void axienet_pcs_an_restart(struct phylink_pcs *pcs)
 {
-	struct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;
+	struct axienet_local *lp = pcs_to_axienet_local(pcs);
 
-	phylink_mii_c22_pcs_an_restart(pcs_phy);
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_1G_10G_25G) {
+		axienet_iow(lp, XXV_AN_CTL1_OFFSET,
+			    (axienet_ior(lp, XXV_AN_CTL1_OFFSET) |
+			     XXV_AN_RESTART_MASK));
+	} else {
+		struct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;
+
+		phylink_mii_c22_pcs_an_restart(pcs_phy);
+	}
 }
 
 static int axienet_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,
@@ -2356,9 +4267,9 @@ static int axienet_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,
 			      const unsigned long *advertising,
 			      bool permit_pause_to_mac)
 {
-	struct mdio_device *pcs_phy = pcs_to_axienet_local(pcs)->pcs_phy;
-	struct net_device *ndev = pcs_to_axienet_local(pcs)->ndev;
-	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_local *lp = pcs_to_axienet_local(pcs);
+	struct mdio_device *pcs_phy = lp->pcs_phy;
+	struct net_device *ndev = lp->ndev;
 	int ret;
 
 	if (lp->switch_x_sgmii) {
@@ -2372,6 +4283,68 @@ static int axienet_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,
 			return ret;
 		}
 	}
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G) {
+		if (!lp->auto_neg)
+			return 0;
+
+		u32 autoneg_complete;
+
+		autoneg_complete = (axienet_ior(lp, XXV_STAT_AN_STS_OFFSET) &
+				    XXV_AN_COMPLETE_MASK);
+
+		/* If auto-negotiation is not completed, restart auto-neg */
+		return (neg_mode == (unsigned int)PHYLINK_PCS_NEG_INBAND_ENABLED &&
+			autoneg_complete == 0);
+	} else if (lp->axienet_config->mactype == XAXIENET_1G_10G_25G) {
+		bool an_enabled = false;
+
+		if (phylink_test(advertising, Autoneg))
+			an_enabled = true;
+
+		if (!an_enabled) {
+			/* Disable autoneg */
+			axienet_iow(lp, XXVS_AN_CTL1_OFFSET,
+				    (axienet_ior(lp, XXVS_AN_CTL1_OFFSET) &
+				    (~XXVS_AN_ENABLE_MASK | XXVS_AN_BYPASS)));
+			axienet_iow(lp, XXVS_RESET_OFFSET, XXVS_RX_SERDES_RESET);
+			axienet_iow(lp, XXVS_LT_CTL_OFFSET, 0);
+			axienet_iow(lp, XXVS_RESET_OFFSET, XXVS_RX_RESET | XXVS_TX_RESET);
+			axienet_iow(lp, XXVS_RESET_OFFSET, 0);
+		}
+
+		/* Workaround: pcs_config expects to configure pcs based on link modes,
+		 * but we're using advertise to depict what to configure.
+		 */
+		if (linkmode_test_bit(ETHTOOL_LINK_MODE_25000baseKR_Full_BIT,
+				      advertising)) {
+			if (an_enabled)
+				axienet_config_autoneg_link_training(lp, XXVS_AN_25G_ABILITY_MASK);
+			else
+				axienet_iow(lp, XXVS_TC_OFFSET, (axienet_ior(lp, XXVS_TC_OFFSET) &
+					     XXVS_CTRL_CORE_SPEED_SEL_CLEAR));
+		} else if (linkmode_test_bit(ETHTOOL_LINK_MODE_10000baseKR_Full_BIT,
+			   advertising)) {
+			if (an_enabled)
+				axienet_config_autoneg_link_training(lp, XXVS_AN_10G_ABILITY_MASK);
+			else
+				axienet_iow(lp, XXVS_TC_OFFSET, (axienet_ior(lp, XXVS_TC_OFFSET) &
+					    XXVS_CTRL_CORE_SPEED_SEL_CLEAR) |
+					    XXVS_CTRL_CORE_SPEED_SEL_10G);
+
+		} else if (linkmode_test_bit(ETHTOOL_LINK_MODE_1000baseKX_Full_BIT,
+			   advertising)) {
+			if (an_enabled)
+				axienet_config_autoneg_link_training(lp, XXVS_AN_1G_ABILITY_MASK);
+			else
+				axienet_iow(lp, XXVS_TC_OFFSET, (axienet_ior(lp, XXVS_TC_OFFSET) &
+					    XXVS_CTRL_CORE_SPEED_SEL_CLEAR) |
+					    XXVS_CTRL_CORE_SPEED_SEL_1G);
+		}
+		return 0;
+	} else if (lp->axienet_config->mactype == XAXIENET_DCMAC) {
+		/* Nothing to change for fixed link */
+		return 0;
+	}
 
 	ret = phylink_mii_c22_pcs_config(pcs_phy, interface, advertising,
 					 neg_mode);
@@ -2381,10 +4354,27 @@ static int axienet_pcs_config(struct phylink_pcs *pcs, unsigned int neg_mode,
 	return ret;
 }
 
+static int axienet_pcs_validate(struct phylink_pcs *pcs, unsigned long *supported,
+				const struct phylink_link_state *state)
+{
+	struct axienet_local *lp = pcs_to_axienet_local(pcs);
+
+	if (lp->axienet_config->mactype == XAXIENET_1G_10G_25G) {
+		int inf;
+
+		for_each_set_bit(inf, lp->phylink_config.supported_interfaces,
+				 PHY_INTERFACE_MODE_MAX)
+			__set_bit(inf, supported);
+	}
+
+	return 0;
+}
+
 static const struct phylink_pcs_ops axienet_pcs_ops = {
 	.pcs_get_state = axienet_pcs_get_state,
 	.pcs_config = axienet_pcs_config,
 	.pcs_an_restart = axienet_pcs_an_restart,
+	.pcs_validate = axienet_pcs_validate,
 };
 
 static struct phylink_pcs *axienet_mac_select_pcs(struct phylink_config *config,
@@ -2393,11 +4383,7 @@ static struct phylink_pcs *axienet_mac_select_pcs(struct phylink_config *config,
 	struct net_device *ndev = to_net_dev(config->dev);
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	if (interface == PHY_INTERFACE_MODE_1000BASEX ||
-	    interface ==  PHY_INTERFACE_MODE_SGMII)
-		return &lp->pcs;
-
-	return NULL;
+	return &lp->pcs;
 }
 
 static void axienet_mac_config(struct phylink_config *config, unsigned int mode,
@@ -2423,10 +4409,20 @@ static void axienet_mac_link_up(struct phylink_config *config,
 	struct axienet_local *lp = netdev_priv(ndev);
 	u32 emmc_reg, fcc_reg;
 
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_1G_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_DCMAC) {
+		/* nothing meaningful to do */
+		return;
+	}
+
 	emmc_reg = axienet_ior(lp, XAE_EMMC_OFFSET);
 	emmc_reg &= ~XAE_EMMC_LINKSPEED_MASK;
 
 	switch (speed) {
+	case SPEED_2500:
+		emmc_reg |= XAE_EMMC_LINKSPD_2500;
+		break;
 	case SPEED_1000:
 		emmc_reg |= XAE_EMMC_LINKSPD_1000;
 		break;
@@ -2463,95 +4459,368 @@ static const struct phylink_mac_ops axienet_phylink_ops = {
 	.mac_link_up = axienet_mac_link_up,
 };
 
-/**
- * axienet_dma_err_handler - Work queue task for Axi DMA Error
- * @work:	pointer to work_struct
- *
- * Resets the Axi DMA and Axi Ethernet devices, and reconfigures the
- * Tx/Rx BDs.
- */
-static void axienet_dma_err_handler(struct work_struct *work)
+static int axienet_clk_init(struct platform_device *pdev,
+			    struct clk **axi_aclk, struct clk **axis_clk,
+			    struct clk **ref_clk, struct clk **tmpclk)
 {
-	u32 i;
-	u32 axienet_status;
-	struct axidma_bd *cur_p;
-	struct axienet_local *lp = container_of(work, struct axienet_local,
-						dma_err_task);
-	struct net_device *ndev = lp->ndev;
+	int err;
 
-	/* Don't bother if we are going to stop anyway */
-	if (READ_ONCE(lp->stopping))
-		return;
+	*tmpclk = NULL;
 
-	napi_disable(&lp->napi_tx);
-	napi_disable(&lp->napi_rx);
+	/* The "ethernet_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
+	 */
+	*axi_aclk = devm_clk_get(&pdev->dev, "ethernet_clk");
+	if (IS_ERR(*axi_aclk)) {
+		if (PTR_ERR(*axi_aclk) != -ENOENT) {
+			err = PTR_ERR(*axi_aclk);
+			return err;
+		}
 
-	axienet_setoptions(ndev, lp->options &
-			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+		*axi_aclk = devm_clk_get(&pdev->dev, "s_axi_lite_clk");
+		if (IS_ERR(*axi_aclk)) {
+			if (PTR_ERR(*axi_aclk) != -ENOENT) {
+				err = PTR_ERR(*axi_aclk);
+				return err;
+			}
+			*axi_aclk = NULL;
+		}
 
-	axienet_dma_stop(lp);
+	} else {
+		dev_warn(&pdev->dev, "ethernet_clk is deprecated and will be removed sometime in the future\n");
+	}
 
-	for (i = 0; i < lp->tx_bd_num; i++) {
-		cur_p = &lp->tx_bd_v[i];
-		if (cur_p->cntrl) {
-			dma_addr_t addr = desc_get_phys_addr(lp, cur_p);
+	*axis_clk = devm_clk_get(&pdev->dev, "axis_clk");
+	if (IS_ERR(*axis_clk)) {
+		if (PTR_ERR(*axis_clk) != -ENOENT) {
+			err = PTR_ERR(*axis_clk);
+			return err;
+		}
+		*axis_clk = NULL;
+	}
 
-			dma_unmap_single(lp->dev, addr,
-					 (cur_p->cntrl &
-					  XAXIDMA_BD_CTRL_LENGTH_MASK),
-					 DMA_TO_DEVICE);
+	*ref_clk = devm_clk_get(&pdev->dev, "ref_clk");
+	if (IS_ERR(*ref_clk)) {
+		if (PTR_ERR(*ref_clk) != -ENOENT) {
+			err = PTR_ERR(*ref_clk);
+			return err;
 		}
-		if (cur_p->skb)
-			dev_kfree_skb_irq(cur_p->skb);
-		cur_p->phys = 0;
-		cur_p->phys_msb = 0;
-		cur_p->cntrl = 0;
-		cur_p->status = 0;
-		cur_p->app0 = 0;
-		cur_p->app1 = 0;
-		cur_p->app2 = 0;
-		cur_p->app3 = 0;
-		cur_p->app4 = 0;
-		cur_p->skb = NULL;
+		*ref_clk = NULL;
 	}
 
-	for (i = 0; i < lp->rx_bd_num; i++) {
-		cur_p = &lp->rx_bd_v[i];
-		cur_p->status = 0;
-		cur_p->app0 = 0;
-		cur_p->app1 = 0;
-		cur_p->app2 = 0;
-		cur_p->app3 = 0;
-		cur_p->app4 = 0;
+	err = clk_prepare_enable(*axi_aclk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axi_aclk/ethernet_clk (%d)\n", err);
+		return err;
 	}
 
-	lp->tx_bd_ci = 0;
-	lp->tx_bd_tail = 0;
-	lp->rx_bd_ci = 0;
+	err = clk_prepare_enable(*axis_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axis_clk (%d)\n", err);
+		goto err_disable_axi_aclk;
+	}
+
+	err = clk_prepare_enable(*ref_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable ref_clk (%d)\n", err);
+		goto err_disable_axis_clk;
+	}
 
-	axienet_dma_start(lp);
+	return 0;
 
-	axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
-	axienet_status &= ~XAE_RCW1_RX_MASK;
-	axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+err_disable_axis_clk:
+	clk_disable_unprepare(*axis_clk);
+err_disable_axi_aclk:
+	clk_disable_unprepare(*axi_aclk);
 
-	axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
-	if (axienet_status & XAE_INT_RXRJECT_MASK)
-		axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
-	axienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?
-		    XAE_INT_RECV_ERROR_MASK : 0);
-	axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	return err;
+}
+
+static int axienet_dma_clk_init(struct platform_device *pdev)
+{
+	int err;
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct axienet_local *lp = netdev_priv(ndev);
 
-	/* Sync default options with HW but leave receiver and
-	 * transmitter disabled.
+	/* The "dma_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
 	 */
-	axienet_setoptions(ndev, lp->options &
-			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
-	axienet_set_mac_address(ndev, NULL);
-	axienet_set_multicast_list(ndev);
-	napi_enable(&lp->napi_rx);
-	napi_enable(&lp->napi_tx);
-	axienet_setoptions(ndev, lp->options);
+	lp->dma_tx_clk = devm_clk_get(&pdev->dev, "dma_clk");
+	if (IS_ERR(lp->dma_tx_clk)) {
+		if (PTR_ERR(lp->dma_tx_clk) != -ENOENT) {
+			err = PTR_ERR(lp->dma_tx_clk);
+			return err;
+		}
+
+		lp->dma_tx_clk = devm_clk_get(&pdev->dev, "m_axi_mm2s_aclk");
+		if (IS_ERR(lp->dma_tx_clk)) {
+			if (PTR_ERR(lp->dma_tx_clk) != -ENOENT) {
+				err = PTR_ERR(lp->dma_tx_clk);
+				return err;
+			}
+			lp->dma_tx_clk = NULL;
+		}
+	} else {
+		dev_warn(&pdev->dev, "dma_clk is deprecated and will be removed sometime in the future\n");
+	}
+
+	lp->dma_rx_clk = devm_clk_get(&pdev->dev, "m_axi_s2mm_aclk");
+	if (IS_ERR(lp->dma_rx_clk)) {
+		if (PTR_ERR(lp->dma_rx_clk) != -ENOENT) {
+			err = PTR_ERR(lp->dma_rx_clk);
+			return err;
+		}
+		lp->dma_rx_clk = NULL;
+	}
+
+	lp->dma_sg_clk = devm_clk_get(&pdev->dev, "m_axi_sg_aclk");
+	if (IS_ERR(lp->dma_sg_clk)) {
+		if (PTR_ERR(lp->dma_sg_clk) != -ENOENT) {
+			err = PTR_ERR(lp->dma_sg_clk);
+			return err;
+		}
+		lp->dma_sg_clk = NULL;
+	}
+
+	err = clk_prepare_enable(lp->dma_tx_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable tx_clk/dma_clk (%d)\n", err);
+		return err;
+	}
+
+	err = clk_prepare_enable(lp->dma_rx_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable rx_clk (%d)\n", err);
+		goto err_disable_txclk;
+	}
+
+	err = clk_prepare_enable(lp->dma_sg_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable sg_clk (%d)\n", err);
+		goto err_disable_rxclk;
+	}
+
+	return 0;
+
+err_disable_rxclk:
+	clk_disable_unprepare(lp->dma_rx_clk);
+err_disable_txclk:
+	clk_disable_unprepare(lp->dma_tx_clk);
+
+	return err;
+}
+
+static void axienet_clk_disable(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	clk_disable_unprepare(lp->dma_sg_clk);
+	clk_disable_unprepare(lp->dma_tx_clk);
+	clk_disable_unprepare(lp->dma_rx_clk);
+	clk_disable_unprepare(lp->eth_sclk);
+	clk_disable_unprepare(lp->eth_refclk);
+	clk_disable_unprepare(lp->eth_dclk);
+	clk_disable_unprepare(lp->aclk);
+}
+
+static int xxvenet_clk_init(struct platform_device *pdev,
+			    struct clk **axi_aclk, struct clk **axis_clk,
+			    struct clk **tmpclk, struct clk **dclk)
+{
+	int err;
+
+	*tmpclk = NULL;
+
+	/* The "ethernet_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
+	 */
+	*axi_aclk = devm_clk_get(&pdev->dev, "ethernet_clk");
+	if (IS_ERR(*axi_aclk)) {
+		if (PTR_ERR(*axi_aclk) != -ENOENT) {
+			err = PTR_ERR(*axi_aclk);
+			return err;
+		}
+
+		*axi_aclk = devm_clk_get(&pdev->dev, "s_axi_aclk");
+		if (IS_ERR(*axi_aclk)) {
+			if (PTR_ERR(*axi_aclk) != -ENOENT) {
+				err = PTR_ERR(*axi_aclk);
+				return err;
+			}
+			*axi_aclk = NULL;
+		}
+
+	} else {
+		dev_warn(&pdev->dev, "ethernet_clk is deprecated and will be removed sometime in the future\n");
+	}
+
+	*axis_clk = devm_clk_get(&pdev->dev, "rx_core_clk");
+	if (IS_ERR(*axis_clk)) {
+		if (PTR_ERR(*axis_clk) != -ENOENT) {
+			err = PTR_ERR(*axis_clk);
+			return err;
+		}
+		*axis_clk = NULL;
+	}
+
+	*dclk = devm_clk_get(&pdev->dev, "dclk");
+	if (IS_ERR(*dclk)) {
+		if (PTR_ERR(*dclk) != -ENOENT) {
+			err = PTR_ERR(*dclk);
+			return err;
+		}
+		*dclk = NULL;
+	}
+
+	err = clk_prepare_enable(*axi_aclk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axi_clk/ethernet_clk (%d)\n", err);
+		return err;
+	}
+
+	err = clk_prepare_enable(*axis_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axis_clk (%d)\n", err);
+		goto err_disable_axi_aclk;
+	}
+
+	err = clk_prepare_enable(*dclk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable dclk (%d)\n", err);
+		goto err_disable_axis_clk;
+	}
+
+	return 0;
+
+err_disable_axis_clk:
+	clk_disable_unprepare(*axis_clk);
+err_disable_axi_aclk:
+	clk_disable_unprepare(*axi_aclk);
+
+	return err;
+}
+
+static const struct axienet_config axienet_1_2p5g_config = {
+	.mactype = XAXIENET_1_2p5G,
+	.setoptions = axienet_setoptions,
+	.clk_init = axienet_clk_init,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+static const struct axienet_config axienet_10g_config = {
+	.mactype = XAXIENET_LEGACY_10G,
+	.setoptions = axienet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+static const struct axienet_config axienet_10g25g_config = {
+	.mactype = XAXIENET_10G_25G,
+	.setoptions = xxvenet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = XXV_TX_PTP_LEN,
+	.ts_header_len = XXVENET_TS_HEADER_LEN,
+	.gt_reset = xxv_gt_reset,
+};
+
+static const struct axienet_config axienet_1g10g25g_config = {
+	.mactype = XAXIENET_1G_10G_25G,
+	.setoptions = xxvenet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = XXV_TX_PTP_LEN,
+	.gt_reset = xxv_gt_reset,
+};
+
+static const struct axienet_config axienet_usxgmii_config = {
+	.mactype = XAXIENET_10G_25G,
+	.setoptions = xxvenet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = 0,
+};
+
+static const struct axienet_config axienet_mrmac_config = {
+	.mactype = XAXIENET_MRMAC,
+	.setoptions = xxvenet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = XXV_TX_PTP_LEN,
+	.ts_header_len = MRMAC_TS_HEADER_LEN,
+	.gt_reset = axienet_mrmac_gt_reset,
+};
+
+static const struct axienet_config axienet_dcmac_config = {
+	.mactype = XAXIENET_DCMAC,
+	.clk_init = xxvenet_clk_init,
+	.gt_reset = dcmac_gt_reset,
+};
+
+/* Match table for of_platform binding */
+static const struct of_device_id axienet_of_match[] = {
+	{ .compatible = "xlnx,axi-ethernet-1.00.a", .data = &axienet_1_2p5g_config},
+	{ .compatible = "xlnx,axi-ethernet-1.01.a", .data = &axienet_1_2p5g_config},
+	{ .compatible = "xlnx,axi-ethernet-2.01.a", .data = &axienet_1_2p5g_config},
+	{ .compatible = "xlnx,axi-2_5-gig-ethernet-1.0",
+						.data = &axienet_1_2p5g_config},
+	{ .compatible = "xlnx,ten-gig-eth-mac", .data = &axienet_10g_config},
+	{ .compatible = "xlnx,xxv-ethernet-1.0",
+						.data = &axienet_10g25g_config},
+	{ .compatible = "xlnx,xxv-usxgmii-ethernet-1.0",
+					.data = &axienet_usxgmii_config},
+	{ .compatible = "xlnx,mrmac-ethernet-1.0",
+					.data = &axienet_mrmac_config},
+	{ .compatible = "xlnx,ethernet-1-10-25g-2.7",
+					.data = &axienet_1g10g25g_config},
+	{ .compatible = "xlnx,dcmac-2.4",
+					.data = &axienet_dcmac_config},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, axienet_of_match);
+
+static int axienet_eoe_netdev_event(struct notifier_block *this, unsigned long event,
+				    void *ptr)
+{
+	struct axienet_local *lp = container_of(this, struct axienet_local,
+						inetaddr_notifier);
+	struct in_ifaddr *ifa = ptr;
+	struct axienet_dma_q *q;
+	int i;
+
+	struct net_device *ndev = ifa->ifa_dev->dev;
+
+	if (lp->ndev != ndev) {
+		dev_err(lp->dev, " ndev is not matched to configure GRO IP address\n");
+	} else {
+		switch (event) {
+		case NETDEV_UP:
+			dev_dbg(lp->dev, "%s:NETDEV_UP\n", __func__);
+			for_each_rx_dma_queue(lp, i) {
+				q = lp->dq[i];
+				if (axienet_eoe_is_channel_gro(lp, q))
+					axienet_eoe_iow(lp,
+							XEOE_UDP_GRO_DST_IP_OFFSET(q->chan_id),
+							ntohl(ifa->ifa_address));
+			}
+		break;
+		case NETDEV_DOWN:
+			dev_dbg(lp->dev, "%s:NETDEV_DOWN\n", __func__);
+			for_each_rx_dma_queue(lp, i) {
+				q = lp->dq[i];
+				if (axienet_eoe_is_channel_gro(lp, q))
+					axienet_eoe_iow(lp,
+							XEOE_UDP_GRO_DST_IP_OFFSET(q->chan_id),
+							0);
+			}
+		break;
+		default:
+			dev_err(lp->dev, "IPv4 Ethernet address is not set\n");
+		}
+	}
+
+	return NOTIFY_DONE;
 }
 
 /**
@@ -2568,16 +4837,32 @@ static void axienet_dma_err_handler(struct work_struct *work)
  */
 static int axienet_probe(struct platform_device *pdev)
 {
-	int ret;
+	int (*axienet_clk_init)(struct platform_device *pdev,
+				struct clk **axi_aclk, struct clk **axis_clk,
+				struct clk **ref_clk, struct clk **tmpclk) =
+					axienet_clk_init;
+	int ret = 0;
 	struct device_node *np;
 	struct axienet_local *lp;
 	struct net_device *ndev;
 	struct resource *ethres;
 	u8 mac_addr[ETH_ALEN];
-	int addr_width = 32;
 	u32 value;
 
-	ndev = alloc_etherdev(sizeof(*lp));
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	struct resource txtsres, rxtsres;
+#endif
+	u16 num_queues = XAE_MAX_QUEUES;
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-queues",
+				   &num_queues);
+	if (ret) {
+#ifndef CONFIG_AXIENET_HAS_MCDMA
+		num_queues = 1;
+#endif
+	}
+
+	ndev = alloc_etherdev_mq(sizeof(*lp), num_queues);
 	if (!ndev)
 		return -ENOMEM;
 
@@ -2595,6 +4880,8 @@ static int axienet_probe(struct platform_device *pdev)
 	lp->ndev = ndev;
 	lp->dev = &pdev->dev;
 	lp->options = XAE_OPTION_DEFAULTS;
+	lp->num_tx_queues = num_queues;
+	lp->num_rx_queues = num_queues;
 	lp->rx_bd_num = RX_BD_NUM_DEFAULT;
 	lp->tx_bd_num = TX_BD_NUM_DEFAULT;
 
@@ -2604,6 +4891,7 @@ static int axienet_probe(struct platform_device *pdev)
 	mutex_init(&lp->stats_lock);
 	seqcount_mutex_init(&lp->hw_stats_seqcount, &lp->stats_lock);
 	INIT_DEFERRABLE_WORK(&lp->stats_work, axienet_refresh_stats);
+	INIT_LIST_HEAD(&lp->rx_fs_list.list);
 
 	lp->axi_clk = devm_clk_get_optional(&pdev->dev, "s_axi_lite_clk");
 	if (!lp->axi_clk) {
@@ -2642,11 +4930,24 @@ static int axienet_probe(struct platform_device *pdev)
 	}
 	lp->regs_start = ethres->start;
 
+	if (pdev->dev.of_node) {
+		const struct of_device_id *match;
+
+		match = of_match_node(axienet_of_match, pdev->dev.of_node);
+		if (match && match->data) {
+			lp->axienet_config = match->data;
+			axienet_clk_init = lp->axienet_config->clk_init;
+		}
+	}
+
 	/* Setup checksum offload, but default to off if not specified */
 	lp->features = 0;
-
-	if (axienet_ior(lp, XAE_ABILITY_OFFSET) & XAE_ABILITY_STATS)
-		lp->features |= XAE_FEATURE_STATS;
+#ifndef CONFIG_AXIENET_HAS_MCDMA
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G) {
+		if (axienet_ior(lp, XAE_ABILITY_OFFSET) & XAE_ABILITY_STATS)
+			lp->features |= XAE_FEATURE_STATS;
+	}
+#endif
 
 	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,txcsum", &value);
 	if (!ret) {
@@ -2688,83 +4989,284 @@ static int axienet_probe(struct platform_device *pdev)
 						   "xlnx,switch-x-sgmii");
 
 	/* Start with the proprietary, and broken phy_type */
-	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phy-type", &value);
-	if (!ret) {
-		netdev_warn(ndev, "Please upgrade your device tree binary blob to use phy-mode");
-		switch (value) {
-		case XAE_PHY_TYPE_MII:
-			lp->phy_mode = PHY_INTERFACE_MODE_MII;
-			break;
-		case XAE_PHY_TYPE_GMII:
-			lp->phy_mode = PHY_INTERFACE_MODE_GMII;
-			break;
-		case XAE_PHY_TYPE_RGMII_2_0:
-			lp->phy_mode = PHY_INTERFACE_MODE_RGMII_ID;
-			break;
-		case XAE_PHY_TYPE_SGMII:
-			lp->phy_mode = PHY_INTERFACE_MODE_SGMII;
-			break;
-		case XAE_PHY_TYPE_1000BASE_X:
-			lp->phy_mode = PHY_INTERFACE_MODE_1000BASEX;
-			break;
-		default:
-			ret = -EINVAL;
-			goto cleanup_clk;
+	if (lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phy-type", &value);
+		if (!ret) {
+			switch (value) {
+			case XAE_PHY_TYPE_MII:
+				lp->phy_mode = PHY_INTERFACE_MODE_MII;
+				break;
+			case XAE_PHY_TYPE_GMII:
+				lp->phy_mode = PHY_INTERFACE_MODE_GMII;
+				break;
+			case XAE_PHY_TYPE_RGMII_2_0:
+				lp->phy_mode = PHY_INTERFACE_MODE_RGMII_ID;
+				break;
+			case XAE_PHY_TYPE_SGMII:
+				lp->phy_mode = PHY_INTERFACE_MODE_SGMII;
+				break;
+			case XAE_PHY_TYPE_1000BASE_X:
+				lp->phy_mode = PHY_INTERFACE_MODE_1000BASEX;
+				break;
+			case XXE_PHY_TYPE_USXGMII:
+				lp->phy_mode = PHY_INTERFACE_MODE_USXGMII;
+				break;
+			default:
+				/* Don't error out as phy-type is an optional property */
+				break;
+			}
+		} else {
+			ret = of_get_phy_mode(pdev->dev.of_node, &lp->phy_mode);
+			if (ret)
+				goto cleanup_clk;
 		}
-	} else {
-		ret = of_get_phy_mode(pdev->dev.of_node, &lp->phy_mode);
-		if (ret)
+	}
+
+	/* Set default USXGMII rate */
+	lp->usxgmii_rate = SPEED_1000;
+	of_property_read_u32(pdev->dev.of_node, "xlnx,usxgmii-rate",
+			     &lp->usxgmii_rate);
+
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G ||
+	    lp->axienet_config->mactype == XAXIENET_MRMAC ||
+	    lp->axienet_config->mactype == XAXIENET_DCMAC) {
+		ret = of_property_read_u32(pdev->dev.of_node, "max-speed",
+					   &lp->max_speed);
+
+		if (ret && lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			ret = of_property_read_u32(pdev->dev.of_node,
+						   "xlnx,mrmac-rate",
+						   &lp->max_speed);
+			if (!ret) {
+				dev_warn(&pdev->dev,
+					 "xlnx,mrmac-rate is deprecated, please use max-speed instead\n");
+			}
+		}
+		if (ret) {
+			dev_err(&pdev->dev, "couldn't find MAC Rate\n");
 			goto cleanup_clk;
+		}
 	}
-	if (lp->switch_x_sgmii && lp->phy_mode != PHY_INTERFACE_MODE_SGMII &&
-	    lp->phy_mode != PHY_INTERFACE_MODE_1000BASEX) {
-		dev_err(&pdev->dev, "xlnx,switch-x-sgmii only supported with SGMII or 1000BaseX\n");
-		ret = -EINVAL;
-		goto cleanup_clk;
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		const char *gt_mode;
+
+		/* Default to GT wide mode */
+		lp->gt_mode_narrow = false;
+
+		ret = of_property_read_string(pdev->dev.of_node,
+					      "xlnx,gt-mode",
+					      &gt_mode);
+		if (ret != -EINVAL && !strcasecmp(gt_mode, GT_MODE_NARROW))
+			lp->gt_mode_narrow = true;
+
+		/* Default AXI4-stream data widths */
+		if (lp->max_speed == SPEED_10000)
+			lp->mrmac_stream_dwidth = MRMAC_STREAM_DWIDTH_32;
+		else
+			lp->mrmac_stream_dwidth = MRMAC_STREAM_DWIDTH_64;
+
+		of_property_read_u32(pdev->dev.of_node,
+				     "xlnx,axistream-dwidth",
+				     &lp->mrmac_stream_dwidth);
 	}
 
-	if (!of_property_present(pdev->dev.of_node, "dmas")) {
-		/* Find the DMA node, map the DMA registers, and decode the DMA IRQs */
-		np = of_parse_phandle(pdev->dev.of_node, "axistream-connected", 0);
+	lp->eth_hasnobuf = of_property_read_bool(pdev->dev.of_node,
+						 "xlnx,eth-hasnobuf");
+	lp->eth_hasptp = of_property_read_bool(pdev->dev.of_node,
+					       "xlnx,eth-hasptp");
 
-		if (np) {
-			struct resource dmares;
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G)
+		lp->auto_neg = of_property_read_bool(pdev->dev.of_node,
+						     "xlnx,has-auto-neg");
 
-			ret = of_address_to_resource(np, 0, &dmares);
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G  ||
+	    lp->axienet_config->mactype == XAXIENET_1G_10G_25G)
+		lp->xxv_ip_version = axienet_ior(lp, XXV_CONFIG_REVISION);
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		struct resource gtpll, gtctrl;
+
+		if (mrmac_pll_reg) {
+			lp->gt_pll = mrmac_gt_pll;
+			lp->gt_ctrl = mrmac_gt_ctrl;
+		} else {
+			np = of_parse_phandle(pdev->dev.of_node,
+					      "xlnx,gtpll", 0);
+			if (IS_ERR(np)) {
+				dev_err(&pdev->dev,
+					"couldn't find GT PLL\n");
+				ret = PTR_ERR(np);
+				goto cleanup_clk;
+			}
+
+			ret = of_address_to_resource(np, 0, &gtpll);
 			if (ret) {
 				dev_err(&pdev->dev,
-					"unable to get DMA resource\n");
-				of_node_put(np);
+					"unable to get GT PLL resource\n");
 				goto cleanup_clk;
 			}
-			lp->dma_regs = devm_ioremap_resource(&pdev->dev,
-							     &dmares);
-			lp->rx_irq = irq_of_parse_and_map(np, 1);
-			lp->tx_irq = irq_of_parse_and_map(np, 0);
-			of_node_put(np);
-			lp->eth_irq = platform_get_irq_optional(pdev, 0);
-		} else {
-			/* Check for these resources directly on the Ethernet node. */
-			lp->dma_regs = devm_platform_get_and_ioremap_resource(pdev, 1, NULL);
-			lp->rx_irq = platform_get_irq(pdev, 1);
-			lp->tx_irq = platform_get_irq(pdev, 0);
-			lp->eth_irq = platform_get_irq_optional(pdev, 2);
-		}
-		if (IS_ERR(lp->dma_regs)) {
-			dev_err(&pdev->dev, "could not map DMA regs\n");
-			ret = PTR_ERR(lp->dma_regs);
+
+			lp->gt_pll = devm_ioremap_resource(&pdev->dev,
+							   &gtpll);
+			if (IS_ERR(lp->gt_pll)) {
+				dev_err(&pdev->dev,
+					"couldn't map GT PLL regs\n");
+				ret = PTR_ERR(lp->gt_pll);
+				goto cleanup_clk;
+			}
+
+			np = of_parse_phandle(pdev->dev.of_node,
+					      "xlnx,gtctrl", 0);
+			if (IS_ERR(np)) {
+				dev_err(&pdev->dev,
+					"couldn't find GT control\n");
+				ret = PTR_ERR(np);
+				goto cleanup_clk;
+			}
+
+			ret = of_address_to_resource(np, 0, &gtctrl);
+			if (ret) {
+				dev_err(&pdev->dev,
+					"unable to get GT control resource\n");
+				goto cleanup_clk;
+			}
+
+			lp->gt_ctrl = devm_ioremap_resource(&pdev->dev,
+							    &gtctrl);
+			if (IS_ERR(lp->gt_ctrl)) {
+				dev_err(&pdev->dev,
+					"couldn't map GT control regs\n");
+				ret = PTR_ERR(lp->gt_ctrl);
+				goto cleanup_clk;
+			}
+
+			mrmac_gt_pll = lp->gt_pll;
+			mrmac_gt_ctrl = lp->gt_ctrl;
+			mrmac_pll_reg = 1;
+		}
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+		ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phcindex",
+					   &lp->phc_index);
+		if (!ret)
+			dev_warn(&pdev->dev, "xlnx,phcindex is deprecated, please use ptp-hardware-clock instead\n");
+#endif
+		ret = of_property_read_u32(pdev->dev.of_node, "xlnx,gtlane",
+					   &lp->gt_lane);
+		if (ret) {
+			dev_err(&pdev->dev, "MRMAC GT lane information missing\n");
 			goto cleanup_clk;
 		}
-		if (lp->rx_irq <= 0 || lp->tx_irq <= 0) {
-			dev_err(&pdev->dev, "could not determine irqs\n");
-			ret = -ENOMEM;
+		dev_info(&pdev->dev, "GT lane: %d\n", lp->gt_lane);
+	} else if (lp->axienet_config->mactype == XAXIENET_DCMAC) {
+		lp->gds_gt_ctrl = devm_gpiod_get_array(&pdev->dev,
+						       "gt_ctrl",
+						       GPIOD_OUT_LOW);
+		if (IS_ERR(lp->gds_gt_ctrl)) {
+			dev_err(&pdev->dev,
+				"Failed to request GT control GPIO\n");
+			ret = PTR_ERR(lp->gds_gt_ctrl);
 			goto cleanup_clk;
 		}
 
-		/* Reset core now that clocks are enabled, prior to accessing MDIO */
-		ret = __axienet_device_reset(lp);
+		lp->gds_gt_rx_dpath = devm_gpiod_get_array(&pdev->dev,
+							   "gt_rx_dpath",
+							    GPIOD_OUT_LOW);
+		if (IS_ERR(lp->gds_gt_rx_dpath)) {
+			dev_err(&pdev->dev,
+				"Failed to request GT Rx dpath GPIO\n");
+			ret = PTR_ERR(lp->gds_gt_rx_dpath);
+			goto cleanup_clk;
+		}
+
+		lp->gds_gt_tx_dpath = devm_gpiod_get_array(&pdev->dev,
+							   "gt_tx_dpath",
+							   GPIOD_OUT_LOW);
+		if (IS_ERR(lp->gds_gt_tx_dpath)) {
+			dev_err(&pdev->dev,
+				"Failed to request GT Tx dpath GPIO\n");
+			ret = PTR_ERR(lp->gds_gt_tx_dpath);
+			goto cleanup_clk;
+		}
+
+		lp->gds_gt_rsts = devm_gpiod_get_array(&pdev->dev,
+						       "gt_rsts",
+						       GPIOD_OUT_LOW);
+		if (IS_ERR(lp->gds_gt_rsts)) {
+			dev_err(&pdev->dev,
+				"Failed to request GT Resets GPIO\n");
+			ret = PTR_ERR(lp->gds_gt_rsts);
+			goto cleanup_clk;
+		}
+
+		lp->gds_gt_tx_reset_done =  devm_gpiod_get_array(&pdev->dev,
+								 "gt_tx_rst_done",
+								 GPIOD_IN);
+		if (IS_ERR(lp->gds_gt_tx_reset_done)) {
+			dev_err(&pdev->dev,
+				"Failed to request GT Tx Reset Done GPIO\n");
+			ret = PTR_ERR(lp->gds_gt_tx_reset_done);
+			goto cleanup_clk;
+		}
+
+		lp->gds_gt_rx_reset_done =  devm_gpiod_get_array(&pdev->dev,
+								 "gt_rx_rst_done",
+								 GPIOD_IN);
+		if (IS_ERR(lp->gds_gt_rx_reset_done)) {
+			dev_err(&pdev->dev,
+				"Failed to request GT Rx Reset Done GPIO\n");
+			ret = PTR_ERR(lp->gds_gt_rx_reset_done);
+			goto cleanup_clk;
+		}
+	}
+
+	if (!of_property_present(pdev->dev.of_node, "dmas")) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		ret = axienet_mcdma_probe(pdev, lp, ndev);
+#else
+		ret = axienet_dma_probe(pdev, ndev);
+#endif
 		if (ret)
 			goto cleanup_clk;
+		if (lp->axienet_config->mactype == XAXIENET_1_2p5G &&
+		    !lp->eth_hasnobuf)
+			lp->eth_irq = platform_get_irq(pdev, 0);
+
+		/* Check for Ethernet core IRQ (optional) */
+		if (lp->eth_irq <= 0)
+			dev_info(&pdev->dev, "Ethernet core IRQ not defined\n");
+
+		if (dma_set_mask_and_coherent(lp->dev, DMA_BIT_MASK(lp->dma_mask)) != 0) {
+			dev_warn(&pdev->dev, "default to %d-bit dma mask\n", XAE_DMA_MASK_MIN);
+			if (dma_set_mask_and_coherent(lp->dev,
+						      DMA_BIT_MASK(XAE_DMA_MASK_MIN)) != -3) {
+				dev_err(&pdev->dev, "dma_set_mask_and_coherent failed, aborting\n");
+				goto cleanup_clk;
+			}
+		}
+
+		ret = axienet_dma_clk_init(pdev);
+		if (ret) {
+			if (ret != -EPROBE_DEFER)
+				dev_err(&pdev->dev, "DMA clock init failed %d\n", ret);
+			goto cleanup_clk;
+		}
+
+		ret = axienet_clk_init(pdev, &lp->aclk, &lp->eth_sclk,
+				       &lp->eth_refclk, &lp->eth_dclk);
+		if (ret) {
+			if (ret != -EPROBE_DEFER)
+				dev_err(&pdev->dev, "Ethernet clock init failed %d\n", ret);
+			goto err_disable_clk;
+		}
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		/* Create sysfs file entries for the device */
+		ret = axeinet_mcdma_create_sysfs(&lp->dev->kobj);
+		if (ret < 0) {
+			dev_err(lp->dev, "unable to create sysfs entries\n");
+			goto err_disable_clk;
+		}
+#endif
 
 		/* Autodetect the need for 64-bit DMA pointers.
 		 * When the IP is configured for a bus width bigger than 32 bits,
@@ -2774,34 +5276,99 @@ static int axienet_probe(struct platform_device *pdev)
 		 * only, those registers are RES0.
 		 * Those MSB registers were introduced in IP v7.1, which we check first.
 		 */
-		if ((axienet_ior(lp, XAE_ID_OFFSET) >> 24) >= 0x9) {
-			void __iomem *desc = lp->dma_regs + XAXIDMA_TX_CDESC_OFFSET + 4;
-
-			iowrite32(0x0, desc);
-			if (ioread32(desc) == 0) {	/* sanity check */
-				iowrite32(0xffffffff, desc);
-				if (ioread32(desc) > 0) {
-					lp->features |= XAE_FEATURE_DMA_64BIT;
-					addr_width = 64;
-					dev_info(&pdev->dev,
-						 "autodetected 64-bit DMA range\n");
-				}
+		if (lp->axienet_config->mactype == XAXIENET_1_2p5G) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			if (lp->dma_mask > XAE_DMA_MASK_MIN)
+				lp->features |= XAE_FEATURE_DMA_64BIT;
+#else
+			if ((axienet_ior(lp, XAE_ID_OFFSET) >> 24) >= 0x9) {
+				void __iomem *desc = lp->dq[0]->dma_regs +
+						     XAXIDMA_TX_CDESC_OFFSET + 4;
 				iowrite32(0x0, desc);
+				if (ioread32(desc) == 0) {	/* sanity check */
+					iowrite32(0xffffffff, desc);
+					if (ioread32(desc) > 0) {
+						lp->features |= XAE_FEATURE_DMA_64BIT;
+						dev_info(&pdev->dev,
+							 "autodetected 64-bit DMA range\n");
+					}
+					iowrite32(0x0, desc);
+				}
 			}
+			if (!IS_ENABLED(CONFIG_64BIT) && lp->features & XAE_FEATURE_DMA_64BIT) {
+				dev_err(&pdev->dev, "64-bit addressable DMA is not compatible with 32-bit archecture\n");
+				ret = -EINVAL;
+				goto err_disable_clk;
+			}
+#endif
+		} else if (lp->dma_mask > XAE_DMA_MASK_MIN) {
+			/* High speed MACs with 64-bit DMA */
+			lp->features |= XAE_FEATURE_DMA_64BIT;
 		}
-		if (!IS_ENABLED(CONFIG_64BIT) && lp->features & XAE_FEATURE_DMA_64BIT) {
-			dev_err(&pdev->dev, "64-bit addressable DMA is not compatible with 32-bit archecture\n");
-			ret = -EINVAL;
-			goto cleanup_clk;
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+		/* Find AXI Stream FIFO */
+		np = of_parse_phandle(pdev->dev.of_node, "axififo-connected", 0);
+		if (IS_ERR(np)) {
+			dev_err(&pdev->dev, "could not find TX Timestamp FIFO\n");
+			ret = PTR_ERR(np);
+			goto err_disable_clk;
 		}
 
-		ret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(addr_width));
+		ret = of_address_to_resource(np, 0, &txtsres);
 		if (ret) {
-			dev_err(&pdev->dev, "No suitable DMA available\n");
-			goto cleanup_clk;
+			dev_err(&pdev->dev, "unable to get Tx Timestamp resource\n");
+			goto err_disable_clk;
+		}
+
+		lp->tx_ts_regs = devm_ioremap_resource(&pdev->dev, &txtsres);
+		if (IS_ERR(lp->tx_ts_regs)) {
+			dev_err(&pdev->dev, "could not map Tx Timestamp regs\n");
+			ret = PTR_ERR(lp->tx_ts_regs);
+			goto err_disable_clk;
+		}
+
+		if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		    lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			np = of_parse_phandle(pdev->dev.of_node, "xlnx,rxtsfifo",
+					      0);
+			if (IS_ERR(np)) {
+				dev_err(&pdev->dev,
+					"couldn't find rx-timestamp FIFO\n");
+				ret = PTR_ERR(np);
+				goto err_disable_clk;
+			}
+
+			ret = of_address_to_resource(np, 0, &rxtsres);
+			if (ret) {
+				dev_err(&pdev->dev,
+					"unable to get rx-timestamp resource\n");
+				goto err_disable_clk;
+			}
+
+			lp->rx_ts_regs = devm_ioremap_resource(&pdev->dev, &rxtsres);
+			if (IS_ERR(lp->rx_ts_regs)) {
+				dev_err(&pdev->dev, "couldn't map rx-timestamp regs\n");
+				ret = PTR_ERR(lp->rx_ts_regs);
+				goto err_disable_clk;
+			}
+			lp->tx_ptpheader = devm_kzalloc(&pdev->dev,
+							XXVENET_TS_HEADER_LEN,
+							GFP_KERNEL);
+		}
+		spin_lock_init(&lp->ptp_tx_lock);
+		of_node_put(np);
+#endif
+		lp->eoe_connected = of_property_read_bool(pdev->dev.of_node,
+							  "xlnx,has-hw-offload");
+
+		if (lp->eoe_connected) {
+			ret = axienet_eoe_probe(pdev);
+			if (ret) {
+				dev_err(&pdev->dev, "Ethernet Offload not Supported\n");
+				goto cleanup_clk;
+			}
 		}
-		netif_napi_add(ndev, &lp->napi_rx, axienet_rx_poll);
-		netif_napi_add(ndev, &lp->napi_tx, axienet_tx_poll);
 	} else {
 		struct xilinx_vdma_config cfg;
 		struct dma_chan *tx_chan;
@@ -2835,6 +5402,8 @@ static int axienet_probe(struct platform_device *pdev)
 		ndev->netdev_ops = &axienet_netdev_dmaengine_ops;
 	else
 		ndev->netdev_ops = &axienet_netdev_ops;
+
+	lp->eth_irq = platform_get_irq(pdev, 0);
 	/* Check for Ethernet core IRQ (optional) */
 	if (lp->eth_irq <= 0)
 		dev_info(&pdev->dev, "Ethernet core IRQ not defined\n");
@@ -2848,32 +5417,53 @@ static int axienet_probe(struct platform_device *pdev)
 			 ret);
 		axienet_set_mac_address(ndev, NULL);
 	}
+	if (!lp->use_dmaengine) {
+		lp->coalesce_count_rx = XAXIDMA_DFT_RX_THRESHOLD;
+		lp->coalesce_count_tx = XAXIDMA_DFT_TX_THRESHOLD;
+		lp->coalesce_usec_rx = XAXIDMA_DFT_RX_USEC;
+		lp->coalesce_usec_tx = XAXIDMA_DFT_TX_USEC;
+
+		/* Set the TX coalesce count to 1. With offload enabled, there are not as
+		 * many interrupts as before and the interrupt for every 64KB segment needs
+		 * to be handled immediately to ensure better performance.
+		 */
+		if (ndev->hw_features & NETIF_F_GSO_UDP_L4)
+			lp->coalesce_count_tx = XMCDMA_DFT_TX_THRESHOLD;
 
-	lp->coalesce_count_rx = XAXIDMA_DFT_RX_THRESHOLD;
-	lp->coalesce_count_tx = XAXIDMA_DFT_TX_THRESHOLD;
-	lp->coalesce_usec_rx = XAXIDMA_DFT_RX_USEC;
-	lp->coalesce_usec_tx = XAXIDMA_DFT_TX_USEC;
-
-	ret = axienet_mdio_setup(lp);
-	if (ret)
-		dev_warn(&pdev->dev,
-			 "error registering MDIO bus: %d\n", ret);
-
-	if (lp->phy_mode == PHY_INTERFACE_MODE_SGMII ||
-	    lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX) {
+		/* Update the required thresholds for Rx HW UDP GRO
+		 * GRO receives 16 segmented data packets from MAC
+		 * and packet coalescing increases performance.
+		 */
+		if (lp->eoe_features & RX_HW_UDP_GRO)
+			lp->coalesce_count_rx = XMCDMA_DFT_RX_THRESHOLD;
+	}
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_1G_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC &&
+	    lp->axienet_config->mactype != XAXIENET_DCMAC) {
 		np = of_parse_phandle(pdev->dev.of_node, "pcs-handle", 0);
 		if (!np) {
-			/* Deprecated: Always use "pcs-handle" for pcs_phy.
-			 * Falling back to "phy-handle" here is only for
-			 * backward compatibility with old device trees.
+			/* For SGMII/1000BaseX:
+			 * "phy-handle" is deprecated; always use "pcs-handle"
+			 * for pcs_phy. Falling back to "phy-handle" here is
+			 * only for backward compatiblility with old device trees"
+			 * For RGMII:
+			 * "phy-handle" is used to describe external PHY.
 			 */
 			np = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
 		}
 		if (!np) {
-			dev_err(&pdev->dev, "pcs-handle (preferred) or phy-handle required for 1000BaseX/SGMII\n");
+			dev_err(&pdev->dev, "pcs-handle (preferred for 1000BaseX/SGMII) or phy-handle required for external PHY\n");
 			ret = -EINVAL;
 			goto cleanup_mdio;
 		}
+		if (np) {
+			ret = axienet_mdio_setup(lp);
+			if (ret)
+				dev_warn(&pdev->dev,
+					 "error registering MDIO bus: %d\n", ret);
+		}
+
 		lp->pcs_phy = of_mdio_find_device(np);
 		if (!lp->pcs_phy) {
 			ret = -EPROBE_DEFER;
@@ -2915,8 +5505,25 @@ static int axienet_probe(struct platform_device *pdev)
 		goto cleanup_phylink;
 	}
 
+	/* Register notifier for inet address additions/deletions.
+	 * It should be called after register_netdev to access the interface's
+	 * network configuration parameters.
+	 */
+
+	if (lp->eoe_features & RX_HW_UDP_GRO) {
+		lp->inetaddr_notifier.notifier_call = axienet_eoe_netdev_event;
+		ret = register_inetaddr_notifier(&lp->inetaddr_notifier);
+		if (ret) {
+			dev_err(lp->dev, "register_netdevice_notifier() error\n");
+			goto err_unregister_netdev;
+		}
+	}
+
 	return 0;
 
+err_unregister_netdev:
+	unregister_netdev(ndev);
+
 cleanup_phylink:
 	phylink_destroy(lp->phylink);
 
@@ -2925,6 +5532,9 @@ static int axienet_probe(struct platform_device *pdev)
 		put_device(&lp->pcs_phy->dev);
 	if (lp->mii_bus)
 		axienet_mdio_teardown(lp);
+err_disable_clk:
+	axienet_clk_disable(pdev);
+
 cleanup_clk:
 	clk_bulk_disable_unprepare(XAE_NUM_MISC_CLOCKS, lp->misc_clks);
 	clk_disable_unprepare(lp->axi_clk);
@@ -2939,8 +5549,18 @@ static void axienet_remove(struct platform_device *pdev)
 {
 	struct net_device *ndev = platform_get_drvdata(pdev);
 	struct axienet_local *lp = netdev_priv(ndev);
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		netif_napi_del(&lp->dq[i]->napi_rx);
+		netif_napi_del(&lp->dq[i]->napi_tx);
+	}
+
+	if (lp->eoe_features & RX_HW_UDP_GRO)
+		unregister_inetaddr_notifier(&lp->inetaddr_notifier);
 
 	unregister_netdev(ndev);
+	axienet_clk_disable(pdev);
 
 	if (lp->phylink)
 		phylink_destroy(lp->phylink);
@@ -2948,10 +5568,14 @@ static void axienet_remove(struct platform_device *pdev)
 	if (lp->pcs_phy)
 		put_device(&lp->pcs_phy->dev);
 
-	axienet_mdio_teardown(lp);
+	if (lp->mii_bus)
+		axienet_mdio_teardown(lp);
 
 	clk_bulk_disable_unprepare(XAE_NUM_MISC_CLOCKS, lp->misc_clks);
 	clk_disable_unprepare(lp->axi_clk);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	axeinet_mcdma_remove_sysfs(&lp->dev->kobj);
+#endif
 
 	free_netdev(ndev);
 }
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_mcdma.c b/drivers/net/ethernet/xilinx/xilinx_axienet_mcdma.c
new file mode 100644
index 000000000..d2793e0a5
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_mcdma.c
@@ -0,0 +1,1096 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx AXI Ethernet (MCDMA programming)
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2010 - 2012 Xilinx, Inc.
+ * Copyright (C) 2018 - 2022 Xilinx, Inc. All rights reserved.
+ * Copyright (c) 2022 Advanced Micro Devices, Inc.
+ *
+ * This file contains helper functions for AXI MCDMA TX and RX programming.
+ */
+
+#include <linux/module.h>
+#include <linux/of_mdio.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/platform_device.h>
+
+#include "xilinx_axienet.h"
+#include "xilinx_axienet_eoe.h"
+
+struct axienet_stat {
+	const char *name;
+};
+
+static struct axienet_stat axienet_get_tx_strings_stats[] = {
+	{ "txq0_packets" },
+	{ "txq0_bytes"   },
+	{ "txq1_packets" },
+	{ "txq1_bytes"   },
+	{ "txq2_packets" },
+	{ "txq2_bytes"   },
+	{ "txq3_packets" },
+	{ "txq3_bytes"   },
+	{ "txq4_packets" },
+	{ "txq4_bytes"   },
+	{ "txq5_packets" },
+	{ "txq5_bytes"   },
+	{ "txq6_packets" },
+	{ "txq6_bytes"   },
+	{ "txq7_packets" },
+	{ "txq7_bytes"   },
+	{ "txq8_packets" },
+	{ "txq8_bytes"   },
+	{ "txq9_packets" },
+	{ "txq9_bytes"   },
+	{ "txq10_packets" },
+	{ "txq10_bytes"   },
+	{ "txq11_packets" },
+	{ "txq11_bytes"   },
+	{ "txq12_packets" },
+	{ "txq12_bytes"   },
+	{ "txq13_packets" },
+	{ "txq13_bytes"   },
+	{ "txq14_packets" },
+	{ "txq14_bytes"   },
+	{ "txq15_packets" },
+	{ "txq15_bytes"   },
+};
+
+static struct axienet_stat axienet_get_rx_strings_stats[] = {
+	{ "rxq0_packets" },
+	{ "rxq0_bytes"   },
+	{ "rxq1_packets" },
+	{ "rxq1_bytes"   },
+	{ "rxq2_packets" },
+	{ "rxq2_bytes"   },
+	{ "rxq3_packets" },
+	{ "rxq3_bytes"   },
+	{ "rxq4_packets" },
+	{ "rxq4_bytes"   },
+	{ "rxq5_packets" },
+	{ "rxq5_bytes"   },
+	{ "rxq6_packets" },
+	{ "rxq6_bytes"   },
+	{ "rxq7_packets" },
+	{ "rxq7_bytes"   },
+	{ "rxq8_packets" },
+	{ "rxq8_bytes"   },
+	{ "rxq9_packets" },
+	{ "rxq9_bytes"   },
+	{ "rxq10_packets" },
+	{ "rxq10_bytes"   },
+	{ "rxq11_packets" },
+	{ "rxq11_bytes"   },
+	{ "rxq12_packets" },
+	{ "rxq12_bytes"   },
+	{ "rxq13_packets" },
+	{ "rxq13_bytes"   },
+	{ "rxq14_packets" },
+	{ "rxq14_bytes"   },
+	{ "rxq15_packets" },
+	{ "rxq15_bytes"   },
+};
+
+/**
+ * axienet_mcdma_tx_bd_free - Release MCDMA Tx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_tx_q_init.
+ */
+void __maybe_unused axienet_mcdma_tx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (q->txq_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+				  q->txq_bd_v,
+				  q->tx_bd_p);
+	}
+	if (q->tx_bufs) {
+		dma_free_coherent(ndev->dev.parent,
+				  XAE_MAX_PKT_LEN * lp->tx_bd_num,
+				  q->tx_bufs,
+				  q->tx_bufs_dma);
+	}
+}
+
+/**
+ * axienet_mcdma_rx_bd_free - Release MCDMA Rx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_rx_q_init.
+ */
+void __maybe_unused axienet_mcdma_rx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t phys;
+	int i;
+
+	if (!q->rxq_bd_v)
+		return;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		phys = mcdma_desc_get_phys_addr(lp, &q->rxq_bd_v[i]);
+		if (phys)
+			dma_unmap_single(ndev->dev.parent, phys,
+					 lp->max_frm_size, DMA_FROM_DEVICE);
+		dev_kfree_skb((struct sk_buff *)
+			      (q->rxq_bd_v[i].sw_id_offset));
+	}
+
+	dma_free_coherent(ndev->dev.parent,
+			  sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+			  q->rxq_bd_v,
+			  q->rx_bd_p);
+	q->rxq_bd_v = NULL;
+}
+
+/**
+ * axienet_mcdma_tx_q_init - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Tx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_tx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+
+	q->txq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+					 &q->tx_bd_p, GFP_KERNEL);
+	if (!q->txq_bd_v)
+		goto out;
+
+	if (!q->eth_hasdre) {
+		q->tx_bufs = dma_alloc_coherent(ndev->dev.parent,
+						XAE_MAX_PKT_LEN * lp->tx_bd_num,
+						&q->tx_bufs_dma,
+						GFP_KERNEL);
+		if (!q->tx_bufs)
+			goto out;
+
+		for (i = 0; i < lp->tx_bd_num; i++)
+			q->tx_buf[i] = &q->tx_bufs[i * XAE_MAX_PKT_LEN];
+	}
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		q->txq_bd_v[i].next = q->tx_bd_p +
+				      sizeof(*q->txq_bd_v) *
+				      ((i + 1) % lp->tx_bd_num);
+	}
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Only set interrupt delay timer if not generating an interrupt on
+	 * the first TX packet. Otherwise leave at 0 to disable delay interrupt.
+	 */
+	if (lp->coalesce_count_tx > 1)
+		cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_tx)
+		       << XAXIDMA_DELAY_SHIFT) | XAXIDMA_IRQ_DELAY_MASK;
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	return 0;
+out:
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+/**
+ * axienet_mcdma_rx_q_init - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Rx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_rx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct sk_buff *skb;
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t mapping;
+	int ret;
+
+	q->rx_bd_ci = 0;
+	q->rx_offset = XMCDMA_CHAN_RX_OFFSET;
+
+	q->rxq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+					 &q->rx_bd_p, GFP_KERNEL);
+	if (!q->rxq_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		q->rxq_bd_v[i].next = q->rx_bd_p +
+				      sizeof(*q->rxq_bd_v) *
+				      ((i + 1) % lp->rx_bd_num);
+
+		if (axienet_eoe_is_channel_gro(lp, q)) {
+			ret = axienet_eoe_mcdma_gro_q_init(ndev, q, i);
+			if (ret)
+				goto out;
+		} else {
+			skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+			if (!skb)
+				goto out;
+
+			/* Ensure that the skb is completely updated
+			 * prior to mapping the DMA
+			 */
+			wmb();
+
+			q->rxq_bd_v[i].sw_id_offset = skb;
+			mapping = dma_map_single(ndev->dev.parent,
+						 skb->data,
+						 lp->max_frm_size,
+						 DMA_FROM_DEVICE);
+			if (unlikely(dma_mapping_error(ndev->dev.parent, mapping))) {
+				dev_err(&ndev->dev, "mcdma map error\n");
+				goto out;
+			}
+
+			mcdma_desc_set_phys_addr(lp, mapping, &q->rxq_bd_v[i]);
+			q->rxq_bd_v[i].cntrl = lp->max_frm_size;
+		}
+	}
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Only set interrupt delay timer if not generating an interrupt on
+	 * the first RX packet. Otherwise leave at 0 to disable delay interrupt.
+	 */
+	if (lp->coalesce_count_rx > 1)
+		cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_rx)
+		       << XAXIDMA_DELAY_SHIFT) | XAXIDMA_IRQ_DELAY_MASK;
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	return 0;
+
+out:
+	for_each_rx_dma_queue(lp, i) {
+		if (axienet_eoe_is_channel_gro(lp, lp->dq[i]))
+			axienet_eoe_mcdma_gro_bd_free(ndev, lp->dq[i]);
+		else
+			axienet_mcdma_rx_bd_free(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+static inline int get_mcdma_tx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_tx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int get_mcdma_rx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int map_dma_q_txirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_TXINT_SER_OFFSET);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+	     i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	u32 cr;
+	unsigned int status;
+	int i, j = map_dma_q_txirq(irq, lp);
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_tx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id));
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id), status);
+		/* Disable further TX completion interrupts and schedule
+		 * NAPI to handle the completions.
+		 */
+		cr =  axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		cr &= ~(XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		if (napi_schedule_prep(&q->napi_tx)) {
+			axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+			__napi_schedule(&q->napi_tx);
+		}
+		goto out;
+	}
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Tx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: 0x%x%08x\n",
+			q->txq_bd_v[q->tx_bd_ci].phys_msb,
+			q->txq_bd_v[q->tx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+out:
+	return IRQ_HANDLED;
+}
+
+static inline int map_dma_q_rxirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_RXINT_SER_OFFSET +
+					q->rx_offset);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+		i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i, j = map_dma_q_rxirq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_rx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		cr &= ~(XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+		napi_schedule(&q->napi_rx);
+	}
+
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Rx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: 0x%x%08x\n",
+			q->rxq_bd_v[q->rx_bd_ci].phys_msb,
+			q->rxq_bd_v[q->rx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+
+	return IRQ_HANDLED;
+}
+
+void axienet_strings(struct net_device *ndev, u32 sset, u8 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i = AXIENET_ETHTOOLS_SSTATS_LEN, j, k = 0;
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+		q = lp->dq[j];
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_tx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+	k = 0;
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+		q = lp->dq[j];
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_rx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+}
+
+int axienet_sset_count(struct net_device *ndev, int sset)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	switch (sset) {
+	case ETH_SS_STATS:
+		return (AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+void axienet_get_stats(struct net_device *ndev,
+		       struct ethtool_stats *stats,
+		       u64 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	unsigned int i = AXIENET_ETHTOOLS_SSTATS_LEN, j;
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+
+		q = lp->dq[j];
+		data[i++] = q->txq_packets;
+		data[i++] = q->txq_bytes;
+		++j;
+	}
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+
+		q = lp->dq[j];
+		data[i++] = q->rxq_packets;
+		data[i++] = q->rxq_bytes;
+		++j;
+	}
+}
+
+/**
+ * axienet_mcdma_err_handler - Tasklet handler for Axi MCDMA Error
+ * @data:	Data passed
+ *
+ * Resets the Axi MCDMA and Axi Ethernet devices, and reconfigures the
+ * Tx/Rx BDs.
+ */
+void __maybe_unused axienet_mcdma_err_handler(unsigned long data)
+{
+	struct axienet_dma_q *q = (struct axienet_dma_q *)data;
+	struct axienet_local *lp = q->lp;
+	struct net_device *ndev = lp->ndev;
+	struct aximcdma_bd *cur_p;
+	dma_addr_t phys;
+	u32 axienet_status;
+	u32 cr, i, chan_en;
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	/* When we do an Axi Ethernet reset, it resets the complete core
+	 * including the MDIO. MDIO must be disabled before resetting.
+	 * Hold MDIO bus lock to avoid MDIO accesses during the reset.
+	 */
+	axienet_lock_mii(lp);
+	__axienet_device_reset(q);
+	axienet_unlock_mii(lp);
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		cur_p = &q->txq_bd_v[i];
+		phys = mcdma_desc_get_phys_addr(lp, cur_p);
+		if (phys)
+			dma_unmap_single(ndev->dev.parent, phys,
+					 (cur_p->cntrl &
+					  XAXIDMA_BD_CTRL_LENGTH_MASK),
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+		cur_p->phys_msb = 0;
+		cur_p->phys = 0;
+		cur_p->cntrl = 0;
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+		cur_p->sw_id_offset = 0;
+		cur_p->tx_skb = 0;
+	}
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		cur_p = &q->rxq_bd_v[i];
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+	}
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+	q->rx_bd_ci = 0;
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Only set interrupt delay timer if not generating an interrupt on
+	 * the first RX packet. Otherwise leave at 0 to disable delay interrupt.
+	 */
+	if (lp->coalesce_count_rx > 1)
+		cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_rx)
+		       << XAXIDMA_DELAY_SHIFT) | XAXIDMA_IRQ_DELAY_MASK;
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Only set interrupt delay timer if not generating an interrupt on
+	 * the first TX packet. Otherwise leave at 0 to disable delay interrupt.
+	 */
+	if (lp->coalesce_count_tx > 1)
+		cr |= (axienet_usec_to_timer(lp, lp->coalesce_usec_tx)
+		       << XAXIDMA_DELAY_SHIFT) | XAXIDMA_IRQ_DELAY_MASK;
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1_2p5G &&
+	    !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+	}
+#endif
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	axienet_set_mac_address(ndev, NULL);
+	axienet_set_multicast_list(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+}
+
+int __maybe_unused axienet_mcdma_tx_probe(struct platform_device *pdev,
+					  struct device_node *np,
+					  struct axienet_local *lp)
+{
+	int i;
+	char dma_name[24];
+
+	for_each_tx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "mm2s_ch%d_introut",
+			 q->chan_id);
+		q->tx_irq = platform_get_irq_byname(pdev, dma_name);
+		q->eth_hasdre = of_property_read_bool(np,
+						      "xlnx,include-dre");
+		netif_napi_add(lp->ndev, &q->napi_tx, axienet_tx_poll);
+		spin_lock_init(&q->tx_lock);
+	}
+	of_node_put(np);
+
+	return 0;
+}
+
+int __maybe_unused axienet_mcdma_rx_probe(struct platform_device *pdev,
+					  struct axienet_local *lp,
+					  struct net_device *ndev)
+{
+	int i;
+	char dma_name[24];
+
+	for_each_rx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "s2mm_ch%d_introut",
+			 q->chan_id);
+		q->rx_irq = platform_get_irq_byname(pdev, dma_name);
+
+		netif_napi_add(lp->ndev, &q->napi_rx, xaxienet_rx_poll);
+	}
+
+	return 0;
+}
+
+static ssize_t rxch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 2 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 3 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 4 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 5 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t txch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 2 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 3 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 4 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 5 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t chan_weight_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	return sprintf(buf, "chan_id is %d and weight is %d\n",
+		       lp->chan_id, lp->weight);
+}
+
+static ssize_t chan_weight_store(struct device *dev,
+				 struct device_attribute *attr,
+				 const char *buf, size_t count)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	int ret;
+	u16 flags, chan_id;
+	u32 val;
+
+	ret = kstrtou16(buf, 16, &flags);
+	if (ret)
+		return ret;
+
+	lp->chan_id = (flags & 0xF0) >> 4;
+	lp->weight = flags & 0x0F;
+
+	if (lp->chan_id < 8)
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT0_OFFSET);
+	else
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT1_OFFSET);
+
+	if (lp->chan_id > 7)
+		chan_id = lp->chan_id - 8;
+	else
+		chan_id = lp->chan_id;
+
+	val &= ~XMCDMA_TXWEIGHT_CH_MASK(chan_id);
+	val |= lp->weight << XMCDMA_TXWEIGHT_CH_SHIFT(chan_id);
+
+	if (lp->chan_id < 8)
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT0_OFFSET, val);
+	else
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT1_OFFSET, val);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(chan_weight);
+static DEVICE_ATTR_RO(rxch_obs1);
+static DEVICE_ATTR_RO(rxch_obs2);
+static DEVICE_ATTR_RO(rxch_obs3);
+static DEVICE_ATTR_RO(rxch_obs4);
+static DEVICE_ATTR_RO(rxch_obs5);
+static DEVICE_ATTR_RO(rxch_obs6);
+static DEVICE_ATTR_RO(txch_obs1);
+static DEVICE_ATTR_RO(txch_obs2);
+static DEVICE_ATTR_RO(txch_obs3);
+static DEVICE_ATTR_RO(txch_obs4);
+static DEVICE_ATTR_RO(txch_obs5);
+static DEVICE_ATTR_RO(txch_obs6);
+static const struct attribute *mcdma_attrs[] = {
+	&dev_attr_chan_weight.attr,
+	&dev_attr_rxch_obs1.attr,
+	&dev_attr_rxch_obs2.attr,
+	&dev_attr_rxch_obs3.attr,
+	&dev_attr_rxch_obs4.attr,
+	&dev_attr_rxch_obs5.attr,
+	&dev_attr_rxch_obs6.attr,
+	&dev_attr_txch_obs1.attr,
+	&dev_attr_txch_obs2.attr,
+	&dev_attr_txch_obs3.attr,
+	&dev_attr_txch_obs4.attr,
+	&dev_attr_txch_obs5.attr,
+	&dev_attr_txch_obs6.attr,
+	NULL,
+};
+
+static const struct attribute_group mcdma_attributes = {
+	.attrs = (struct attribute **)mcdma_attrs,
+};
+
+int axeinet_mcdma_create_sysfs(struct kobject *kobj)
+{
+	return sysfs_create_group(kobj, &mcdma_attributes);
+}
+
+void axeinet_mcdma_remove_sysfs(struct kobject *kobj)
+{
+	sysfs_remove_group(kobj, &mcdma_attributes);
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_emaclite.c b/drivers/net/ethernet/xilinx/xilinx_emaclite.c
index 258096543..10d6f19ad 100644
--- a/drivers/net/ethernet/xilinx/xilinx_emaclite.c
+++ b/drivers/net/ethernet/xilinx/xilinx_emaclite.c
@@ -7,6 +7,7 @@
  * Copyright (c) 2007 - 2013 Xilinx, Inc.
  */
 
+#include <linux/clk.h>
 #include <linux/module.h>
 #include <linux/platform_device.h>
 #include <linux/uaccess.h>
@@ -1091,13 +1092,14 @@ static int xemaclite_of_probe(struct platform_device *ofdev)
 	struct net_device *ndev = NULL;
 	struct net_local *lp = NULL;
 	struct device *dev = &ofdev->dev;
+	struct clk *clkin;
 
 	int rc = 0;
 
 	dev_info(dev, "Device Tree Probing\n");
 
 	/* Create an ethernet device instance */
-	ndev = alloc_etherdev(sizeof(struct net_local));
+	ndev = devm_alloc_etherdev(dev, sizeof(struct net_local));
 	if (!ndev)
 		return -ENOMEM;
 
@@ -1110,15 +1112,13 @@ static int xemaclite_of_probe(struct platform_device *ofdev)
 	/* Get IRQ for the device */
 	rc = platform_get_irq(ofdev, 0);
 	if (rc < 0)
-		goto error;
+		return rc;
 
 	ndev->irq = rc;
 
 	lp->base_addr = devm_platform_get_and_ioremap_resource(ofdev, 0, &res);
-	if (IS_ERR(lp->base_addr)) {
-		rc = PTR_ERR(lp->base_addr);
-		goto error;
-	}
+	if (IS_ERR(lp->base_addr))
+		return PTR_ERR(lp->base_addr);
 
 	ndev->mem_start = res->start;
 	ndev->mem_end = res->end;
@@ -1129,6 +1129,11 @@ static int xemaclite_of_probe(struct platform_device *ofdev)
 	lp->tx_ping_pong = get_bool(ofdev, "xlnx,tx-ping-pong");
 	lp->rx_ping_pong = get_bool(ofdev, "xlnx,rx-ping-pong");
 
+	clkin = devm_clk_get_optional_enabled(&ofdev->dev, NULL);
+	if (IS_ERR(clkin))
+		return dev_err_probe(&ofdev->dev, PTR_ERR(clkin),
+				"Failed to get and enable clock from Device Tree\n");
+
 	rc = of_get_ethdev_address(ofdev->dev.of_node, ndev);
 	if (rc) {
 		dev_warn(dev, "No MAC address found, using random\n");
@@ -1167,8 +1172,6 @@ static int xemaclite_of_probe(struct platform_device *ofdev)
 
 put_node:
 	of_node_put(lp->phy_node);
-error:
-	free_netdev(ndev);
 	return rc;
 }
 
@@ -1197,8 +1200,6 @@ static void xemaclite_of_remove(struct platform_device *of_dev)
 
 	of_node_put(lp->phy_node);
 	lp->phy_node = NULL;
-
-	free_netdev(ndev);
 }
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
