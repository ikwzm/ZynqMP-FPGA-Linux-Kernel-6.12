diff --git a/Documentation/devicetree/bindings/staging/display/amd,mmi-dptx.yaml b/Documentation/devicetree/bindings/staging/display/amd,mmi-dptx.yaml
new file mode 100644
index 000000000..ec83fad95
--- /dev/null
+++ b/Documentation/devicetree/bindings/staging/display/amd,mmi-dptx.yaml
@@ -0,0 +1,182 @@
+# SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/staging/display/amd,mmi-dptx.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: AMD Multimedia Integrated Display Port Transmitter.
+
+description:
+  The AMD Multimedia Integrated DisplayPort Tx subsystem block implements the
+  DisplayPort transmitter and outputs video data using DisplayPort protocol.
+
+maintainers:
+  - Vishal Sagar <vishal.sagar@amd.com>
+
+properties:
+  compatible:
+    const: amd,mmi-dptx-1.0
+
+  reg:
+    items:
+      - description: dptx registers
+      - description: hdcp registers
+
+  reg-names:
+    items:
+      - const: dp
+      - const: hdcp
+
+  interrupts:
+    items:
+      - description: DP Tx event
+      - description: HDCP event
+
+  interrupt-names:
+    items:
+      - const: dptx
+      - const: hdcp
+
+  clocks:
+    items:
+      - description: APB Clock
+      - description: Video pixel clock
+      - description: Audio i2s clock
+
+  clock-names:
+    items:
+      - const: apb_clk
+      - const: pixel_clk
+      - const: i2s_clk
+
+  xlnx,hdcp-1x:
+    type: boolean
+    description: boolean present when HDCP version is 1.3.
+
+  xlnx,hdcp-2x:
+    type: boolean
+    description: boolean present when HDCP version is 2.3.
+
+  xlnx,dptx-streams:
+    description: number of input streams to DP Tx
+    $ref: /schemas/types.yaml#/definitions/uint32
+    enum: [1, 2, 4]
+
+  xlnx,dptx-pixel-mode:
+    description: Pixels per clock
+    $ref: /schemas/types.yaml#/definitions/uint32
+    enum: [1, 2, 4]
+
+  xlnx,dp-lanes:
+    description: Number of lanes for DP
+    $ref: /schemas/types.yaml#/definitions/uint32
+    enum: [1, 2, 4]
+
+  ports:
+    $ref: /schemas/graph.yaml#/properties/ports
+    description:
+      Connections to the Display Controller(DC) and DP monitor. Each port
+      shall have a single endpoint. Initial 4 are gets input from DC.
+      Next 1 is output port connected to DP monitor.
+
+    properties:
+      "#address-cells":
+        const: 1
+
+      "#size-cells":
+        const: 0
+
+    patternProperties:
+      "^port@([0-3])$":
+        type: object
+        description:
+          Input port number, describing input coming from DC.
+          port 0 is blended video output from DisplayPort Tx
+          port 1-3 are connected in bypass / MST mode only.
+
+        properties:
+          reg:
+            enum: [0, 1, 2, 3]
+
+          endpoint:
+            type: object
+
+            properties:
+              remote-endpoint: true
+
+            required:
+              - remote-endpoint
+
+            additionalProperties: false
+
+        required:
+          - reg
+          - endpoint
+
+        additionalProperties: false
+
+    required:
+      - "#address-cells"
+      - "#size-cells"
+      - port@0
+
+required:
+  - compatible
+  - reg
+  - reg-names
+  - interrupts
+  - interrupt-names
+  - ports
+
+additionalProperties: false
+
+examples:
+  - |
+    axi {
+        #address-cells = <2>;
+        #size-cells = <2>;
+
+        mmi_dptx@ede00000 {
+            compatible = "amd,mmi-dptx-1.0";
+            reg = <0x0 0xede00000 0x0 0x40000>, <0x0 0xedeb0000 0x0 0x100>;
+            reg-names = "dp", "hdcp";
+            interrupts = <0 178 4>, <0 167 4>;
+            interrupt-names = "dptx", "hdcp";
+            interrupt-parent = <&imux>;
+            clocks = <&mmi_apb_clk>, <&mmi_pixel_clk>, <&mmi_i2s_clk>;
+            clock-names = "apb_clk", "pixel_clk", "i2s_clk";
+
+            xlnx,dp-lanes = <2>;
+
+            ports {
+                #address-cells = <1>;
+                #size-cells = <0>;
+
+                port@0 {
+                    reg = <0>;
+                    dptx_in_0: endpoint {
+                        remote-endpoint = <&dc_out_0>;
+                    };
+                };
+                port@1 {
+                    reg = <1>;
+                    dptx_in_1: endpoint {
+                        remote-endpoint = <&dc_out_1>;
+                    };
+                };
+                port@2 {
+                    reg = <2>;
+                    dptx_in_2: endpoint {
+                        remote-endpoint = <&dc_out_2>;
+                    };
+                };
+                port@3 {
+                    reg = <3>;
+                    dptx_in_3: endpoint {
+                        remote-endpoint = <&dc_out_3>;
+                    };
+                };
+            };
+        };
+    };
+...
diff --git a/Documentation/devicetree/bindings/staging/net/xilinx-tsn-ethernet.txt b/Documentation/devicetree/bindings/staging/net/xilinx-tsn-ethernet.txt
new file mode 100644
index 000000000..e66b64bc1
--- /dev/null
+++ b/Documentation/devicetree/bindings/staging/net/xilinx-tsn-ethernet.txt
@@ -0,0 +1,54 @@
+Xilinx TSN (time sensitive networking) TEMAC axi ethernet driver (xilinx_axienet)
+-----------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ethernet-1.00.a".
+- reg			: Physical base address and size of the TSN registers map.
+- interrupts		: Property with a value describing the interrupt
+			  number.
+- interrupts-names	: Property denotes the interrupt names.
+- interrupt-parent	: Must be core interrupt controller.
+- phy-handle		: See ethernet.txt file [1].
+- local-mac-address	: See ethernet.txt file [1].
+- phy-mode		: see ethernet.txt file [1].
+
+Optional properties:
+- xlnx,tsn		: Denotes a ethernet with TSN capabilities.
+- xlnx,tsn-slave	: Denotes a TSN slave port.
+- xlnx,txcsum		: Tx checksum mode (Full, Partial and None).
+- xlnx,rxcsum		: Rx checksum mode (Full, Partial and None).
+- xlnx,phy-type		: Xilinx phy device type. See xilinx-phy.txt [2].
+- xlnx,eth-hasnobuf	: Used when 1G MAC is configured in non-processor mode.
+- xlnx,num-queue	: Number of queue supported in current design, range is
+			  2 to 5 and default value is 5.
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,qbv-addr		: Denotes mac scheduler physical base address.
+- xlnx,qbv-size		: Denotes mac scheduler address space size.
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+[2] Documentation/devicetree/bindings/net/xilinx-phy.txt
+
+Example:
+
+	tsn_emac_0: tsn_mac@80040000 {
+		compatible = "xlnx,tsn-ethernet-1.00.a";
+		interrupt-parent = <&gic>;
+		interrupts = <0 104 4 0 106 4 0 91 4 0 110 4>;
+		interrupt-names = "interrupt_ptp_rx_1", "interrupt_ptp_tx_1", "mac_irq_1", "interrupt_ptp_timer";
+		local-mac-address = [ 00 0A 35 00 01 0e ];
+		phy-mode = "rgmii";
+		reg = <0x0 0x80040000 0x0 0x14000>;
+		tsn,endpoint = <&tsn_ep>;
+		xlnx,tsn;
+		xlnx,tsn-slave;
+		xlnx,phy-type = <0x3>;
+		xlnx,eth-hasnobuf;
+		xlnx,num-queue = <0x2>;
+		xlnx,num-tc = <0x3>;
+		xlnx,qbv-addr = <0x80054000>;
+		xlnx,qbv-size = <0x2000>;
+		xlnx,txsum = <0>;
+		xlnx,rxsum = <0>;
+	};
diff --git a/Documentation/devicetree/bindings/staging/net/xilinx_tsn.txt b/Documentation/devicetree/bindings/staging/net/xilinx_tsn.txt
new file mode 100644
index 000000000..b715d7c1c
--- /dev/null
+++ b/Documentation/devicetree/bindings/staging/net/xilinx_tsn.txt
@@ -0,0 +1,17 @@
+Xilinx TSN (time sensitive networking) IP driver (xilinx_tsn_ip)
+-----------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be one of "xlnx,tsn-endpoint-ethernet-mac-1.0",
+			  "xlnx,tsn-endpoint-ethernet-mac-2.0" for TSN.
+- reg			: Physical base address and size of the TSN registers map.
+- ranges		: Specifies child address ranges of TSN IP subsystem including
+			  TEMACs, endpoint and switch. Leave this property as empty
+			  because parent node has the same mapping as all the child nodes.
+Example:
+
+	tsn_endpoint_ip_0: tsn_endpoint_ip_0 {
+		compatible = "xlnx,tsn-endpoint-ethernet-mac-2.0";
+		reg = <0x0 0x80040000 0x0 0x40000>;
+		ranges;
+	};
diff --git a/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep.txt b/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep.txt
new file mode 100644
index 000000000..7902c8fd5
--- /dev/null
+++ b/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep.txt
@@ -0,0 +1,95 @@
+Xilinx TSN (time sensitive networking) EndPoint Driver (xilinx_tsn_ep)
+-------------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ep"
+- reg			: Physical base address and size of the TSN Endpoint
+				registers map
+- interrupts		: Property with a value describing the interrupt
+- interrupts-names	: Property denotes the interrupt names.
+- interrupt-parent	: Must be core interrupt controller.
+- xlnx,tsn-tx-config	: Multiple TX Queue parameters. Phandle to a node that
+			  implements the 'tx-queues-config'.
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is [2, 8]. It denotes the traffic classes based
+			  on VLAN-PCP value.
+
+Optional properties:
+- xlnx,channel-ids 	: Queue Identifier associated with the MCDMA Channel, range
+			  is Tx: "1 to 2" and Rx: "2 to 5", default value is "1 to 5".
+- xlnx,eth-hasnobuf	: Used when 1G MAC is configured in non processor mode.
+- axistream-connected-rx	: Reference to another node which contains the
+				  resources for the MCDMA controller used by
+				  this device. The MCDMA related resources
+				  (registers and interrupts) will be used from
+				  this node.
+
+Required TX Queues config properties:
+- xlnx,num-tx-queues	: Number of TX queues to be used and it should match with
+			  the number of traffic classes supported.
+- xlnx,dma-channel-num	: DMA channel id connected to the TX queue.
+
+Optional TX Queues config properties:
+- xlnx,is-tadma		: Defined when the TX queue is connected with TADMA channel.
+
+Optional TADMA properties:
+- xlnx,num-buffers-per-stream	: Number of TADMA buffers per stream in design. Default is 64.
+- xlnx,num-streams		: Number of streams. Default is 8.
+- xlnx,num-fetch-entries	: Maximum number of entries that can be programmed to be
+				  fetched by TADMA. Default is 8.
+- axistream-connected-tx	: Reference to another node which contains the
+				  resources for the TADMA controller used by
+				  this device. The TADMA-related resources
+				  (registers and interrupts) will be used from
+				  this node.
+
+Optional MCDMA properties:
+- xlnx,num-mm2s-channels	: Number of MM2S(read) channels in MCDMA
+- xlnx,num-s2mm-channels	: Number of S2MM(write) channels in MCDMA
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+
+Example:
+
+	tsn_ep: tsn_ep@80056000 {
+		compatible = "xlnx,tsn-ep";
+		reg = <0x0 0x80056000 0x0 0xA000>;
+		xlnx,num-tc = <0x3>;
+		interrupt-names = "tsn_ep_scheduler_irq";
+		interrupt-parent = <&gic>;
+		interrupts = <0 111 4>;
+		local-mac-address = [00 0A 35 00 01 10];
+		xlnx,channel-ids = "1","2","3","4","5";
+		xlnx,eth-hasnobuf ;
+		xlnx,tsn-tx-config = <&tsn_tx_config>;
+	};
+
+	tsn_tx_config: tx-queues-config {
+		xlnx,num-tx-queues = <0x8>;
+		queue0 {
+			xlnx,dma-channel-num = <0x6>;
+		};
+		queue1 {
+			xlnx,dma-channel-num = <0x5>;
+		};
+		queue2 {
+			xlnx,dma-channel-num = <0x4>;
+		};
+		queue3 {
+			xlnx,dma-channel-num = <0x3>;
+		};
+		queue4 {
+			xlnx,dma-channel-num = <0x2>;
+		};
+		queue5 {
+			xlnx,dma-channel-num = <0x1>;
+		};
+		queue6 {
+			xlnx,is-tadma;
+			xlnx,dma-channel-num = <0x1>;
+		};
+		queue7 {
+			xlnx,is-tadma;
+			xlnx,dma-channel-num = <0x0>;
+		};
+	};
diff --git a/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep_ex.txt b/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep_ex.txt
new file mode 100644
index 000000000..b01e12e51
--- /dev/null
+++ b/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep_ex.txt
@@ -0,0 +1,31 @@
+Xilinx TSN (time sensitive networking) Extended EndPoint Driver (xilinx_tsn_ep_ex)
+-------------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ex-ep"
+- reg			: Physical base address and size of the TSN Endpoint
+				registers map
+- local-mac-address	: See ethernet.txt [1].
+
+Optional properties:
+- packet-switch		: set to 1 when packet switching on ex-ep is
+			  enabled in the design.
+			  This property deprecated.
+			  To identify the packet switching feature is enabled
+			  or not in the design, please use the
+			  xlnx, packet-switch property from the switch node
+			  instead of packet-switch property.
+- tsn,endpoint		: This is a handle to the endpoint node.
+			  The necessary ep resource details are obtained
+			  from this reference.
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+
+Example:
+
+	tsn_ep_ex: tsn_ep_ex@80056000 {
+		compatible = "xlnx,tsn-ex-ep";
+		reg = <0x0 0x80056000 0x0 0xA000>;
+		local-mac-address = [00 0A 35 00 01 20];
+		tsn,endpoint = <&tsn_ep>;
+	};
diff --git a/Documentation/devicetree/bindings/staging/net/xilinx_tsn_switch.txt b/Documentation/devicetree/bindings/staging/net/xilinx_tsn_switch.txt
new file mode 100644
index 000000000..96e62b6dc
--- /dev/null
+++ b/Documentation/devicetree/bindings/staging/net/xilinx_tsn_switch.txt
@@ -0,0 +1,27 @@
+Xilinx TSN (time sensitive networking) Switch Driver (xilinx_tsn_switch)
+-----------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-switch"
+- reg			: Physical base address and size of the TSN registers map.
+- xlnx,num-ports	: Number of network ports in subsystems. For ex., for an
+			  EP + Switch system to two TEMACs, this value should be 3
+			  (ep, temac1 and temac 2).
+
+Optional properties:
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,has-hwaddr-learning	: Denotes hardware address learning support
+- xlnx,has-inband-mgmt-tag	: Denotes inband management support
+- xlnx,packet-switch		: Denotes packet switching support.
+
+Example:
+
+	epswitch: tsn_switch@80078000 {
+		compatible = "xlnx,tsn-switch";
+		reg = <0x0 0x80078000 0x0 0x4000>;
+		xlnx,num-tc = <0x3>;
+		xlnx,has-hwaddr-learning ;
+		xlnx,has-inband-mgmt-tag ;
+	};
diff --git a/drivers/staging/Kconfig b/drivers/staging/Kconfig
index 3fb68d60d..1cc85f601 100644
--- a/drivers/staging/Kconfig
+++ b/drivers/staging/Kconfig
@@ -64,4 +64,22 @@ source "drivers/staging/fieldbus/Kconfig"
 
 source "drivers/staging/vme_user/Kconfig"
 
+source "drivers/staging/fclk/Kconfig"
+
+source "drivers/staging/xlnxsync/Kconfig"
+
+source "drivers/staging/xroeframer/Kconfig"
+
+source "drivers/staging/xroetrafficgen/Kconfig"
+
+source "drivers/staging/uartlite-rs485/Kconfig"
+
+source "drivers/staging/xilinx-tsn/Kconfig"
+
+source "drivers/staging/xlnx_hdcp1x/Kconfig"
+
+source "drivers/staging/xilinx_hdcp/Kconfig"
+
+source "drivers/staging/xlnx-mmi-dptx/Kconfig"
+
 endif # STAGING
diff --git a/drivers/staging/Makefile b/drivers/staging/Makefile
index c977aa13f..a78cdf1f7 100644
--- a/drivers/staging/Makefile
+++ b/drivers/staging/Makefile
@@ -21,3 +21,11 @@ obj-$(CONFIG_GREYBUS)		+= greybus/
 obj-$(CONFIG_BCM2835_VCHIQ)	+= vc04_services/
 obj-$(CONFIG_XIL_AXIS_FIFO)	+= axis-fifo/
 obj-$(CONFIG_FIELDBUS_DEV)     += fieldbus/
+obj-$(CONFIG_XILINX_FCLK)	+= fclk/
+obj-$(CONFIG_XLNX_HDCP1X_CIPHER)	+= xlnx_hdcp1x/
+obj-$(CONFIG_XLNX_SYNC)		+= xlnxsync/
+obj-$(CONFIG_XROE_FRAMER)	+= xroeframer/
+obj-$(CONFIG_SERIAL_UARTLITE_RS485)	+= uartlite-rs485/
+obj-$(CONFIG_XILINX_TSN)	+= xilinx-tsn/
+obj-$(CONFIG_XILINX_HDCP_COMMON)	+= xilinx_hdcp/
+obj-$(CONFIG_DRM_AMD_MMI_DPTX)	+= xlnx-mmi-dptx/
diff --git a/drivers/staging/fclk/Kconfig b/drivers/staging/fclk/Kconfig
new file mode 100644
index 000000000..3e13f2170
--- /dev/null
+++ b/drivers/staging/fclk/Kconfig
@@ -0,0 +1,9 @@
+#
+# Xilinx PL clk enabler
+#
+
+config XILINX_FCLK
+	tristate "Xilinx PL clock enabler"
+	depends on COMMON_CLK && OF
+	help
+	  Support for the Xilinx fclk clock enabler.
diff --git a/drivers/staging/fclk/Makefile b/drivers/staging/fclk/Makefile
new file mode 100644
index 000000000..71723036c
--- /dev/null
+++ b/drivers/staging/fclk/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_XILINX_FCLK)	+= xilinx_fclk.o
diff --git a/drivers/staging/fclk/TODO b/drivers/staging/fclk/TODO
new file mode 100644
index 000000000..912325fe5
--- /dev/null
+++ b/drivers/staging/fclk/TODO
@@ -0,0 +1,2 @@
+TODO:
+	- Remove this hack and clock adapt all the drivers.
diff --git a/drivers/staging/fclk/xilinx_fclk.c b/drivers/staging/fclk/xilinx_fclk.c
new file mode 100644
index 000000000..c923d509f
--- /dev/null
+++ b/drivers/staging/fclk/xilinx_fclk.c
@@ -0,0 +1,114 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx fclk clock driver.
+ * Copyright (c) 2017 - 2020 Xilinx Inc.
+ */
+
+#include <linux/clk.h>
+#include <linux/clk-provider.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+
+struct fclk_state {
+	struct device	*dev;
+	struct clk	*pl;
+};
+
+/* Match table for of_platform binding */
+static const struct of_device_id fclk_of_match[] = {
+	{ .compatible = "xlnx,fclk",},
+	{ /* end of list */ },
+};
+MODULE_DEVICE_TABLE(of, fclk_of_match);
+
+static ssize_t set_rate_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
+{
+	struct fclk_state *st = dev_get_drvdata(dev);
+
+	return scnprintf(buf, PAGE_SIZE, "%lu\n", clk_get_rate(st->pl));
+}
+
+static ssize_t set_rate_store(struct device *dev,
+			      struct device_attribute *attr,
+			      const char *buf, size_t count)
+{
+	int ret = 0;
+	unsigned long rate;
+	struct fclk_state *st = dev_get_drvdata(dev);
+
+	ret = kstrtoul(buf, 0, &rate);
+	if (ret)
+		return -EINVAL;
+
+	rate = clk_round_rate(st->pl, rate);
+	ret = clk_set_rate(st->pl, rate);
+
+	return ret ? ret : count;
+}
+
+static DEVICE_ATTR_RW(set_rate);
+
+static const struct attribute *fclk_ctrl_attrs[] = {
+	&dev_attr_set_rate.attr,
+	NULL,
+};
+
+static const struct attribute_group fclk_ctrl_attr_grp = {
+	.attrs = (struct attribute **)fclk_ctrl_attrs,
+};
+
+static int fclk_probe(struct platform_device *pdev)
+{
+	struct fclk_state *st;
+	int ret;
+	struct device *dev = &pdev->dev;
+
+	st = devm_kzalloc(&pdev->dev, sizeof(*st), GFP_KERNEL);
+	if (!st)
+		return -ENOMEM;
+
+	st->dev = dev;
+	platform_set_drvdata(pdev, st);
+
+	st->pl = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(st->pl))
+		return PTR_ERR(st->pl);
+
+	ret = clk_prepare_enable(st->pl);
+	if (ret) {
+		dev_err(&pdev->dev, "Unable to enable clock.\n");
+		return ret;
+	}
+
+	ret = sysfs_create_group(&dev->kobj, &fclk_ctrl_attr_grp);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static void fclk_remove(struct platform_device *pdev)
+{
+	struct fclk_state *st = platform_get_drvdata(pdev);
+
+	clk_disable_unprepare(st->pl);
+}
+
+static struct platform_driver fclk_driver = {
+	.driver = {
+		.name = KBUILD_MODNAME,
+		.of_match_table = fclk_of_match,
+	},
+	.probe		= fclk_probe,
+	.remove		= fclk_remove,
+};
+
+module_platform_driver(fclk_driver);
+
+MODULE_AUTHOR("Shubhrajyoti Datta <shubhrajyoti.datta@xilinx.com>");
+MODULE_DESCRIPTION("fclk enable");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/uartlite-rs485/Kconfig b/drivers/staging/uartlite-rs485/Kconfig
new file mode 100644
index 000000000..c814d3293
--- /dev/null
+++ b/drivers/staging/uartlite-rs485/Kconfig
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Uartlite RS485 Serial device configuration
+#
+
+config SERIAL_UARTLITE_RS485
+	tristate "Xilinx uartlite rs485 serial port support"
+	depends on HAS_IOMEM && TTY
+	select SERIAL_CORE
+	help
+	  Say Y here if you want to use the Xilinx uartlite with rs485 serial controller.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called uartlite_485.
+
diff --git a/drivers/staging/uartlite-rs485/Makefile b/drivers/staging/uartlite-rs485/Makefile
new file mode 100644
index 000000000..535529284
--- /dev/null
+++ b/drivers/staging/uartlite-rs485/Makefile
@@ -0,0 +1,6 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the kernel serial device drivers.
+#
+
+obj-$(CONFIG_SERIAL_UARTLITE_RS485) += uartlite-rs485.o
diff --git a/drivers/staging/uartlite-rs485/uartlite-rs485.c b/drivers/staging/uartlite-rs485/uartlite-rs485.c
new file mode 100644
index 000000000..8b048c82a
--- /dev/null
+++ b/drivers/staging/uartlite-rs485/uartlite-rs485.c
@@ -0,0 +1,717 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * uartlite.c: Serial driver for Xilinx uartlite serial controller
+ *
+ * Copyright (C) 2006 Peter Korsgaard <jacmet@sunsite.dk>
+ * Copyright (C) 2007 Secret Lab Technologies Ltd.
+ */
+
+#include <linux/platform_device.h>
+#include <linux/module.h>
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/serial_core.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/clk.h>
+#include <linux/pm_runtime.h>
+
+#define ULITE_NAME		"ttyULR"
+#define ULITE_DRV_NAME		"uartlite_rs485"
+#define ULITE_MAJOR		205
+#define ULITE_MINOR		187
+#define ULITE_NR_UARTS		CONFIG_SERIAL_UARTLITE_NR_UARTS
+
+/* ---------------------------------------------------------------------
+ * Register definitions
+ *
+ * For register details see datasheet:
+ * https://www.xilinx.com/support/documentation/ip_documentation/opb_uartlite.pdf
+ */
+
+#define ULITE_RX		0x00
+#define ULITE_TX		0x04
+#define ULITE_STATUS		0x08
+#define ULITE_CONTROL		0x0c
+
+#define ULITE_REGION		16
+
+#define ULITE_STATUS_RXVALID	0x01
+#define ULITE_STATUS_RXFULL	0x02
+#define ULITE_STATUS_TXEMPTY	0x04
+#define ULITE_STATUS_TXFULL	0x08
+#define ULITE_STATUS_IE		0x10
+#define ULITE_STATUS_OVERRUN	0x20
+#define ULITE_STATUS_FRAME	0x40
+#define ULITE_STATUS_PARITY	0x80
+
+#define ULITE_CONTROL_RST_TX	0x01
+#define ULITE_CONTROL_RST_RX	0x02
+#define ULITE_CONTROL_IE	0x10
+#define UART_AUTOSUSPEND_TIMEOUT	3000	/* ms */
+
+struct uartlite_data {
+	const struct uartlite_reg_ops *reg_ops;
+	struct clk *clk;
+};
+
+struct uartlite_reg_ops {
+	u32 (*in)(void __iomem *addr);
+	void (*out)(u32 val, void __iomem *addr);
+};
+
+static u32 uartlite_inbe32(void __iomem *addr)
+{
+	return ioread32be(addr);
+}
+
+static void uartlite_outbe32(u32 val, void __iomem *addr)
+{
+	iowrite32be(val, addr);
+}
+
+static const struct uartlite_reg_ops uartlite_be = {
+	.in = uartlite_inbe32,
+	.out = uartlite_outbe32,
+};
+
+static u32 uartlite_inle32(void __iomem *addr)
+{
+	return ioread32(addr);
+}
+
+static void uartlite_outle32(u32 val, void __iomem *addr)
+{
+	iowrite32(val, addr);
+}
+
+static const struct uartlite_reg_ops uartlite_le = {
+	.in = uartlite_inle32,
+	.out = uartlite_outle32,
+};
+
+static inline u32 uart_in32(u32 offset, struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+
+	return pdata->reg_ops->in(port->membase + offset);
+}
+
+static inline void uart_out32(u32 val, u32 offset, struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+
+	pdata->reg_ops->out(val, port->membase + offset);
+}
+
+static struct uart_port ulite_ports[ULITE_NR_UARTS];
+
+/* ---------------------------------------------------------------------
+ * Core UART driver operations
+ */
+
+static int ulite_receive(struct uart_port *port, int stat)
+{
+	struct tty_port *tport = &port->state->port;
+	unsigned char ch = 0;
+	char flag = TTY_NORMAL;
+
+	if ((stat & (ULITE_STATUS_RXVALID | ULITE_STATUS_OVERRUN
+		     | ULITE_STATUS_FRAME)) == 0)
+		return 0;
+
+	/* stats */
+	if (stat & ULITE_STATUS_RXVALID) {
+		port->icount.rx++;
+		ch = uart_in32(ULITE_RX, port);
+
+		if (stat & ULITE_STATUS_PARITY)
+			port->icount.parity++;
+	}
+
+	if (stat & ULITE_STATUS_OVERRUN)
+		port->icount.overrun++;
+
+	if (stat & ULITE_STATUS_FRAME)
+		port->icount.frame++;
+
+	/* drop byte with parity error if IGNPAR specified */
+	if (stat & port->ignore_status_mask & ULITE_STATUS_PARITY)
+		stat &= ~ULITE_STATUS_RXVALID;
+
+	stat &= port->read_status_mask;
+
+	if (stat & ULITE_STATUS_PARITY)
+		flag = TTY_PARITY;
+
+	stat &= ~port->ignore_status_mask;
+
+	if (stat & ULITE_STATUS_RXVALID)
+		tty_insert_flip_char(tport, ch, flag);
+
+	if (stat & ULITE_STATUS_FRAME)
+		tty_insert_flip_char(tport, 0, TTY_FRAME);
+
+	if (stat & ULITE_STATUS_OVERRUN)
+		tty_insert_flip_char(tport, 0, TTY_OVERRUN);
+
+	return 1;
+}
+
+static int ulite_transmit(struct uart_port *port, int stat)
+{
+	struct tty_port *tport = &port->state->port;
+	unsigned char ch;
+
+	if (stat & ULITE_STATUS_TXFULL)
+		return 0;
+
+	if (port->x_char) {
+		uart_out32(port->x_char, ULITE_TX, port);
+		port->x_char = 0;
+		port->icount.tx++;
+		return 1;
+	}
+
+	if (uart_tx_stopped(port))
+		return 0;
+
+	if (!uart_fifo_get(port, &ch))
+		return 0;
+
+	uart_out32(ch, ULITE_TX, port);
+
+	/* wake up */
+	if (kfifo_len(&tport->xmit_fifo) < WAKEUP_CHARS)
+		uart_write_wakeup(port);
+
+	return 1;
+}
+
+static irqreturn_t ulite_isr(int irq, void *dev_id)
+{
+	struct uart_port *port = dev_id;
+	int stat, busy, n = 0;
+	unsigned long flags;
+
+	do {
+		spin_lock_irqsave(&port->lock, flags);
+		stat = uart_in32(ULITE_STATUS, port);
+		busy  = ulite_receive(port, stat);
+		busy |= ulite_transmit(port, stat);
+		spin_unlock_irqrestore(&port->lock, flags);
+		n++;
+	} while (busy);
+
+	/* work done? */
+	if (n > 1) {
+		tty_flip_buffer_push(&port->state->port);
+		return IRQ_HANDLED;
+	} else {
+		return IRQ_NONE;
+	}
+}
+
+static unsigned int ulite_tx_empty(struct uart_port *port)
+{
+	unsigned long flags;
+	unsigned int ret;
+
+	spin_lock_irqsave(&port->lock, flags);
+	ret = uart_in32(ULITE_STATUS, port);
+	spin_unlock_irqrestore(&port->lock, flags);
+
+	return ret & ULITE_STATUS_TXEMPTY ? TIOCSER_TEMT : 0;
+}
+
+static unsigned int ulite_get_mctrl(struct uart_port *port)
+{
+	return TIOCM_CTS | TIOCM_DSR | TIOCM_CAR;
+}
+
+static void ulite_set_mctrl(struct uart_port *port, unsigned int mctrl)
+{
+	/* N/A */
+}
+
+static void ulite_stop_tx(struct uart_port *port)
+{
+	/* N/A */
+}
+
+static void ulite_start_tx(struct uart_port *port)
+{
+	ulite_transmit(port, uart_in32(ULITE_STATUS, port));
+}
+
+static void ulite_stop_rx(struct uart_port *port)
+{
+	/* don't forward any more data (like !CREAD) */
+	port->ignore_status_mask = ULITE_STATUS_RXVALID | ULITE_STATUS_PARITY
+		| ULITE_STATUS_FRAME | ULITE_STATUS_OVERRUN;
+}
+
+static void ulite_break_ctl(struct uart_port *port, int ctl)
+{
+	/* N/A */
+}
+
+static int ulite_startup(struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+	int ret;
+
+	ret = clk_enable(pdata->clk);
+	if (ret) {
+		dev_err(port->dev, "Failed to enable clock\n");
+		return ret;
+	}
+
+	ret = request_irq(port->irq, ulite_isr, IRQF_SHARED | IRQF_TRIGGER_RISING,
+			  ULITE_DRV_NAME, port);
+	if (ret)
+		return ret;
+
+	uart_out32(ULITE_CONTROL_RST_RX | ULITE_CONTROL_RST_TX,
+		   ULITE_CONTROL, port);
+	uart_out32(ULITE_CONTROL_IE, ULITE_CONTROL, port);
+
+	return 0;
+}
+
+static void ulite_shutdown(struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+
+	uart_out32(0, ULITE_CONTROL, port);
+	free_irq(port->irq, port);
+	clk_disable(pdata->clk);
+}
+
+static void ulite_set_termios(struct uart_port *port, struct ktermios *termios,
+			      const struct ktermios *old)
+{
+	unsigned long flags;
+	unsigned int baud;
+
+	spin_lock_irqsave(&port->lock, flags);
+
+	port->read_status_mask = ULITE_STATUS_RXVALID | ULITE_STATUS_OVERRUN
+		| ULITE_STATUS_TXFULL;
+
+	if (termios->c_iflag & INPCK)
+		port->read_status_mask |=
+			ULITE_STATUS_PARITY | ULITE_STATUS_FRAME;
+
+	port->ignore_status_mask = 0;
+	if (termios->c_iflag & IGNPAR)
+		port->ignore_status_mask |= ULITE_STATUS_PARITY
+			| ULITE_STATUS_FRAME | ULITE_STATUS_OVERRUN;
+
+	/* ignore all characters if CREAD is not set */
+	if ((termios->c_cflag & CREAD) == 0)
+		port->ignore_status_mask |=
+			ULITE_STATUS_RXVALID | ULITE_STATUS_PARITY
+			| ULITE_STATUS_FRAME | ULITE_STATUS_OVERRUN;
+
+	/* update timeout */
+	baud = uart_get_baud_rate(port, termios, old, 0, 460800);
+	uart_update_timeout(port, termios->c_cflag, baud);
+
+	spin_unlock_irqrestore(&port->lock, flags);
+}
+
+static const char *ulite_type(struct uart_port *port)
+{
+	return port->type == PORT_UARTLITE ? ULITE_DRV_NAME : NULL;
+}
+
+static void ulite_release_port(struct uart_port *port)
+{
+	release_mem_region(port->mapbase, ULITE_REGION);
+	iounmap(port->membase);
+	port->membase = NULL;
+}
+
+static int ulite_request_port(struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+	int ret;
+
+	pr_debug("ulite console: port=%p; port->mapbase=%llx\n",
+		 port, (unsigned long long)port->mapbase);
+
+	if (!request_mem_region(port->mapbase, ULITE_REGION, ULITE_DRV_NAME)) {
+		dev_err(port->dev, "Memory region busy\n");
+		return -EBUSY;
+	}
+
+	port->membase = ioremap(port->mapbase, ULITE_REGION);
+	if (!port->membase) {
+		dev_err(port->dev, "Unable to map registers\n");
+		release_mem_region(port->mapbase, ULITE_REGION);
+		return -EBUSY;
+	}
+
+	pdata->reg_ops = &uartlite_be;
+	uart_out32(ULITE_CONTROL_RST_TX, ULITE_CONTROL, port);
+	ret = uart_in32(ULITE_STATUS, port);
+	/* Endianness detection */
+	if ((ret & ULITE_STATUS_TXEMPTY) != ULITE_STATUS_TXEMPTY)
+		pdata->reg_ops = &uartlite_le;
+
+	return 0;
+}
+
+static void ulite_config_port(struct uart_port *port, int flags)
+{
+	if (!ulite_request_port(port))
+		port->type = PORT_UARTLITE;
+}
+
+static int ulite_verify_port(struct uart_port *port, struct serial_struct *ser)
+{
+	/* we don't want the core code to modify any port params */
+	return -EINVAL;
+}
+
+static void ulite_pm(struct uart_port *port, unsigned int state,
+		     unsigned int oldstate)
+{
+	int ret;
+
+	if (!state) {
+		ret = pm_runtime_get_sync(port->dev);
+		if (ret < 0)
+			dev_err(port->dev, "Failed to enable clocks\n");
+	} else {
+		pm_runtime_mark_last_busy(port->dev);
+		pm_runtime_put_autosuspend(port->dev);
+	}
+}
+
+static int ulite_config_rs485(struct uart_port *port,
+			      struct ktermios *termios,
+			      struct serial_rs485 *rs485conf)
+{
+	port->rs485 = *rs485conf;
+
+	if (rs485conf->flags & SER_RS485_ENABLED)
+		dev_dbg(port->dev, "Setting UART to RS485\n");
+	else
+		dev_dbg(port->dev, "Setting UART to RS232\n");
+
+	return 0;
+}
+
+static const struct uart_ops ulite_ops = {
+	.tx_empty	= ulite_tx_empty,
+	.set_mctrl	= ulite_set_mctrl,
+	.get_mctrl	= ulite_get_mctrl,
+	.stop_tx	= ulite_stop_tx,
+	.start_tx	= ulite_start_tx,
+	.stop_rx	= ulite_stop_rx,
+	.break_ctl	= ulite_break_ctl,
+	.startup	= ulite_startup,
+	.shutdown	= ulite_shutdown,
+	.set_termios	= ulite_set_termios,
+	.type		= ulite_type,
+	.release_port	= ulite_release_port,
+	.request_port	= ulite_request_port,
+	.config_port	= ulite_config_port,
+	.verify_port	= ulite_verify_port,
+	.pm		= ulite_pm,
+};
+
+static struct uart_driver ulite_uart_driver = {
+	.owner		= THIS_MODULE,
+	.driver_name	= ULITE_DRV_NAME,
+	.dev_name	= ULITE_NAME,
+	.major		= ULITE_MAJOR,
+	.minor		= ULITE_MINOR,
+	.nr		= ULITE_NR_UARTS,
+};
+
+/* ---------------------------------------------------------------------
+ * Port assignment functions (mapping devices to uart_port structures)
+ */
+
+/** ulite_assign: register a uartlite device with the driver
+ *
+ * @dev: pointer to device structure
+ * @id: requested id number.  Pass -1 for automatic port assignment
+ * @base: base address of uartlite registers
+ * @irq: irq number for uartlite
+ * @pdata: private data for uartlite
+ *
+ * Returns: 0 on success, <0 otherwise
+ */
+static int ulite_assign(struct device *dev, int id, phys_addr_t base, int irq,
+			struct uartlite_data *pdata)
+{
+	struct uart_port *port;
+	int rc;
+
+	/* if id = -1; then scan for a free id and use that */
+	if (id < 0) {
+		for (id = 0; id < ULITE_NR_UARTS; id++)
+			if (ulite_ports[id].mapbase == 0)
+				break;
+	}
+	if (id < 0 || id >= ULITE_NR_UARTS) {
+		dev_err(dev, "%s%i too large\n", ULITE_NAME, id);
+		return -EINVAL;
+	}
+
+	if ((ulite_ports[id].mapbase) && (ulite_ports[id].mapbase != base)) {
+		dev_err(dev, "cannot assign to %s%i; it is already in use\n",
+			ULITE_NAME, id);
+		return -EBUSY;
+	}
+
+	port = &ulite_ports[id];
+
+	spin_lock_init(&port->lock);
+	port->fifosize = 16;
+	port->regshift = 2;
+	port->iotype = UPIO_MEM;
+	port->iobase = 1; /* mark port in use */
+	port->mapbase = base;
+	port->membase = NULL;
+	port->ops = &ulite_ops;
+	port->irq = irq;
+	port->flags = UPF_BOOT_AUTOCONF;
+	port->dev = dev;
+	port->type = PORT_UNKNOWN;
+	port->line = id;
+	port->private_data = pdata;
+
+	port->rs485.flags |= SER_RS485_ENABLED;
+	port->rs485_config = ulite_config_rs485;
+
+	dev_set_drvdata(dev, port);
+
+	/* Register the port */
+	rc = uart_add_one_port(&ulite_uart_driver, port);
+	if (rc) {
+		dev_err(dev, "uart_add_one_port() failed; err=%i\n", rc);
+		port->mapbase = 0;
+		dev_set_drvdata(dev, NULL);
+		return rc;
+	}
+
+	return 0;
+}
+
+/** ulite_release: register a uartlite device with the driver
+ *
+ * @dev: pointer to device structure
+ */
+static int ulite_release(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+
+	if (port) {
+		uart_remove_one_port(&ulite_uart_driver, port);
+		dev_set_drvdata(dev, NULL);
+		port->mapbase = 0;
+	}
+
+	return 0;
+}
+
+/**
+ * ulite_suspend - Stop the device.
+ *
+ * @dev: handle to the device structure.
+ * Return: 0 always.
+ */
+static int __maybe_unused ulite_suspend(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+
+	if (port)
+		uart_suspend_port(&ulite_uart_driver, port);
+
+	return 0;
+}
+
+/**
+ * ulite_resume - Resume the device.
+ *
+ * @dev: handle to the device structure.
+ * Return: 0 on success, errno otherwise.
+ */
+static int __maybe_unused ulite_resume(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+
+	if (port)
+		uart_resume_port(&ulite_uart_driver, port);
+
+	return 0;
+}
+
+static int __maybe_unused ulite_runtime_suspend(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+	struct uartlite_data *pdata = port->private_data;
+
+	clk_disable(pdata->clk);
+	return 0;
+};
+
+static int __maybe_unused ulite_runtime_resume(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+	struct uartlite_data *pdata = port->private_data;
+	int ret;
+
+	ret = clk_enable(pdata->clk);
+	if (ret) {
+		dev_err(dev, "Cannot enable clock.\n");
+		return ret;
+	}
+	return 0;
+}
+
+/* ---------------------------------------------------------------------
+ * Platform bus binding
+ */
+
+static const struct dev_pm_ops ulite_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(ulite_suspend, ulite_resume)
+	SET_RUNTIME_PM_OPS(ulite_runtime_suspend,
+			   ulite_runtime_resume, NULL)
+};
+
+#if defined(CONFIG_OF)
+/* Match table for of_platform binding */
+static const struct of_device_id ulite_of_match[] = {
+	{ .compatible = "xlnx,axi-uartlite-rs485", },
+	{}
+};
+MODULE_DEVICE_TABLE(of, ulite_of_match);
+#endif /* CONFIG_OF */
+
+static int ulite_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	struct uartlite_data *pdata;
+	int irq, ret;
+	int id = pdev->id;
+#ifdef CONFIG_OF
+	const __be32 *prop;
+
+	prop = of_get_property(pdev->dev.of_node, "port-number", NULL);
+	if (prop)
+		id = be32_to_cpup(prop);
+#endif
+	pdata = devm_kzalloc(&pdev->dev, sizeof(struct uartlite_data),
+			     GFP_KERNEL);
+	if (!pdata)
+		return -ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		return -ENODEV;
+
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0)
+		return irq;
+
+	pdata->clk = devm_clk_get(&pdev->dev, "s_axi_aclk");
+	if (IS_ERR(pdata->clk)) {
+		if (PTR_ERR(pdata->clk) != -ENOENT)
+			return PTR_ERR(pdata->clk);
+
+		/*
+		 * Clock framework support is optional, continue on
+		 * anyways if we don't find a matching clock.
+		 */
+		pdata->clk = NULL;
+	}
+
+	ret = clk_prepare_enable(pdata->clk);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to prepare clock\n");
+		return ret;
+	}
+
+	pm_runtime_use_autosuspend(&pdev->dev);
+	pm_runtime_set_autosuspend_delay(&pdev->dev, UART_AUTOSUSPEND_TIMEOUT);
+	pm_runtime_set_active(&pdev->dev);
+	pm_runtime_enable(&pdev->dev);
+
+	if (!ulite_uart_driver.state) {
+		dev_dbg(&pdev->dev, "uartlite: calling uart_register_driver()\n");
+		ret = uart_register_driver(&ulite_uart_driver);
+		if (ret < 0) {
+			dev_err(&pdev->dev, "Failed to register driver\n");
+			clk_disable_unprepare(pdata->clk);
+			return ret;
+		}
+	}
+
+	ret = ulite_assign(&pdev->dev, id, res->start, irq, pdata);
+
+	pm_runtime_mark_last_busy(&pdev->dev);
+	pm_runtime_put_autosuspend(&pdev->dev);
+
+	return ret;
+}
+
+static void ulite_remove(struct platform_device *pdev)
+{
+	struct uart_port *port = dev_get_drvdata(&pdev->dev);
+	struct uartlite_data *pdata = port->private_data;
+
+	clk_disable_unprepare(pdata->clk);
+	ulite_release(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+	pm_runtime_set_suspended(&pdev->dev);
+	pm_runtime_dont_use_autosuspend(&pdev->dev);
+}
+
+static struct platform_driver ulite_platform_driver = {
+	.probe = ulite_probe,
+	.remove = ulite_remove,
+	.driver = {
+		.name  = ULITE_DRV_NAME,
+		.of_match_table = of_match_ptr(ulite_of_match),
+		.pm = &ulite_pm_ops,
+	},
+};
+
+/* ---------------------------------------------------------------------
+ * Module setup/teardown
+ */
+
+static int __init ulite_init(void)
+{
+	pr_debug("uartlite: calling platform_driver_register()\n");
+	return platform_driver_register(&ulite_platform_driver);
+}
+
+static void __exit ulite_exit(void)
+{
+	platform_driver_unregister(&ulite_platform_driver);
+	if (ulite_uart_driver.state)
+		uart_unregister_driver(&ulite_uart_driver);
+}
+
+module_init(ulite_init);
+module_exit(ulite_exit);
+
+MODULE_AUTHOR("Peter Korsgaard <jacmet@sunsite.dk>");
+MODULE_DESCRIPTION("Xilinx uartlite serial driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/xilinx-tsn/Kconfig b/drivers/staging/xilinx-tsn/Kconfig
new file mode 100644
index 000000000..8e8b685c7
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/Kconfig
@@ -0,0 +1,91 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Xilinx device configuration
+#
+
+if NET_VENDOR_XILINX
+
+config XILINX_TSN
+	tristate "Enable Xilinx's TSN IP"
+	select PHYLIB
+	help
+	  Enable Xilinx's TSN IP.
+
+	  If built as a module (m) or static (y), then all its submodules
+	  follow the same build mode.
+
+if XILINX_TSN
+config AXIENET_HAS_TADMA
+	bool "AxiEthernet is configured with TADMA"
+	depends on XILINX_TSN
+	help
+	  When hardware is generated with Axi Ethernet with TADMA select this option.
+	  This driver enables Time Aware DMA to support TSN Qbv clause accurately.
+	  It is used to fetch schedule traffic.
+	  It contains one queue.
+
+	  If XILINX_TSN is built as a module (m) or static (y), then this
+	  submodule follows the same build mode.
+
+config XILINX_TSN_PTP
+	bool "Generate hardware packet timestamps using Xilinx's TSN IP"
+	depends on XILINX_TSN && PTP_1588_CLOCK
+	default XILINX_TSN
+	help
+	  Generate hardware packet timestamps. This is to facilitate IEEE 1588.
+
+	  If XILINX_TSN is built as a module (m) or static (y), then this
+	  submodule follows the same build mode.
+
+config XILINX_TSN_QBV
+	bool "Support Qbv protocol in TSN"
+	depends on XILINX_TSN_PTP
+	default XILINX_TSN
+	help
+	  Enables TSN Qbv protocol.
+
+	  If XILINX_TSN is built as a module (m) or static (y), then this
+	  submodule follows the same build mode.
+
+config XILINX_TSN_SWITCH
+	bool "Support TSN switch"
+	depends on XILINX_TSN && NET_SWITCHDEV
+	default XILINX_TSN
+	help
+	  Enable Xilinx's TSN Switch support.
+
+	  If XILINX_TSN is built as a module (m) or static (y), then this
+	  submodule follows the same build mode.
+
+config XILINX_TSN_QCI
+	bool "Support Qci protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default XILINX_TSN
+	help
+	  Enable TSN QCI protocol.
+
+	  If XILINX_TSN is built as a module (m) or static (y), then this
+	  submodule follows the same build mode.
+
+config XILINX_TSN_CB
+	bool "Support CB protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default XILINX_TSN
+	help
+	  Enable TSN CB protocol support.
+
+	  If XILINX_TSN is built as a module (m) or static (y), then this
+	  submodule follows the same build mode.
+
+config XILINX_TSN_QBR
+	bool "Support QBR protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default XILINX_TSN
+	help
+	  Enable TSN QBR protocol support.
+
+	  If XILINX_TSN is built as a module (m) or static (y), then this
+	  submodule follows the same build mode.
+
+endif #XILINX_TSN
+endif # NET_VENDOR_XILINX
diff --git a/drivers/staging/xilinx-tsn/Makefile b/drivers/staging/xilinx-tsn/Makefile
new file mode 100644
index 000000000..94296a9db
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/Makefile
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the Xilinx network device drivers.
+#
+
+obj-$(CONFIG_XILINX_TSN) :=xilinx_emac_tsn.o
+xilinx_emac_tsn-objs := xilinx_axienet_main_tsn.o xilinx_axienet_mcdma_tsn.o xilinx_axienet_mdio_tsn.o xilinx_tsn_ep.o xilinx_tsn_ep_ex.o xilinx_tsn_ip.o
+xilinx_emac_tsn-$(CONFIG_XILINX_TSN_PTP) += xilinx_tsn_ptp_xmit.o xilinx_tsn_ptp_clock.o
+xilinx_emac_tsn-$(CONFIG_XILINX_TSN_QBV) += xilinx_tsn_shaper.o
+xilinx_emac_tsn-$(CONFIG_XILINX_TSN_QCI) += xilinx_tsn_qci.o
+xilinx_emac_tsn-$(CONFIG_XILINX_TSN_CB) += xilinx_tsn_cb.o
+xilinx_emac_tsn-$(CONFIG_XILINX_TSN_SWITCH) += xilinx_tsn_switch.o xilinx_tsn_switchdev.o
+xilinx_emac_tsn-$(CONFIG_XILINX_TSN_QBR) += xilinx_tsn_preemption.o
+xilinx_emac_tsn-$(CONFIG_AXIENET_HAS_TADMA) += xilinx_tsn_tadma.o
diff --git a/drivers/staging/xilinx-tsn/xilinx_axienet_main_tsn.c b/drivers/staging/xilinx-tsn/xilinx_axienet_main_tsn.c
new file mode 100644
index 000000000..096563346
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_axienet_main_tsn.c
@@ -0,0 +1,1995 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx Axi Ethernet device driver
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2019 SED Systems, a division of Calian Ltd.
+ * Copyright (c) 2010 - 2012 Xilinx, Inc. All rights reserved.
+ *
+ * This is a driver for the Xilinx Axi Ethernet which is used in the Virtex6
+ * and Spartan6.
+ *
+ * TODO:
+ *  - Add Axi Fifo support.
+ *  - Factor out Axi DMA code into separate driver.
+ *  - Test and fix basic multicast filtering.
+ *  - Add support for extended multicast filtering.
+ *  - Test basic VLAN support.
+ *  - Add support for extended VLAN support.
+ */
+
+#include <linux/clk.h>
+#include <linux/circ_buf.h>
+#include <linux/delay.h>
+#include <linux/etherdevice.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <linux/phy.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/iopoll.h>
+#include <linux/ptp_classify.h>
+#include <linux/platform_device.h>
+#include <linux/net_tstamp.h>
+#include <linux/random.h>
+#include <net/sock.h>
+#include <linux/xilinx_phy.h>
+
+#include "xilinx_axienet_tsn.h"
+
+/* Descriptors defines for Tx and Rx DMA */
+#define TX_BD_NUM_DEFAULT		64
+#define RX_BD_NUM_DEFAULT		128
+#define TX_BD_NUM_MAX			4096
+#define RX_BD_NUM_MAX			4096
+
+/* Must be shorter than length of ethtool_drvinfo.driver field to fit */
+#define DRIVER_NAME		"xaxienet"
+#define DRIVER_DESCRIPTION	"Xilinx Axi Ethernet driver"
+#define DRIVER_VERSION		"1.00a"
+
+#define AXIENET_REGS_N		40
+#define AXIENET_TS_HEADER_LEN	8
+#define XXVENET_TS_HEADER_LEN	4
+#define MRMAC_TS_HEADER_LEN		16
+#define MRMAC_TS_HEADER_WORDS   (MRMAC_TS_HEADER_LEN / 4)
+#define NS_PER_SEC              1000000000ULL /* Nanoseconds per second */
+
+#define MRMAC_RESET_DELAY	1 /* Delay in msecs*/
+
+/* IEEE1588 Message Type field values  */
+#define PTP_TYPE_SYNC		0
+#define PTP_TYPE_PDELAY_REQ	2
+#define PTP_TYPE_PDELAY_RESP	3
+#define PTP_TYPE_OFFSET		42
+/* SW flags used to convey message type for command FIFO handling */
+#define MSG_TYPE_SHIFT			4
+#define MSG_TYPE_SYNC_FLAG		((PTP_TYPE_SYNC + 1) << MSG_TYPE_SHIFT)
+#define MSG_TYPE_PDELAY_RESP_FLAG	((PTP_TYPE_PDELAY_RESP + 1) << \
+									 MSG_TYPE_SHIFT)
+
+#define FILTER_SELECT		0x100	   /* Filter select */
+#define ETHERTYPE_FILTER_IPV4	0x00000008 /* Ethertype field 0x08 for IPv4 packets */
+#define ETHERTYPE_FILTER_PTP    0x0000F788 /* Ethertype field 0x88F7 for PTP packets */
+#define PROTO_FILTER_UDP	0x11000000 /* protocol field 0x11 for UDP packets */
+#define PTP_UDP_PORT		0x00003F01 /* dest port field 0x013F for PTP over UDP packes */
+#define PTP_VERSION		0x02000000 /* PTPv2 */
+
+#define DESTMAC_FILTER_ENABLE_MASK_MSB		0xFFFFFFFF /* Enable filtering for bytes 0-3 in a
+							    * packet corresponding to 4 Most
+							    * significant bytes of
+							    * Destination MAC address
+							    */
+#define DESTMAC_FILTER_ENABLE_MASK_LSB		0xFF000000 /* Enable filtering for bytes
+							    * 4-5 in a packet
+							    * corresponding to 2 Least
+							    * significant bytes of
+							    * Destination MAC address
+							    */
+#define PROTO_FILTER_DISABLE_MASK		0x0 /* Disable protocol based filtering */
+#define PORT_NUM_FILTER_DISABLE_MASK		0x0 /* Disable port number based filtering */
+#define VERSION_FILTER_DISABLE_MASK		0x0 /* Disable filtering based on PTP version */
+
+#define DESTMAC_FILTER_DISABLE_MASK_MSB		0 /* Disable Dest MAC address filtering(4 MSB'S) */
+#define DESTMAC_FILTER_DISABLE_MASK_LSB		0 /* Disable Dest MAC address filtering(2 LSB's) */
+#define PROTO_FILTER_ENABLE_MASK		0xFF000000 /* Enable protocol based filtering */
+#define PORT_NUM_FILTER_ENABLE_MASK		0x0000FFFF /* Enable port number based filtering */
+#define VERSION_FILTER_ENABLE_MASK		0xFF000000 /* Enable PTP version based filtering */
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+int axienet_phc_index = -1;
+EXPORT_SYMBOL(axienet_phc_index);
+#endif
+
+/* Option table for setting up Axi Ethernet hardware options */
+static struct axienet_option axienet_options[] = {
+	/* Turn on jumbo packet support for both Rx and Tx */
+	{
+		.opt = XAE_OPTION_JUMBO,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_JUM_MASK,
+	}, {
+		.opt = XAE_OPTION_JUMBO,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_JUM_MASK,
+	}, { /* Turn on VLAN packet support for both Rx and Tx */
+		.opt = XAE_OPTION_VLAN,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_VLAN_MASK,
+	}, {
+		.opt = XAE_OPTION_VLAN,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_VLAN_MASK,
+	}, { /* Turn on FCS stripping on receive packets */
+		.opt = XAE_OPTION_FCS_STRIP,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_FCS_MASK,
+	}, { /* Turn on FCS insertion on transmit packets */
+		.opt = XAE_OPTION_FCS_INSERT,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_FCS_MASK,
+	}, { /* Turn off length/type field checking on receive packets */
+		.opt = XAE_OPTION_LENTYPE_ERR,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_LT_DIS_MASK,
+	}, { /* Turn on Rx flow control */
+		.opt = XAE_OPTION_FLOW_CONTROL,
+		.reg = XAE_FCC_OFFSET,
+		.m_or = XAE_FCC_FCRX_MASK,
+	}, { /* Turn on Tx flow control */
+		.opt = XAE_OPTION_FLOW_CONTROL,
+		.reg = XAE_FCC_OFFSET,
+		.m_or = XAE_FCC_FCTX_MASK,
+	}, { /* Turn on promiscuous frame filtering */
+		.opt = XAE_OPTION_PROMISC,
+		.reg = XAE_FMC_OFFSET,
+		.m_or = XAE_FMC_PM_MASK,
+	}, { /* Enable transmitter */
+		.opt = XAE_OPTION_TXEN,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_TX_MASK,
+	}, { /* Enable receiver */
+		.opt = XAE_OPTION_RXEN,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_RX_MASK,
+	},
+	{}
+};
+
+struct axienet_ethtools_stat {
+	const char *name;
+};
+
+static struct axienet_ethtools_stat axienet_get_ethtools_strings_stats[] = {
+	{ "tx_packets" },
+	{ "rx_packets" },
+	{ "tx_bytes" },
+	{ "rx_bytes" },
+	{ "tx_errors" },
+	{ "rx_errors" },
+};
+
+/**
+ * axienet_dma_bd_release_tsn - Release buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_dma_bd_init. axienet_dma_bd_release is called when Axi Ethernet
+ * driver stop api is called.
+ */
+void axienet_dma_bd_release_tsn(struct net_device *ndev)
+{
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+	for_each_rx_dma_queue(lp, i) {
+		axienet_mcdma_rx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+}
+
+/**
+ * axienet_set_mac_address_tsn - Write the MAC address
+ * @ndev:	Pointer to the net_device structure
+ * @address:	6 byte Address to be written as MAC address
+ *
+ * This function is called to initialize the MAC address of the Axi Ethernet
+ * core. It writes to the UAW0 and UAW1 registers of the core.
+ */
+void axienet_set_mac_address_tsn(struct net_device *ndev,
+				 const void *address)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (address)
+		eth_hw_addr_set(ndev, address);
+	if (!is_valid_ether_addr(ndev->dev_addr))
+		eth_hw_addr_random(ndev);
+
+	if (lp->axienet_config->mactype != XAXIENET_1G &&
+	    lp->axienet_config->mactype != XAXIENET_2_5G)
+		return;
+
+	/* Set up unicast MAC address filter set its mac address */
+	axienet_iow(lp, XAE_UAW0_OFFSET,
+		    (ndev->dev_addr[0]) |
+		    (ndev->dev_addr[1] << 8) |
+		    (ndev->dev_addr[2] << 16) |
+		    (ndev->dev_addr[3] << 24));
+	axienet_iow(lp, XAE_UAW1_OFFSET,
+		    (((axienet_ior(lp, XAE_UAW1_OFFSET)) &
+		      ~XAE_UAW1_UNICASTADDR_MASK) |
+		     (ndev->dev_addr[4] |
+		     (ndev->dev_addr[5] << 8))));
+}
+
+/**
+ * netdev_set_mac_address - Write the MAC address (from outside the driver)
+ * @ndev:	Pointer to the net_device structure
+ * @p:		6 byte Address to be written as MAC address
+ *
+ * Return: 0 for all conditions. Presently, there is no failure case.
+ *
+ * This function is called to initialize the MAC address of the Axi Ethernet
+ * core. It calls the core specific axienet_set_mac_address_tsn. This is the
+ * function that goes into net_device_ops structure entry ndo_set_mac_address.
+ */
+static int netdev_set_mac_address(struct net_device *ndev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	axienet_set_mac_address_tsn(ndev, addr->sa_data);
+	return 0;
+}
+
+/**
+ * axienet_set_multicast_list_tsn - Prepare the multicast table
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is called to initialize the multicast table during
+ * initialization. The Axi Ethernet basic multicast support has a four-entry
+ * multicast table which is initialized here. Additionally this function
+ * goes into the net_device_ops structure entry ndo_set_multicast_list. This
+ * means whenever the multicast table entries need to be updated this
+ * function gets called.
+ */
+void axienet_set_multicast_list_tsn(struct net_device *ndev)
+{
+	int i;
+	u32 reg, af0reg, af1reg;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (lp->axienet_config->mactype != XAXIENET_1G || lp->eth_hasnobuf)
+		return;
+
+	if (ndev->flags & (IFF_ALLMULTI | IFF_PROMISC) ||
+	    netdev_mc_count(ndev) > XAE_MULTICAST_CAM_TABLE_NUM) {
+		/* We must make the kernel realize we had to move into
+		 * promiscuous mode. If it was a promiscuous mode request
+		 * the flag is already set. If not we set it.
+		 */
+		ndev->flags |= IFF_PROMISC;
+		reg = axienet_ior(lp, XAE_FMC_OFFSET);
+		reg |= XAE_FMC_PM_MASK;
+		axienet_iow(lp, XAE_FMC_OFFSET, reg);
+		dev_info(&ndev->dev, "Promiscuous mode enabled.\n");
+	} else if (!netdev_mc_empty(ndev)) {
+		struct netdev_hw_addr *ha;
+
+		i = 0;
+		netdev_for_each_mc_addr(ha, ndev) {
+			if (i >= XAE_MULTICAST_CAM_TABLE_NUM)
+				break;
+
+			af0reg = (ha->addr[0]);
+			af0reg |= (ha->addr[1] << 8);
+			af0reg |= (ha->addr[2] << 16);
+			af0reg |= (ha->addr[3] << 24);
+
+			af1reg = (ha->addr[4]);
+			af1reg |= (ha->addr[5] << 8);
+
+			reg = axienet_ior(lp, XAE_FMC_OFFSET) & 0xFFFFFF00;
+			reg |= i;
+
+			axienet_iow(lp, XAE_FMC_OFFSET, reg);
+			axienet_iow(lp, XAE_AF0_OFFSET, af0reg);
+			axienet_iow(lp, XAE_AF1_OFFSET, af1reg);
+			i++;
+		}
+	} else {
+		reg = axienet_ior(lp, XAE_FMC_OFFSET);
+		reg &= ~XAE_FMC_PM_MASK;
+
+		axienet_iow(lp, XAE_FMC_OFFSET, reg);
+
+		for (i = 0; i < XAE_MULTICAST_CAM_TABLE_NUM; i++) {
+			reg = axienet_ior(lp, XAE_FMC_OFFSET) & 0xFFFFFF00;
+			reg |= i;
+
+			axienet_iow(lp, XAE_FMC_OFFSET, reg);
+			axienet_iow(lp, XAE_AF0_OFFSET, 0);
+			axienet_iow(lp, XAE_AF1_OFFSET, 0);
+		}
+
+		dev_info(&ndev->dev, "Promiscuous mode disabled.\n");
+	}
+}
+
+/**
+ * axienet_setoptions_tsn - Set an Axi Ethernet option
+ * @ndev:	Pointer to the net_device structure
+ * @options:	Option to be enabled/disabled
+ *
+ * The Axi Ethernet core has multiple features which can be selectively turned
+ * on or off. The typical options could be jumbo frame option, basic VLAN
+ * option, promiscuous mode option etc. This function is used to set or clear
+ * these options in the Axi Ethernet hardware. This is done through
+ * axienet_option structure .
+ */
+void axienet_setoptions_tsn(struct net_device *ndev, u32 options)
+{
+	int reg;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_option *tp = &axienet_options[0];
+
+	while (tp->opt) {
+		reg = ((axienet_ior(lp, tp->reg)) & ~(tp->m_or));
+		if (options & tp->opt)
+			reg |= tp->m_or;
+		axienet_iow(lp, tp->reg, reg);
+		tp++;
+	}
+
+	lp->options |= options;
+}
+
+void __axienet_device_reset_tsn(struct axienet_dma_q *q)
+{
+	u32 timeout;
+	/* Reset Axi DMA. This would reset Axi Ethernet core as well. The reset
+	 * process of Axi DMA takes a while to complete as all pending
+	 * commands/transfers will be flushed or completed during this
+	 * reset process.
+	 * Note that even though both TX and RX have their own reset register,
+	 * they both reset the entire DMA core, so only one needs to be used.
+	 */
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, XAXIDMA_CR_RESET_MASK);
+	timeout = DELAY_OF_ONE_MILLISEC;
+	while (axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET) &
+				XAXIDMA_CR_RESET_MASK) {
+		udelay(1);
+		if (--timeout == 0) {
+			netdev_err(q->lp->ndev, "%s: DMA reset timeout!\n",
+				   __func__);
+			break;
+		}
+	}
+}
+
+/**
+ * axienet_adjust_link_tsn - Adjust the PHY link speed/duplex.
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is called to change the speed and duplex setting after
+ * auto negotiation is done by the PHY. This is the function that gets
+ * registered with the PHY interface through the "of_phy_connect" call.
+ */
+void axienet_adjust_link_tsn(struct net_device *ndev)
+{
+	u32 emmc_reg;
+	u32 link_state;
+	u32 setspeed = 1;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct phy_device *phy = ndev->phydev;
+
+	link_state = phy->speed | (phy->duplex << 1) | phy->link;
+	if (lp->last_link != link_state) {
+		if (phy->speed == SPEED_10 || phy->speed == SPEED_100) {
+			if (lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX)
+				setspeed = 0;
+		} else {
+			if (phy->speed == SPEED_1000 &&
+			    lp->phy_mode == PHY_INTERFACE_MODE_MII)
+				setspeed = 0;
+		}
+
+		if (setspeed == 1) {
+			emmc_reg = axienet_ior(lp, XAE_EMMC_OFFSET);
+			emmc_reg &= ~XAE_EMMC_LINKSPEED_MASK;
+
+			switch (phy->speed) {
+			case SPEED_2500:
+				emmc_reg |= XAE_EMMC_LINKSPD_2500;
+				break;
+			case SPEED_1000:
+				emmc_reg |= XAE_EMMC_LINKSPD_1000;
+				break;
+			case SPEED_100:
+				emmc_reg |= XAE_EMMC_LINKSPD_100;
+				break;
+			case SPEED_10:
+				emmc_reg |= XAE_EMMC_LINKSPD_10;
+				break;
+			default:
+				dev_err(&ndev->dev, "Speed other than 10, 100 ");
+				dev_err(&ndev->dev, "or 1Gbps is not supported\n");
+				break;
+			}
+
+			axienet_iow(lp, XAE_EMMC_OFFSET, emmc_reg);
+			phy_print_status(phy);
+		} else {
+			netdev_err(ndev,
+				   "Error setting Axi Ethernet mac speed\n");
+		}
+
+		lp->last_link = link_state;
+	}
+}
+
+/**
+ * axienet_start_xmit_done_tsn - Invoked once a transmit is completed by the
+ * Axi DMA Tx channel.
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is invoked from the Axi DMA Tx isr to notify the completion
+ * of transmit operation. It clears fields in the corresponding Tx BDs and
+ * unmaps the corresponding buffer so that CPU can regain ownership of the
+ * buffer. It finally invokes "netif_wake_subqueue" to restart transmission if
+ * required.
+ */
+void axienet_start_xmit_done_tsn(struct net_device *ndev,
+				 struct axienet_dma_q *q)
+{
+	u32 size = 0;
+	u32 packets = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct aximcdma_bd *cur_p;
+	unsigned int status = 0;
+
+	cur_p = &q->txq_bd_v[q->tx_bd_ci];
+	status = cur_p->sband_stats;
+	while (status & XAXIDMA_BD_STS_COMPLETE_MASK) {
+		if (cur_p->tx_desc_mapping == DESC_DMA_MAP_PAGE)
+			dma_unmap_page(ndev->dev.parent, cur_p->phys,
+				       cur_p->cntrl &
+				       XAXIDMA_BD_CTRL_LENGTH_MASK,
+				       DMA_TO_DEVICE);
+		else
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 cur_p->cntrl &
+					 XAXIDMA_BD_CTRL_LENGTH_MASK,
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq(cur_p->tx_skb);
+		/*cur_p->phys = 0;*/
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app4 = 0;
+		cur_p->status = 0;
+		cur_p->tx_skb = 0;
+		cur_p->sband_stats = 0;
+
+		size += status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+		packets++;
+
+		if (++q->tx_bd_ci >= lp->tx_bd_num)
+			q->tx_bd_ci = 0;
+		cur_p = &q->txq_bd_v[q->tx_bd_ci];
+		status = cur_p->sband_stats;
+	}
+
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += size;
+	q->tx_packets += packets;
+	q->tx_bytes += size;
+
+	/* Matches barrier in axienet_start_xmit */
+	smp_mb();
+
+	if (__netif_subqueue_stopped(ndev, q->txq_idx) && packets)
+		netif_wake_subqueue(ndev, q->txq_idx);
+}
+
+/**
+ * axienet_check_tx_bd_space - Checks if a BD/group of BDs are currently busy
+ * @q:		Pointer to DMA queue structure
+ * @num_frag:	The number of BDs to check for
+ *
+ * Return: 0, on success
+ *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ *
+ * This function is invoked before BDs are allocated and transmission starts.
+ * This function returns 0 if a BD or group of BDs can be allocated for
+ * transmission. If the BD or any of the BDs are not free the function
+ * returns a busy status. This is invoked from axienet_start_xmit.
+ */
+static inline int axienet_check_tx_bd_space(struct axienet_dma_q *q,
+					    int num_frag)
+{
+	struct axienet_local *lp = q->lp;
+	struct aximcdma_bd *cur_p;
+
+	if (CIRC_SPACE(q->tx_bd_tail, q->tx_bd_ci, lp->tx_bd_num) < (num_frag + 1))
+		return NETDEV_TX_BUSY;
+
+	cur_p = &q->txq_bd_v[(q->tx_bd_tail + num_frag) % lp->tx_bd_num];
+	if (cur_p->sband_stats & XMCDMA_BD_STS_ALL_MASK)
+		return NETDEV_TX_BUSY;
+	return 0;
+}
+
+int axienet_queue_xmit_tsn(struct sk_buff *skb,
+			   struct net_device *ndev, u16 map)
+{
+	u32 csum_start_off, csum_index_off, num_frag, ii;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct aximcdma_bd *cur_p;
+	struct axienet_dma_q *q;
+	unsigned long flags;
+	dma_addr_t tail_p;
+	u8 dmaq_idx;
+
+	num_frag = skb_shinfo(skb)->nr_frags;
+
+	dmaq_idx = lp->txqs[map].dmaq_idx;
+	q = lp->dq[dmaq_idx];
+
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+	spin_lock_irqsave(&q->tx_lock, flags);
+	if (axienet_check_tx_bd_space(q, num_frag)) {
+		if (__netif_subqueue_stopped(ndev, q->txq_idx)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
+		netif_stop_subqueue(ndev, q->txq_idx);
+
+		/* Matches barrier in axienet_start_xmit_done_tsn */
+		smp_mb();
+
+		/* Space might have just been freed - check again */
+		if (axienet_check_tx_bd_space(q, num_frag)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
+		netif_wake_subqueue(ndev, q->txq_idx);
+	}
+
+	if (skb->ip_summed == CHECKSUM_PARTIAL && !lp->eth_hasnobuf &&
+	    lp->axienet_config->mactype == XAXIENET_1G) {
+		if (lp->features & XAE_FEATURE_FULL_TX_CSUM) {
+			/* Tx Full Checksum Offload Enabled */
+			cur_p->app0 |= 2;
+		} else if (lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) {
+			csum_start_off = skb_transport_offset(skb);
+			csum_index_off = csum_start_off + skb->csum_offset;
+			/* Tx Partial Checksum Offload Enabled */
+			cur_p->app0 |= 1;
+			cur_p->app1 = (csum_start_off << 16) | csum_index_off;
+		}
+	} else if (skb->ip_summed == CHECKSUM_UNNECESSARY &&
+		   !lp->eth_hasnobuf &&
+		   (lp->axienet_config->mactype == XAXIENET_1G)) {
+		cur_p->app0 |= 2; /* Tx Full Checksum Offload Enabled */
+	}
+
+	cur_p->cntrl = (skb_headlen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK);
+
+	if (!q->eth_hasdre &&
+	    (((uintptr_t)skb->data & 0x3) || num_frag > 0)) {
+		skb_copy_and_csum_dev(skb, q->tx_buf[q->tx_bd_tail]);
+
+		cur_p->phys = q->tx_bufs_dma +
+			      (q->tx_buf[q->tx_bd_tail] - q->tx_bufs);
+
+		cur_p->cntrl = skb_pagelen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK;
+		goto out;
+	} else {
+		cur_p->phys = dma_map_single(ndev->dev.parent, skb->data,
+					     skb_headlen(skb), DMA_TO_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, cur_p->phys))) {
+			cur_p->phys = 0;
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			dev_err(&ndev->dev, "TX buffer map failed\n");
+			return NETDEV_TX_BUSY;
+		}
+	}
+	cur_p->tx_desc_mapping = DESC_DMA_MAP_SINGLE;
+
+	for (ii = 0; ii < num_frag; ii++) {
+		u32 len;
+		skb_frag_t *frag;
+
+		if (++q->tx_bd_tail >= lp->tx_bd_num)
+			q->tx_bd_tail = 0;
+
+		cur_p = &q->txq_bd_v[q->tx_bd_tail];
+		frag = &skb_shinfo(skb)->frags[ii];
+		len = skb_frag_size(frag);
+		cur_p->phys = skb_frag_dma_map(ndev->dev.parent, frag, 0, len,
+					       DMA_TO_DEVICE);
+		cur_p->cntrl = len;
+		cur_p->tx_desc_mapping = DESC_DMA_MAP_PAGE;
+	}
+
+out:
+	cur_p->cntrl |= XMCDMA_BD_CTRL_TXEOF_MASK;
+	tail_p = q->tx_bd_p + sizeof(*q->txq_bd_v) * q->tx_bd_tail;
+	cur_p->tx_skb = skb;
+
+	/* Ensure BD write before starting transfer */
+	wmb();
+
+	/* Start the transfer */
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id),
+			  tail_p);
+	if (++q->tx_bd_tail >= lp->tx_bd_num)
+		q->tx_bd_tail = 0;
+
+	spin_unlock_irqrestore(&q->tx_lock, flags);
+
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_recv - Is called from Axi DMA Rx Isr to complete the received
+ *		  BD processing.
+ * @ndev:	Pointer to net_device structure.
+ * @budget:	NAPI budget
+ * @q:		Pointer to axienet DMA queue structure
+ *
+ * This function is invoked from the Axi DMA Rx isr(poll) to process the Rx BDs
+ * It does minimal processing and invokes "netif_receive_skb" to complete
+ * further processing.
+ * Return: Number of BD's processed.
+ */
+static int axienet_recv(struct net_device *ndev, int budget,
+			struct axienet_dma_q *q)
+{
+	u32 length;
+	u32 csumstatus;
+	u32 size = 0;
+	u32 packets = 0;
+	dma_addr_t tail_p = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct sk_buff *skb, *new_skb;
+	u32 sband_status = 0;
+	struct net_device *temp_ndev = NULL;
+	struct aximcdma_bd *cur_p;
+	unsigned int numbdfree = 0;
+
+	/* Get relevat BD status value */
+	rmb();
+	cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+	sband_status = cur_p->sband_stats;
+
+	while ((numbdfree < budget) &&
+	       (cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
+		new_skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!new_skb)
+			break;
+		tail_p = q->rx_bd_p + sizeof(*q->rxq_bd_v) * q->rx_bd_ci;
+
+		dma_unmap_single(ndev->dev.parent, cur_p->phys,
+				 lp->max_frm_size,
+				 DMA_FROM_DEVICE);
+
+		skb = (cur_p->sw_id_offset);
+
+		if (lp->eth_hasnobuf ||
+		    lp->axienet_config->mactype != XAXIENET_1G)
+			length = cur_p->status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+		else
+			length = cur_p->app4 & 0x0000FFFF;
+
+		skb_put(skb, length);
+		skb->protocol = eth_type_trans(skb, ndev);
+		/*skb_checksum_none_assert(skb);*/
+		skb->ip_summed = CHECKSUM_NONE;
+
+		/* if we're doing Rx csum offload, set it up */
+		if (lp->features & XAE_FEATURE_FULL_RX_CSUM &&
+		    lp->axienet_config->mactype == XAXIENET_1G &&
+		    !lp->eth_hasnobuf) {
+			csumstatus = (cur_p->app2 &
+				      XAE_FULL_CSUM_STATUS_MASK) >> 3;
+			if (csumstatus == XAE_IP_TCP_CSUM_VALIDATED ||
+			    csumstatus == XAE_IP_UDP_CSUM_VALIDATED) {
+				skb->ip_summed = CHECKSUM_UNNECESSARY;
+			}
+		} else if ((lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) != 0 &&
+			   skb->protocol == htons(ETH_P_IP) &&
+			   skb->len > 64 && !lp->eth_hasnobuf &&
+			   (lp->axienet_config->mactype == XAXIENET_1G)) {
+			skb->csum = be32_to_cpu(cur_p->app3 & 0xFFFF);
+			skb->ip_summed = CHECKSUM_COMPLETE;
+		}
+		if (unlikely(sband_status & XMCDMA_BD_SD_MGMT_VALID_MASK)) {
+			/* received packet on mgmt channel */
+			if ((sband_status & XMCDMA_BD_SD_STS_ALL_MASK)
+			    == XMCDMA_BD_SD_STS_TUSER_MAC_1) {
+				temp_ndev = lp->slaves[0];
+			} else if ((sband_status & XMCDMA_BD_SD_STS_ALL_MASK)
+				 == XMCDMA_BD_SD_STS_TUSER_MAC_2) {
+				temp_ndev = lp->slaves[1];
+			} else if ((sband_status & XMCDMA_BD_SD_STS_ALL_MASK)
+				 == XMCDMA_BD_SD_STS_TUSER_EP) {
+				temp_ndev = lp->ndev;
+			} else if (lp->ex_ep && ((sband_status &
+				XMCDMA_BD_SD_STS_ALL_MASK) ==
+				XMCDMA_BD_SD_STS_TUSER_EX_EP)) {
+				temp_ndev = lp->ex_ep;
+			}
+
+			/* send to one of the front panel port */
+			if (temp_ndev && netif_running(temp_ndev)) {
+				skb->dev = temp_ndev;
+				netif_receive_skb(skb);
+			} else {
+				kfree(skb); /* dont send up the stack */
+			}
+		} else if (unlikely(q->flags & MCDMA_EP_EX_CHAN)) {
+			temp_ndev = lp->ex_ep;
+			if (temp_ndev && netif_running(temp_ndev)) {
+				skb->dev = temp_ndev;
+				netif_receive_skb(skb);
+			} else {
+				kfree(skb); /* dont send up the stack */
+			}
+		} else {
+			netif_receive_skb(skb); /* send on normal data path */
+		}
+
+		size += length;
+		packets++;
+
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		cur_p->phys = dma_map_single(ndev->dev.parent, new_skb->data,
+					     lp->max_frm_size,
+					   DMA_FROM_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, cur_p->phys))) {
+			cur_p->phys = 0;
+			dev_kfree_skb(new_skb);
+			dev_err(lp->dev, "RX buffer map failed\n");
+			break;
+		}
+		cur_p->cntrl = lp->max_frm_size;
+		cur_p->status = 0;
+		cur_p->sw_id_offset = new_skb;
+
+		if (++q->rx_bd_ci >= lp->rx_bd_num)
+			q->rx_bd_ci = 0;
+
+		/* Get relevat BD status value */
+		rmb();
+		cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+		numbdfree++;
+	}
+
+	ndev->stats.rx_packets += packets;
+	ndev->stats.rx_bytes += size;
+	q->rx_packets += packets;
+	q->rx_bytes += size;
+
+	if (tail_p) {
+		axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+				  q->rx_offset, tail_p);
+	}
+
+	return numbdfree;
+}
+
+/**
+ * xaxienet_rx_poll_tsn - Poll routine for rx packets (NAPI)
+ * @napi:	napi structure pointer
+ * @quota:	Max number of rx packets to be processed.
+ *
+ * This is the poll routine for rx part.
+ * It will process the packets maximux quota value.
+ *
+ * Return: number of packets received
+ */
+int xaxienet_rx_poll_tsn(struct napi_struct *napi, int quota)
+{
+	struct net_device *ndev = napi->dev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int work_done = 0;
+	unsigned int status, cr;
+
+	int map = napi - lp->napi;
+
+	struct axienet_dma_q *q = lp->dq[map];
+
+	pr_debug("index %d, chan_id %d\n", map, q->chan_id);
+	spin_lock(&q->rx_lock);
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	while ((status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) &&
+	       (work_done < quota)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+		if (status & XMCDMA_IRQ_ERR_MASK) {
+			dev_err(lp->dev, "Rx error 0x%x\n\r", status);
+			break;
+		}
+		work_done += axienet_recv(lp->ndev, quota - work_done, q);
+		status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+					  q->rx_offset);
+	}
+	spin_unlock(&q->rx_lock);
+
+	if (work_done < quota) {
+		napi_complete(napi);
+		/* Enable the interrupts again */
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      XMCDMA_RX_OFFSET);
+		cr |= (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  XMCDMA_RX_OFFSET, cr);
+	}
+
+	return work_done;
+}
+
+/**
+ * axienet_change_mtu - Driver change mtu routine.
+ * @ndev:	Pointer to net_device structure
+ * @new_mtu:	New mtu value to be applied
+ *
+ * Return: Always returns 0 (success).
+ *
+ * This is the change mtu driver routine. It checks if the Axi Ethernet
+ * hardware supports jumbo frames before changing the mtu. This can be
+ * called only when the device is not up.
+ */
+static int axienet_change_mtu(struct net_device *ndev, int new_mtu)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (netif_running(ndev))
+		return -EBUSY;
+
+	if ((new_mtu + VLAN_ETH_HLEN +
+		XAE_TRL_SIZE) > lp->rxmem)
+		return -EINVAL;
+
+	ndev->mtu = new_mtu;
+
+	return 0;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/**
+ * axienet_poll_controller - Axi Ethernet poll mechanism.
+ * @ndev:	Pointer to net_device structure
+ *
+ * This implements Rx/Tx ISR poll mechanisms. The interrupts are disabled prior
+ * to polling the ISRs and are enabled back after the polling is done.
+ */
+static void axienet_poll_controller(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i;
+
+	for_each_tx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->rx_irq);
+
+	for_each_rx_dma_queue(lp, i)
+		axienet_mcdma_rx_irq_tsn(lp->dq[i]->rx_irq, ndev);
+	for_each_tx_dma_queue(lp, i)
+		axienet_mcdma_tx_irq_tsn(lp->dq[i]->tx_irq, ndev);
+	for_each_tx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->rx_irq);
+}
+#endif
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+/**
+ *  axienet_set_timestamp_mode - sets up the hardware for the requested mode
+ *  @lp: Pointer to axienet local structure
+ *  @config: the hwtstamp configuration requested
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_timestamp_mode(struct axienet_local *lp,
+				      struct hwtstamp_config *config)
+{
+	u32 regval;
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	/* reserved for future extensions */
+	if (config->flags)
+		return -EINVAL;
+
+	if (config->tx_type < HWTSTAMP_TX_OFF ||
+	    config->tx_type > HWTSTAMP_TX_ONESTEP_SYNC)
+		return -ERANGE;
+
+	lp->ptp_ts_type = config->tx_type;
+
+	/* On RX always timestamp everything */
+	switch (config->rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		break;
+	default:
+		config->rx_filter = lp->current_rx_filter;
+	}
+	return 0;
+
+#endif
+
+	/* reserved for future extensions */
+	if (config->flags)
+		return -EINVAL;
+
+	/* Read the current value in the MAC TX CTRL register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_TC_OFFSET);
+
+	switch (config->tx_type) {
+	case HWTSTAMP_TX_OFF:
+		regval &= ~XAE_TC_INBAND1588_MASK;
+		break;
+	case HWTSTAMP_TX_ON:
+		config->tx_type = HWTSTAMP_TX_ON;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, 0x0);
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+		config->tx_type = HWTSTAMP_TX_ONESTEP_SYNC;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		break;
+	case HWTSTAMP_TX_ONESTEP_P2P:
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			config->tx_type = HWTSTAMP_TX_ONESTEP_P2P;
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		} else {
+			return -ERANGE;
+		}
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_TC_OFFSET, regval);
+
+	/* Read the current value in the MAC RX RCW1 register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_RCW1_OFFSET);
+
+	/* On RX always timestamp everything */
+	switch (config->rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		regval &= ~XAE_RCW1_INBAND1588_MASK;
+		break;
+	default:
+		config->rx_filter = HWTSTAMP_FILTER_ALL;
+		regval |= XAE_RCW1_INBAND1588_MASK;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_RCW1_OFFSET, regval);
+
+	return 0;
+}
+
+static void change_filter_values_to_udp(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_FMC_OFFSET, FILTER_SELECT);
+	/**
+	 * axienet_iow(lp, 0x70C, 0x0); values may not
+	 * be written on to the specified address if this is not given
+	 */
+	axienet_iow(lp, XAE_FF_3_OFFSET, ETHERTYPE_FILTER_IPV4);
+	axienet_iow(lp, XAE_FF_5_OFFSET, PROTO_FILTER_UDP);
+	axienet_iow(lp, XAE_FF_9_OFFSET, PTP_UDP_PORT);
+	axienet_iow(lp, XAE_FF_10_OFFSET, PTP_VERSION);
+
+	axienet_iow(lp, XAE_AF0_MASK_OFFSET, DESTMAC_FILTER_DISABLE_MASK_MSB);
+	axienet_iow(lp, XAE_AF1_MASK_OFFSET, DESTMAC_FILTER_DISABLE_MASK_LSB);
+	axienet_iow(lp, XAE_FF_5_MASK_OFFSET, PROTO_FILTER_ENABLE_MASK);
+	axienet_iow(lp, XAE_FF_9_MASK_OFFSET, PORT_NUM_FILTER_ENABLE_MASK);
+	axienet_iow(lp, XAE_FF_10_MASK_OFFSET, VERSION_FILTER_DISABLE_MASK);
+}
+
+static void change_filter_values_to_gptp(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_FF_3_OFFSET, ETHERTYPE_FILTER_PTP);
+	axienet_iow(lp, XAE_AF0_MASK_OFFSET, DESTMAC_FILTER_ENABLE_MASK_MSB);
+	axienet_iow(lp, XAE_AF1_MASK_OFFSET, DESTMAC_FILTER_ENABLE_MASK_LSB);
+	axienet_iow(lp, XAE_FF_5_MASK_OFFSET, PROTO_FILTER_DISABLE_MASK);
+	axienet_iow(lp, XAE_FF_9_MASK_OFFSET, PORT_NUM_FILTER_ENABLE_MASK);
+	axienet_iow(lp, XAE_FF_10_MASK_OFFSET, VERSION_FILTER_DISABLE_MASK);
+}
+
+/**
+ * axienet_set_ts_config - user entry point for timestamp mode
+ * @lp: Pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Set hardware to the requested more. If unsupported return an error
+ * with no changes. Otherwise, store the mode for future reference
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config config;
+	int err;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+	if (config.rx_filter == HWTSTAMP_FILTER_PTP_V2_L2_EVENT &&
+	    lp->current_rx_filter == HWTSTAMP_FILTER_PTP_V2_L4_EVENT) {
+		lp->current_rx_filter = HWTSTAMP_FILTER_PTP_V2_L2_EVENT;
+		change_filter_values_to_gptp(lp);
+	}
+	if (config.rx_filter == HWTSTAMP_FILTER_PTP_V2_L4_EVENT &&
+	    lp->current_rx_filter == HWTSTAMP_FILTER_PTP_V2_L2_EVENT) {
+		lp->current_rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_EVENT;
+		change_filter_values_to_udp(lp);
+	}
+	err = axienet_set_timestamp_mode(lp, &config);
+	if (err)
+		return err;
+
+	/* save these settings for future reference */
+	memcpy(&lp->tstamp_config, &config, sizeof(lp->tstamp_config));
+
+	return copy_to_user(ifr->ifr_data, &config,
+			    sizeof(config)) ? -EFAULT : 0;
+}
+
+/**
+ * axienet_get_ts_config - return the current timestamp configuration
+ * to the user
+ * @lp: pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_get_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config *config = &lp->tstamp_config;
+
+	return copy_to_user(ifr->ifr_data, config,
+			    sizeof(*config)) ? -EFAULT : 0;
+}
+#endif
+
+/* Ioctl MII Interface */
+static int axienet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	struct axienet_local *lp = netdev_priv(dev);
+#endif
+
+	if (!netif_running(dev))
+		return -EINVAL;
+
+	switch (cmd) {
+	case SIOCGMIIPHY:
+	case SIOCGMIIREG:
+	case SIOCSMIIREG:
+		if (!dev->phydev)
+			return -EOPNOTSUPP;
+		return phy_mii_ioctl(dev->phydev, rq, cmd);
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	case SIOCSHWTSTAMP:
+		return axienet_set_ts_config(lp, rq);
+	case SIOCGHWTSTAMP:
+		return axienet_get_ts_config(lp, rq);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int axienet_ioctl_siocdevprivate(struct net_device *dev,
+					struct ifreq *rq, void __user *data, int cmd)
+{
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV) || \
+	IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	struct axienet_local *lp = netdev_priv(dev);
+#endif
+
+	switch (cmd) {
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	case SIOCCHIOCTL:
+		if (lp->qbv_regs)
+			return axienet_set_schedule(dev, data);
+		return -EINVAL;
+	case SIOC_GET_SCHED:
+		if (lp->qbv_regs)
+			return axienet_get_schedule(dev, data);
+		return -EINVAL;
+#endif
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	case SIOC_TADMA_OFF:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_off(dev, data);
+	case SIOC_TADMA_STR_ADD:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_add_stream(dev, data);
+	case SIOC_TADMA_PROG_ALL:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_program(dev, data);
+	case SIOC_TADMA_STR_FLUSH:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_flush_stream(dev, data);
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBR)
+	case SIOC_PREEMPTION_CFG:
+		return axienet_preemption(dev, data);
+	case SIOC_PREEMPTION_CTRL:
+		return axienet_preemption_ctrl(dev, data);
+	case SIOC_PREEMPTION_STS:
+		return axienet_preemption_sts(dev, data);
+	case SIOC_PREEMPTION_RECEIVE:
+		return axienet_preemption_receive(dev);
+	case SIOC_PREEMPTION_COUNTER:
+		return axienet_preemption_cnt(dev, data);
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	case SIOC_QBU_USER_OVERRIDE:
+		return axienet_qbu_user_override(dev, data);
+	case SIOC_QBU_STS:
+		return axienet_qbu_sts(dev, data);
+#endif
+#endif
+
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static const struct net_device_ops axienet_netdev_ops = {
+	.ndo_open = axienet_tsn_open,
+	.ndo_stop = axienet_tsn_stop,
+	.ndo_start_xmit = axienet_tsn_xmit,
+	.ndo_change_mtu	= axienet_change_mtu,
+	.ndo_set_mac_address = netdev_set_mac_address,
+	.ndo_validate_addr = eth_validate_addr,
+	.ndo_eth_ioctl = axienet_ioctl,
+	.ndo_siocdevprivate = axienet_ioctl_siocdevprivate,
+	.ndo_set_rx_mode = axienet_set_multicast_list_tsn,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller = axienet_poll_controller,
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN)
+	.ndo_select_queue = axienet_tsn_select_queue,
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+	.ndo_get_port_parent_id = tsn_switch_get_port_parent_id,
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	.ndo_setup_tc = axienet_tsn_shaper_tc,
+#endif
+#endif
+};
+
+bool xlnx_is_port_temac_netdev(const struct net_device *ndev)
+{
+	return ndev && (ndev->netdev_ops == &axienet_netdev_ops);
+}
+
+/**
+ * axienet_ethtools_get_drvinfo - Get various Axi Ethernet driver information.
+ * @ndev:	Pointer to net_device structure
+ * @ed:		Pointer to ethtool_drvinfo structure
+ *
+ * This implements ethtool command for getting the driver information.
+ * Issue "ethtool -i ethX" under linux prompt to execute this function.
+ */
+static void axienet_ethtools_get_drvinfo(struct net_device *ndev,
+					 struct ethtool_drvinfo *ed)
+{
+	strscpy(ed->driver, DRIVER_NAME, sizeof(ed->driver));
+	strscpy(ed->version, DRIVER_VERSION, sizeof(ed->version));
+}
+
+/**
+ * axienet_ethtools_get_regs_len - Get the total regs length present in the
+ *				   AxiEthernet core.
+ * @ndev:	Pointer to net_device structure
+ *
+ * This implements ethtool command for getting the total register length
+ * information.
+ *
+ * Return: the total regs length
+ */
+static int axienet_ethtools_get_regs_len(struct net_device *ndev)
+{
+	return sizeof(u32) * AXIENET_REGS_N;
+}
+
+/**
+ * axienet_ethtools_get_regs - Dump the contents of all registers present
+ *			       in AxiEthernet core.
+ * @ndev:	Pointer to net_device structure
+ * @regs:	Pointer to ethtool_regs structure
+ * @ret:	Void pointer used to return the contents of the registers.
+ *
+ * This implements ethtool command for getting the Axi Ethernet register dump.
+ * Issue "ethtool -d ethX" to execute this function.
+ */
+static void axienet_ethtools_get_regs(struct net_device *ndev,
+				      struct ethtool_regs *regs, void *ret)
+{
+	u32 *data = (u32 *)ret;
+	size_t len = sizeof(u32) * AXIENET_REGS_N;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	regs->version = 0;
+	regs->len = len;
+
+	memset(data, 0, len);
+	data[13] = axienet_ior(lp, XAE_RCW0_OFFSET);
+	data[14] = axienet_ior(lp, XAE_RCW1_OFFSET);
+	data[15] = axienet_ior(lp, XAE_TC_OFFSET);
+	data[16] = axienet_ior(lp, XAE_FCC_OFFSET);
+	data[17] = axienet_ior(lp, XAE_EMMC_OFFSET);
+	data[18] = axienet_ior(lp, XAE_RMFC_OFFSET);
+	data[19] = axienet_ior(lp, XAE_MDIO_MC_OFFSET);
+	data[20] = axienet_ior(lp, XAE_MDIO_MCR_OFFSET);
+	data[21] = axienet_ior(lp, XAE_MDIO_MWD_OFFSET);
+	data[22] = axienet_ior(lp, XAE_MDIO_MRD_OFFSET);
+	data[23] = axienet_ior(lp, XAE_TEMAC_IS_OFFSET);
+	data[24] = axienet_ior(lp, XAE_TEMAC_IP_OFFSET);
+	data[25] = axienet_ior(lp, XAE_TEMAC_IE_OFFSET);
+	data[26] = axienet_ior(lp, XAE_TEMAC_IC_OFFSET);
+	data[27] = axienet_ior(lp, XAE_UAW0_OFFSET);
+	data[28] = axienet_ior(lp, XAE_UAW1_OFFSET);
+	data[29] = axienet_ior(lp, XAE_FMC_OFFSET);
+	data[30] = axienet_ior(lp, XAE_AF0_OFFSET);
+	data[31] = axienet_ior(lp, XAE_AF1_OFFSET);
+	/* Support only single DMA queue */
+	data[32] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CR_OFFSET);
+	data[33] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_SR_OFFSET);
+	data[34] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CDESC_OFFSET);
+	data[35] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_TDESC_OFFSET);
+	data[36] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CR_OFFSET);
+	data[37] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_SR_OFFSET);
+	data[38] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CDESC_OFFSET);
+	data[39] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_TDESC_OFFSET);
+}
+
+static void
+axienet_ethtools_get_ringparam(struct net_device *ndev,
+			       struct ethtool_ringparam *ering,
+			       struct kernel_ethtool_ringparam *kernel_ering,
+			       struct netlink_ext_ack *extack)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	ering->rx_max_pending = RX_BD_NUM_MAX;
+	ering->rx_mini_max_pending = 0;
+	ering->rx_jumbo_max_pending = 0;
+	ering->tx_max_pending = TX_BD_NUM_MAX;
+	ering->rx_pending = lp->rx_bd_num;
+	ering->rx_mini_pending = 0;
+	ering->rx_jumbo_pending = 0;
+	ering->tx_pending = lp->tx_bd_num;
+}
+
+static int
+axienet_ethtools_set_ringparam(struct net_device *ndev,
+			       struct ethtool_ringparam *ering,
+			       struct kernel_ethtool_ringparam *kernel_ering,
+			       struct netlink_ext_ack *extack)
+
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (ering->rx_pending > RX_BD_NUM_MAX ||
+	    ering->rx_mini_pending ||
+	    ering->rx_jumbo_pending ||
+	    ering->rx_pending > TX_BD_NUM_MAX)
+		return -EINVAL;
+
+	if (netif_running(ndev))
+		return -EBUSY;
+
+	lp->rx_bd_num = ering->rx_pending;
+	lp->tx_bd_num = ering->tx_pending;
+	return 0;
+}
+
+/**
+ * axienet_ethtools_get_pauseparam - Get the pause parameter setting for
+ *				     Tx and Rx paths.
+ * @ndev:	Pointer to net_device structure
+ * @epauseparm:	Pointer to ethtool_pauseparam structure.
+ *
+ * This implements ethtool command for getting axi ethernet pause frame
+ * setting. Issue "ethtool -a ethX" to execute this function.
+ */
+static void
+axienet_ethtools_get_pauseparam(struct net_device *ndev,
+				struct ethtool_pauseparam *epauseparm)
+{
+	u32 regval;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	epauseparm->autoneg  = 0;
+	regval = axienet_ior(lp, XAE_FCC_OFFSET);
+	epauseparm->tx_pause = regval & XAE_FCC_FCTX_MASK;
+	epauseparm->rx_pause = regval & XAE_FCC_FCRX_MASK;
+}
+
+/**
+ * axienet_ethtools_set_pauseparam - Set device pause parameter(flow control)
+ *				     settings.
+ * @ndev:	Pointer to net_device structure
+ * @epauseparm:	Pointer to ethtool_pauseparam structure
+ *
+ * This implements ethtool command for enabling flow control on Rx and Tx
+ * paths. Issue "ethtool -A ethX tx on|off" under linux prompt to execute this
+ * function.
+ *
+ * Return: 0 on success, -EFAULT if device is running
+ */
+static int
+axienet_ethtools_set_pauseparam(struct net_device *ndev,
+				struct ethtool_pauseparam *epauseparm)
+{
+	u32 regval = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (netif_running(ndev)) {
+		netdev_err(ndev,
+			   "Please stop netif before applying configuration\n");
+		return -EFAULT;
+	}
+
+	regval = axienet_ior(lp, XAE_FCC_OFFSET);
+	if (epauseparm->tx_pause)
+		regval |= XAE_FCC_FCTX_MASK;
+	else
+		regval &= ~XAE_FCC_FCTX_MASK;
+	if (epauseparm->rx_pause)
+		regval |= XAE_FCC_FCRX_MASK;
+	else
+		regval &= ~XAE_FCC_FCRX_MASK;
+	axienet_iow(lp, XAE_FCC_OFFSET, regval);
+
+	return 0;
+}
+
+/**
+ * axienet_ethtools_get_coalesce - Get DMA interrupt coalescing count.
+ * @ndev:	Pointer to net_device structure
+ * @ecoalesce:	Pointer to ethtool_coalesce structure
+ * @kernel_coal: ethtool CQE mode setting structure
+ * @extack:	extack for reporting error messages
+ *
+ * This implements ethtool command for getting the DMA interrupt coalescing
+ * count on Tx and Rx paths. Issue "ethtool -c ethX" under linux prompt to
+ * execute this function.
+ *
+ * Return: 0 always
+ */
+int axienet_ethtools_get_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack)
+{
+	u32 regval = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		if (!q)
+			return 0;
+
+		regval = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		ecoalesce->rx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+	}
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		if (!q)
+			return 0;
+		regval = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		ecoalesce->tx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+	}
+	return 0;
+}
+
+/**
+ * axienet_ethtools_set_coalesce - Set DMA interrupt coalescing count.
+ * @ndev:	Pointer to net_device structure
+ * @ecoalesce:	Pointer to ethtool_coalesce structure
+ * @kernel_coal: ethtool CQE mode setting structure
+ * @extack:	extack for reporting error messages
+ *
+ * This implements ethtool command for setting the DMA interrupt coalescing
+ * count on Tx and Rx paths. Issue "ethtool -C ethX rx-frames 5" under linux
+ * prompt to execute this function.
+ *
+ * Return: 0, on success, Non-zero error value on failure.
+ */
+int axienet_ethtools_set_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (netif_running(ndev)) {
+		netdev_err(ndev,
+			   "Please stop netif before applying configuration\n");
+		return -EFAULT;
+	}
+
+	if (ecoalesce->rx_coalesce_usecs ||
+	    ecoalesce->rx_coalesce_usecs_irq ||
+	    ecoalesce->rx_max_coalesced_frames_irq ||
+	    ecoalesce->tx_coalesce_usecs ||
+	    ecoalesce->tx_coalesce_usecs_irq ||
+	    ecoalesce->tx_max_coalesced_frames_irq ||
+	    ecoalesce->stats_block_coalesce_usecs ||
+	    ecoalesce->use_adaptive_rx_coalesce ||
+	    ecoalesce->use_adaptive_tx_coalesce ||
+	    ecoalesce->pkt_rate_low ||
+	    ecoalesce->rx_coalesce_usecs_low ||
+	    ecoalesce->rx_max_coalesced_frames_low ||
+	    ecoalesce->tx_coalesce_usecs_low ||
+	    ecoalesce->tx_max_coalesced_frames_low ||
+	    ecoalesce->pkt_rate_high ||
+	    ecoalesce->rx_coalesce_usecs_high ||
+	    ecoalesce->rx_max_coalesced_frames_high ||
+	    ecoalesce->tx_coalesce_usecs_high ||
+	    ecoalesce->tx_max_coalesced_frames_high ||
+	    ecoalesce->rate_sample_interval)
+		return -EOPNOTSUPP;
+	if (ecoalesce->rx_max_coalesced_frames)
+		lp->coalesce_count_rx = ecoalesce->rx_max_coalesced_frames;
+	if (ecoalesce->tx_max_coalesced_frames)
+		lp->coalesce_count_tx = ecoalesce->tx_max_coalesced_frames;
+
+	return 0;
+}
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+/**
+ * axienet_ethtools_get_ts_info - Get h/w timestamping capabilities.
+ * @ndev:	Pointer to net_device structure
+ * @info:	Pointer to ethtool_ts_info structure
+ *
+ * Return: 0, on success, Non-zero error value on failure.
+ */
+static int axienet_ethtools_get_ts_info(struct net_device *ndev,
+					struct kernel_ethtool_ts_info *info)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
+				SOF_TIMESTAMPING_RX_HARDWARE |
+				SOF_TIMESTAMPING_RAW_HARDWARE;
+	info->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON) |
+			(1 << HWTSTAMP_TX_ONESTEP_SYNC) |
+			(1 << HWTSTAMP_TX_ONESTEP_P2P);
+	info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
+			   (1 << HWTSTAMP_FILTER_ALL);
+	info->phc_index = lp->phc_index;
+
+	info->phc_index = axienet_phc_index;
+	return 0;
+}
+#endif
+
+/**
+ * axienet_ethtools_sset_count - Get number of strings that
+ *				 get_strings will write.
+ * @ndev:	Pointer to net_device structure
+ * @sset:	Get the set strings
+ *
+ * Return: number of strings, on success, Non-zero error value on
+ *	   failure.
+ */
+static int axienet_ethtools_sset_count(struct net_device *ndev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return axienet_sset_count_tsn(ndev, sset);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+/**
+ * axienet_ethtools_get_stats - Get the extended statistics
+ *				about the device.
+ * @ndev:	Pointer to net_device structure
+ * @stats:	Pointer to ethtool_stats structure
+ * @data:	To store the statistics values
+ *
+ * Return: None.
+ */
+static void axienet_ethtools_get_stats(struct net_device *ndev,
+				       struct ethtool_stats *stats,
+				       u64 *data)
+{
+	unsigned int i = 0;
+
+	data[i++] = ndev->stats.tx_packets;
+	data[i++] = ndev->stats.rx_packets;
+	data[i++] = ndev->stats.tx_bytes;
+	data[i++] = ndev->stats.rx_bytes;
+	data[i++] = ndev->stats.tx_errors;
+	data[i++] = ndev->stats.rx_missed_errors + ndev->stats.rx_frame_errors;
+
+	axienet_get_stats_tsn(ndev, stats, data);
+}
+
+/**
+ * axienet_ethtools_strings - Set of strings that describe
+ *			 the requested objects.
+ * @ndev:	Pointer to net_device structure
+ * @sset:	Get the set strings
+ * @data:	Data of Transmit and Receive statistics
+ *
+ * Return: None.
+ */
+static void axienet_ethtools_strings(struct net_device *ndev, u32 sset, u8 *data)
+{
+	int i;
+
+	for (i = 0; i < AXIENET_ETHTOOLS_SSTATS_LEN; i++) {
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_ethtools_strings_stats[i].name,
+			       ETH_GSTRING_LEN);
+	}
+	axienet_strings_tsn(ndev, sset, data);
+}
+
+static const struct ethtool_ops axienet_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES,
+	.get_drvinfo    = axienet_ethtools_get_drvinfo,
+	.get_regs_len   = axienet_ethtools_get_regs_len,
+	.get_regs       = axienet_ethtools_get_regs,
+	.get_link       = ethtool_op_get_link,
+	.get_ringparam	= axienet_ethtools_get_ringparam,
+	.set_ringparam  = axienet_ethtools_set_ringparam,
+	.get_pauseparam = axienet_ethtools_get_pauseparam,
+	.set_pauseparam = axienet_ethtools_set_pauseparam,
+	.get_coalesce   = axienet_ethtools_get_coalesce,
+	.set_coalesce   = axienet_ethtools_set_coalesce,
+	.get_sset_count	= axienet_ethtools_sset_count,
+	.get_ethtool_stats = axienet_ethtools_get_stats,
+	.get_strings = axienet_ethtools_strings,
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	.get_ts_info    = axienet_ethtools_get_ts_info,
+#endif
+	.get_link_ksettings = phy_ethtool_get_link_ksettings,
+	.set_link_ksettings = phy_ethtool_set_link_ksettings,
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBR)
+	.get_mm		= axienet_preemption_sts_ethtool,
+	.set_mm		= axienet_preemption_ctrl_ethtool,
+	.get_mm_stats	= axienet_preemption_cnt_ethtool,
+#endif
+};
+
+static int axienet_clk_init(struct platform_device *pdev,
+			    struct clk **axi_aclk, struct clk **axis_clk,
+			    struct clk **ref_clk, struct clk **tmpclk)
+{
+	int err;
+
+	*tmpclk = NULL;
+
+	/* The "ethernet_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
+	 */
+	*axi_aclk = devm_clk_get_enabled(&pdev->dev, "ethernet_clk");
+	if (IS_ERR(*axi_aclk)) {
+		if (PTR_ERR(*axi_aclk) != -ENOENT) {
+			err = PTR_ERR(*axi_aclk);
+			return err;
+		}
+
+		*axi_aclk = devm_clk_get_enabled(&pdev->dev, "s_axi_lite_clk");
+		if (IS_ERR(*axi_aclk)) {
+			if (PTR_ERR(*axi_aclk) != -ENOENT) {
+				err = PTR_ERR(*axi_aclk);
+				return err;
+			}
+			*axi_aclk = NULL;
+		}
+
+	} else {
+		dev_warn(&pdev->dev, "ethernet_clk is deprecated and will be removed sometime in the future\n");
+	}
+
+	*axis_clk = devm_clk_get_enabled(&pdev->dev, "axis_clk");
+	if (IS_ERR(*axis_clk)) {
+		if (PTR_ERR(*axis_clk) != -ENOENT) {
+			err = PTR_ERR(*axis_clk);
+			return err;
+		}
+		*axis_clk = NULL;
+	}
+
+	*ref_clk = devm_clk_get_enabled(&pdev->dev, "ref_clk");
+	if (IS_ERR(*ref_clk)) {
+		if (PTR_ERR(*ref_clk) != -ENOENT) {
+			err = PTR_ERR(*ref_clk);
+			return err;
+		}
+		*ref_clk = NULL;
+	}
+
+	return 0;
+}
+
+static const struct axienet_config axienet_1g_config_tsn = {
+	.mactype = XAXIENET_1G,
+	.setoptions = axienet_setoptions_tsn,
+	.clk_init = axienet_clk_init,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+/* Match table for of_platform binding */
+static const struct of_device_id axienet_of_match[] = {
+	{ .compatible = "xlnx,tsn-ethernet-1.00.a", .data = &axienet_1g_config_tsn},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, axienet_of_match);
+
+/**
+ * axienet_probe - Axi Ethernet probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for Axi Ethernet driver. This is called before
+ * any other driver routines are invoked. It allocates and sets up the Ethernet
+ * device. Parses through device tree and populates fields of
+ * axienet_local. It registers the Ethernet device.
+ */
+static int axienet_probe(struct platform_device *pdev)
+{
+	int (*axienet_clk_init)(struct platform_device *pdev,
+				struct clk **axi_aclk, struct clk **axis_clk,
+				struct clk **ref_clk, struct clk **tmpclk) =
+					axienet_clk_init;
+	int ret = 0;
+	struct axienet_local *lp;
+	struct net_device *ndev;
+	u8 mac_addr[ETH_ALEN];
+	struct resource *ethres;
+	u32 value;
+	u16 num_queues = XAE_MAX_QUEUES;
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-queues",
+				   &num_queues);
+	num_queues = clamp_val(num_queues, XAE_TSN_MIN_QUEUES, XAE_MAX_QUEUES);
+
+	ndev = alloc_etherdev_mq(sizeof(*lp), num_queues);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
+	ndev->features = NETIF_F_SG;
+	ndev->hw_features |= NETIF_F_HW_TC;
+	ndev->netdev_ops = &axienet_netdev_ops;
+	ndev->ethtool_ops = &axienet_ethtool_ops;
+
+	/* MTU range: 64 - 9000 */
+	ndev->min_mtu = 64;
+	ndev->max_mtu = XAE_JUMBO_MTU;
+
+	lp = netdev_priv(ndev);
+	lp->ndev = ndev;
+	lp->dev = &pdev->dev;
+	lp->options = XAE_OPTION_DEFAULTS;
+	lp->num_tx_queues = num_queues;
+	lp->num_rx_queues = num_queues;
+	lp->rx_bd_num = RX_BD_NUM_DEFAULT;
+	lp->tx_bd_num = TX_BD_NUM_DEFAULT;
+
+	lp->axi_clk = devm_clk_get_optional_enabled(&pdev->dev, "s_axi_lite_clk");
+	if (IS_ERR(lp->axi_clk)) {
+		ret = PTR_ERR(lp->axi_clk);
+		goto free_netdev;
+	}
+	if (!lp->axi_clk) {
+		/* For backward compatibility, if named AXI clock is not present,
+		 * treat the first clock specified as the AXI clock.
+		 */
+		lp->axi_clk = devm_clk_get_optional_enabled(&pdev->dev, NULL);
+		if (IS_ERR(lp->axi_clk)) {
+			ret = PTR_ERR(lp->axi_clk);
+			goto free_netdev;
+		}
+	}
+
+	lp->misc_clks[0].clk = devm_clk_get_optional_enabled(&pdev->dev, "axis_clk");
+	if (IS_ERR(lp->misc_clks[0].clk)) {
+		ret = PTR_ERR(lp->misc_clks[0].clk);
+		goto free_netdev;
+	}
+	lp->misc_clks[1].clk = devm_clk_get_optional_enabled(&pdev->dev, "ref_clk");
+	if (IS_ERR(lp->misc_clks[1].clk)) {
+		ret = PTR_ERR(lp->misc_clks[1].clk);
+		goto free_netdev;
+	}
+	lp->misc_clks[2].clk = devm_clk_get_optional_enabled(&pdev->dev, "mgt_clk");
+	if (IS_ERR(lp->misc_clks[2].clk)) {
+		ret = PTR_ERR(lp->misc_clks[2].clk);
+		goto free_netdev;
+	}
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &lp->num_tc);
+	if (ret || !axienet_tsn_num_tc_valid(lp->num_tc)) {
+		dev_err(&pdev->dev, "xlnx,num-tc parameter not defined\n");
+		goto free_netdev;
+	}
+
+	/* Map device registers */
+	lp->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &ethres);
+	if (IS_ERR(lp->regs)) {
+		ret = PTR_ERR(lp->regs);
+		goto free_netdev;
+	}
+	lp->regs_start = ethres->start;
+
+	/* Setup checksum offload, but default to off if not specified */
+	lp->features = 0;
+
+	if (pdev->dev.of_node) {
+		const struct of_device_id *match;
+
+		match = of_match_node(axienet_of_match, pdev->dev.of_node);
+		if (match && match->data) {
+			lp->axienet_config = match->data;
+			axienet_clk_init = lp->axienet_config->clk_init;
+		}
+	}
+
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,txcsum", &value);
+	if (!ret) {
+		dev_info(&pdev->dev, "TX_CSUM %d\n", value);
+
+		switch (value) {
+		case 1:
+			lp->csum_offload_on_tx_path =
+				XAE_FEATURE_PARTIAL_TX_CSUM;
+			lp->features |= XAE_FEATURE_PARTIAL_TX_CSUM;
+			/* Can checksum TCP/UDP over IPv4. */
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;
+			break;
+		case 2:
+			lp->csum_offload_on_tx_path =
+				XAE_FEATURE_FULL_TX_CSUM;
+			lp->features |= XAE_FEATURE_FULL_TX_CSUM;
+			/* Can checksum TCP/UDP over IPv4. */
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;
+			break;
+		default:
+			lp->csum_offload_on_tx_path = XAE_NO_CSUM_OFFLOAD;
+		}
+	}
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,rxcsum", &value);
+	if (!ret) {
+		dev_info(&pdev->dev, "RX_CSUM %d\n", value);
+
+		switch (value) {
+		case 1:
+			lp->csum_offload_on_rx_path =
+				XAE_FEATURE_PARTIAL_RX_CSUM;
+			lp->features |= XAE_FEATURE_PARTIAL_RX_CSUM;
+			break;
+		case 2:
+			lp->csum_offload_on_rx_path =
+				XAE_FEATURE_FULL_RX_CSUM;
+			lp->features |= XAE_FEATURE_FULL_RX_CSUM;
+			break;
+		default:
+			lp->csum_offload_on_rx_path = XAE_NO_CSUM_OFFLOAD;
+		}
+	}
+	/* For supporting jumbo frames, the Axi Ethernet hardware must have
+	 * a larger Rx/Tx Memory. Typically, the size must be large so that
+	 * we can enable jumbo option and start supporting jumbo frames.
+	 * Here we check for memory allocated for Rx/Tx in the hardware from
+	 * the device-tree and accordingly set flags.
+	 */
+	of_property_read_u32(pdev->dev.of_node, "xlnx,rxmem", &lp->rxmem);
+
+	/* The phy_mode is optional but when it is not specified it should not
+	 *  be a value that alters the driver behavior so set it to an invalid
+	 *  value as the default.
+	 */
+	lp->phy_mode = PHY_INTERFACE_MODE_NA;
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phy-type", &lp->phy_mode);
+	if (!ret)
+		netdev_warn(ndev, "xlnx,phy-type is deprecated, Please upgrade your device tree to use phy-mode");
+
+	/* Set default USXGMII rate */
+	lp->usxgmii_rate = SPEED_1000;
+	of_property_read_u32(pdev->dev.of_node, "xlnx,usxgmii-rate",
+			     &lp->usxgmii_rate);
+
+	/* Set default MRMAC rate */
+	lp->mrmac_rate = SPEED_10000;
+	of_property_read_u32(pdev->dev.of_node, "xlnx,mrmac-rate",
+			     &lp->mrmac_rate);
+
+	lp->eth_hasnobuf = of_property_read_bool(pdev->dev.of_node,
+						 "xlnx,eth-hasnobuf");
+	lp->eth_hasptp = of_property_read_bool(pdev->dev.of_node,
+					       "xlnx,eth-hasptp");
+
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf)
+		lp->eth_irq = platform_get_irq(pdev, 0);
+
+	ret = axienet_tsn_probe(pdev, lp, ndev);
+
+	if (ret)
+		goto free_netdev;
+
+	ret = axienet_clk_init(pdev, &lp->aclk, &lp->eth_sclk,
+			       &lp->eth_refclk, &lp->eth_dclk);
+	if (ret) {
+		if (ret != -EPROBE_DEFER)
+			dev_err(&pdev->dev, "Ethernet clock init failed %d\n", ret);
+		goto free_netdev;
+	}
+
+	lp->eth_irq = platform_get_irq(pdev, 0);
+	/* Check for Ethernet core IRQ (optional) */
+	if (lp->eth_irq <= 0)
+		dev_info(&pdev->dev, "Ethernet core IRQ not defined\n");
+
+	/* Retrieve the MAC address */
+	ret = of_get_mac_address(pdev->dev.of_node, mac_addr);
+	if (!ret) {
+		axienet_set_mac_address_tsn(ndev, mac_addr);
+	} else {
+		dev_warn(&pdev->dev, "could not find MAC address property: %d\n",
+			 ret);
+		axienet_set_mac_address_tsn(ndev, NULL);
+	}
+
+	lp->coalesce_count_rx = XAXIDMA_DFT_RX_THRESHOLD;
+	lp->coalesce_count_tx = XAXIDMA_DFT_TX_THRESHOLD;
+
+	ret = of_get_phy_mode(pdev->dev.of_node, &lp->phy_mode);
+	if (ret < 0)
+		dev_warn(&pdev->dev, "couldn't find phy i/f\n");
+	if (lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX)
+		lp->phy_flags = XAE_PHY_TYPE_1000BASE_X;
+
+	lp->phy_node = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
+	if (!lp->phy_node) {
+		dev_err(&pdev->dev, "Failed to get 'phy-handle' from device tree\n");
+		ret = -EINVAL;
+		goto free_netdev;
+	}
+	ret = axienet_mdio_setup_tsn(lp);
+	if (ret) {
+		dev_warn(&pdev->dev, "error registering MDIO bus: %d\n", ret);
+		goto err_of_put;
+	}
+
+	/* Create sysfs file entries for the device */
+	ret = axeinet_mcdma_create_sysfs_tsn(&lp->dev->kobj);
+	if (ret < 0) {
+		dev_err(lp->dev, "unable to create sysfs entries\n");
+		goto err_mdio;
+	}
+
+	ret = register_netdev(lp->ndev);
+	if (ret) {
+		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
+		goto err_clear_sysfs;
+	}
+
+	return 0;
+
+err_clear_sysfs:
+	axeinet_mcdma_remove_sysfs_tsn(&lp->dev->kobj);
+err_mdio:
+	axienet_mdio_teardown_tsn(lp);
+err_of_put:
+	of_node_put(lp->phy_node);
+free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static void axienet_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	unregister_netdev(ndev);
+
+	if (lp->mii_bus)
+		axienet_mdio_teardown_tsn(lp);
+
+	axeinet_mcdma_remove_sysfs_tsn(&lp->dev->kobj);
+	of_node_put(lp->phy_node);
+	lp->phy_node = NULL;
+
+	free_netdev(ndev);
+}
+
+static void axienet_shutdown(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+
+	rtnl_lock();
+	netif_device_detach(ndev);
+
+	if (netif_running(ndev))
+		dev_close(ndev);
+
+	rtnl_unlock();
+}
+
+struct platform_driver axienet_driver_tsn = {
+	.probe = axienet_probe,
+	.remove = axienet_remove,
+	.shutdown = axienet_shutdown,
+	.driver = {
+		 .name = "xilinx_axienet_tsn",
+		 .of_match_table = axienet_of_match,
+	},
+};
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/xilinx-tsn/xilinx_axienet_mcdma_tsn.c b/drivers/staging/xilinx-tsn/xilinx_axienet_mcdma_tsn.c
new file mode 100644
index 000000000..41f253822
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_axienet_mcdma_tsn.c
@@ -0,0 +1,1233 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx AXI Ethernet (MCDMA programming)
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2010 - 2012 Xilinx, Inc.
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * This file contains helper functions for AXI MCDMA TX and RX programming.
+ */
+
+#include <linux/module.h>
+#include <linux/of_mdio.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+
+#include "xilinx_axienet_tsn.h"
+
+struct axienet_stat {
+	const char *name;
+};
+
+/* TODO
+ * The channel numbers for managemnet frames in 5 channel mcdma on EP+Switch
+ * system. These are not exposed via hdf/dtsi, so need to hardcode here
+ */
+#define TSN_MAX_RX_Q_EX_EPSWITCH 7
+#define TSN_MIN_RX_Q_EX_EPSWITCH 5
+#define TSN_MGMT_CHAN 3
+#define TSN_MAX_EX_EP_BE_CHAN 5
+#define TSN_MAX_EX_EP_ST_CHAN 6
+#define TSN_MAX_EX_EP_RES_CHAN 7
+#define TSN_MIN_EX_EP_BE_CHAN 4
+#define TSN_MIN_EX_EP_ST_CHAN 5
+
+static struct axienet_stat axienet_get_tx_strings_stats[] = {
+	{ "txq0_packets" },
+	{ "txq0_bytes"   },
+	{ "txq1_packets" },
+	{ "txq1_bytes"   },
+	{ "txq2_packets" },
+	{ "txq2_bytes"   },
+	{ "txq3_packets" },
+	{ "txq3_bytes"   },
+	{ "txq4_packets" },
+	{ "txq4_bytes"   },
+	{ "txq5_packets" },
+	{ "txq5_bytes"   },
+	{ "txq6_packets" },
+	{ "txq6_bytes"   },
+	{ "txq7_packets" },
+	{ "txq7_bytes"   },
+	{ "txq8_packets" },
+	{ "txq8_bytes"   },
+	{ "txq9_packets" },
+	{ "txq9_bytes"   },
+	{ "txq10_packets" },
+	{ "txq10_bytes"   },
+	{ "txq11_packets" },
+	{ "txq11_bytes"   },
+	{ "txq12_packets" },
+	{ "txq12_bytes"   },
+	{ "txq13_packets" },
+	{ "txq13_bytes"   },
+	{ "txq14_packets" },
+	{ "txq14_bytes"   },
+	{ "txq15_packets" },
+	{ "txq15_bytes"   },
+};
+
+static struct axienet_stat axienet_get_rx_strings_stats[] = {
+	{ "rxq0_packets" },
+	{ "rxq0_bytes"   },
+	{ "rxq1_packets" },
+	{ "rxq1_bytes"   },
+	{ "rxq2_packets" },
+	{ "rxq2_bytes"   },
+	{ "rxq3_packets" },
+	{ "rxq3_bytes"   },
+	{ "rxq4_packets" },
+	{ "rxq4_bytes"   },
+	{ "rxq5_packets" },
+	{ "rxq5_bytes"   },
+	{ "rxq6_packets" },
+	{ "rxq6_bytes"   },
+	{ "rxq7_packets" },
+	{ "rxq7_bytes"   },
+	{ "rxq8_packets" },
+	{ "rxq8_bytes"   },
+	{ "rxq9_packets" },
+	{ "rxq9_bytes"   },
+	{ "rxq10_packets" },
+	{ "rxq10_bytes"   },
+	{ "rxq11_packets" },
+	{ "rxq11_bytes"   },
+	{ "rxq12_packets" },
+	{ "rxq12_bytes"   },
+	{ "rxq13_packets" },
+	{ "rxq13_bytes"   },
+	{ "rxq14_packets" },
+	{ "rxq14_bytes"   },
+	{ "rxq15_packets" },
+	{ "rxq15_bytes"   },
+};
+
+/**
+ * axienet_mcdma_disable_tx_q - Disable MCDMA queue corresponding to the Tx
+ * queue map
+ * @ndev:	Pointer to the net_device structure
+ * @map:	Tx queue map identifier
+ *
+ * This function disables the MCDMA channel completion interrupt, disables
+ * the channel and frees any pending skb buffers.
+ */
+int axienet_mcdma_disable_tx_q(struct net_device *ndev, u8 map)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct aximcdma_bd *cur_p;
+	struct axienet_dma_q *q;
+	unsigned long flags;
+	u32 reg, cr;
+	u8 dmaq_idx;
+	int err;
+
+	dmaq_idx = lp->txqs[map].dmaq_idx;
+	q = lp->dq[dmaq_idx];
+
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr & ~XMCDMA_IRQ_IOC_MASK);
+
+	netif_stop_subqueue(ndev, q->txq_idx);
+
+	err = readl_poll_timeout(q->dma_regs +
+				 XMCDMA_CHAN_SR_OFFSET(q->chan_id),
+				 reg,
+				 (reg & XMCDMA_CHAN_SR_IDLE_MASK), 10,
+				 (5 * DELAY_OF_ONE_MILLISEC));
+	if (err) {
+		dev_err(&ndev->dev, "Failed to disable MCDMA ch%d of Q%d\n",
+			q->chan_id, q->txq_idx);
+		return err;
+	}
+
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr & ~XMCDMA_CR_RUNSTOP_MASK);
+
+	spin_lock_irqsave(&q->tx_lock, flags);
+	cur_p = &q->txq_bd_v[q->tx_bd_ci];
+	while (q->tx_bd_ci != q->tx_bd_tail) {
+		if (cur_p->tx_desc_mapping == DESC_DMA_MAP_PAGE)
+			dma_unmap_page(ndev->dev.parent, cur_p->phys,
+				       cur_p->cntrl &
+				       XAXIDMA_BD_CTRL_LENGTH_MASK,
+				       DMA_TO_DEVICE);
+		else
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 cur_p->cntrl &
+					 XAXIDMA_BD_CTRL_LENGTH_MASK,
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app4 = 0;
+		cur_p->status = 0;
+		cur_p->tx_skb = NULL;
+		cur_p->sband_stats = 0;
+
+		if (++q->tx_bd_ci >= lp->tx_bd_num)
+			q->tx_bd_ci = 0;
+
+		cur_p = &q->txq_bd_v[q->tx_bd_ci];
+	}
+	spin_unlock_irqrestore(&q->tx_lock, flags);
+
+	dev_dbg(&ndev->dev, "MCDMA ch%d of Q%d disabled\n", q->chan_id,
+		q->txq_idx);
+	return 0;
+}
+
+/**
+ * axienet_mcdma_enable_tx_q - Enable MCDMA queue corresponding to the Tx
+ * queue map
+ * @ndev:	Pointer to the net_device structure
+ * @map:	Tx queue map identifier
+ *
+ * This function enables the MCDMA channel completion interrupt, initializes the
+ * channel's current and tail descriptors and enables the channel.
+ */
+void axienet_mcdma_enable_tx_q(struct net_device *ndev, u8 map)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	u8 dmaq_idx;
+	u32 cr;
+
+	dmaq_idx = lp->txqs[map].dmaq_idx;
+	q = lp->dq[dmaq_idx];
+
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_IRQ_IOC_MASK);
+
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id), 0);
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	netif_wake_subqueue(ndev, q->txq_idx);
+
+	dev_dbg(&ndev->dev, "MCDMA ch%d of Q%d enabled\n", q->chan_id,
+		q->txq_idx);
+}
+
+/**
+ * axienet_mcdma_tx_bd_free_tsn - Release MCDMA Tx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_tx_q_init_tsn.
+ */
+void __maybe_unused axienet_mcdma_tx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (q->txq_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+				  q->txq_bd_v,
+				  q->tx_bd_p);
+	}
+	if (q->tx_bufs) {
+		dma_free_coherent(ndev->dev.parent,
+				  XAE_MAX_PKT_LEN * lp->tx_bd_num,
+				  q->tx_bufs,
+				  q->tx_bufs_dma);
+	}
+}
+
+/**
+ * axienet_mcdma_rx_bd_free_tsn - Release MCDMA Rx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_rx_q_init_tsn.
+ */
+void __maybe_unused axienet_mcdma_rx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q)
+{
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (!q->rxq_bd_v)
+		return;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		if (q->rxq_bd_v[i].phys)
+			dma_unmap_single(ndev->dev.parent, q->rxq_bd_v[i].phys,
+					 lp->max_frm_size, DMA_FROM_DEVICE);
+		dev_kfree_skb((struct sk_buff *)
+			      (q->rxq_bd_v[i].sw_id_offset));
+	}
+
+	dma_free_coherent(ndev->dev.parent,
+			  sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+			  q->rxq_bd_v,
+			  q->rx_bd_p);
+	q->rxq_bd_v = NULL;
+}
+
+/**
+ * axienet_mcdma_tx_q_init_tsn - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Tx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_tx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+
+	q->txq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+					 &q->tx_bd_p, GFP_KERNEL);
+	if (!q->txq_bd_v)
+		goto out;
+
+	if (!q->eth_hasdre) {
+		q->tx_bufs = dma_alloc_coherent(ndev->dev.parent,
+						XAE_MAX_PKT_LEN * lp->tx_bd_num,
+						&q->tx_bufs_dma,
+						GFP_KERNEL);
+		if (!q->tx_bufs)
+			goto out;
+
+		for (i = 0; i < lp->tx_bd_num; i++)
+			q->tx_buf[i] = &q->tx_bufs[i * XAE_MAX_PKT_LEN];
+	}
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		q->txq_bd_v[i].next = q->tx_bd_p +
+				      sizeof(*q->txq_bd_v) *
+				      ((i + 1) % lp->tx_bd_num);
+	}
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XMCDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	return 0;
+out:
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+/**
+ * axienet_mcdma_rx_q_init_tsn - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Rx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_rx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct sk_buff *skb;
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t mapping;
+
+	q->rx_bd_ci = 0;
+	q->rx_offset = XMCDMA_CHAN_RX_OFFSET;
+
+	q->rxq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+					 &q->rx_bd_p, GFP_KERNEL);
+	if (!q->rxq_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		q->rxq_bd_v[i].next = q->rx_bd_p +
+				      sizeof(*q->rxq_bd_v) *
+				      ((i + 1) % lp->rx_bd_num);
+
+		skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!skb)
+			goto out;
+
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		q->rxq_bd_v[i].sw_id_offset = skb;
+		mapping = dma_map_single(ndev->dev.parent,
+					 skb->data,
+					 lp->max_frm_size,
+					 DMA_FROM_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, mapping))) {
+			dev_err(&ndev->dev, "mcdma map error\n");
+			goto out;
+		}
+
+		q->rxq_bd_v[i].phys = mapping;
+		q->rxq_bd_v[i].cntrl = lp->max_frm_size;
+	}
+
+	/* check if this is a mgmt channel */
+	if (lp->num_tc == XAE_MAX_LEGACY_TSN_TC &&
+	    lp->num_rx_queues == TSN_MAX_RX_Q_EX_EPSWITCH &&
+	    (q->chan_id == TSN_MAX_EX_EP_BE_CHAN ||
+	     q->chan_id == TSN_MAX_EX_EP_ST_CHAN ||
+	     q->chan_id == TSN_MAX_EX_EP_RES_CHAN)) {
+		q->flags = MCDMA_EP_EX_CHAN;
+	} else if ((lp->num_tc == XAE_MIN_LEGACY_TSN_TC) &&
+		   (lp->num_rx_queues == TSN_MIN_RX_Q_EX_EPSWITCH) &&
+		   ((q->chan_id == TSN_MIN_EX_EP_BE_CHAN) ||
+		    (q->chan_id == TSN_MIN_EX_EP_ST_CHAN))) {
+		q->flags = MCDMA_EP_EX_CHAN;
+	}
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XMCDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	return 0;
+
+out:
+	for_each_rx_dma_queue(lp, i) {
+		axienet_mcdma_rx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+static inline int get_mcdma_tx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_tx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int get_mcdma_rx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int map_dma_q_txirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_TXINT_SER_OFFSET);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+	     i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq_tsn(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i, j = map_dma_q_txirq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_tx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id));
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id), status);
+		axienet_start_xmit_done_tsn(lp->ndev, q);
+		goto out;
+	}
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Tx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->txq_bd_v[q->tx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+out:
+	return IRQ_HANDLED;
+}
+
+static inline int map_dma_q_rxirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_RXINT_SER_OFFSET +
+					q->rx_offset);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+		i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq_tsn(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i, j = map_dma_q_rxirq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_rx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		cr &= ~(XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+		napi_schedule(&lp->napi[i]);
+	}
+
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Rx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->rxq_bd_v[q->rx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+
+	return IRQ_HANDLED;
+}
+
+void axienet_strings_tsn(struct net_device *ndev, u32 sset, u8 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i = AXIENET_ETHTOOLS_SSTATS_LEN, j, k = 0, l = 0;
+	static const char tx_packets[] = "tx_packets";
+	static const char tx_bytes[] = "tx_bytes";
+	static const char rx_packets[] = "rx_packets";
+	static const char rx_bytes[] = "rx_bytes";
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+		q = lp->dq[j];
+		if (!q) {
+			if (sset == ETH_SS_STATS) {
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_packets,
+				       sizeof(tx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_bytes, sizeof(tx_bytes));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_packets,
+				       sizeof(rx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_bytes, sizeof(rx_bytes));
+			}
+			return;
+		}
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_tx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+	k = 0;
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+		q = lp->dq[j];
+		if (!q) {
+			if (sset == ETH_SS_STATS) {
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_packets,
+				       sizeof(tx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_bytes, sizeof(tx_bytes));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_packets,
+				       sizeof(rx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_bytes, sizeof(rx_bytes));
+			}
+			return;
+		}
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_rx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+}
+
+int axienet_sset_count_tsn(struct net_device *ndev, int sset)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i;
+
+	switch (sset) {
+	case ETH_SS_STATS:
+		for (i = 0; i < AXIENET_TX_SSTATS_LEN(lp); i++) {
+			if (!(lp->dq[i]))
+				return 4;
+		}
+		return (AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+void axienet_get_stats_tsn(struct net_device *ndev,
+			   struct ethtool_stats *stats,
+			   u64 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i = AXIENET_ETHTOOLS_SSTATS_LEN, j, k = 0;
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+
+		q = lp->dq[j];
+		if (!q) {
+			data[k++] = ndev->stats.tx_packets;
+			data[k++] = ndev->stats.tx_bytes;
+			data[k++] = ndev->stats.rx_packets;
+			data[k++] = ndev->stats.rx_bytes;
+			return;
+		}
+
+		data[i++] = q->tx_packets;
+		data[i++] = q->tx_bytes;
+		++j;
+	}
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+
+		q = lp->dq[j];
+		if (!q) {
+			data[k++] = ndev->stats.tx_packets;
+			data[k++] = ndev->stats.tx_bytes;
+			data[k++] = ndev->stats.rx_packets;
+			data[k++] = ndev->stats.rx_bytes;
+			return;
+		}
+		data[i++] = q->rx_packets;
+		data[i++] = q->rx_bytes;
+		++j;
+	}
+}
+
+/**
+ * axienet_mcdma_err_handler_tsn - Tasklet handler for Axi MCDMA Error
+ * @data:	Data passed
+ *
+ * Resets the Axi MCDMA and Axi Ethernet devices, and reconfigures the
+ * Tx/Rx BDs.
+ */
+void __maybe_unused axienet_mcdma_err_handler_tsn(unsigned long data)
+{
+	u32 axienet_status;
+	u32 cr, i, chan_en;
+	struct axienet_dma_q *q = (struct axienet_dma_q *)data;
+	struct axienet_local *lp = q->lp;
+	struct net_device *ndev = lp->ndev;
+	struct aximcdma_bd *cur_p;
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	__axienet_device_reset_tsn(q);
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		cur_p = &q->txq_bd_v[i];
+		if (cur_p->phys)
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 (cur_p->cntrl &
+					  XAXIDMA_BD_CTRL_LENGTH_MASK),
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq(cur_p->tx_skb);
+		cur_p->phys = 0;
+		cur_p->cntrl = 0;
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+		cur_p->sw_id_offset = 0;
+		cur_p->tx_skb = 0;
+	}
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		cur_p = &q->rxq_bd_v[i];
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+	}
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+	q->rx_bd_ci = 0;
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XMCDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XMCDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	axienet_set_mac_address_tsn(ndev, NULL);
+	axienet_set_multicast_list_tsn(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+}
+
+int __maybe_unused axienet_mcdma_tx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct axienet_local *lp)
+{
+	int i;
+	char dma_name[24];
+
+	u32 num = XAE_TSN_MIN_QUEUES;
+	int ret = 0;
+	/* get number of associated queues */
+	ret = of_property_read_u32(np, "xlnx,num-mm2s-channels", &num);
+	if (ret)
+		num = XAE_TSN_MIN_QUEUES;
+	lp->num_tx_queues = num;
+
+	for_each_tx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "mm2s_ch%d_introut",
+			 q->chan_id);
+		q->tx_irq = of_irq_get_byname(np, dma_name);
+		q->eth_hasdre = of_property_read_bool(np,
+						      "xlnx,include-mm2s-dre");
+		spin_lock_init(&q->tx_lock);
+	}
+
+	return 0;
+}
+
+int __maybe_unused axienet_mcdma_rx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct net_device *ndev)
+{
+	int i;
+	char dma_name[24];
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	for_each_rx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "s2mm_ch%d_introut",
+			 q->chan_id);
+		q->rx_irq = of_irq_get_byname(np, dma_name);
+		spin_lock_init(&q->rx_lock);
+
+		netif_napi_add(ndev, &lp->napi[i], xaxienet_rx_poll_tsn);
+	}
+
+	return 0;
+}
+
+static ssize_t rxch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 2 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 3 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 4 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 5 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t txch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 2 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 3 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 4 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 5 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t chan_weight_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	return sprintf(buf, "chan_id is %d and weight is %d\n",
+		       lp->chan_id, lp->weight);
+}
+
+static ssize_t chan_weight_store(struct device *dev,
+				 struct device_attribute *attr,
+				 const char *buf, size_t count)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	int ret;
+	u16 flags, chan_id;
+	u32 val;
+
+	ret = kstrtou16(buf, 16, &flags);
+	if (ret)
+		return ret;
+
+	lp->chan_id = (flags & 0xF0) >> 4;
+	lp->weight = flags & 0x0F;
+
+	if (lp->chan_id < 8)
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT0_OFFSET);
+	else
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT1_OFFSET);
+
+	if (lp->chan_id > 7)
+		chan_id = lp->chan_id - 8;
+	else
+		chan_id = lp->chan_id;
+
+	val &= ~XMCDMA_TXWEIGHT_CH_MASK(chan_id);
+	val |= lp->weight << XMCDMA_TXWEIGHT_CH_SHIFT(chan_id);
+
+	if (lp->chan_id < 8)
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT0_OFFSET, val);
+	else
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT1_OFFSET, val);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(chan_weight);
+static DEVICE_ATTR_RO(rxch_obs1);
+static DEVICE_ATTR_RO(rxch_obs2);
+static DEVICE_ATTR_RO(rxch_obs3);
+static DEVICE_ATTR_RO(rxch_obs4);
+static DEVICE_ATTR_RO(rxch_obs5);
+static DEVICE_ATTR_RO(rxch_obs6);
+static DEVICE_ATTR_RO(txch_obs1);
+static DEVICE_ATTR_RO(txch_obs2);
+static DEVICE_ATTR_RO(txch_obs3);
+static DEVICE_ATTR_RO(txch_obs4);
+static DEVICE_ATTR_RO(txch_obs5);
+static DEVICE_ATTR_RO(txch_obs6);
+static const struct attribute *mcdma_attrs[] = {
+	&dev_attr_chan_weight.attr,
+	&dev_attr_rxch_obs1.attr,
+	&dev_attr_rxch_obs2.attr,
+	&dev_attr_rxch_obs3.attr,
+	&dev_attr_rxch_obs4.attr,
+	&dev_attr_rxch_obs5.attr,
+	&dev_attr_rxch_obs6.attr,
+	&dev_attr_txch_obs1.attr,
+	&dev_attr_txch_obs2.attr,
+	&dev_attr_txch_obs3.attr,
+	&dev_attr_txch_obs4.attr,
+	&dev_attr_txch_obs5.attr,
+	&dev_attr_txch_obs6.attr,
+	NULL,
+};
+
+static const struct attribute_group mcdma_attributes = {
+	.attrs = (struct attribute **)mcdma_attrs,
+};
+
+int axeinet_mcdma_create_sysfs_tsn(struct kobject *kobj)
+{
+	return sysfs_create_group(kobj, &mcdma_attributes);
+}
+
+void axeinet_mcdma_remove_sysfs_tsn(struct kobject *kobj)
+{
+	sysfs_remove_group(kobj, &mcdma_attributes);
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_axienet_mdio_tsn.c b/drivers/staging/xilinx-tsn/xilinx_axienet_mdio_tsn.c
new file mode 100644
index 000000000..3eff4449b
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_axienet_mdio_tsn.c
@@ -0,0 +1,297 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * MDIO bus driver for the Xilinx Axi Ethernet device
+ *
+ * Copyright (c) 2009 Secret Lab Technologies, Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2019 SED Systems, a division of Calian Ltd.
+ * Copyright (c) 2010 - 2012 Xilinx, Inc. All rights reserved.
+ */
+
+#include <linux/clk.h>
+#include <linux/of_address.h>
+#include <linux/of_mdio.h>
+#include <linux/jiffies.h>
+#include <linux/iopoll.h>
+
+#include "xilinx_axienet_tsn.h"
+
+#define MAX_MDIO_FREQ		2500000 /* 2.5 MHz */
+#define DEFAULT_HOST_CLOCK	150000000 /* 150 MHz */
+
+/* Wait till MDIO interface is ready to accept a new transaction */
+int axienet_mdio_wait_until_ready_tsn(struct axienet_local *lp)
+{
+	u32 val;
+
+	return readx_poll_timeout(axinet_ior_read_mcr, lp,
+				  val, val & XAE_MDIO_MCR_READY_MASK,
+				  1, 20000);
+}
+
+/* Enable the MDIO MDC. Called prior to a read/write operation */
+static void axienet_mdio_mdc_enable(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET,
+		    ((u32)lp->mii_clk_div | XAE_MDIO_MC_MDIOEN_MASK));
+}
+
+/* Disable the MDIO MDC. Called after a read/write operation */
+static void axienet_mdio_mdc_disable(struct axienet_local *lp)
+{
+	u32 mc_reg;
+
+	mc_reg = axienet_ior(lp, XAE_MDIO_MC_OFFSET);
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET,
+		    (mc_reg & ~XAE_MDIO_MC_MDIOEN_MASK));
+}
+
+/**
+ * axienet_mdio_read - MDIO interface read function
+ * @bus:	Pointer to mii bus structure
+ * @phy_id:	Address of the PHY device
+ * @reg:	PHY register to read
+ *
+ * Return:	The register contents on success, -ETIMEDOUT on a timeout
+ *
+ * Reads the contents of the requested register from the requested PHY
+ * address by first writing the details into MCR register. After a while
+ * the register MRD is read to obtain the PHY register content.
+ */
+static int axienet_mdio_read(struct mii_bus *bus, int phy_id, int reg)
+{
+	u32 rc;
+	int ret;
+	struct axienet_local *lp = bus->priv;
+
+	axienet_mdio_mdc_enable(lp);
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+
+	axienet_iow(lp, XAE_MDIO_MCR_OFFSET,
+		    (((phy_id << XAE_MDIO_MCR_PHYAD_SHIFT) &
+		      XAE_MDIO_MCR_PHYAD_MASK) |
+		     ((reg << XAE_MDIO_MCR_REGAD_SHIFT) &
+		      XAE_MDIO_MCR_REGAD_MASK) |
+		     XAE_MDIO_MCR_INITIATE_MASK |
+		     XAE_MDIO_MCR_OP_READ_MASK));
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+
+	rc = axienet_ior(lp, XAE_MDIO_MRD_OFFSET) & 0x0000FFFF;
+
+	dev_dbg(lp->dev, "%s (phy_id=%i, reg=%x) == %x\n",
+		__func__, phy_id, reg, rc);
+
+	axienet_mdio_mdc_disable(lp);
+	return rc;
+}
+
+/**
+ * axienet_mdio_write - MDIO interface write function
+ * @bus:	Pointer to mii bus structure
+ * @phy_id:	Address of the PHY device
+ * @reg:	PHY register to write to
+ * @val:	Value to be written into the register
+ *
+ * Return:	0 on success, -ETIMEDOUT on a timeout
+ *
+ * Writes the value to the requested register by first writing the value
+ * into MWD register. The MCR register is then appropriately setup
+ * to finish the write operation.
+ */
+static int axienet_mdio_write(struct mii_bus *bus, int phy_id, int reg,
+			      u16 val)
+{
+	int ret;
+	struct axienet_local *lp = bus->priv;
+
+	dev_dbg(lp->dev, "%s (phy_id=%i, reg=%x, val=%x)\n",
+		__func__, phy_id, reg, val);
+
+	axienet_mdio_mdc_enable(lp);
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+
+	axienet_iow(lp, XAE_MDIO_MWD_OFFSET, (u32)val);
+	axienet_iow(lp, XAE_MDIO_MCR_OFFSET,
+		    (((phy_id << XAE_MDIO_MCR_PHYAD_SHIFT) &
+		      XAE_MDIO_MCR_PHYAD_MASK) |
+		     ((reg << XAE_MDIO_MCR_REGAD_SHIFT) &
+		      XAE_MDIO_MCR_REGAD_MASK) |
+		     XAE_MDIO_MCR_INITIATE_MASK |
+		     XAE_MDIO_MCR_OP_WRITE_MASK));
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+	axienet_mdio_mdc_disable(lp);
+	return 0;
+}
+
+/**
+ * axienet_mdio_enable_tsn - MDIO hardware setup function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Return:	0 on success, -ETIMEDOUT on a timeout.
+ *
+ * Sets up the MDIO interface by initializing the MDIO clock and enabling the
+ * MDIO interface in hardware.
+ */
+int axienet_mdio_enable_tsn(struct axienet_local *lp)
+{
+	u32 host_clock;
+
+	lp->mii_clk_div = 0;
+
+	if (lp->axi_clk) {
+		host_clock = clk_get_rate(lp->axi_clk);
+	} else {
+		struct device_node *np1;
+
+		/* Legacy fallback: detect CPU clock frequency and use as AXI
+		 * bus clock frequency. This only works on certain platforms.
+		 */
+		np1 = of_find_node_by_name(NULL, "cpu");
+		if (!np1) {
+			netdev_warn(lp->ndev, "Could not find CPU device node.\n");
+			host_clock = DEFAULT_HOST_CLOCK;
+		} else {
+			int ret = of_property_read_u32(np1, "clock-frequency",
+						       &host_clock);
+			if (ret) {
+				netdev_warn(lp->ndev, "CPU clock-frequency property not found.\n");
+				host_clock = DEFAULT_HOST_CLOCK;
+			}
+			of_node_put(np1);
+		}
+		netdev_info(lp->ndev, "Setting assumed host clock to %u\n",
+			    host_clock);
+	}
+
+	/* clk_div can be calculated by deriving it from the equation:
+	 * fMDIO = fHOST / ((1 + clk_div) * 2)
+	 *
+	 * Where fMDIO <= 2500000, so we get:
+	 * fHOST / ((1 + clk_div) * 2) <= 2500000
+	 *
+	 * Then we get:
+	 * 1 / ((1 + clk_div) * 2) <= (2500000 / fHOST)
+	 *
+	 * Then we get:
+	 * 1 / (1 + clk_div) <= ((2500000 * 2) / fHOST)
+	 *
+	 * Then we get:
+	 * 1 / (1 + clk_div) <= (5000000 / fHOST)
+	 *
+	 * So:
+	 * (1 + clk_div) >= (fHOST / 5000000)
+	 *
+	 * And finally:
+	 * clk_div >= (fHOST / 5000000) - 1
+	 *
+	 * fHOST can be read from the flattened device tree as property
+	 * "clock-frequency" from the CPU
+	 */
+
+	lp->mii_clk_div = (host_clock / (MAX_MDIO_FREQ * 2)) - 1;
+	/* If there is any remainder from the division of
+	 * fHOST / (MAX_MDIO_FREQ * 2), then we need to add
+	 * 1 to the clock divisor or we will surely be above 2.5 MHz
+	 */
+	if (host_clock % (MAX_MDIO_FREQ * 2))
+		lp->mii_clk_div++;
+
+	netdev_dbg(lp->ndev,
+		   "Setting MDIO clock divisor to %u/%u Hz host clock.\n",
+		   lp->mii_clk_div, host_clock);
+
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET, lp->mii_clk_div | XAE_MDIO_MC_MDIOEN_MASK);
+
+	return axienet_mdio_wait_until_ready_tsn(lp);
+}
+
+/**
+ * axienet_mdio_disable_tsn - MDIO hardware disable function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Disable the MDIO interface in hardware.
+ */
+void axienet_mdio_disable_tsn(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET, 0);
+}
+
+/**
+ * axienet_mdio_setup_tsn - MDIO setup function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Return:	0 on success, -ETIMEDOUT on a timeout, -ENOMEM when
+ *		mdiobus_alloc (to allocate memory for mii bus structure) fails.
+ *
+ * Sets up the MDIO interface by initializing the MDIO clock.
+ * Register the MDIO interface.
+ */
+int axienet_mdio_setup_tsn(struct axienet_local *lp)
+{
+	struct device_node *mdio_node;
+	struct mii_bus *bus;
+	int ret;
+
+	ret = axienet_mdio_enable_tsn(lp);
+	if (ret < 0)
+		return ret;
+
+	bus = mdiobus_alloc();
+	if (!bus)
+		return -ENOMEM;
+
+	snprintf(bus->id, MII_BUS_ID_SIZE, "axienet-%.8llx",
+		 (unsigned long long)lp->regs_start);
+
+	bus->priv = lp;
+	bus->name = "Xilinx Axi Ethernet MDIO";
+	bus->read = axienet_mdio_read;
+	bus->write = axienet_mdio_write;
+	bus->parent = lp->dev;
+	lp->mii_bus = bus;
+
+	mdio_node = of_get_child_by_name(lp->dev->of_node, "mdio");
+	ret = of_mdiobus_register(bus, mdio_node);
+	of_node_put(mdio_node);
+	if (ret) {
+		mdiobus_free(bus);
+		lp->mii_bus = NULL;
+		return ret;
+	}
+	axienet_mdio_mdc_disable(lp);
+	return 0;
+}
+
+/**
+ * axienet_mdio_teardown_tsn - MDIO remove function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Unregisters the MDIO and frees any associate memory for mii bus.
+ */
+void axienet_mdio_teardown_tsn(struct axienet_local *lp)
+{
+	mdiobus_unregister(lp->mii_bus);
+	mdiobus_free(lp->mii_bus);
+	lp->mii_bus = NULL;
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_axienet_tsn.h b/drivers/staging/xilinx-tsn/xilinx_axienet_tsn.h
new file mode 100644
index 000000000..0e43255fa
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_axienet_tsn.h
@@ -0,0 +1,1357 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Definitions for Xilinx Axi Ethernet device driver.
+ *
+ * Copyright (c) 2009 Secret Lab Technologies, Ltd.
+ * Copyright (c) 2010 - 2012 Xilinx, Inc. All rights reserved.
+ */
+
+#ifndef XILINX_AXIENET_TSN_H
+#define XILINX_AXIENET_TSN_H
+
+#include <linux/netdevice.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/if_vlan.h>
+#include <linux/net_tstamp.h>
+#include <linux/phy.h>
+#include <linux/of_platform.h>
+#include <linux/clk.h>
+#include <linux/io-64-nonatomic-lo-hi.h>
+
+/* Packet size info */
+#define XAE_HDR_SIZE			14 /* Size of Ethernet header */
+#define XAE_TRL_SIZE			 4 /* Size of Ethernet trailer (FCS) */
+#define XAE_MTU			      1500 /* Max MTU of an Ethernet frame */
+#define XAE_JUMBO_MTU		      9000 /* Max MTU of a jumbo Eth. frame */
+
+#define XAE_MAX_FRAME_SIZE	 (XAE_MTU + XAE_HDR_SIZE + XAE_TRL_SIZE)
+#define XAE_MAX_VLAN_FRAME_SIZE  (XAE_MTU + VLAN_ETH_HLEN + XAE_TRL_SIZE)
+#define XAE_MAX_JUMBO_FRAME_SIZE (XAE_JUMBO_MTU + XAE_HDR_SIZE + XAE_TRL_SIZE)
+
+/* Queue 0 is the lowest priority queue */
+#define BE_QUEUE_NUMBER  0
+
+/* DMA address width min and max range */
+#define XAE_DMA_MASK_MIN	32
+#define XAE_DMA_MASK_MAX	64
+
+/* In AXI DMA Tx and Rx queue count is same */
+#define for_each_tx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_tx_queues; (var)++)
+
+#define for_each_rx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_rx_queues; (var)++)
+/* Configuration options */
+
+/* Accept all incoming packets. Default: disabled (cleared) */
+#define XAE_OPTION_PROMISC			BIT(0)
+
+/* Jumbo frame support for Tx & Rx. Default: disabled (cleared) */
+#define XAE_OPTION_JUMBO			BIT(1)
+
+/* VLAN Rx & Tx frame support. Default: disabled (cleared) */
+#define XAE_OPTION_VLAN				BIT(2)
+
+/* Enable recognition of flow control frames on Rx. Default: enabled (set) */
+#define XAE_OPTION_FLOW_CONTROL			BIT(4)
+
+/* Strip FCS and PAD from incoming frames. Note: PAD from VLAN frames is not
+ * stripped. Default: disabled (set)
+ */
+#define XAE_OPTION_FCS_STRIP			BIT(5)
+
+/* Generate FCS field and add PAD automatically for outgoing frames.
+ * Default: enabled (set)
+ */
+#define XAE_OPTION_FCS_INSERT			BIT(6)
+
+/* Enable Length/Type error checking for incoming frames. When this option is
+ * set, the MAC will filter frames that have a mismatched type/length field
+ * and if XAE_OPTION_REPORT_RXERR is set, the user is notified when these
+ * types of frames are encountered. When this option is cleared, the MAC will
+ * allow these types of frames to be received. Default: enabled (set)
+ */
+#define XAE_OPTION_LENTYPE_ERR			BIT(7)
+
+/* Enable the transmitter. Default: enabled (set) */
+#define XAE_OPTION_TXEN				BIT(11)
+
+/*  Enable the receiver. Default: enabled (set) */
+#define XAE_OPTION_RXEN				BIT(12)
+
+/*  Default options set when device is initialized or reset */
+#define XAE_OPTION_DEFAULTS				   \
+				(XAE_OPTION_TXEN |	   \
+				 XAE_OPTION_FLOW_CONTROL | \
+				 XAE_OPTION_RXEN)
+
+/* Axi DMA Register definitions */
+
+#define XAXIDMA_TX_CR_OFFSET	0x00000000 /* Channel control */
+#define XAXIDMA_TX_SR_OFFSET	0x00000004 /* Status */
+#define XAXIDMA_TX_CDESC_OFFSET	0x00000008 /* Current descriptor pointer */
+#define XAXIDMA_TX_TDESC_OFFSET	0x00000010 /* Tail descriptor pointer */
+
+#define XAXIDMA_RX_CR_OFFSET	0x00000030 /* Channel control */
+#define XAXIDMA_RX_SR_OFFSET	0x00000034 /* Status */
+#define XAXIDMA_RX_CDESC_OFFSET	0x00000038 /* Current descriptor pointer */
+#define XAXIDMA_RX_TDESC_OFFSET	0x00000040 /* Tail descriptor pointer */
+
+#define XAXIDMA_CR_RUNSTOP_MASK	0x00000001 /* Start/stop DMA channel */
+#define XAXIDMA_CR_RESET_MASK	0x00000004 /* Reset DMA engine */
+
+#define XAXIDMA_SR_HALT_MASK	0x00000001 /* Indicates DMA channel halted */
+
+#define XAXIDMA_BD_NDESC_OFFSET		0x00 /* Next descriptor pointer */
+#define XAXIDMA_BD_BUFA_OFFSET		0x08 /* Buffer address */
+#define XAXIDMA_BD_CTRL_LEN_OFFSET	0x18 /* Control/buffer length */
+#define XAXIDMA_BD_STS_OFFSET		0x1C /* Status */
+#define XAXIDMA_BD_USR0_OFFSET		0x20 /* User IP specific word0 */
+#define XAXIDMA_BD_USR1_OFFSET		0x24 /* User IP specific word1 */
+#define XAXIDMA_BD_USR2_OFFSET		0x28 /* User IP specific word2 */
+#define XAXIDMA_BD_USR3_OFFSET		0x2C /* User IP specific word3 */
+#define XAXIDMA_BD_USR4_OFFSET		0x30 /* User IP specific word4 */
+#define XAXIDMA_BD_ID_OFFSET		0x34 /* Sw ID */
+#define XAXIDMA_BD_HAS_STSCNTRL_OFFSET	0x38 /* Whether has stscntrl strm */
+#define XAXIDMA_BD_HAS_DRE_OFFSET	0x3C /* Whether has DRE */
+
+#define XAXIDMA_BD_HAS_DRE_SHIFT	8 /* Whether has DRE shift */
+#define XAXIDMA_BD_HAS_DRE_MASK		0xF00 /* Whether has DRE mask */
+#define XAXIDMA_BD_WORDLEN_MASK		0xFF /* Whether has DRE mask */
+
+#define XAXIDMA_BD_CTRL_LENGTH_MASK	0x007FFFFF /* Requested len */
+#define XAXIDMA_BD_CTRL_TXSOF_MASK	0x08000000 /* First tx packet */
+#define XAXIDMA_BD_CTRL_TXEOF_MASK	0x04000000 /* Last tx packet */
+#define XAXIDMA_BD_CTRL_ALL_MASK	0x0C000000 /* All control bits */
+
+#define XAXIDMA_DELAY_MASK		0xFF000000 /* Delay timeout counter */
+#define XAXIDMA_COALESCE_MASK		0x00FF0000 /* Coalesce counter */
+
+#define XAXIDMA_DELAY_SHIFT		24
+#define XAXIDMA_COALESCE_SHIFT		16
+
+#define XAXIDMA_IRQ_IOC_MASK		0x00001000 /* Completion intr */
+#define XAXIDMA_IRQ_DELAY_MASK		0x00002000 /* Delay interrupt */
+#define XAXIDMA_IRQ_ERROR_MASK		0x00004000 /* Error interrupt */
+#define XAXIDMA_IRQ_ALL_MASK		0x00007000 /* All interrupts */
+
+/* Default TX/RX Threshold and waitbound values for SGDMA mode */
+#define XAXIDMA_DFT_TX_THRESHOLD	24
+#define XAXIDMA_DFT_TX_WAITBOUND	254
+#define XAXIDMA_DFT_RX_THRESHOLD	1
+#define XAXIDMA_DFT_RX_WAITBOUND	254
+
+#define XAXIDMA_BD_CTRL_TXSOF_MASK	0x08000000 /* First tx packet */
+#define XAXIDMA_BD_CTRL_TXEOF_MASK	0x04000000 /* Last tx packet */
+#define XAXIDMA_BD_CTRL_ALL_MASK	0x0C000000 /* All control bits */
+
+#define XAXIDMA_BD_STS_ACTUAL_LEN_MASK	0x007FFFFF /* Actual len */
+#define XAXIDMA_BD_STS_COMPLETE_MASK	0x80000000 /* Completed */
+#define XAXIDMA_BD_STS_DEC_ERR_MASK	0x40000000 /* Decode error */
+#define XAXIDMA_BD_STS_SLV_ERR_MASK	0x20000000 /* Slave error */
+#define XAXIDMA_BD_STS_INT_ERR_MASK	0x10000000 /* Internal err */
+#define XAXIDMA_BD_STS_ALL_ERR_MASK	0x70000000 /* All errors */
+#define XAXIDMA_BD_STS_RXSOF_MASK	0x08000000 /* First rx pkt */
+#define XAXIDMA_BD_STS_RXEOF_MASK	0x04000000 /* Last rx pkt */
+#define XAXIDMA_BD_STS_ALL_MASK		0xFC000000 /* All status bits */
+
+#define XAXIDMA_BD_MINIMUM_ALIGNMENT	0x40
+
+/* AXI Tx Timestamp Stream FIFO Register Definitions */
+#define XAXIFIFO_TXTS_ISR	0x00000000 /* Interrupt Status Register */
+#define XAXIFIFO_TXTS_TDFV	0x0000000C /* Transmit Data FIFO Vacancy */
+#define XAXIFIFO_TXTS_TXFD	0x00000010 /* Tx Data Write Port */
+#define XAXIFIFO_TXTS_TLR	0x00000014 /* Transmit Length Register */
+#define XAXIFIFO_TXTS_RFO	0x0000001C /* Rx Fifo Occupancy */
+#define XAXIFIFO_TXTS_RDFR	0x00000018 /* Rx Fifo reset */
+#define XAXIFIFO_TXTS_RXFD	0x00000020 /* Rx Data Read Port */
+#define XAXIFIFO_TXTS_RLR	0x00000024 /* Receive Length Register */
+#define XAXIFIFO_TXTS_SRR	0x00000028 /* AXI4-Stream Reset */
+
+#define XAXIFIFO_TXTS_INT_RC_MASK	0x04000000
+#define XAXIFIFO_TXTS_RXFD_MASK		0x7FFFFFFF
+#define XAXIFIFO_TXTS_RESET_MASK	0x000000A5
+#define XAXIFIFO_TXTS_TAG_MASK		0xFFFF0000
+#define XAXIFIFO_TXTS_TAG_SHIFT		16
+#define XAXIFIFO_TXTS_TAG_MAX		0xFFFE
+
+/* Axi Ethernet registers definition */
+#define XAE_RAF_OFFSET		0x00000000 /* Reset and Address filter */
+#define XAE_TPF_OFFSET		0x00000004 /* Tx Pause Frame */
+#define XAE_IFGP_OFFSET		0x00000008 /* Tx Inter-frame gap adjustment*/
+#define XAE_IS_OFFSET		0x0000000C /* Interrupt status */
+#define XAE_IP_OFFSET		0x00000010 /* Interrupt pending */
+#define XAE_IE_OFFSET		0x00000014 /* Interrupt enable */
+#define XAE_TTAG_OFFSET		0x00000018 /* Tx VLAN TAG */
+#define XAE_RTAG_OFFSET		0x0000001C /* Rx VLAN TAG */
+#define XAE_UAWL_OFFSET		0x00000020 /* Unicast address word lower */
+#define XAE_UAWU_OFFSET		0x00000024 /* Unicast address word upper */
+#define XAE_TPID0_OFFSET	0x00000028 /* VLAN TPID0 register */
+#define XAE_TPID1_OFFSET	0x0000002C /* VLAN TPID1 register */
+#define XAE_PPST_OFFSET		0x00000030 /* PCS PMA Soft Temac Status Reg */
+#define XAE_RCW0_OFFSET		0x00000400 /* Rx Configuration Word 0 */
+#define XAE_RCW1_OFFSET		0x00000404 /* Rx Configuration Word 1 */
+#define XAE_TC_OFFSET		0x00000408 /* Tx Configuration */
+#define XAE_FCC_OFFSET		0x0000040C /* Flow Control Configuration */
+#define XAE_ID_OFFSET		0x000004F8 /* Identification register */
+#define XAE_EMMC_OFFSET		0x00000410 /* MAC speed configuration */
+#define XAE_RMFC_OFFSET		0x00000414 /* RX Max Frame Configuration */
+#define XAE_TSN_ABL_OFFSET	0x000004FC /* Ability Register */
+#define XAE_MDIO_MC_OFFSET	0x00000500 /* MDIO Setup */
+#define XAE_MDIO_MCR_OFFSET	0x00000504 /* MDIO Control */
+#define XAE_MDIO_MWD_OFFSET	0x00000508 /* MDIO Write Data */
+#define XAE_MDIO_MRD_OFFSET	0x0000050C /* MDIO Read Data */
+#define XAE_TEMAC_IS_OFFSET	0x00000600 /* TEMAC Interrupt Status */
+#define XAE_TEMAC_IP_OFFSET	0x00000610 /* TEMAC Interrupt Pending Status */
+#define XAE_TEMAC_IE_OFFSET	0x00000620 /* TEMAC Interrupt Enable Status */
+#define XAE_TEMAC_IC_OFFSET	0x00000630 /* TEMAC Interrupt Clear Status */
+#define XAE_UAW0_OFFSET		0x00000700 /* Unicast address word 0 */
+#define XAE_UAW1_OFFSET		0x00000704 /* Unicast address word 1 */
+#define XAE_FMC_OFFSET		0x00000708 /* Frame Filter Control */
+#define XAE_AF0_OFFSET		0x00000710 /* Address Filter 0 */
+#define XAE_AF1_OFFSET		0x00000714 /* Address Filter 1 */
+#define XAE_FF_3_OFFSET		0x0000071C /* Frame Filter 3 */
+#define XAE_FF_5_OFFSET		0x00000724 /* Frame Filter 5 */
+#define XAE_FF_9_OFFSET		0x00000734 /* Frame Filter 9 */
+#define XAE_FF_10_OFFSET	0x00000738 /* Frame Filter 10 */
+#define XAE_AF0_MASK_OFFSET	0x00000750 /* Address Filter Mask 0 */
+#define XAE_AF1_MASK_OFFSET	0x00000754 /* Address Filter Mask 1 */
+#define XAE_FF_5_MASK_OFFSET	0x00000764 /* Frame Filter Mask register 5 */
+#define XAE_FF_9_MASK_OFFSET	0x00000774 /* Frame Filter Mask register 9 */
+#define XAE_FF_10_MASK_OFFSET	0x00000778 /* Frame Filter Mask register 10 */
+
+#define XAE_TX_VLAN_DATA_OFFSET 0x00004000 /* TX VLAN data table address */
+#define XAE_RX_VLAN_DATA_OFFSET 0x00008000 /* RX VLAN data table address */
+#define XAE_MCAST_TABLE_OFFSET	0x00020000 /* Multicast table address */
+
+/* Bit Masks for Axi Ethernet RAF register */
+/* Reject receive multicast destination address */
+#define XAE_RAF_MCSTREJ_MASK		0x00000002
+/* Reject receive broadcast destination address */
+#define XAE_RAF_BCSTREJ_MASK		0x00000004
+#define XAE_RAF_TXVTAGMODE_MASK		0x00000018 /* Tx VLAN TAG mode */
+#define XAE_RAF_RXVTAGMODE_MASK		0x00000060 /* Rx VLAN TAG mode */
+#define XAE_RAF_TXVSTRPMODE_MASK	0x00000180 /* Tx VLAN STRIP mode */
+#define XAE_RAF_RXVSTRPMODE_MASK	0x00000600 /* Rx VLAN STRIP mode */
+#define XAE_RAF_NEWFNCENBL_MASK		0x00000800 /* New function mode */
+/* Extended Multicast Filtering mode */
+#define XAE_RAF_EMULTIFLTRENBL_MASK	0x00001000
+#define XAE_RAF_STATSRST_MASK		0x00002000 /* Stats. Counter Reset */
+#define XAE_RAF_RXBADFRMEN_MASK		0x00004000 /* Recv Bad Frame Enable */
+#define XAE_RAF_TXVTAGMODE_SHIFT	3 /* Tx Tag mode shift bits */
+#define XAE_RAF_RXVTAGMODE_SHIFT	5 /* Rx Tag mode shift bits */
+#define XAE_RAF_TXVSTRPMODE_SHIFT	7 /* Tx strip mode shift bits*/
+#define XAE_RAF_RXVSTRPMODE_SHIFT	9 /* Rx Strip mode shift bits*/
+
+/* Bit Masks for Axi Ethernet TPF and IFGP registers */
+#define XAE_TPF_TPFV_MASK		0x0000FFFF /* Tx pause frame value */
+/* Transmit inter-frame gap adjustment value */
+#define XAE_IFGP0_IFGP_MASK		0x0000007F
+
+/* Bit Masks for Axi Ethernet IS, IE and IP registers, Same masks apply
+ * for all 3 registers.
+ */
+/* Hard register access complete */
+#define XAE_INT_HARDACSCMPLT_MASK	0x00000001
+/* Auto negotiation complete */
+#define XAE_INT_AUTONEG_MASK		0x00000002
+#define XAE_INT_RXCMPIT_MASK		0x00000004 /* Rx complete */
+#define XAE_INT_RXRJECT_MASK		0x00000008 /* Rx frame rejected */
+#define XAE_INT_RXFIFOOVR_MASK		0x00000010 /* Rx fifo overrun */
+#define XAE_INT_TXCMPIT_MASK		0x00000020 /* Tx complete */
+#define XAE_INT_RXDCMLOCK_MASK		0x00000040 /* Rx Dcm Lock */
+#define XAE_INT_MGTRDY_MASK		0x00000080 /* MGT clock Lock */
+#define XAE_INT_PHYRSTCMPLT_MASK	0x00000100 /* Phy Reset complete */
+#define XAE_INT_ALL_MASK		0x0000003F /* All the ints */
+
+/* INT bits that indicate receive errors */
+#define XAE_INT_RECV_ERROR_MASK				\
+	(XAE_INT_RXRJECT_MASK | XAE_INT_RXFIFOOVR_MASK)
+
+/* Bit masks for Axi Ethernet VLAN TPID Word 0 register */
+#define XAE_TPID_0_MASK		0x0000FFFF /* TPID 0 */
+#define XAE_TPID_1_MASK		0xFFFF0000 /* TPID 1 */
+
+/* Bit masks for Axi Ethernet VLAN TPID Word 1 register */
+#define XAE_TPID_2_MASK		0x0000FFFF /* TPID 0 */
+#define XAE_TPID_3_MASK		0xFFFF0000 /* TPID 1 */
+
+/* Bit masks for Axi Ethernet RCW1 register */
+#define XAE_RCW1_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
+#define XAE_RCW1_RST_MASK	0x80000000 /* Reset */
+#define XAE_RCW1_JUM_MASK	0x40000000 /* Jumbo frame enable */
+/* In-Band FCS enable (FCS not stripped) */
+#define XAE_RCW1_FCS_MASK	0x20000000
+#define XAE_RCW1_RX_MASK	0x10000000 /* Receiver enable */
+#define XAE_RCW1_VLAN_MASK	0x08000000 /* VLAN frame enable */
+/* Length/type field valid check disable */
+#define XAE_RCW1_LT_DIS_MASK	0x02000000
+/* Control frame Length check disable */
+#define XAE_RCW1_CL_DIS_MASK	0x01000000
+/* Pause frame source address bits [47:32]. Bits [31:0] are
+ * stored in register RCW0
+ */
+#define XAE_RCW1_PAUSEADDR_MASK 0x0000FFFF
+
+/* Bit masks for Axi Ethernet TC register */
+#define XAE_TC_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
+#define XAE_TC_RST_MASK		0x80000000 /* Reset */
+#define XAE_TC_JUM_MASK		0x40000000 /* Jumbo frame enable */
+/* In-Band FCS enable (FCS not generated) */
+#define XAE_TC_FCS_MASK		0x20000000
+#define XAE_TC_TX_MASK		0x10000000 /* Transmitter enable */
+#define XAE_TC_VLAN_MASK	0x08000000 /* VLAN frame enable */
+/* Inter-frame gap adjustment enable */
+#define XAE_TC_IFG_MASK		0x02000000
+
+/* Bit masks for Axi Ethernet FCC register */
+#define XAE_FCC_FCRX_MASK	0x20000000 /* Rx flow control enable */
+#define XAE_FCC_FCTX_MASK	0x40000000 /* Tx flow control enable */
+
+/* Bit masks for Axi Ethernet EMMC register */
+#define XAE_EMMC_LINKSPEED_MASK	0xC0000000 /* Link speed */
+#define XAE_EMMC_RGMII_MASK	0x20000000 /* RGMII mode enable */
+#define XAE_EMMC_SGMII_MASK	0x10000000 /* SGMII mode enable */
+#define XAE_EMMC_GPCS_MASK	0x08000000 /* 1000BaseX mode enable */
+#define XAE_EMMC_HOST_MASK	0x04000000 /* Host interface enable */
+#define XAE_EMMC_TX16BIT	0x02000000 /* 16 bit Tx client enable */
+#define XAE_EMMC_RX16BIT	0x01000000 /* 16 bit Rx client enable */
+#define XAE_EMMC_LINKSPD_10	0x00000000 /* Link Speed mask for 10 Mbit */
+#define XAE_EMMC_LINKSPD_100	0x40000000 /* Link Speed mask for 100 Mbit */
+#define XAE_EMMC_LINKSPD_1000	0x80000000 /* Link Speed mask for 1000 Mbit */
+#define XAE_EMMC_LINKSPD_2500	0x80000000 /* Link Speed mask for 2500 Mbit */
+
+/* Bit masks for Axi Ethernet MDIO interface MC register */
+#define XAE_MDIO_MC_MDIOEN_MASK		0x00000040 /* MII management enable */
+#define XAE_MDIO_MC_CLOCK_DIVIDE_MAX	0x3F	   /* Maximum MDIO divisor */
+
+/* Bit masks for Axi Ethernet MDIO interface MCR register */
+#define XAE_MDIO_MCR_PHYAD_MASK		0x1F000000 /* Phy Address Mask */
+#define XAE_MDIO_MCR_PHYAD_SHIFT	24	   /* Phy Address Shift */
+#define XAE_MDIO_MCR_REGAD_MASK		0x001F0000 /* Reg Address Mask */
+#define XAE_MDIO_MCR_REGAD_SHIFT	16	   /* Reg Address Shift */
+#define XAE_MDIO_MCR_OP_MASK		0x0000C000 /* Operation Code Mask */
+#define XAE_MDIO_MCR_OP_SHIFT		13	   /* Operation Code Shift */
+#define XAE_MDIO_MCR_OP_READ_MASK	0x00008000 /* Op Code Read Mask */
+#define XAE_MDIO_MCR_OP_WRITE_MASK	0x00004000 /* Op Code Write Mask */
+#define XAE_MDIO_MCR_INITIATE_MASK	0x00000800 /* Ready Mask */
+#define XAE_MDIO_MCR_READY_MASK		0x00000080 /* Ready Mask */
+
+/* Bit masks for Axi Ethernet UAW1 register */
+/* Station address bits [47:32]; Station address
+ * bits [31:0] are stored in register UAW0
+ */
+#define XAE_UAW1_UNICASTADDR_MASK	0x0000FFFF
+
+/* Bit masks for Axi Ethernet FMC register */
+#define XAE_FMC_PM_MASK			0x80000000 /* Promis. mode enable */
+#define XAE_FMC_IND_MASK		0x00000003 /* Index Mask */
+
+#define XAE_MDIO_DIV_DFT		29 /* Default MDIO clock divisor */
+
+/* Total number of entries in the hardware multicast table. */
+#define XAE_MULTICAST_CAM_TABLE_NUM	4
+
+/* Axi Ethernet Synthesis features */
+#define XAE_FEATURE_PARTIAL_RX_CSUM	BIT(0)
+#define XAE_FEATURE_PARTIAL_TX_CSUM	BIT(1)
+#define XAE_FEATURE_FULL_RX_CSUM	BIT(2)
+#define XAE_FEATURE_FULL_TX_CSUM	BIT(3)
+#define XAE_FEATURE_DMA_64BIT		BIT(4)
+
+#define XAE_NO_CSUM_OFFLOAD		0
+
+#define XAE_FULL_CSUM_STATUS_MASK	0x00000038
+#define XAE_IP_UDP_CSUM_VALIDATED	0x00000003
+#define XAE_IP_TCP_CSUM_VALIDATED	0x00000002
+
+#define DELAY_OF_ONE_MILLISEC		1000
+
+#define XAXIENET_NAPI_WEIGHT		64
+
+/* Definition of 1588 PTP in Axi Ethernet IP */
+#define TX_TS_OP_NOOP           0x0
+#define TX_TS_OP_ONESTEP        0x1
+#define TX_TS_OP_TWOSTEP        0x2
+#define TX_TS_CSUM_UPDATE       0x1
+#define TX_TS_CSUM_UPDATE_MRMAC		0x4
+#define TX_TS_PDELAY_UPDATE_MRMAC	0x8
+#define TX_PTP_CSUM_OFFSET      0x28
+#define TX_PTP_TS_OFFSET        0x4C
+#define TX_PTP_CF_OFFSET        0x32
+
+/* XXV MAC Register Definitions */
+#define XXV_GT_RESET_OFFSET		0x00000000
+#define XXV_TC_OFFSET			0x0000000C
+#define XXV_RCW1_OFFSET			0x00000014
+#define XXV_JUM_OFFSET			0x00000018
+#define XXV_TICKREG_OFFSET		0x00000020
+#define XXV_STATRX_BLKLCK_OFFSET	0x0000040C
+#define XXV_USXGMII_AN_OFFSET		0x000000C8
+#define XXV_USXGMII_AN_STS_OFFSET	0x00000458
+#define XXV_STAT_GTWIZ_OFFSET		0x000004A0
+#define XXV_CONFIG_REVISION		0x00000024
+
+/* XXV MAC Register Mask Definitions */
+#define XXV_GT_RESET_MASK	BIT(0)
+#define XXV_TC_TX_MASK		BIT(0)
+#define XXV_RCW1_RX_MASK	BIT(0)
+#define XXV_RCW1_FCS_MASK	BIT(1)
+#define XXV_TC_FCS_MASK		BIT(1)
+#define XXV_MIN_JUM_MASK	GENMASK(7, 0)
+#define XXV_MAX_JUM_MASK	GENMASK(10, 8)
+#define XXV_RX_BLKLCK_MASK	BIT(0)
+#define XXV_TICKREG_STATEN_MASK BIT(0)
+#define XXV_MAC_MIN_PKT_LEN	64
+#define XXV_GTWIZ_RESET_DONE	(BIT(0) | BIT(1))
+#define XXV_MAJ_MASK		GENMASK(7, 0)
+#define XXV_MIN_MASK		GENMASK(15, 8)
+
+/* USXGMII Register Mask Definitions  */
+#define USXGMII_AN_EN		BIT(5)
+#define USXGMII_AN_RESET	BIT(6)
+#define USXGMII_AN_RESTART	BIT(7)
+#define USXGMII_EN		BIT(16)
+#define USXGMII_RATE_MASK	0x0E000700
+#define USXGMII_RATE_1G		0x04000200
+#define USXGMII_RATE_2G5	0x08000400
+#define USXGMII_RATE_10M	0x0
+#define USXGMII_RATE_100M	0x02000100
+#define USXGMII_RATE_5G		0x0A000500
+#define USXGMII_RATE_10G	0x06000300
+#define USXGMII_FD		BIT(28)
+#define USXGMII_LINK_STS	BIT(31)
+
+/* USXGMII AN STS register mask definitions */
+#define USXGMII_AN_STS_COMP_MASK	BIT(16)
+
+/* MCDMA Register Definitions */
+#define XMCDMA_CR_OFFSET	0x00
+#define XMCDMA_SR_OFFSET	0x04
+#define XMCDMA_CHEN_OFFSET	0x08
+#define XMCDMA_CHSER_OFFSET	0x0C
+#define XMCDMA_ERR_OFFSET	0x10
+#define XMCDMA_PKTDROP_OFFSET	0x14
+#define XMCDMA_TXWEIGHT0_OFFSET 0x18
+#define XMCDMA_TXWEIGHT1_OFFSET 0x1C
+#define XMCDMA_RXINT_SER_OFFSET 0x20
+#define XMCDMA_TXINT_SER_OFFSET 0x28
+
+#define XMCDMA_CHOBS1_OFFSET	0x440
+#define XMCDMA_CHOBS2_OFFSET	0x444
+#define XMCDMA_CHOBS3_OFFSET	0x448
+#define XMCDMA_CHOBS4_OFFSET	0x44C
+#define XMCDMA_CHOBS5_OFFSET	0x450
+#define XMCDMA_CHOBS6_OFFSET	0x454
+
+#define XMCDMA_CHAN_RX_OFFSET  0x500
+
+/* Per Channel Registers */
+#define XMCDMA_CHAN_CR_OFFSET(chan_id)		(0x40 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_SR_OFFSET(chan_id)		(0x44 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_CURDESC_OFFSET(chan_id)	(0x48 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_TAILDESC_OFFSET(chan_id)	(0x50 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_PKTDROP_OFFSET(chan_id)	(0x58 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_SR_IDLE_MASK		BIT(0)
+
+#define XMCDMA_RX_OFFSET	0x500
+
+/* MCDMA Mask registers */
+#define XMCDMA_CR_RUNSTOP_MASK		BIT(0) /* Start/stop DMA channel */
+#define XMCDMA_CR_RESET_MASK		BIT(2) /* Reset DMA engine */
+
+#define XMCDMA_SR_HALTED_MASK		BIT(0)
+#define XMCDMA_SR_IDLE_MASK		BIT(1)
+
+#define XMCDMA_IRQ_ERRON_OTHERQ_MASK	BIT(3)
+#define XMCDMA_IRQ_PKTDROP_MASK		BIT(4)
+#define XMCDMA_IRQ_IOC_MASK		BIT(5)
+#define XMCDMA_IRQ_DELAY_MASK		BIT(6)
+#define XMCDMA_IRQ_ERR_MASK		BIT(7)
+#define XMCDMA_IRQ_ALL_MASK		GENMASK(7, 5)
+#define XMCDMA_PKTDROP_COALESCE_MASK	GENMASK(15, 8)
+#define XMCDMA_COALESCE_MASK		GENMASK(23, 16)
+#define XMCDMA_DELAY_MASK		GENMASK(31, 24)
+
+#define XMCDMA_CHEN_MASK		GENMASK(7, 0)
+#define XMCDMA_CHID_MASK		GENMASK(7, 0)
+
+#define XMCDMA_ERR_INTERNAL_MASK	BIT(0)
+#define XMCDMA_ERR_SLAVE_MASK		BIT(1)
+#define XMCDMA_ERR_DECODE_MASK		BIT(2)
+#define XMCDMA_ERR_SG_INT_MASK		BIT(4)
+#define XMCDMA_ERR_SG_SLV_MASK		BIT(5)
+#define XMCDMA_ERR_SG_DEC_MASK		BIT(6)
+
+#define XMCDMA_PKTDROP_CNT_MASK		GENMASK(31, 0)
+
+#define XMCDMA_BD_CTRL_TXSOF_MASK	0x80000000 /* First tx packet */
+#define XMCDMA_BD_CTRL_TXEOF_MASK	0x40000000 /* Last tx packet */
+#define XMCDMA_BD_CTRL_ALL_MASK		0xC0000000 /* All control bits */
+#define XMCDMA_BD_STS_ALL_MASK		0xF0000000 /* All status bits */
+#define XMCDMA_BD_SD_STS_ALL_MASK	0x00000030 /* TUSER input port bits */
+#define XMCDMA_BD_SD_MGMT_VALID_MASK	BIT(0) /* Management frame */
+
+#define XMCDMA_BD_SD_STS_TUSER_EP	0x00000000
+#define XMCDMA_BD_SD_STS_TUSER_MAC_1	0x00000010
+#define XMCDMA_BD_SD_STS_TUSER_MAC_2	0x00000020
+#define XMCDMA_BD_SD_STS_TUSER_EX_EP	0x00000030
+
+#define XMCDMA_COALESCE_SHIFT		16
+#define XMCDMA_DELAY_SHIFT		24
+#define XMCDMA_DFT_TX_THRESHOLD		1
+
+#define XMCDMA_TXWEIGHT_CH_MASK(chan_id)	GENMASK(((chan_id) * 4 + 3), \
+							(chan_id) * 4)
+#define XMCDMA_TXWEIGHT_CH_SHIFT(chan_id)	((chan_id) * 4)
+
+/* PTP Packet length */
+#define XAE_TX_PTP_LEN		16
+#define XXV_TX_PTP_LEN		12
+
+/* Macros used when AXI DMA h/w is configured without DRE */
+#define XAE_TX_BUFFERS		64
+#define XAE_MAX_PKT_LEN		8192
+
+/* MRMAC Register Definitions */
+/* Configuration Registers */
+#define MRMAC_REV_OFFSET		0x00000000
+#define MRMAC_RESET_OFFSET		0x00000004
+#define MRMAC_MODE_OFFSET		0x00000008
+#define MRMAC_CONFIG_TX_OFFSET		0x0000000C
+#define MRMAC_CONFIG_RX_OFFSET		0x00000010
+#define MRMAC_TICK_OFFSET		0x0000002C
+#define MRMAC_CFG1588_OFFSET	0x00000040
+
+/* Status Registers */
+#define MRMAC_TX_STS_OFFSET		0x00000740
+#define MRMAC_RX_STS_OFFSET		0x00000744
+#define MRMAC_TX_RT_STS_OFFSET		0x00000748
+#define MRMAC_RX_RT_STS_OFFSET		0x0000074C
+#define MRMAC_STATRX_BLKLCK_OFFSET	0x00000754
+#define MRMAC_STATRX_VALID_CTRL_OFFSET	0x000007B8
+
+/* Register bit masks */
+#define MRMAC_RX_SERDES_RST_MASK	(BIT(3) | BIT(2) | BIT(1) | BIT(0))
+#define MRMAC_TX_SERDES_RST_MASK	BIT(4)
+#define MRMAC_RX_RST_MASK		BIT(5)
+#define MRMAC_TX_RST_MASK		BIT(6)
+#define MRMAC_RX_AXI_RST_MASK		BIT(8)
+#define MRMAC_TX_AXI_RST_MASK		BIT(9)
+#define MRMAC_STS_ALL_MASK		0xFFFFFFFF
+
+#define MRMAC_RX_EN_MASK		BIT(0)
+#define MRMAC_RX_DEL_FCS_MASK		BIT(1)
+
+#define MRMAC_TX_EN_MASK		BIT(0)
+#define MRMAC_TX_INS_FCS_MASK		BIT(1)
+
+#define MRMAC_RX_BLKLCK_MASK		BIT(0)
+#define MRMAC_RX_STATUS_MASK		BIT(0)
+#define MRMAC_RX_VALID_MASK		BIT(0)
+
+#define MRMAC_CTL_DATA_RATE_MASK	GENMASK(2, 0)
+#define MRMAC_CTL_DATA_RATE_10G		0
+#define MRMAC_CTL_DATA_RATE_25G		1
+#define MRMAC_CTL_DATA_RATE_40G		2
+#define MRMAC_CTL_DATA_RATE_50G		3
+#define MRMAC_CTL_DATA_RATE_100G	4
+
+#define MRMAC_CTL_AXIS_CFG_MASK		GENMASK(11, 9)
+#define MRMAC_CTL_AXIS_CFG_SHIFT	9
+#define MRMAC_CTL_AXIS_CFG_10G_IND	1
+#define MRMAC_CTL_AXIS_CFG_25G_IND	1
+
+#define MRMAC_CTL_SERDES_WIDTH_MASK	GENMASK(6, 4)
+#define MRMAC_CTL_SERDES_WIDTH_SHIFT	4
+#define MRMAC_CTL_SERDES_WIDTH_10G	4
+#define MRMAC_CTL_SERDES_WIDTH_25G	6
+
+#define MRMAC_CTL_RATE_CFG_MASK		(MRMAC_CTL_DATA_RATE_MASK |	\
+					 MRMAC_CTL_AXIS_CFG_MASK |	\
+					 MRMAC_CTL_SERDES_WIDTH_MASK)
+
+#define MRMAC_CTL_PM_TICK_MASK		BIT(30)
+#define MRMAC_TICK_TRIGGER		BIT(0)
+#define MRMAC_ONE_STEP_EN		BIT(0)
+
+/* MRMAC GT wrapper registers */
+#define MRMAC_GT_PLL_OFFSET		0x0
+#define MRMAC_GT_PLL_STS_OFFSET		0x8
+#define MRMAC_GT_RATE_OFFSET		0x0
+#define MRMAC_GT_CTRL_OFFSET		0x8
+
+#define MRMAC_GT_PLL_RST_MASK		0x00030003
+#define MRMAC_GT_PLL_DONE_MASK		0xFF
+#define MRMAC_GT_RST_ALL_MASK		BIT(0)
+#define MRMAC_GT_RST_RX_MASK		BIT(1)
+#define MRMAC_GT_RST_TX_MASK		BIT(2)
+#define MRMAC_GT_10G_MASK		0x00000001
+#define MRMAC_GT_25G_MASK		0x00000002
+
+#define MRMAC_GT_LANE_OFFSET		BIT(16)
+#define MRMAC_MAX_GT_LANES		4
+/**
+ * struct aximcdma_bd - Axi MCDMA buffer descriptor layout
+ * @next:         MM2S/S2MM Next Descriptor Pointer
+ * @reserved1:    Reserved and not used for 32-bit
+ * @phys:         MM2S/S2MM Buffer Address
+ * @reserved2:    Reserved and not used for 32-bit
+ * @reserved3:    Reserved and not used
+ * @cntrl:        MM2S/S2MM Control value
+ * @status:       S2MM Status value
+ * @sband_stats:  S2MM Sideband Status value
+ *		  MM2S Status value
+ * @app0:         MM2S/S2MM User Application Field 0.
+ * @app1:         MM2S/S2MM User Application Field 1.
+ * @app2:         MM2S/S2MM User Application Field 2.
+ * @app3:         MM2S/S2MM User Application Field 3.
+ * @app4:         MM2S/S2MM User Application Field 4.
+ * @sw_id_offset: MM2S/S2MM Sw ID
+ * @ptp_tx_skb:   If timestamping is enabled used for timestamping skb
+ *		  Otherwise reserved.
+ * @ptp_tx_ts_tag: Tag value of 2 step timestamping if timestamping is enabled
+ *		   Otherwise reserved.
+ * @tx_skb:	  Transmit skb address
+ * @tx_desc_mapping: Tx Descriptor DMA mapping type.
+ */
+struct aximcdma_bd {
+	phys_addr_t next;	/* Physical address of next buffer descriptor */
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved1;
+#endif
+	phys_addr_t phys;
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved2;
+#endif
+	u32 reserved3;
+	u32 cntrl;
+	u32 status;
+	u32 sband_stats;
+	u32 app0;
+	u32 app1;	/* TX start << 16 | insert */
+	u32 app2;	/* TX csum seed */
+	u32 app3;
+	u32 app4;
+	struct sk_buff *sw_id_offset; /* first unused field by h/w */
+	struct sk_buff *ptp_tx_skb;
+	u32 ptp_tx_ts_tag;
+	struct sk_buff *tx_skb;
+	u32 tx_desc_mapping;
+} __aligned(XAXIDMA_BD_MINIMUM_ALIGNMENT);
+
+#define XAE_NUM_MISC_CLOCKS 3
+#define DESC_DMA_MAP_SINGLE 0
+#define DESC_DMA_MAP_PAGE 1
+
+/* In Legacy design: TSN queues range is 2 to 5.
+ * For eg: for num_tc = 2 minimum queues = 2;
+ * for num_tc = 3 with sideband signalling maximum queues = 5
+ *
+ * In eight queue design: 8 queues are for EP and 8 queues for EX_EP
+ */
+#define XAE_MAX_QUEUES		16
+
+#define XAE_MAX_TSN_TC		8
+#define XAE_MIN_LEGACY_TSN_TC	2
+#define XAE_MAX_LEGACY_TSN_TC	3
+#define XAE_TSN_MIN_QUEUES	4
+
+#define TSN_BRIDGEEP_EPONLY	BIT(29)
+
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+#define TADMA_MAX_NO_STREAM	128
+struct axitadma_bd {
+	u32 phys;
+	struct sk_buff *tx_skb;
+	u32 tx_desc_mapping;
+	u32 num_frag;
+	u32 len;
+};
+#endif
+
+enum axienet_tsn_ioctl {
+	SIOCCHIOCTL = SIOCDEVPRIVATE,
+	SIOC_GET_SCHED,
+	SIOC_PREEMPTION_CFG,
+	SIOC_PREEMPTION_CTRL,
+	SIOC_PREEMPTION_STS,
+	SIOC_PREEMPTION_COUNTER,
+	SIOC_QBU_USER_OVERRIDE,
+	SIOC_QBU_STS,
+	SIOC_TADMA_STR_ADD,
+	SIOC_TADMA_PROG_ALL,
+	SIOC_TADMA_STR_FLUSH,
+	SIOC_PREEMPTION_RECEIVE,
+	SIOC_TADMA_OFF,
+};
+
+/**
+ * struct axienet_tsn_txq - TX queues to DMA channel mapping
+ * @is_tadma: True if the queue is connected via TADMA channel
+ * @dmaq_idx: DMA queue data index for MCDMA and queue type for TADMA
+ * @disable_cnt: Counter which indicates the number of times the queue is
+ *		 disabled.
+ */
+struct axienet_tsn_txq {
+	bool is_tadma;
+	u8 dmaq_idx;
+	u8 disable_cnt;
+};
+
+/**
+ * struct axienet_local - axienet private per device data
+ * @ndev:	Pointer for net_device to which it will be attached.
+ * @dev:	Pointer to device structure
+ * @phy_node:	Pointer to device node structure
+ * @axi_clk:	AXI4-Lite bus clock
+ * @misc_clks:	Misc ethernet clocks (AXI4-Stream, Ref, MGT clocks)
+ * @mii_bus:	Pointer to MII bus structure
+ * @mii_clk_div: MII bus clock divider value
+ * @regs_start: Resource start for axienet device addresses
+ * @regs:	Base address for the axienet_local device address space
+ * @mcdma_regs:	Base address for the aximcdma device address space
+ * @napi:	Napi Structure array for all dma queues
+ * @num_tx_queues: Total number of Tx DMA queues
+ * @num_rx_queues: Total number of Rx DMA queues
+ * @dq:		DMA queues data
+ * @phy_mode:	Phy type to identify between MII/GMII/RGMII/SGMII/1000 Base-X
+ * @num_tc:	Total number of TSN Traffic classes
+ * @abl_reg:	TSN abilities register
+ * @master:	Master endpoint
+ * @slaves:	Front panel ports
+ * @ex_ep:	extended end point
+ * @packet_switch: packet switching parameter
+ * @switch_prt: Switch port number
+ * @timer_priv: PTP timer private data pointer
+ * @ptp_tx_irq: PTP tx irq
+ * @ptp_rx_irq: PTP rx irq
+ * @rtc_irq:	PTP RTC irq
+ * @qbv_irq:	QBV shed irq
+ * @ptp_ts_type: ptp time stamp type - 1 or 2 step mode
+ * @ptp_rx_hw_pointer: ptp rx hw pointer
+ * @ptp_rx_sw_pointer: ptp rx sw pointer
+ * @ptp_txq:	PTP tx queue header
+ * @tx_tstamp_work: PTP timestamping work queue
+ * @qbv_regs:	pointer to qbv registers base address
+ * @tadma_regs: pointer to tadma registers base address
+ * @tadma_irq: TADMA IRQ number
+ * @t_cb: pointer to tadma_cb
+ * @tadma_queues: Bitmask of the TADMA queues connected in design
+ * @active_sfm: current active stream fetch memory
+ * @num_tadma_buffers: number of TADMA buffers per stream
+ * @num_streams: maximum number of streams TADMA can fetch
+ * @num_entries: maximum number of entries in TADMA streams config
+ * @get_sid: Number of TADMA streams currently active
+ * @get_sfm: Number of SFM entries currently in use
+ * @default_res_sid: RES queue stream id in TADMA continuous mode
+ * @default_st_sid: ST queue stream id in TADMA continuous mode
+ * @tadma_hash_bits: Number of bits required to represent TADMA streams
+ * @tx_bd: tadma transmit buffer descriptor
+ * @tx_bd_head: transmit BD head indices
+ * @tx_bd_tail: transmit BD tail indices
+ * @tx_bd_rd: TADMA read pointer offset
+ * @sid_txq_idx: Maps TADMA sid to TX subqueue index
+ * @tadma_tx_lock: TADMA tx lock
+ * @ptp_tx_lock: PTP tx lock
+ * @dma_err_tasklet: Tasklet structure to process Axi DMA errors
+ * @eth_irq:	Axi Ethernet IRQ number
+ * @options:	AxiEthernet option word
+ * @last_link:	Phy link state in which the PHY was negotiated earlier
+ * @features:	Stores the extended features supported by the axienet hw
+ * @tx_bd_num:	Number of TX buffer descriptors.
+ * @rx_bd_num:	Number of RX buffer descriptors.
+ * @max_frm_size: Stores the maximum size of the frame that can be that
+ *		  Txed/Rxed in the existing hardware. If jumbo option is
+ *		  supported, the maximum frame size would be 9k. Else it is
+ *		  1522 bytes (assuming support for basic VLAN)
+ * @rxmem:	Stores rx memory size for jumbo frame handling.
+ * @csum_offload_on_tx_path:	Stores the checksum selection on TX side.
+ * @csum_offload_on_rx_path:	Stores the checksum selection on RX side.
+ * @coalesce_count_rx:	Store the irq coalesce on RX side.
+ * @coalesce_count_tx:	Store the irq coalesce on TX side.
+ * @phy_flags:	Phy interface flags.
+ * @eth_hasnobuf: Ethernet is configured in Non buf mode.
+ * @eth_hasptp: Ethernet is configured for ptp.
+ * @axienet_config: Ethernet config structure
+ * @tx_ts_regs:	  Base address for the axififo device address space.
+ * @rx_ts_regs:	  Base address for the rx axififo device address space.
+ * @tstamp_config: Hardware timestamp config structure.
+ * @current_rx_filter : Current rx filter.
+ * @tx_ptpheader: Stores the tx ptp header.
+ * @aclk: AXI4-Lite clock for ethernet and dma.
+ * @eth_sclk: AXI4-Stream interface clock.
+ * @eth_refclk: Stable clock used by signal delay primitives and transceivers.
+ * @eth_dclk: Dynamic Reconfiguration Port(DRP) clock.
+ * @dma_sg_clk: DMA Scatter Gather Clock.
+ * @dma_rx_clk: DMA S2MM Primary Clock.
+ * @dma_tx_clk: DMA MM2S Primary Clock.
+ * @qnum:     Axi Ethernet queue number to be operate on.
+ * @chan_num: MCDMA Channel number to be operate on.
+ * @chan_id:  MCMDA Channel id used in conjunction with weight parameter.
+ * @weight:   MCDMA Channel weight value to be configured for.
+ * @dma_mask: Specify the width of the DMA address space.
+ * @usxgmii_rate: USXGMII PHY speed.
+ * @mrmac_rate: MRMAC speed.
+ * @gt_pll: Common GT PLL mask control register space.
+ * @gt_ctrl: GT speed and reset control register space.
+ * @phc_index: Index to corresponding PTP clock used.
+ * @gt_lane: MRMAC GT lane index used.
+ * @ptp_os_cf: CF TS of PTP PDelay req for one step usage.
+ * @xxv_ip_version: XXV IP version
+ * @txqs: TX queues to MCDMA & TADMA channel mapping
+ * @pcpmap: PCP to queues mapping information
+ * @qbv_enabled: Bitmask of the QBV enabled queues
+ */
+struct axienet_local {
+	struct net_device *ndev;
+	struct device *dev;
+
+	struct device_node *phy_node;
+
+	struct clk *axi_clk;
+	struct clk_bulk_data misc_clks[XAE_NUM_MISC_CLOCKS];
+
+	struct mii_bus *mii_bus;
+	u8 mii_clk_div;
+
+	resource_size_t regs_start;
+	void __iomem *regs;
+	void __iomem *mcdma_regs;
+
+	struct tasklet_struct dma_err_tasklet[XAE_MAX_QUEUES];
+	struct napi_struct napi[XAE_MAX_QUEUES];	/* NAPI Structure */
+
+	u16    num_tx_queues;	/* Number of TX DMA queues */
+	u16    num_rx_queues;	/* Number of RX DMA queues */
+	struct axienet_dma_q *dq[XAE_MAX_QUEUES];	/* DMA queue data*/
+
+	phy_interface_t phy_mode;
+
+	u16   num_tc;
+	u32   abl_reg;
+	struct net_device *master; /* master endpoint */
+	struct net_device *slaves[2]; /* two front panel ports */
+	struct net_device *ex_ep; /* extended endpoint*/
+	u8	packet_switch;
+	u8      switch_prt;	/* port on the switch */
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	void *timer_priv;
+	int ptp_tx_irq;
+	int ptp_rx_irq;
+	int rtc_irq;
+	int qbv_irq;
+	int ptp_ts_type;
+	u8  ptp_rx_hw_pointer;
+	u8  ptp_rx_sw_pointer;
+	struct sk_buff_head ptp_txq;
+	struct work_struct tx_tstamp_work;
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	void __iomem *qbv_regs;
+#endif
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	void __iomem *tadma_regs;
+	int tadma_irq;
+	void *t_cb;
+	u8 tadma_queues;
+	int active_sfm;
+	int num_tadma_buffers;
+	int num_streams;
+	int num_entries;
+	u32 get_sid;
+	u32 get_sfm;
+	int default_res_sid;
+	int default_st_sid;
+	u8 tadma_hash_bits;
+	struct axitadma_bd **tx_bd;
+	u32 tx_bd_head[TADMA_MAX_NO_STREAM];
+	u32 tx_bd_tail[TADMA_MAX_NO_STREAM];
+	u32 tx_bd_rd[TADMA_MAX_NO_STREAM];
+	u8 sid_txq_idx[TADMA_MAX_NO_STREAM];
+	spinlock_t tadma_tx_lock;               /* TSN TADMA tx lock*/
+#endif
+	spinlock_t ptp_tx_lock;		/* PTP tx lock*/
+	int eth_irq;
+
+	u32 options;			/* Current options word */
+	u32 last_link;
+	u32 features;
+
+	u16 tx_bd_num;
+	u32 rx_bd_num;
+
+	u32 max_frm_size;
+	u32 rxmem;
+
+	int csum_offload_on_tx_path;
+	int csum_offload_on_rx_path;
+
+	u32 coalesce_count_rx;
+	u32 coalesce_count_tx;
+	u32 phy_flags;
+	bool eth_hasnobuf;
+	bool eth_hasptp;
+	const struct axienet_config *axienet_config;
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	void __iomem *tx_ts_regs;
+	void __iomem *rx_ts_regs;
+	struct hwtstamp_config tstamp_config;
+	int current_rx_filter;
+	u8 *tx_ptpheader;
+#endif
+	struct clk *aclk;
+	struct clk *eth_sclk;
+	struct clk *eth_refclk;
+	struct clk *eth_dclk;
+	struct clk *dma_sg_clk;
+	struct clk *dma_rx_clk;
+	struct clk *dma_tx_clk;
+
+	/* MCDMA Fields */
+	int qnum[XAE_MAX_QUEUES];
+	int chan_num[XAE_MAX_QUEUES];
+	/* WRR Fields */
+	u16 chan_id;
+	u16 weight;
+
+	u8 dma_mask;
+	u32 usxgmii_rate;
+
+	u32 mrmac_rate;		/* MRMAC speed */
+	void __iomem *gt_pll;	/* Common GT PLL mask control register space */
+	void __iomem *gt_ctrl;	/* GT speed and reset control register space */
+	u32 phc_index;		/* Index to corresponding PTP clock used  */
+	u32 gt_lane;		/* MRMAC GT lane index used */
+	u64 ptp_os_cf;		/* CF TS of PTP PDelay req for one step usage */
+	u32 xxv_ip_version;
+	struct axienet_tsn_txq txqs[XAE_MAX_TSN_TC];
+	u8 pcpmap[XAE_MAX_TSN_TC];
+	u32 qbv_enabled;
+};
+
+/**
+ * struct axienet_dma_q - axienet private per dma queue data
+ * @lp:		Parent pointer
+ * @dma_regs:	Base address for the axidma device address space
+ * @tx_irq:	Axidma TX IRQ number
+ * @rx_irq:	Axidma RX IRQ number
+ * @tx_lock:	Spin lock for tx path
+ * @rx_lock:	Spin lock for tx path
+ * @tx_bd_v:	Virtual address of the TX buffer descriptor ring
+ * @tx_bd_p:	Physical address(start address) of the TX buffer descr. ring
+ * @rx_bd_v:	Virtual address of the RX buffer descriptor ring
+ * @rx_bd_p:	Physical address(start address) of the RX buffer descr. ring
+ * @tx_buf:	Virtual address of the Tx buffer pool used by the driver when
+ *		DMA h/w is configured without DRE.
+ * @tx_bufs:	Virutal address of the Tx buffer address.
+ * @tx_bufs_dma: Physical address of the Tx buffer address used by the driver
+ *		 when DMA h/w is configured without DRE.
+ * @eth_hasdre: Tells whether DMA h/w is configured with dre or not.
+ * @txq_idx:	TX subqueue index of this MCDMA queue.
+ * @tx_bd_ci:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while alloc. BDs before a TX starts
+ * @tx_bd_tail:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while processing BDs after the TX
+ *		completed.
+ * @rx_bd_ci:	Stores the index of the Rx buffer descriptor in the ring being
+ *		accessed currently.
+ * @flags:      MCDMA management channel flags
+ * @chan_id:    MCDMA channel to operate on.
+ * @rx_offset:	MCDMA S2MM channel starting offset.
+ * @txq_bd_v:	Virtual address of the MCDMA TX buffer descriptor ring
+ * @rxq_bd_v:	Virtual address of the MCDMA RX buffer descriptor ring
+ * @tx_packets: Number of transmit packets processed by the dma queue.
+ * @tx_bytes:   Number of transmit bytes processed by the dma queue.
+ * @rx_packets: Number of receive packets processed by the dma queue.
+ * @rx_bytes:	Number of receive bytes processed by the dma queue.
+ */
+struct axienet_dma_q {
+	struct axienet_local	*lp; /* parent */
+	void __iomem *dma_regs;
+
+	int tx_irq;
+	int rx_irq;
+
+	spinlock_t tx_lock;		/* tx lock */
+	spinlock_t rx_lock;		/* rx lock */
+
+	/* Buffer descriptors */
+	dma_addr_t rx_bd_p;
+	dma_addr_t tx_bd_p;
+
+	unsigned char *tx_buf[XAE_TX_BUFFERS];
+	unsigned char *tx_bufs;
+	dma_addr_t tx_bufs_dma;
+	bool eth_hasdre;
+	u8 txq_idx;
+
+	u32 tx_bd_ci;
+	u32 rx_bd_ci;
+	u32 tx_bd_tail;
+
+	/* MCDMA fields */
+#if IS_ENABLED(CONFIG_XILINX_TSN)
+#define MCDMA_MGMT_CHAN		BIT(0)
+#define MCDMA_EP_EX_CHAN	BIT(1)
+	u32 flags;
+#endif
+	u16 chan_id;
+	u32 rx_offset;
+	struct aximcdma_bd *txq_bd_v;
+	struct aximcdma_bd *rxq_bd_v;
+
+	unsigned long tx_packets;
+	unsigned long tx_bytes;
+	unsigned long rx_packets;
+	unsigned long rx_bytes;
+};
+
+#define AXIENET_ETHTOOLS_SSTATS_LEN 6
+#define AXIENET_TX_SSTATS_LEN(lp) ((lp)->num_tx_queues * 2)
+#define AXIENET_RX_SSTATS_LEN(lp) ((lp)->num_rx_queues * 2)
+
+/**
+ * enum axienet_ip_type - AXIENET IP/MAC type.
+ *
+ * @XAXIENET_1G:	 IP is 1G MAC
+ * @XAXIENET_2_5G:	 IP type is 2.5G MAC.
+ * @XAXIENET_LEGACY_10G: IP type is legacy 10G MAC.
+ * @XAXIENET_10G_25G:	 IP type is 10G/25G MAC(XXV MAC).
+ * @XAXIENET_MRMAC:	 IP type is hardened Multi Rate MAC (MRMAC).
+ *
+ */
+enum axienet_ip_type {
+	XAXIENET_1G = 0,
+	XAXIENET_2_5G,
+	XAXIENET_LEGACY_10G,
+	XAXIENET_10G_25G,
+	XAXIENET_MRMAC,
+};
+
+struct axienet_config {
+	enum axienet_ip_type mactype;
+	void (*setoptions)(struct net_device *ndev, u32 options);
+	int (*clk_init)(struct platform_device *pdev, struct clk **axi_aclk,
+			struct clk **axis_clk, struct clk **ref_clk,
+			struct clk **dclk);
+	u32 tx_ptplen;
+	u8 ts_header_len;
+};
+
+/**
+ * struct axienet_option - Used to set axi ethernet hardware options
+ * @opt:	Option to be set.
+ * @reg:	Register offset to be written for setting the option
+ * @m_or:	Mask to be ORed for setting the option in the register
+ */
+struct axienet_option {
+	u32 opt;
+	u32 reg;
+	u32 m_or;
+};
+
+struct xxvenet_option {
+	u32 opt;
+	u32 reg;
+	u32 m_or;
+};
+
+extern void __iomem *mrmac_gt_pll;
+extern void __iomem *mrmac_gt_ctrl;
+extern int mrmac_pll_reg;
+extern int mrmac_pll_rst;
+
+extern struct platform_driver tsn_ex_ep_driver;
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+extern struct platform_driver tsnswitch_driver;
+#endif
+extern struct platform_driver axienet_driver_tsn;
+extern struct platform_driver tsn_ep_driver;
+
+/**
+ * axienet_ior - Memory mapped Axi Ethernet register read
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ *
+ * Return: The contents of the Axi Ethernet register
+ *
+ * This function returns the contents of the corresponding register.
+ */
+static inline u32 axienet_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->regs + offset);
+}
+
+/**
+ * axienet_ior64 - Memory mapped Axi Ethernet register read from two consecutive registers
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ *
+ * Return: 64 bit value, where the LSB 32 bits correspond to the contents of the register
+ * at baseaddress+offset and MSB 32 bits correspond to the contents of the register at
+ * baseaddress+offset+4
+ *
+ * This function returns the 64 bit value corresponding to two consecutive registers.
+ */
+static inline u64 axienet_ior64(struct axienet_local *lp, off_t offset)
+{
+	return ioread64_lo_hi(lp->regs + offset);
+}
+
+static inline u32 axinet_ior_read_mcr(struct axienet_local *lp)
+{
+	return axienet_ior(lp, XAE_MDIO_MCR_OFFSET);
+}
+
+/**
+ * axienet_iow - Memory mapped Axi Ethernet register write
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ * @value:      Value to be written into the Axi Ethernet register
+ *
+ * This function writes the desired value into the corresponding Axi Ethernet
+ * register.
+ */
+static inline void axienet_iow(struct axienet_local *lp, off_t offset,
+			       u32 value)
+{
+	iowrite32(value, lp->regs + offset);
+}
+
+/**
+ * axienet_get_mrmac_blocklock - Write to Clear MRMAC RX block lock status register
+ * and read the latest status
+ * @lp:         Pointer to axienet local structure
+ *
+ * Return: The contents of the Contents of MRMAC RX block lock status register
+ */
+
+static inline u32 axienet_get_mrmac_blocklock(struct axienet_local *lp)
+{
+	axienet_iow(lp, MRMAC_STATRX_BLKLCK_OFFSET, MRMAC_STS_ALL_MASK);
+	return axienet_ior(lp, MRMAC_STATRX_BLKLCK_OFFSET);
+}
+
+/**
+ * axienet_get_mrmac_rx_status - Write to Clear MRMAC RX status register
+ * and read the latest status
+ * @lp:		Pointer to axienet local structure
+ *
+ * Return: The contents of the Contents of MRMAC RX status register
+ */
+
+static inline u32 axienet_get_mrmac_rx_status(struct axienet_local *lp)
+{
+	axienet_iow(lp, MRMAC_RX_STS_OFFSET, MRMAC_STS_ALL_MASK);
+	return axienet_ior(lp, MRMAC_RX_STS_OFFSET);
+}
+
+/**
+ * axienet_dma_in32 - Memory mapped Axi DMA register read
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ *
+ * Return: The contents of the Axi DMA register
+ *
+ * This function returns the contents of the corresponding Axi DMA register.
+ */
+static inline u32 axienet_dma_in32(struct axienet_dma_q *q, off_t reg)
+{
+	return ioread32(q->dma_regs + reg);
+}
+
+/**
+ * axienet_dma_out32 - Memory mapped Axi DMA register write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_out32(struct axienet_dma_q *q,
+				     off_t reg, u32 value)
+{
+	iowrite32(value, q->dma_regs + reg);
+}
+
+/**
+ * axienet_dma_bdout - Memory mapped Axi DMA register Buffer Descriptor write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_bdout(struct axienet_dma_q *q,
+				     off_t reg, dma_addr_t value)
+{
+#if defined(CONFIG_PHYS_ADDR_T_64BIT)
+	writeq(value, (q->dma_regs + reg));
+#else
+	writel(value, (q->dma_regs + reg));
+#endif
+}
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+/**
+ * axienet_qbv_ior - Memory mapped TSN QBV register read
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ *
+ * Return: The contents of the Axi Ethernet register
+ *
+ * This function returns the contents of the corresponding register.
+ */
+static inline u32 axienet_qbv_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->qbv_regs + offset);
+}
+
+/**
+ * axienet_qbv_iow - Memory mapped TSN QBV register write
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ * @value:      Value to be written into the Axi Ethernet register
+ *
+ * This function writes the desired value into the corresponding Axi Ethernet
+ * register.
+ */
+static inline void axienet_qbv_iow(struct axienet_local *lp, off_t offset,
+				   u32 value)
+{
+	iowrite32(value, (lp->qbv_regs + offset));
+}
+#endif
+
+static inline bool axienet_tsn_num_tc_valid(int num_tc)
+{
+	return (num_tc >= XAE_MIN_LEGACY_TSN_TC && num_tc <= XAE_MAX_TSN_TC);
+}
+
+/* Function prototypes visible in xilinx_axienet_mdio.c for other files */
+int axienet_mdio_enable_tsn(struct axienet_local *lp);
+void axienet_mdio_disable_tsn(struct axienet_local *lp);
+int axienet_mdio_setup_tsn(struct axienet_local *lp);
+void axienet_mdio_teardown_tsn(struct axienet_local *lp);
+void axienet_adjust_link_tsn(struct net_device *ndev);
+int axienet_tsn_open(struct net_device *ndev);
+int axienet_tsn_stop(struct net_device *ndev);
+int axienet_tsn_probe(struct platform_device *pdev,
+		      struct axienet_local *lp,
+		      struct net_device *ndev);
+int tsn_mcdma_probe(struct platform_device *pdev, struct axienet_local *lp,
+		    struct net_device *ndev);
+int axienet_tsn_xmit(struct sk_buff *skb, struct net_device *ndev);
+u16 axienet_tsn_select_queue(struct net_device *ndev, struct sk_buff *skb,
+			     struct net_device *sb_dev);
+u16 axienet_tsn_pcp_to_queue(struct net_device *ndev, struct sk_buff *skb);
+void axienet_set_pcpmap(struct axienet_local *lp);
+int axienet_init_tsn_txqs(struct axienet_local *lp, u16 num_tc);
+int tsn_data_path_open(struct net_device *ndev);
+int tsn_data_path_close(struct net_device *ndev);
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+void *axienet_ptp_timer_probe(void __iomem *base, struct platform_device *pdev);
+void axienet_tx_tstamp(struct work_struct *work);
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+int axienet_qbv_init(struct net_device *ndev);
+int axienet_set_schedule(struct net_device *ndev, void __user *useraddr);
+int axienet_get_schedule(struct net_device *ndev, void __user *useraddr);
+int axienet_tsn_shaper_tc(struct net_device *dev, enum tc_setup_type type, void *type_data);
+#endif
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBR)
+int axienet_preemption(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_ctrl(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_sts(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_receive(struct net_device *ndev);
+int axienet_preemption_cnt(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_sts_ethtool(struct net_device *ndev, struct ethtool_mm_state *state);
+void axienet_preemption_cnt_ethtool(struct net_device *ndev, struct ethtool_mm_stats *stats);
+int axienet_preemption_ctrl_ethtool(struct net_device *ndev, struct ethtool_mm_cfg *config_data,
+				    struct netlink_ext_ack *extack);
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+int axienet_qbu_user_override(struct net_device *ndev, void __user *useraddr);
+int axienet_qbu_sts(struct net_device *ndev, void __user *useraddr);
+#endif
+#endif
+
+int axienet_mdio_wait_until_ready_tsn(struct axienet_local *lp);
+void __maybe_unused axienet_bd_free(struct net_device *ndev,
+				    struct axienet_dma_q *q);
+
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+int axienet_tadma_add_stream(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_flush_stream(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_off(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_program(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_probe(struct platform_device *pdev, struct net_device *ndev);
+int axienet_tadma_xmit(struct sk_buff *skb, struct net_device *ndev, u16 queue_type);
+int axienet_tadma_open(struct net_device *ndev);
+int axienet_tadma_stop(struct net_device *ndev);
+#endif
+
+int __maybe_unused axienet_dma_q_init(struct net_device *ndev,
+				      struct axienet_dma_q *q);
+void axienet_dma_err_handler(unsigned long data);
+irqreturn_t __maybe_unused axienet_tx_irq(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_rx_irq(int irq, void *_ndev);
+void axienet_start_xmit_done_tsn(struct net_device *ndev, struct axienet_dma_q *q);
+void axienet_dma_bd_release_tsn(struct net_device *ndev);
+void __axienet_device_reset_tsn(struct axienet_dma_q *q);
+void axienet_set_mac_address_tsn(struct net_device *ndev, const void *address);
+void axienet_set_multicast_list_tsn(struct net_device *ndev);
+int xaxienet_rx_poll_tsn(struct napi_struct *napi, int quota);
+void axienet_setoptions_tsn(struct net_device *ndev, u32 options);
+int axienet_queue_xmit_tsn(struct sk_buff *skb, struct net_device *ndev,
+			   u16 map);
+
+int __maybe_unused axienet_mcdma_rx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q);
+int __maybe_unused axienet_mcdma_tx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_tx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_rx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q);
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq_tsn(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq_tsn(int irq, void *_ndev);
+void __maybe_unused axienet_mcdma_err_handler_tsn(unsigned long data);
+void axienet_strings_tsn(struct net_device *ndev, u32 sset, u8 *data);
+int axienet_sset_count_tsn(struct net_device *ndev, int sset);
+void axienet_get_stats_tsn(struct net_device *ndev,
+			   struct ethtool_stats *stats,
+			   u64 *data);
+int axeinet_mcdma_create_sysfs_tsn(struct kobject *kobj);
+void axeinet_mcdma_remove_sysfs_tsn(struct kobject *kobj);
+int __maybe_unused axienet_mcdma_tx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct axienet_local *lp);
+int __maybe_unused axienet_mcdma_rx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct net_device *ndev);
+int axienet_mcdma_disable_tx_q(struct net_device *ndev, u8 map);
+void axienet_mcdma_enable_tx_q(struct net_device *ndev, u8 map);
+
+int axienet_ethtools_get_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack);
+int axienet_ethtools_set_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack);
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+int tsn_switch_get_port_parent_id(struct net_device *dev,
+				  struct netdev_phys_item_id *ppid);
+#endif
+bool xlnx_is_port_ep_netdev(const struct net_device *ndev);
+bool xlnx_is_port_temac_netdev(const struct net_device *ndev);
+bool xlnx_is_port_ep_ex_netdev(const struct net_device *ndev);
+
+#endif /* XILINX_AXI_ENET_TSN_H */
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_cb.c b/drivers/staging/xilinx-tsn/xilinx_tsn_cb.c
new file mode 100644
index 000000000..97764fed6
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_cb.c
@@ -0,0 +1,199 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QCI Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include "xilinx_tsn_switch.h"
+
+#define IN_PORTID_MASK				0x3
+#define IN_PORTID_SHIFT				24
+#define MAX_SEQID_MASK				0x0000FFFF
+#define REMAINING_TICKS_MASK			0x0000FFFF
+
+#define SEQ_REC_HIST_LEN_MASK			0x000000FF
+#define SEQ_REC_HIST_LEN_SHIFT			16
+#define SPLIT_STREAM_INPORTID_SHIFT		12
+#define SPLIT_STREAM_INPORTID_MASK		0x3
+#define SPLIT_STREAM_VLANID_MASK		0x00000FFF
+
+#define GATE_ID_SHIFT				24
+#define MEMBER_ID_SHIFT				8
+#define SEQ_RESET_SHIFT				7
+#define REC_TIMEOUT_SHIFT			6
+#define GATE_STATE_SHIFT			5
+#define FRER_VALID_SHIFT			4
+#define WR_OP_TYPE_SHIFT			2
+#define OP_TYPE_SHIFT				1
+#define WR_OP_TYPE_MASK				0x3
+#define FRER_EN_CONTROL_MASK			0x1
+
+/**
+ * frer_control - Configure thr control for frer
+ * @data:	Value to be programmed
+ */
+void frer_control(struct frer_ctrl data)
+{
+	u32 mask = 0;
+
+	mask = data.gate_id << GATE_ID_SHIFT;
+	mask |= data.memb_id << MEMBER_ID_SHIFT;
+	mask |= data.seq_reset << SEQ_RESET_SHIFT;
+	mask |= data.gate_state << GATE_STATE_SHIFT;
+	mask |= data.rcvry_tmout << REC_TIMEOUT_SHIFT;
+	mask |= data.frer_valid << FRER_VALID_SHIFT;
+	mask |= (data.wr_op_type & WR_OP_TYPE_MASK) << WR_OP_TYPE_SHIFT;
+	mask |= data.op_type << OP_TYPE_SHIFT;
+	mask |= FRER_EN_CONTROL_MASK;
+
+	axienet_iow(&lp, FRER_CONTROL_OFFSET, mask);
+
+	/* wait for write to complete */
+	while ((axienet_ior(&lp, FRER_CONTROL_OFFSET) & FRER_EN_CONTROL_MASK))
+		;
+}
+
+/**
+ * get_ingress_filter_config -  Get Ingress Filter Configuration
+ * @data:	Value returned
+ */
+void get_ingress_filter_config(struct in_fltr *data)
+{
+	u32 reg_val = 0;
+
+	reg_val = axienet_ior(&lp, INGRESS_FILTER_OFFSET);
+
+	data->max_seq_id = reg_val & MAX_SEQID_MASK;
+	data->in_port_id = (reg_val >> IN_PORTID_SHIFT) & IN_PORTID_MASK;
+}
+
+/**
+ * config_ingress_filter -  Configure Ingress Filter Configuration
+ * @data:	Value to be programmed
+ */
+void config_ingress_filter(struct cb data)
+{
+	u32 conf_r1;
+
+	conf_r1 = axienet_ior(&lp, INGRESS_FILTER_OFFSET);
+	conf_r1 &= ~(IN_PORTID_MASK << IN_PORTID_SHIFT);
+	conf_r1 |= ((data.in_fltr_data.in_port_id & IN_PORTID_MASK) <<
+			IN_PORTID_SHIFT);
+	axienet_iow(&lp, INGRESS_FILTER_OFFSET, conf_r1);
+
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG1);
+	conf_r1 &= ~(SEQ_REC_HIST_LEN_MASK << SEQ_REC_HIST_LEN_SHIFT);
+	conf_r1 |= (data.frer_memb_config_data.seq_rec_hist_len &
+			SEQ_REC_HIST_LEN_MASK)
+			<< SEQ_REC_HIST_LEN_SHIFT;
+	axienet_iow(&lp, FRER_CONFIG_REG1, conf_r1);
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG2);
+	conf_r1 &= ~(REMAINING_TICKS_MASK);
+	conf_r1 |= data.frer_memb_config_data.rem_ticks;
+	axienet_iow(&lp, FRER_CONFIG_REG2, conf_r1);
+}
+
+/**
+ * get_member_reg -  Read frer member Configuration registers value
+ * @data:	Value returned
+ */
+void get_member_reg(struct frer_memb_config *data)
+{
+	u32 conf_r1 = 0;
+
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG1);
+	data->rem_ticks = axienet_ior(&lp, FRER_CONFIG_REG2);
+
+	data->seq_rec_hist_len = (conf_r1 >> SEQ_REC_HIST_LEN_SHIFT)
+						& SEQ_REC_HIST_LEN_MASK;
+	data->split_strm_egport_id = (conf_r1 >> SPLIT_STREAM_INPORTID_SHIFT)
+						& SPLIT_STREAM_INPORTID_MASK;
+	data->split_strm_vlan_id = conf_r1 & SPLIT_STREAM_VLANID_MASK;
+}
+
+/**
+ * program_member_reg -  configure frer member Configuration registers
+ * @data:	Value to be programmed
+ */
+void program_member_reg(struct cb data)
+{
+	u32 conf_r1 = 0;
+
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG1);
+	conf_r1 &= ~(SPLIT_STREAM_INPORTID_MASK << SPLIT_STREAM_INPORTID_SHIFT);
+	conf_r1 &= ~(SPLIT_STREAM_VLANID_MASK);
+	conf_r1 |= ((data.frer_memb_config_data.split_strm_egport_id
+					& SPLIT_STREAM_INPORTID_MASK)
+					<< SPLIT_STREAM_INPORTID_SHIFT);
+	conf_r1 |= (data.frer_memb_config_data.split_strm_vlan_id &
+					SPLIT_STREAM_VLANID_MASK);
+
+	axienet_iow(&lp, FRER_CONFIG_REG1, conf_r1);
+	axienet_iow(&lp, FRER_CONFIG_REG2,
+		    data.frer_memb_config_data.rem_ticks);
+
+	conf_r1 = axienet_ior(&lp, INGRESS_FILTER_OFFSET);
+	conf_r1 &= ~MAX_SEQID_MASK;
+	conf_r1 |= (data.in_fltr_data.max_seq_id & MAX_SEQID_MASK);
+	axienet_iow(&lp, INGRESS_FILTER_OFFSET, conf_r1);
+}
+
+/**
+ * get_frer_static_counter -  get frer static counters value
+ * @data:	return value, containing counter value
+ */
+void get_frer_static_counter(struct frer_static_counter *data)
+{
+	int offset = (data->num) * 8;
+
+	data->frer_fr_count.lsb = axienet_ior(&lp, TOTAL_FRER_FRAMES_OFFSET +
+									offset);
+	data->frer_fr_count.msb = axienet_ior(&lp, TOTAL_FRER_FRAMES_OFFSET +
+								offset + 0x4);
+
+	data->disc_frames_in_portid.lsb = axienet_ior(&lp,
+						      FRER_DISCARD_INGS_FLTR_OFFSET + offset);
+	data->disc_frames_in_portid.msb = axienet_ior(&lp,
+						      FRER_DISCARD_INGS_FLTR_OFFSET + offset + 0x4);
+
+	data->pass_frames_ind_recv.lsb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_INDV_OFFSET + offset);
+	data->pass_frames_ind_recv.msb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_INDV_OFFSET + offset + 0x4);
+
+	data->disc_frames_ind_recv.lsb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_INDV_OFFSET + offset);
+	data->disc_frames_ind_recv.msb =
+	axienet_ior(&lp, FRER_DISCARD_FRAMES_INDV_OFFSET + offset + 0x4);
+
+	data->pass_frames_seq_recv.lsb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_SEQ_OFFSET + offset);
+	data->pass_frames_seq_recv.msb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->disc_frames_seq_recv.lsb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_SEQ_OFFSET + offset);
+	data->disc_frames_seq_recv.msb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->rogue_frames_seq_recv.lsb = axienet_ior(&lp,
+						      FRER_ROGUE_FRAMES_SEQ_OFFSET + offset);
+	data->rogue_frames_seq_recv.msb = axienet_ior(&lp,
+						      FRER_ROGUE_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->seq_recv_rst.lsb = axienet_ior(&lp,
+					     SEQ_RECV_RESETS_OFFSET + offset);
+	data->seq_recv_rst.msb = axienet_ior(&lp,
+					     SEQ_RECV_RESETS_OFFSET + offset + 0x4);
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_ep.c b/drivers/staging/xilinx-tsn/xilinx_tsn_ep.c
new file mode 100644
index 000000000..45d7bab4c
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_ep.c
@@ -0,0 +1,836 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN End point driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/etherdevice.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/platform_device.h>
+#include <linux/skbuff.h>
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+#include "xilinx_tsn_tadma.h"
+#include "xilinx_tsn_timer.h"
+
+#define TX_BD_NUM_DEFAULT	64
+#define RX_BD_NUM_DEFAULT	1024
+
+static u8 st_pcp[8] = {4};
+static uint st_count = 1;
+module_param_array(st_pcp, byte, &st_count, 0644);
+MODULE_PARM_DESC(st_pcp, "Array of pcp values mapped to ST class at the compile time");
+
+static u8 res_pcp[8] = {2, 3};
+static uint res_count = 2;
+module_param_array(res_pcp, byte, &res_count, 0644);
+MODULE_PARM_DESC(res_pcp, "Array of pcp values mapped to RES class at the compile time");
+
+static inline int mcdma_qidx_to_txq_idx(struct axienet_local *lp, int qidx)
+{
+	int qno;
+
+	for (qno = 0; qno < lp->num_tc; qno++) {
+		if (!lp->txqs[qno].is_tadma &&
+		    lp->txqs[qno].dmaq_idx == qidx) {
+			return qno;
+		}
+	}
+
+	return -EINVAL;
+}
+
+int tsn_data_path_open(struct net_device *ndev)
+{
+	/* The highest possible count of IRQs is twice the maximum number of
+	 * queues, considering both the Tx and Rx channels
+	 */
+	static char irq_name[XAE_MAX_QUEUES * 2][32];
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct net_device *emac0_ndev;
+	struct net_device *emac1_ndev;
+	u8 hw_addr_mask[ETH_ALEN];
+	struct axienet_dma_q *q;
+	u8 irq_cnt = 0;
+	int ret, i = 0;
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		/*MCDMA TX RESET*/
+		__axienet_device_reset_tsn(q);
+		ret = mcdma_qidx_to_txq_idx(lp, i);
+		if (ret < 0) {
+			dev_err(&ndev->dev, "Failed to get txq for MCMDA Q%d\n",
+				i);
+			return -EINVAL;
+		}
+
+		q->txq_idx = ret;
+	}
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		ret = axienet_mcdma_rx_q_init_tsn(ndev, q);
+		/* Enable interrupts for Axi MCDMA Rx
+		 */
+		snprintf(irq_name[irq_cnt], sizeof(irq_name[irq_cnt]),
+			 "%s_mcdma_rx_%d", ndev->name, i + 1);
+		ret = request_irq(q->rx_irq, axienet_mcdma_rx_irq_tsn,
+				  IRQF_SHARED, irq_name[irq_cnt], ndev);
+		if (ret)
+			goto err_dma_rx_irq;
+
+		tasklet_init(&lp->dma_err_tasklet[i],
+			     axienet_mcdma_err_handler_tsn,
+			     (unsigned long)lp->dq[i]);
+		napi_enable(&lp->napi[i]);
+		irq_cnt++;
+	}
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		ret = axienet_mcdma_tx_q_init_tsn(ndev, q);
+		/* Enable interrupts for Axi MCDMA Tx */
+		snprintf(irq_name[irq_cnt], sizeof(irq_name[irq_cnt]),
+			 "%s_mcdma_tx_%d", ndev->name, i + 1);
+		ret = request_irq(q->tx_irq, axienet_mcdma_tx_irq_tsn,
+				  IRQF_SHARED, irq_name[irq_cnt], ndev);
+		if (ret)
+			goto err_dma_tx_irq;
+		irq_cnt++;
+	}
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	ret = axienet_tadma_open(ndev);
+	if (ret)
+		goto err_tadma;
+#endif
+	if (lp->slaves[0] && lp->slaves[1]) {
+		emac0_ndev = lp->slaves[0];
+		emac1_ndev = lp->slaves[1];
+
+		for (i = 0; i < ETH_ALEN; i++)
+			hw_addr_mask[i] = 0xFF;
+		hw_addr_mask[5] &= 0x000000F0;
+		if (!ether_addr_equal_masked(emac0_ndev->dev_addr, emac1_ndev->dev_addr,
+					     hw_addr_mask))
+			netdev_warn(ndev,
+				    "MSB 44 bits of the MAC addresses of TSN EMAC0 and TSN EMAC1 are different");
+		if (!ether_addr_equal_masked(emac0_ndev->dev_addr, ndev->dev_addr, hw_addr_mask))
+			netdev_warn(ndev, "MSB 44 bits of the MAC addresses of TSN EMAC0 and TSN EP are different");
+		if (!ether_addr_equal_masked(emac0_ndev->dev_addr, ndev->dev_addr, hw_addr_mask))
+			netdev_warn(ndev, "MSB 44 bits of the MAC addresses of TSN EMAC1 and TSN EP are different");
+	}
+
+	netif_tx_start_all_queues(ndev);
+	return 0;
+
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+err_tadma:
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		free_irq(q->tx_irq, ndev);
+	}
+#endif
+err_dma_tx_irq:
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		free_irq(q->rx_irq, ndev);
+	}
+err_dma_rx_irq:
+	return ret;
+}
+
+/**
+ * tsn_ep_open - TSN EP driver open routine.
+ * @ndev:       Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *          non-zero error value on failure
+ *
+ * This is the driver open routine. It also allocates interrupt service
+ * routines, enables the interrupt lines and ISR handling. Axi Ethernet
+ * core is reset through Axi DMA core. Buffer descriptors are initialized.
+ */
+static int tsn_ep_open(struct net_device *ndev)
+{
+	return tsn_data_path_open(ndev);
+}
+
+int tsn_data_path_close(struct net_device *ndev)
+{
+	u32 cr;
+	u32 i;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET,
+				  cr & (~XAXIDMA_CR_RUNSTOP_MASK));
+		if (netif_running(ndev))
+			netif_stop_subqueue(ndev, q->txq_idx);
+		free_irq(q->tx_irq, ndev);
+	}
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET,
+				  cr & (~XAXIDMA_CR_RUNSTOP_MASK));
+		napi_disable(&lp->napi[i]);
+		tasklet_kill(&lp->dma_err_tasklet[i]);
+
+		free_irq(q->rx_irq, ndev);
+	}
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	axienet_tadma_stop(ndev);
+#endif
+	axienet_dma_bd_release_tsn(ndev);
+
+	return 0;
+}
+
+/**
+ * tsn_ep_stop - TSN EP driver stop routine.
+ * @ndev:	Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *
+ * This is the driver stop routine. It also removes the interrupt handlers
+ * and disables the interrupts. The Axi DMA Tx/Rx BDs are released.
+ */
+static int tsn_ep_stop(struct net_device *ndev)
+{
+	return tsn_data_path_close(ndev);
+}
+
+/**
+ * tsn_ep_ioctl - TSN endpoint ioctl interface.
+ * @dev: Pointer to the net_device structure
+ * @rq: Socket ioctl interface request structure
+ * @data: User data
+ * @cmd: Ioctl case
+ *
+ * Return: 0 on success, Non-zero error value on failure.
+ *
+ * This is the ioctl interface for TSN end point. Currently this
+ * supports only gate programming.
+ */
+static int tsn_ep_ioctl(struct net_device *dev, struct ifreq *rq, void __user *data, int cmd)
+{
+	switch (cmd) {
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	case SIOCCHIOCTL:
+		return axienet_set_schedule(dev, data);
+	case SIOC_GET_SCHED:
+		return axienet_get_schedule(dev, data);
+#endif
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	case SIOC_TADMA_OFF:
+		return axienet_tadma_off(dev, data);
+	case SIOC_TADMA_STR_ADD:
+		return axienet_tadma_add_stream(dev, data);
+	case SIOC_TADMA_PROG_ALL:
+		return axienet_tadma_program(dev, data);
+	case SIOC_TADMA_STR_FLUSH:
+		return axienet_tadma_flush_stream(dev, data);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+u16 axienet_tsn_pcp_to_queue(struct net_device *ndev, struct sk_buff *skb)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethhdr *hdr = (struct ethhdr *)skb->data;
+	u16 ether_type = ntohs(hdr->h_proto);
+	u16 vlan_tci;
+	u8 pcp = 0;
+
+	if (unlikely(ether_type == ETH_P_8021Q)) {
+		struct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)skb->data;
+
+		/* ether_type = ntohs(vhdr->h_vlan_encapsulated_proto); */
+
+		vlan_tci = ntohs(vhdr->h_vlan_TCI);
+
+		pcp = (vlan_tci & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
+		return lp->pcpmap[pcp];
+	}
+
+	return BE_QUEUE_NUMBER;
+}
+
+static u16 axienet_tsn_ep_select_queue(struct net_device *ndev, struct sk_buff *skb,
+				       struct net_device *sb_dev)
+{
+	return axienet_tsn_pcp_to_queue(ndev, skb);
+}
+
+/**
+ * tsn_ep_xmit - TSN endpoint xmit routine.
+ * @skb: Packet data
+ * @ndev: Pointer to the net_device structure
+ *
+ * Return: Always returns NETDEV_TX_OK.
+ *
+ * This is dummy xmit function for endpoint as all the data path is assumed to
+ * be connected by TEMAC1 as per linux view
+ */
+static int tsn_ep_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct axienet_tsn_txq *txq;
+	struct axienet_local *lp;
+	u16 map;
+
+	map = skb_get_queue_mapping(skb);
+	lp = netdev_priv(ndev);
+	txq = &lp->txqs[map];
+	pr_debug("map %d, is_tadma %d\n", map, txq->is_tadma);
+
+	if (txq->is_tadma) /* ST Traffic */
+		return axienet_tadma_xmit(skb, ndev, map);
+
+	return axienet_queue_xmit_tsn(skb, ndev, map);
+}
+
+static void tsn_ep_set_mac_address(struct net_device *ndev, const void *address)
+{
+	if (address)
+		eth_hw_addr_set(ndev, address);
+	if (!address || !is_valid_ether_addr(ndev->dev_addr))
+		eth_hw_addr_random(ndev);
+}
+
+/**
+ * netdev_set_mac_address - Write the MAC address (from outside the driver)
+ * @ndev:	Pointer to the net_device structure
+ * @p:		6 byte Address to be written as MAC address
+ *
+ * Return: 0 for all conditions. Presently, there is no failure case.
+ *
+ * This function is called to initialize the MAC address of the Axi Ethernet
+ * core. This is the function that goes into net_device_ops structure entry
+ * ndo_set_mac_address.
+ */
+static int netdev_set_mac_address(struct net_device *ndev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	tsn_ep_set_mac_address(ndev, addr->sa_data);
+	return 0;
+}
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+/**
+ * tsn_ethtools_get_ts_info - Get h/w timestamping capabilities.
+ * @ndev:       Pointer to net_device structure
+ * @info:       Pointer to ethtool_ts_info structure
+ *
+ * Return: 0
+ */
+static int tsn_ethtools_get_ts_info(struct net_device *ndev,
+				    struct kernel_ethtool_ts_info *info)
+{
+	info->phc_index = axienet_phc_index;
+	return 0;
+}
+#endif
+
+static const struct ethtool_ops ep_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES,
+	.get_sset_count	 = axienet_sset_count_tsn,
+	.get_ethtool_stats = axienet_get_stats_tsn,
+	.get_coalesce   = axienet_ethtools_get_coalesce,
+	.set_coalesce   = axienet_ethtools_set_coalesce,
+	.get_strings = axienet_strings_tsn,
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	.get_ts_info    = tsn_ethtools_get_ts_info,
+#endif
+};
+
+static const struct net_device_ops ep_netdev_ops = {
+	.ndo_open = tsn_ep_open,
+	.ndo_stop = tsn_ep_stop,
+	.ndo_siocdevprivate = tsn_ep_ioctl,
+	.ndo_start_xmit = tsn_ep_xmit,
+	.ndo_set_mac_address = netdev_set_mac_address,
+	.ndo_select_queue = axienet_tsn_ep_select_queue,
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+	.ndo_get_port_parent_id = tsn_switch_get_port_parent_id,
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	.ndo_setup_tc = axienet_tsn_shaper_tc,
+#endif
+};
+
+bool xlnx_is_port_ep_netdev(const struct net_device *ndev)
+{
+	return ndev && (ndev->netdev_ops == &ep_netdev_ops);
+}
+
+static const struct of_device_id tsn_ep_of_match[] = {
+	{ .compatible = "xlnx,tsn-ep"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ep_of_match);
+
+/* separate function is needed to probe tsn mcdma
+ * as there is asymmetry between rx channels and tx channels
+ * having unique probe for both tsn and axienet with mcdma is not possible
+ */
+int __maybe_unused tsn_mcdma_probe(struct platform_device *pdev, struct axienet_local *lp,
+				   struct net_device *ndev)
+{
+	int i, ret = 0;
+	struct axienet_dma_q *q;
+	struct device_node *np;
+	struct resource dmares;
+	const char *str;
+	u32 num;
+
+	ret = of_property_count_strings(pdev->dev.of_node, "xlnx,channel-ids");
+	if (ret < 0)
+		return -EINVAL;
+
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected-rx",
+			      0);
+	if (!np) {
+		dev_err(&pdev->dev, "could not find DMA node\n");
+		return -EINVAL;
+	}
+
+	/* get number of associated queues */
+	ret = of_property_read_u32(np, "xlnx,num-s2mm-channels", &num);
+	if (ret < 0)
+		return -EINVAL;
+
+	lp->num_rx_queues = num;
+	pr_info("%s: num_rx_queues: %d\n", __func__, lp->num_rx_queues);
+
+	for_each_rx_dma_queue(lp, i) {
+		q = devm_kzalloc(&pdev->dev, sizeof(*q), GFP_KERNEL);
+		if (!q) {
+			ret = -ENOMEM;
+			goto err_put_node;
+		}
+
+		/* parent */
+		q->lp = lp;
+		lp->dq[i] = q;
+		ret = of_property_read_string_index(pdev->dev.of_node,
+						    "xlnx,channel-ids", i,
+						    &str);
+		if (ret < 0) {
+			dev_err(&pdev->dev, "Failed to read channel ID for queue %d\n", i);
+			goto err_put_node;
+		}
+		ret = kstrtou16(str, 16, &q->chan_id);
+		lp->qnum[i] = i;
+		lp->chan_num[i] = q->chan_id;
+	}
+
+	ret = of_address_to_resource(np, 0, &dmares);
+	if (ret) {
+		dev_err(&pdev->dev, "unable to get DMA resource\n");
+		goto err_put_node;
+	}
+
+	lp->mcdma_regs = devm_ioremap_resource(&pdev->dev, &dmares);
+	if (IS_ERR(lp->mcdma_regs)) {
+		dev_err(&pdev->dev, "iormeap failed for the dma\n");
+		ret = PTR_ERR(lp->mcdma_regs);
+		goto err_put_node;
+	}
+
+	axienet_mcdma_rx_probe_tsn(pdev, np, ndev);
+	axienet_mcdma_tx_probe_tsn(pdev, np, lp);
+
+	of_node_put(np);
+	return 0;
+
+err_put_node:
+	of_node_put(np);
+	return ret;
+}
+
+static const struct axienet_config tsn_endpoint_cfg = {
+	.mactype = XAXIENET_1G,
+	.setoptions = NULL,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+static void set_pcpmap_legacy(struct axienet_local *lp)
+{
+	int i;
+
+	/* First map all PCP values to lowest priority queue */
+	for (i = 0; i < XAE_MAX_TSN_TC; i++)
+		lp->pcpmap[i] = 0;
+
+	/* Map ST PCP values to Highest priority queue */
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	for (i = 0; i < st_count; i++) {
+		if (st_pcp[i] >= XAE_MAX_TSN_TC) {
+			pr_warn("invalid st pcp value %d ignored\n",
+				st_pcp[i]);
+			continue;
+		}
+
+		lp->pcpmap[st_pcp[i]] = lp->num_tc - 1;
+	}
+#endif
+
+	/* Map RES PCP values to Second Highest priority queue */
+	for (i = 0; i < res_count && lp->num_tc == XAE_MAX_LEGACY_TSN_TC; i++) {
+		if (res_pcp[i] >= XAE_MAX_TSN_TC)
+			pr_warn("invalid st pcp value %d ignored\n",
+				res_pcp[i]);
+		else if (lp->pcpmap[res_pcp[i]] == (lp->num_tc - 1))
+			pr_warn("pcp value %d already mapped to ST\n",
+				res_pcp[i]);
+		else
+			lp->pcpmap[res_pcp[i]] = lp->num_tc - 2;
+	}
+}
+
+static void set_pcpmap_new(struct axienet_local *lp)
+{
+	int i;
+
+	for (i = 0; i < lp->num_tc; i++)
+		lp->pcpmap[i] = i;
+}
+
+/**
+ * axienet_set_pcpmap - Gets the compile time pcp values that
+ * are mapped to ST and RES traffic from st_pcp and res_pcp
+ * fields and sets the pcpmap field for legacy design.
+ * For eight queue design, pcpmap field is one to one mapping from
+ * PCP to the queue.
+ *
+ * @lp:	netdev private data
+ */
+void axienet_set_pcpmap(struct axienet_local *lp)
+{
+	if (lp->num_tc <= XAE_MAX_LEGACY_TSN_TC)
+		set_pcpmap_legacy(lp);
+	else
+		set_pcpmap_new(lp);
+}
+
+static int init_tsn_txqs_legacy(struct axienet_local *lp, u16 num_tc)
+{
+	struct axienet_tsn_txq *txq;
+	int i;
+
+	for (i = 0; i < lp->num_tx_queues; i++) {
+		txq = &lp->txqs[i];
+		txq->is_tadma = false;
+		txq->dmaq_idx = i;
+	}
+
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	txq = &lp->txqs[num_tc - 1];
+	txq->is_tadma = true;
+	txq->dmaq_idx = qt_st;
+	lp->tadma_queues |= BIT(qt_st);
+#endif
+	return 0;
+}
+
+static int tsn_mcdma_chan_to_qidx(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_tx_dma_queue(lp, i) {
+		if (lp->dq[i]->chan_id == chan_id)
+			return i;
+	}
+
+	return -EINVAL;
+}
+
+static int init_tsn_txqs_new(struct axienet_local *lp, u16 num_tc)
+{
+	u32 total_tx_queues, queue = 0;
+	struct device_node *queue_node;
+	struct platform_device *pdev;
+	struct axienet_tsn_txq *txq;
+	struct device_node *tx_node;
+	int ret = 0;
+
+	pdev = to_platform_device(lp->dev);
+	tx_node = of_parse_phandle(pdev->dev.of_node, "xlnx,tsn-tx-config", 0);
+	if (!tx_node) {
+		dev_err(lp->dev, "xlnx,tsn-tx-config property not defined\n");
+		return -EINVAL;
+	}
+
+	ret = of_property_read_u32(tx_node, "xlnx,num-tx-queues",
+				   &total_tx_queues);
+	if (ret) {
+		dev_err(lp->dev, "xlnx,num-tx-queues property not defined\n");
+		goto exit;
+	}
+
+	if (total_tx_queues != num_tc) {
+		dev_err(lp->dev,
+			"total_tx_queues %d not equals to num_tc %d\n",
+			total_tx_queues, num_tc);
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	for_each_child_of_node(tx_node, queue_node) {
+		u32 dma_chan;
+
+		txq = &lp->txqs[queue];
+		ret = of_property_read_u32(queue_node, "xlnx,dma-channel-num",
+					   &dma_chan);
+		if (ret) {
+			dev_err(lp->dev,
+				"Q%d xlnx,dma-channel-num not defined\n",
+				queue);
+			of_node_put(queue_node);
+			goto exit;
+		}
+
+		if (of_property_read_bool(queue_node, "xlnx,is-tadma")) {
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+			if (dma_chan != qt_st && dma_chan != qt_res) {
+				dev_err(lp->dev, "Invalid TADMA stream %d\n",
+					dma_chan);
+				ret = -EINVAL;
+				of_node_put(queue_node);
+				goto exit;
+			}
+
+			lp->tadma_queues |= BIT(dma_chan);
+			txq->is_tadma = true;
+			txq->dmaq_idx = dma_chan;
+			queue++;
+#endif
+		} else {
+			int dmaq_idx = tsn_mcdma_chan_to_qidx(lp, dma_chan);
+
+			if (dmaq_idx < 0) {
+				dev_err(lp->dev,
+					"DMA chan %d not connected to TSN IP\n",
+					dma_chan);
+				ret = -EINVAL;
+				of_node_put(queue_node);
+				goto exit;
+			}
+
+			txq->is_tadma = false;
+			txq->dmaq_idx = dmaq_idx;
+			queue++;
+		}
+	}
+
+	if (queue != total_tx_queues) {
+		dev_err(lp->dev,
+			"No queues %d in DT not equal to total_tx_queues %d\n",
+			queue, total_tx_queues);
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	ret = 0;
+exit:
+	of_node_put(tx_node);
+	return ret;
+}
+
+int axienet_init_tsn_txqs(struct axienet_local *lp, u16 num_tc)
+{
+	if (num_tc <= XAE_MAX_LEGACY_TSN_TC)
+		return init_tsn_txqs_legacy(lp, num_tc);
+	else
+		return init_tsn_txqs_new(lp, num_tc);
+}
+
+/**
+ * tsn_ep_probe - TSN ep pointer probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for TSN endpoint driver.
+ */
+static int tsn_ep_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct axienet_local *lp;
+	struct net_device *ndev;
+	struct resource *ethres;
+	u16 num_queues = XAE_MAX_QUEUES;
+	struct device_node *np;
+	u8 mac_addr[ETH_ALEN];
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	char irq_name[32];
+#endif
+
+	ndev = alloc_netdev_mq(sizeof(*lp), "ep",
+			       NET_NAME_UNKNOWN, ether_setup, num_queues);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
+	ndev->features = NETIF_F_SG;
+	ndev->netdev_ops = &ep_netdev_ops;
+	ndev->ethtool_ops = &ep_ethtool_ops;
+
+	/* MTU range: 64 - 9000 */
+	ndev->min_mtu = 64;
+	ndev->max_mtu = XAE_JUMBO_MTU;
+
+	lp = netdev_priv(ndev);
+	lp->ndev = ndev;
+	lp->dev = &pdev->dev;
+	lp->options = XAE_OPTION_DEFAULTS;
+	lp->tx_bd_num = TX_BD_NUM_DEFAULT;
+	lp->rx_bd_num = RX_BD_NUM_DEFAULT;
+	lp->switch_prt = PORT_EP;
+
+	/* TODO
+	 * there are two temacs or two slaves to ep
+	 * get this infor from design?
+	 */
+	lp->slaves[0] = NULL;
+	lp->slaves[1] = NULL;
+	lp->ex_ep = NULL;
+	lp->packet_switch = 0;
+
+	lp->axienet_config = &tsn_endpoint_cfg;
+
+	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
+
+	/* check if ep has dma connected
+	 * in a ep_only system dma(mcdma/tadma) is connected to temac1
+	 */
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected-rx", 0);
+	if (!np) {
+		/* dont expose ep dev in ep_only system
+		 * all functionality handled by temac1/eth1
+		 */
+		free_netdev(ndev);
+		return 0;
+	}
+
+	/* Setup checksum offload, but default to off if not specified */
+	lp->features = 0;
+
+	lp->eth_hasnobuf = of_property_read_bool(pdev->dev.of_node,
+						 "xlnx,eth-hasnobuf");
+
+	/* Retrieve the MAC address */
+	ret = of_get_mac_address(pdev->dev.of_node, mac_addr);
+	if (ret)
+		dev_err(&pdev->dev, "could not find MAC address\n");
+	tsn_ep_set_mac_address(ndev, mac_addr);
+	ret = tsn_mcdma_probe(pdev, lp, ndev);
+	if (ret) {
+		dev_err(&pdev->dev, "Getting MCDMA resource failed\n");
+		goto err_put_node;
+	}
+
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+	ret = axienet_tadma_probe(pdev, ndev);
+	if (ret) {
+		dev_err(&pdev->dev, "Getting TADMA resource failed\n");
+		goto err_put_node;
+	}
+#endif
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &lp->num_tc);
+	if (ret || !axienet_tsn_num_tc_valid(lp->num_tc)) {
+		dev_err(&pdev->dev,
+			"xlnx,num-tc parameter not defined or valid\n");
+		goto err_put_node;
+	}
+
+	axienet_set_pcpmap(lp);
+	ret = axienet_init_tsn_txqs(lp, lp->num_tc);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to initialize TX queues\n");
+		goto err_put_node;
+	}
+
+	/* Map device registers */
+	ethres = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp->regs = devm_ioremap_resource(&pdev->dev, ethres);
+	if (IS_ERR(lp->regs)) {
+		ret = PTR_ERR(lp->regs);
+		goto err_put_node;
+	}
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	lp->qbv_regs = lp->regs;
+
+	sprintf(irq_name, "tsn_ep_scheduler_irq");
+	lp->qbv_irq = platform_get_irq_byname(pdev, irq_name);
+	axienet_qbv_init(ndev);
+#endif
+
+	ret = register_netdev(lp->ndev);
+	if (ret) {
+		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
+		goto err_put_node;
+	}
+	of_node_put(np);
+	return ret;
+
+err_put_node:
+	of_node_put(np);
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static void tsn_ep_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+
+	unregister_netdev(ndev);
+
+	free_netdev(ndev);
+}
+
+struct platform_driver tsn_ep_driver = {
+	.probe = tsn_ep_probe,
+	.remove = tsn_ep_remove,
+	.driver = {
+		 .name = "tsn_ep_axienet",
+		 .of_match_table = tsn_ep_of_match,
+	},
+};
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_ep_ex.c b/drivers/staging/xilinx-tsn/xilinx_tsn_ep_ex.c
new file mode 100644
index 000000000..32022086d
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_ep_ex.c
@@ -0,0 +1,168 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN End point driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/etherdevice.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/platform_device.h>
+#include <linux/skbuff.h>
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+
+static const struct of_device_id tsn_ex_ep_of_match[] = {
+	{ .compatible = "xlnx,tsn-ex-ep"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ex_ep_of_match);
+
+static int tsn_ex_ep_open(struct net_device *ndev)
+{
+	return 0;
+}
+
+static int tsn_ex_ep_stop(struct net_device *ndev)
+{
+	return 0;
+}
+
+static int tsn_ex_ep_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct net_device *master = lp->master;
+
+	skb->dev = master;
+	dev_queue_xmit(skb);
+	return NETDEV_TX_OK;
+}
+
+static void tsn_ex_ep_set_mac_address(struct net_device *ndev, const void *address)
+{
+	if (address)
+		eth_hw_addr_set(ndev, address);
+	if (!is_valid_ether_addr(ndev->dev_addr))
+		eth_hw_addr_random(ndev);
+}
+
+static int netdev_set_ex_ep_mac_address(struct net_device *ndev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	tsn_ex_ep_set_mac_address(ndev, addr->sa_data);
+	return 0;
+}
+
+static const struct net_device_ops ex_ep_netdev_ops = {
+	.ndo_open = tsn_ex_ep_open,
+	.ndo_stop = tsn_ex_ep_stop,
+	.ndo_start_xmit = tsn_ex_ep_xmit,
+	.ndo_set_mac_address = netdev_set_ex_ep_mac_address,
+};
+
+bool xlnx_is_port_ep_ex_netdev(const struct net_device *ndev)
+{
+	return ndev && (ndev->netdev_ops == &ex_ep_netdev_ops);
+}
+
+static int tsn_ex_ep_probe(struct platform_device *pdev)
+{
+	struct axienet_local *lp;
+	struct net_device *ndev;
+	struct device_node *ep_node;
+	struct axienet_local *ep_lp;
+	u8 mac_addr[ETH_ALEN];
+	int ret = 0;
+	const void *packet_switch;
+
+	ndev = alloc_netdev(sizeof(*lp), "exep",
+			    NET_NAME_UNKNOWN, ether_setup);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
+	ndev->features = NETIF_F_SG;
+	ndev->netdev_ops = &ex_ep_netdev_ops;
+
+	/* MTU range: 64 - 9000 */
+	ndev->min_mtu = 64;
+	ndev->max_mtu = XAE_JUMBO_MTU;
+
+	lp = netdev_priv(ndev);
+	lp->ndev = ndev;
+	lp->dev = &pdev->dev;
+	lp->options = XAE_OPTION_DEFAULTS;
+	/* Retrieve the MAC address */
+	ret = of_get_mac_address(pdev->dev.of_node, mac_addr);
+	if (ret) {
+		dev_err(&pdev->dev, "could not find MAC address\n");
+		goto free_netdev;
+	}
+	tsn_ex_ep_set_mac_address(ndev, mac_addr);
+	packet_switch = of_get_property(pdev->dev.of_node, "packet-switch", NULL);
+	ep_node = of_parse_phandle(pdev->dev.of_node, "tsn,endpoint", 0);
+
+	lp->master = of_find_net_device_by_node(ep_node);
+	ret = register_netdev(lp->ndev);
+	if (ret) {
+		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
+		goto free_of_node;
+	}
+	ep_lp = netdev_priv(lp->master);
+	ep_lp->ex_ep = ndev;
+	if (packet_switch) {
+		ep_lp->packet_switch = 1;
+		dev_warn(&pdev->dev,
+			 "packet-switch is deprecated and will be removed.Please use \"xlnx,packet-switch\"instead\n");
+	}
+	of_node_put(ep_node);
+	return ret;
+free_of_node:
+	of_node_put(ep_node);
+free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static void tsn_ex_ep_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+
+	unregister_netdev(ndev);
+	free_netdev(ndev);
+}
+
+struct platform_driver tsn_ex_ep_driver = {
+	.probe = tsn_ex_ep_probe,
+	.remove = tsn_ex_ep_remove,
+	.driver = {
+		 .name = "tsn_ex_ep_axienet",
+		 .of_match_table = tsn_ex_ep_of_match,
+	},
+};
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_ip.c b/drivers/staging/xilinx-tsn/xilinx_tsn_ip.c
new file mode 100644
index 000000000..373f9329d
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_ip.c
@@ -0,0 +1,517 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx FPGA Xilinx TSN IP driver.
+ *
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+#include <linux/phy.h>
+#include <linux/udp.h>
+#include <linux/mii.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_address.h>
+#include <linux/xilinx_phy.h>
+#include <linux/platform_device.h>
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+#include "xilinx_tsn_ptp.h"
+#include "xilinx_tsn_timer.h"
+#endif
+
+#define TSN_TX_BE_QUEUE  0
+#define TSN_TX_RES_QUEUE 1
+#define TSN_TX_ST_QUEUE  2
+
+#define XAE_TEMAC1 0
+#define XAE_TEMAC2 1
+static const struct of_device_id tsn_ip_of_match[] = {
+	{ .compatible = "xlnx,tsn-endpoint-ethernet-mac-1.0"},
+	{ .compatible = "xlnx,tsn-endpoint-ethernet-mac-2.0"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ip_of_match);
+
+/**
+ * tsn_ip_probe - TSN ip pointer probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for TSN driver.
+ */
+static int tsn_ip_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+
+	pr_info("TSN endpoint ethernet mac Probe\n");
+
+	ret = of_platform_populate(pdev->dev.of_node, NULL, NULL, &pdev->dev);
+	if (ret)
+		pr_err("TSN endpoint probe error (%i)\n", ret);
+
+	return ret;
+}
+
+static void tsn_ip_remove(struct platform_device *pdev)
+{
+	of_platform_depopulate(&pdev->dev);
+}
+
+u16 axienet_tsn_select_queue(struct net_device *ndev, struct sk_buff *skb,
+			     struct net_device *sb_dev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	struct ethhdr *hdr = (struct ethhdr *)skb->data;
+	const struct udphdr *udp;
+
+	udp = udp_hdr(skb);
+	if (hdr->h_proto == htons(ETH_P_1588) ||
+	    (lp->current_rx_filter == HWTSTAMP_FILTER_PTP_V2_L4_EVENT &&
+	     hdr->h_proto == htons(ETH_P_IP) && udp->dest == htons(0x013f))) {
+		/* It is not possible to use a static queue number for PTP
+		 * across various designs with multiple queues support as the
+		 * hardcoded queue number may conflict with actual TX queue.
+		 * Using num_tc is safe in all designs since the TX queues will
+		 * be in the range [0, num_tc-1]
+		 */
+		return lp->num_tc;
+	}
+#endif
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY)
+		return axienet_tsn_pcp_to_queue(ndev, skb);
+
+	return BE_QUEUE_NUMBER;
+}
+
+/**
+ * axienet_tsn_xmit - Starts the TSN transmission.
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    Non-zero error value on failure.
+ *
+ * This function is invoked from upper layers to initiate transmission. The
+ * function uses the next available free BDs and populates their fields to
+ * start the transmission. Use axienet_ptp_xmit() for PTP 1588 packets and
+ * use master EP xmit for other packets transmission.
+ */
+int axienet_tsn_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct net_device *master = lp->master;
+	u16 map = skb_get_queue_mapping(skb);
+	struct axienet_tsn_txq *txq = NULL;
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	/* check if skb is a PTP frame ? */
+	if (map == lp->num_tc)
+		return axienet_ptp_xmit(skb, ndev);
+#endif
+	txq = &lp->txqs[map];
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY) {
+		if (txq->is_tadma) /* ST Traffic */
+			return axienet_tadma_xmit(skb, ndev, map);
+
+		return axienet_queue_xmit_tsn(skb, ndev, map);
+	}
+	/* use EP to xmit non-PTP frames */
+	skb->dev = master;
+	dev_queue_xmit(skb);
+
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_tsn_probe - TSN mac probe function.
+ * @pdev:	Pointer to platform device structure.
+ * @lp:		Pointer to axienet local structure
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe for TSN mac nodes.
+ */
+int axienet_tsn_probe(struct platform_device *pdev,
+		      struct axienet_local *lp,
+		      struct net_device *ndev)
+{
+	int ret = 0;
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	char irq_name[32];
+#endif
+	bool slave = false;
+	u8     temac_no;
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	u32 qbv_addr, qbv_size;
+#endif
+	struct device_node *ep_node;
+	struct axienet_local *ep_lp;
+
+	slave = of_property_read_bool(pdev->dev.of_node,
+				      "xlnx,tsn-slave");
+	if (slave) {
+		temac_no = XAE_TEMAC2;
+		lp->switch_prt = PORT_MAC2;
+	} else {
+		temac_no = XAE_TEMAC1;
+		lp->switch_prt = PORT_MAC1;
+	}
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	lp->current_rx_filter = HWTSTAMP_FILTER_PTP_V2_L2_EVENT;
+	sprintf(irq_name, "interrupt_ptp_rx_%d", temac_no + 1);
+	lp->ptp_rx_irq = platform_get_irq_byname(pdev, irq_name);
+
+	pr_info("ptp RX irq: %d %s\n", lp->ptp_rx_irq, irq_name);
+	sprintf(irq_name, "interrupt_ptp_tx_%d", temac_no + 1);
+	lp->ptp_tx_irq = platform_get_irq_byname(pdev, irq_name);
+	pr_info("ptp TX irq: %d %s\n", lp->ptp_tx_irq, irq_name);
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	sprintf(irq_name, "tsn_switch_scheduler_irq_%d", temac_no + 1);
+	lp->qbv_irq = platform_get_irq_byname(pdev, irq_name);
+
+	/*Ignoring if the qbv_irq is not exist*/
+	if (lp->qbv_irq > 0)
+		pr_info("qbv_irq: %d %s\n", lp->qbv_irq, irq_name);
+#endif
+	spin_lock_init(&lp->ptp_tx_lock);
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	if (temac_no == XAE_TEMAC1)
+		lp->timer_priv = axienet_ptp_timer_probe((lp->regs + XAE_RTC_OFFSET), pdev);
+#endif
+
+	/* enable VLAN */
+	lp->options |= XAE_OPTION_VLAN;
+	axienet_setoptions_tsn(lp->ndev, lp->options);
+
+	/* get the ep device */
+	ep_node = of_parse_phandle(pdev->dev.of_node, "tsn,endpoint", 0);
+
+	lp->abl_reg = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+
+	/* in switch-mode, get the endpoint network device. in ep-only mode,
+	 * the endpoint driver frees itself
+	 */
+	if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY) && ep_node) {
+		lp->master = of_find_net_device_by_node(ep_node);
+		if (!lp->master) {
+			netdev_err(ndev, "Defer probe as EP is not probed\n");
+			ret = -EPROBE_DEFER;
+			goto err_1;
+		}
+	}
+
+	/* in ep only case tie the data path to eth1 */
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY && temac_no == XAE_TEMAC1) {
+		axienet_set_pcpmap(lp);
+		ret = tsn_mcdma_probe(pdev, lp, ndev);
+		if (ret) {
+			dev_err(&pdev->dev, "Getting MCDMA resource failed\n");
+			goto err_1;
+		}
+#if IS_ENABLED(CONFIG_AXIENET_HAS_TADMA)
+		ret = axienet_tadma_probe(pdev, ndev);
+		if (ret) {
+			dev_err(&pdev->dev, "Getting TADMA resource failed\n");
+			goto err_1;
+		}
+#endif
+		axienet_init_tsn_txqs(lp, lp->num_tc);
+		if (ret) {
+			dev_err(&pdev->dev, "Failed to initialize TX queues\n");
+			goto err_1;
+		}
+	}
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QBV)
+	lp->qbv_regs = NULL;
+	if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY)) {
+		of_property_read_u32(pdev->dev.of_node, "xlnx,qbv-addr",
+				     &qbv_addr);
+		of_property_read_u32(pdev->dev.of_node, "xlnx,qbv-size",
+				     &qbv_size);
+	} else {
+		struct resource res;
+
+		/* get qbv info from ep_node */
+		if (of_address_to_resource(ep_node, 0, &res) < 0)
+			dev_err(&pdev->dev, "error reading reg property\n");
+		qbv_addr = res.start;
+		qbv_size = res.end - res.start;
+	}
+	lp->qbv_regs = devm_ioremap(&pdev->dev, qbv_addr, qbv_size);
+	if (IS_ERR(lp->qbv_regs)) {
+		dev_err(&pdev->dev, "ioremap failed for the qbv\n");
+		ret = PTR_ERR(lp->qbv_regs);
+		goto err_1;
+	}
+	ret = axienet_qbv_init(ndev);
+#endif
+	if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY)) {
+		/* EP+Switch */
+		/* store the slaves to master(ep) */
+		ep_lp = netdev_priv(lp->master);
+		ep_lp->slaves[temac_no] = ndev;
+	}
+
+	of_node_put(ep_node);
+	return 0;
+err_1:
+	of_node_put(ep_node);
+	return ret;
+}
+
+/**
+ * axienet_device_reset - Reset and initialize the Axi Ethernet hardware.
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is called to reset and initialize the Axi Ethernet core. This
+ * is typically called during initialization. It does a reset of the Axi DMA
+ * Rx/Tx channels and initializes the Axi DMA BDs. Since Axi DMA reset lines
+ * areconnected to Axi Ethernet reset lines, this in turn resets the Axi
+ * Ethernet core. No separate hardware reset is done for the Axi Ethernet
+ * core.
+ */
+static void axienet_device_reset(struct net_device *ndev)
+{
+	u32 axienet_status;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
+
+	lp->options |= XAE_OPTION_VLAN;
+	lp->options &= (~XAE_OPTION_JUMBO);
+
+	if (ndev->mtu > XAE_MTU && ndev->mtu <= XAE_JUMBO_MTU) {
+		lp->max_frm_size = ndev->mtu + VLAN_ETH_HLEN +
+					XAE_TRL_SIZE;
+		if (lp->max_frm_size <= lp->rxmem)
+			lp->options |= XAE_OPTION_JUMBO;
+	}
+
+	axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+	axienet_status &= ~XAE_RCW1_RX_MASK;
+	axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+
+	if (lp->axienet_config->mactype == XAXIENET_1G &&
+	    !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+
+		/* Enable Receive errors */
+		axienet_iow(lp, XAE_IE_OFFSET, XAE_INT_RECV_ERROR_MASK);
+	}
+
+	axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+
+	axienet_set_mac_address_tsn(ndev, NULL);
+	axienet_set_multicast_list_tsn(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+
+	netif_trans_update(ndev);
+}
+
+/**
+ * axienet_tsn_open - TSN driver open routine.
+ * @ndev:	Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *	    non-zero error value on failure
+ *
+ * This is the driver open routine. It calls phy_start to start the PHY device.
+ * It also allocates interrupt service routines, enables the interrupt lines
+ * and ISR handling. Axi Ethernet core is reset through Axi DMA core.
+ */
+int axienet_tsn_open(struct net_device *ndev)
+{
+	int ret = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct phy_device *phydev = NULL;
+	struct net_device *emac0_ndev;
+	struct net_device *emac1_ndev;
+	struct axienet_local *ep_node;
+	u8 hw_addr_mask[ETH_ALEN];
+	int i;
+
+	axienet_device_reset(ndev);
+
+	if (lp->phy_node) {
+		phydev = of_phy_connect(lp->ndev, lp->phy_node,
+					axienet_adjust_link_tsn,
+					lp->phy_flags,
+					lp->phy_mode);
+		if (!phydev)
+			dev_err(lp->dev, "of_phy_connect() failed\n");
+		else
+			phy_start(phydev);
+	}
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	INIT_WORK(&lp->tx_tstamp_work, axienet_tx_tstamp);
+	skb_queue_head_init(&lp->ptp_txq);
+
+	lp->ptp_rx_hw_pointer = 0;
+	lp->ptp_rx_sw_pointer = 0xff;
+
+	axienet_iow(lp, PTP_RX_CONTROL_OFFSET, PTP_RX_PACKET_CLEAR);
+
+	ret = request_irq(lp->ptp_rx_irq, axienet_ptp_rx_irq,
+			  0, "ptp_rx", ndev);
+	if (ret)
+		goto err_ptp_rx_irq;
+
+	ret = request_irq(lp->ptp_tx_irq, axienet_ptp_tx_irq,
+			  0, "ptp_tx", ndev);
+	if (ret)
+		goto err_ptp_tx_irq;
+#endif
+
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY)
+		tsn_data_path_open(ndev);
+
+	if (lp->master) {
+		ep_node = netdev_priv(lp->master);
+		emac0_ndev = ep_node->slaves[0];
+		emac1_ndev = ep_node->slaves[1];
+
+		for (i = 0; i < ETH_ALEN; i++)
+			hw_addr_mask[i] = 0xFF;
+		hw_addr_mask[5] &= 0x000000F0;
+		if (!ether_addr_equal_masked(emac0_ndev->dev_addr, emac1_ndev->dev_addr,
+					     hw_addr_mask))
+			netdev_warn(ndev, "MSB 44 bits of the MAC addresses of TSN EMAC0 and TSN EMAC1 are different");
+		if (!ether_addr_equal_masked(emac0_ndev->dev_addr, ndev->dev_addr, hw_addr_mask))
+			netdev_warn(ndev, "MSB 44 bits of the MAC addresses of TSN EMAC0 and TSN EP are different");
+		if (!ether_addr_equal_masked(emac0_ndev->dev_addr, ndev->dev_addr, hw_addr_mask))
+			netdev_warn(ndev, "MSB 44 bits of the MAC addresses of TSN EMAC1 and TSN EP are different");
+	}
+
+	netif_tx_start_all_queues(ndev);
+
+	return ret;
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+err_ptp_tx_irq:
+	free_irq(lp->ptp_rx_irq, ndev);
+err_ptp_rx_irq:
+	return ret;
+#endif
+}
+
+int axienet_tsn_stop(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_PTP)
+	free_irq(lp->ptp_tx_irq, ndev);
+	free_irq(lp->ptp_rx_irq, ndev);
+#endif
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf)
+		free_irq(lp->eth_irq, ndev);
+
+	if (ndev->phydev)
+		phy_disconnect(ndev->phydev);
+
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY)
+		tsn_data_path_close(ndev);
+
+	return 0;
+}
+
+static struct platform_driver tsn_ip_driver = {
+	.probe = tsn_ip_probe,
+	.remove = tsn_ip_remove,
+	.driver = {
+		 .name = "tsn_ip_axienet",
+		 .of_match_table = tsn_ip_of_match,
+	},
+};
+
+static int tsn_ip_init(void)
+{
+	int ret;
+
+	ret = platform_driver_register(&tsn_ip_driver);
+	if (ret)
+		return ret;
+
+	ret = platform_driver_register(&tsn_ep_driver);
+	if (ret)
+		goto err_unregister_ip;
+
+	ret = platform_driver_register(&axienet_driver_tsn);
+	if (ret)
+		goto err_unregister_ep;
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+	ret = platform_driver_register(&tsnswitch_driver);
+	if (ret)
+		goto err_unregister_axienet;
+#endif
+
+	ret = platform_driver_register(&tsn_ex_ep_driver);
+	if (ret)
+		goto err_unregister_switch;
+
+	return 0;
+
+err_unregister_switch:
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+	platform_driver_unregister(&tsnswitch_driver);
+
+err_unregister_axienet:
+#endif
+	platform_driver_unregister(&axienet_driver_tsn);
+
+err_unregister_ep:
+	platform_driver_unregister(&tsn_ep_driver);
+
+err_unregister_ip:
+	platform_driver_unregister(&tsn_ip_driver);
+	return ret;
+}
+
+static void tsn_ip_exit(void)
+{
+	platform_driver_unregister(&tsn_ex_ep_driver);
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+	platform_driver_unregister(&tsnswitch_driver);
+#endif
+	platform_driver_unregister(&axienet_driver_tsn);
+	platform_driver_unregister(&tsn_ep_driver);
+	platform_driver_unregister(&tsn_ip_driver);
+}
+
+module_init(tsn_ip_init);
+module_exit(tsn_ip_exit);
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.c b/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.c
new file mode 100644
index 000000000..1522f2626
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.c
@@ -0,0 +1,360 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QBU/QBR - Frame Preemption module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_preemption.h"
+
+/**
+ * axienet_preemption -  Configure Frame Preemption
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u8 preemp;
+
+	if (copy_from_user(&preemp, useraddr, sizeof(preemp)))
+		return -EFAULT;
+
+	axienet_iow(lp, PREEMPTION_ENABLE_REG, preemp & PREEMPTION_ENABLE);
+	return 0;
+}
+
+/**
+ * axienet_preemption_ctrl_ethtool -  Configure Frame Preemption Control register
+ * @ndev: Pointer to the net_device structure
+ * @config_data: Pointer to the mm cfg data struct
+ * @extack: Pointer to the netlink_ext_ack struct
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_ctrl_ethtool(struct net_device *ndev, struct ethtool_mm_cfg *config_data,
+				    struct netlink_ext_ack *extack)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	bool preemption_support;
+	int add_frag_size, err;
+	u32 value;
+
+	value = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+	preemption_support = (value & PREEMPTION_SUPPORT) ? 1 : 0;
+	if (!preemption_support)
+		return -EOPNOTSUPP;
+
+	err = ethtool_mm_frag_size_min_to_add(config_data->tx_min_frag_size,
+					      &add_frag_size, extack);
+	if (err)
+		return err;
+
+	if (config_data->tx_enabled) {
+		value = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+		value = value | config_data->tx_enabled;
+		axienet_iow(lp, PREEMPTION_ENABLE_REG, value);
+	}
+
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+
+	value &= ~(VERIFY_TIMER_VALUE_MASK << VERIFY_TIMER_VALUE_SHIFT);
+	value |= (config_data->verify_time << VERIFY_TIMER_VALUE_SHIFT);
+	value &= ~(ADDITIONAL_FRAG_SIZE_MASK << ADDITIONAL_FRAG_SIZE_SHIFT);
+	value |= (add_frag_size << ADDITIONAL_FRAG_SIZE_SHIFT);
+
+	if (!config_data->verify_enabled) {
+		value &= ~(DISABLE_PREEMPTION_VERIFY);
+		value |= !(config_data->verify_enabled);
+	}
+
+	axienet_iow(lp, PREEMPTION_CTRL_STS_REG, value);
+	return 0;
+}
+
+/**
+ * axienet_preemption_ctrl -  Configure Frame Preemption Control register
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_ctrl(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct preempt_ctrl_sts data;
+	u32 value;
+
+	if (copy_from_user(&data, useraddr, sizeof(struct preempt_ctrl_sts)))
+		return -EFAULT;
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+
+	value &= ~(VERIFY_TIMER_VALUE_MASK << VERIFY_TIMER_VALUE_SHIFT);
+	value |= (data.verify_timer_value << VERIFY_TIMER_VALUE_SHIFT);
+	value &= ~(ADDITIONAL_FRAG_SIZE_MASK << ADDITIONAL_FRAG_SIZE_SHIFT);
+	value |= (data.additional_frag_size << ADDITIONAL_FRAG_SIZE_SHIFT);
+	value &= ~(DISABLE_PREEMPTION_VERIFY);
+	value |= (data.disable_preemp_verify);
+
+	axienet_iow(lp, PREEMPTION_CTRL_STS_REG, value);
+	return 0;
+}
+
+/**
+ * axienet_preemption_sts_ethtool -  Get Frame Preemption Status
+ * @ndev: Pointer to the net_device structure
+ * @state: Pointer to ethtool_mm_state struct
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_sts_ethtool(struct net_device *ndev, struct ethtool_mm_state *state)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 value;
+
+	value = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+	state->pmac_enabled = (value & PREEMPTION_SUPPORT) ? 1 : 0;
+	if (!state->pmac_enabled)
+		return -EOPNOTSUPP;
+
+	state->max_verify_time = MAX_VERIFY_TIME;
+	value = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	state->tx_enabled = value & PREEMPTION_ENABLE;
+
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+	state->tx_active = (value & TX_PREEMPTION_STS) ? 1 : 0;
+	state->verify_status = ((value >> MAC_MERGE_TX_VERIFY_STS_SHIFT)
+				 & MAC_MERGE_TX_VERIFY_STS_MASK) + 1;
+	state->verify_time = (value >> VERIFY_TIMER_VALUE_SHIFT) &
+					VERIFY_TIMER_VALUE_MASK;
+	state->tx_min_frag_size = ethtool_mm_frag_size_add_to_min((value
+								   >> ADDITIONAL_FRAG_SIZE_SHIFT)
+								   & ADDITIONAL_FRAG_SIZE_MASK);
+	state->rx_min_frag_size = ETH_ZLEN;
+	state->verify_enabled = ~(value & DISABLE_PREEMPTION_VERIFY);
+
+	return 0;
+}
+
+/**
+ * axienet_preemption_sts -  Get Frame Preemption Status
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing Frame Preemption status
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_sts(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct preempt_status status;
+	u32 value;
+
+	value = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	status.preemp_en = value & PREEMPTION_ENABLE;
+	value = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+	status.preemp_sup = (value & PREEMPTION_SUPPORT) ? 1 : 0;
+
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+	status.ctrl.tx_preemp_sts = (value & TX_PREEMPTION_STS) ? 1 : 0;
+	status.ctrl.mac_tx_verify_sts = (value >> MAC_MERGE_TX_VERIFY_STS_SHIFT)
+					& MAC_MERGE_TX_VERIFY_STS_MASK;
+	status.ctrl.verify_timer_value = (value >> VERIFY_TIMER_VALUE_SHIFT) &
+					VERIFY_TIMER_VALUE_MASK;
+	status.ctrl.additional_frag_size = (value >> ADDITIONAL_FRAG_SIZE_SHIFT)
+						& ADDITIONAL_FRAG_SIZE_MASK;
+	status.ctrl.disable_preemp_verify = value & DISABLE_PREEMPTION_VERIFY;
+
+	if (copy_to_user(useraddr, &status, sizeof(struct preempt_status)))
+		return -EFAULT;
+	return 0;
+}
+
+int axienet_preemption_receive(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 value;
+	u8 preemption_support;
+
+	value = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+	preemption_support = (value & PREEMPTION_SUPPORT) ? 1 : 0;
+
+	if (!preemption_support)
+		return -EOPNOTSUPP;
+
+	value = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	value = value | PREEMPTION_ENABLE;
+	axienet_iow(lp, PREEMPTION_ENABLE_REG, value);
+	return 0;
+}
+
+/**
+ * statistic_cnts -  Read statistics counter registers
+ * @ndev: Pointer to the net_device structure
+ * @ptr: Buffer addr to fill the counter values
+ * @count: read #count number of registers
+ * @addr_off: Register address to be read
+ */
+static void statistic_cnts(struct net_device *ndev, void *ptr,
+			   unsigned int count, unsigned int addr_off)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int *buf = (int *)ptr;
+	int i = 0;
+
+	for (i = 0; i < count; i++) {
+		buf[i] = axienet_ior(lp, addr_off);
+		addr_off += 4;
+	}
+}
+
+/**
+ * axienet_preemption_cnt_ethtool -  Get Frame Preemption Statistics counter
+ * @ndev: Pointer to the net_device structure
+ * @stats: return value, containing counters value
+ * Return: 0 on success, Non-zero error value on failure
+ */
+void axienet_preemption_cnt_ethtool(struct net_device *ndev, struct ethtool_mm_stats *stats)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	stats->MACMergeFrameAssErrorCount = axienet_ior64(lp,
+							  MAC_MERGE_FRAME_ASSEMBLY_ERROR_COUNT_REG);
+	stats->MACMergeFrameSmdErrorCount = axienet_ior64(lp, MAC_MERGE_FRAME_SMD_ERROR_COUNT_REG);
+	stats->MACMergeFrameAssOkCount = axienet_ior64(lp,
+						       MAC_MERGE_FRAME_ASSEMBLY_OK_COUNT_RX_REG);
+	stats->MACMergeFragCountRx = axienet_ior64(lp, MAC_MERGE_FRAG_COUNT_RX_REG);
+	stats->MACMergeFragCountTx = axienet_ior64(lp, MAC_MERGE_FRAG_COUNT_TX_REG);
+	stats->MACMergeHoldCount = axienet_ior64(lp, MAC_MERGE_HOLD_COUNT_REG);
+}
+
+/**
+ * axienet_preemption_cnt -  Get Frame Preemption Statistics counter
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing counters value
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_cnt(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct emac_pmac_stats stats;
+
+	statistic_cnts(ndev, &stats.emac,
+		       sizeof(struct statistics_counters) / 4,
+		       RX_BYTES_EMAC_REG);
+
+	stats.preemp_en = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	if (stats.preemp_en) {
+		statistic_cnts(ndev, &stats.pmac.sts,
+			       sizeof(struct statistics_counters) / 4,
+			       RX_BYTES_PMAC_REG);
+		statistic_cnts(ndev, &stats.pmac.merge,
+			       sizeof(struct mac_merge_counters) / 4,
+			       MAC_MERGE_HOLD_COUNT_REG);
+	}
+
+	if (copy_to_user(useraddr, &stats, sizeof(struct emac_pmac_stats)))
+		return -EFAULT;
+	return 0;
+}
+
+/**
+ * axienet_qbu_user_override -  Configure QBU user override register
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_qbu_user_override(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct qbu_prog data;
+	u32 value;
+
+	if (copy_from_user(&data, useraddr, sizeof(struct qbu_prog)))
+		return -EFAULT;
+
+	value = axienet_ior(lp, QBU_USER_OVERRIDE_REG);
+
+	if (data.set & QBU_WINDOW) {
+		if (data.user.hold_rel_window) {
+			value |= USER_HOLD_REL_ENABLE_VALUE;
+			value |= HOLD_REL_WINDOW_OVERRIDE;
+		} else {
+			value &= ~(USER_HOLD_REL_ENABLE_VALUE);
+			value &= ~(HOLD_REL_WINDOW_OVERRIDE);
+		}
+	}
+	if (data.set & QBU_GUARD_BAND) {
+		if (data.user.guard_band)
+			value |= GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE;
+		else
+			value &= ~(GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE);
+	}
+	if (data.set & QBU_HOLD_TIME) {
+		if (data.user.hold_time_override) {
+			value |= HOLD_TIME_OVERRIDE;
+			value &= ~(USER_HOLD_TIME_MASK << USER_HOLD_TIME_SHIFT);
+			value |= data.user.user_hold_time <<
+					USER_HOLD_TIME_SHIFT;
+		} else {
+			value &= ~(HOLD_TIME_OVERRIDE);
+			value &= ~(USER_HOLD_TIME_MASK << USER_HOLD_TIME_SHIFT);
+		}
+	}
+	if (data.set & QBU_REL_TIME) {
+		if (data.user.rel_time_override) {
+			value |= REL_TIME_OVERRIDE;
+			value &= ~(USER_REL_TIME_MASK << USER_REL_TIME_SHIFT);
+			value |= data.user.user_rel_time << USER_REL_TIME_SHIFT;
+		} else {
+			value &= ~(REL_TIME_OVERRIDE);
+			value &= ~(USER_REL_TIME_MASK << USER_REL_TIME_SHIFT);
+		}
+	}
+
+	axienet_iow(lp, QBU_USER_OVERRIDE_REG, value);
+	return 0;
+}
+
+/**
+ * axienet_qbu_sts -  Get QBU Core status
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing QBU core status value
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_qbu_sts(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct qbu_all_status status;
+	u32 value = 0;
+
+	value = axienet_ior(lp, QBU_USER_OVERRIDE_REG);
+	status.prog.hold_rel_window = (value & USER_HOLD_REL_ENABLE_VALUE)
+					? 1 : 0;
+	status.prog.guard_band = (value & GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE)
+					? 1 : 0;
+	status.prog.user_hold_time = (value >> USER_HOLD_TIME_SHIFT) &
+					USER_HOLD_TIME_MASK;
+	status.prog.user_rel_time = (value >> USER_REL_TIME_SHIFT) &
+					USER_REL_TIME_MASK;
+
+	value = axienet_ior(lp, QBU_CORE_STS_REG);
+	status.core.hold_time = (value >> HOLD_TIME_STS_SHIFT) &
+					HOLD_TIME_STS_MASK;
+	status.core.rel_time = (value >> REL_TIME_STS_SHIFT) &
+					REL_TIME_STS_MASK;
+	status.core.hold_rel_en = (value & HOLD_REL_ENABLE_STS) ? 1 : 0;
+	status.core.pmac_hold_req = value & PMAC_HOLD_REQ_STS;
+
+	if (copy_to_user(useraddr, &status, sizeof(struct qbu_all_status)))
+		return -EFAULT;
+	return 0;
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.h b/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.h
new file mode 100644
index 000000000..bd01d3ca8
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.h
@@ -0,0 +1,177 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN QBU/QBR - Frame Preemption header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_PREEMPTION_H
+#define XILINX_TSN_PREEMPTION_H
+
+#define PREEMPTION_ENABLE_REG			0x00000440
+#define PREEMPTION_CTRL_STS_REG			0x00000444
+#define QBU_USER_OVERRIDE_REG			0x00000448
+#define QBU_CORE_STS_REG			0x0000044c
+#define RX_BYTES_EMAC_REG			0x00000200
+#define RX_BYTES_PMAC_REG			0x00000800
+#define MAC_MERGE_HOLD_COUNT_REG		0x00000910
+#define MAC_MERGE_FRAG_COUNT_TX_REG		0x00000918
+#define MAC_MERGE_FRAME_ASSEMBLY_OK_COUNT_RX_REG	0x00000920
+#define MAC_MERGE_FRAME_ASSEMBLY_ERROR_COUNT_REG	0x00000928
+#define MAC_MERGE_FRAME_SMD_ERROR_COUNT_REG		0x00000930
+#define MAC_MERGE_FRAG_COUNT_RX_REG			0x00000938
+
+#define PREEMPTION_ENABLE			BIT(0)
+#define PREEMPTION_SUPPORT			BIT(15)
+
+#define TX_PREEMPTION_STS			BIT(31)
+#define MAC_MERGE_TX_VERIFY_STS_MASK		0x7
+#define MAC_MERGE_TX_VERIFY_STS_SHIFT		24
+#define VERIFY_TIMER_VALUE_MASK			0x7F
+#define VERIFY_TIMER_VALUE_SHIFT		8
+#define ADDITIONAL_FRAG_SIZE_MASK		0x3
+#define ADDITIONAL_FRAG_SIZE_SHIFT		4
+#define DISABLE_PREEMPTION_VERIFY		BIT(0)
+#define MAX_VERIFY_TIME				0x7F	/* 7-bit field */
+
+#define USER_HOLD_REL_ENABLE_VALUE		BIT(31)
+#define USER_HOLD_TIME_MASK			0x1FF
+#define USER_HOLD_TIME_SHIFT			16
+#define USER_REL_TIME_MASK			0x3F
+#define USER_REL_TIME_SHIFT			8
+#define GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE	BIT(3)
+#define HOLD_REL_WINDOW_OVERRIDE		BIT(2)
+#define HOLD_TIME_OVERRIDE			BIT(1)
+#define REL_TIME_OVERRIDE			BIT(0)
+
+#define HOLD_REL_ENABLE_STS			BIT(31)
+#define HOLD_TIME_STS_MASK			0x1FF
+#define HOLD_TIME_STS_SHIFT			16
+#define REL_TIME_STS_MASK			0x3F
+#define REL_TIME_STS_SHIFT			8
+#define PMAC_HOLD_REQ_STS			BIT(0)
+
+struct preempt_ctrl_sts {
+	u8 tx_preemp_sts;
+	u8 mac_tx_verify_sts;
+	u8 verify_timer_value;
+	u8 additional_frag_size;
+	u8 disable_preemp_verify;
+};
+
+struct qbu_prog_override {
+	u8 enable_value:1;
+	u16 user_hold_time:9;
+	u8 user_rel_time:6;
+	u8 guard_band:1;
+	u8 hold_rel_window:1;
+	u8 hold_time_override:1;
+	u8 rel_time_override:1;
+} __packed;
+
+struct qbu_prog {
+	struct qbu_prog_override user;
+	u8 set;
+};
+
+#define QBU_WINDOW BIT(0)
+#define QBU_GUARD_BAND BIT(1)
+#define QBU_HOLD_TIME BIT(2)
+#define QBU_REL_TIME BIT(3)
+
+struct qbu_core_status {
+	u16 hold_time;
+	u8 rel_time;
+	u8 hold_rel_en:1;
+	u8 pmac_hold_req:1;
+} __packed;
+
+struct qbu_all_status {
+	struct qbu_prog_override prog;
+	struct qbu_core_status core;
+};
+
+struct cnt_64 {
+	unsigned int msb;
+	unsigned int lsb;
+};
+
+union static_cntr {
+	u64 cnt;
+	struct cnt_64 word;
+};
+
+struct mac_merge_counters {
+	union static_cntr tx_hold_cnt;
+	union static_cntr tx_frag_cnt;
+	union static_cntr rx_assembly_ok_cnt;
+	union static_cntr rx_assembly_err_cnt;
+	union static_cntr rx_smd_err_cnt;
+	union static_cntr rx_frag_cnt;
+};
+
+struct statistics_counters {
+	union static_cntr rx_bytes_cnt;
+	union static_cntr tx_bytes_cnt;
+	union static_cntr undersize_frames_cnt;
+	union static_cntr frag_frames_cnt;
+	union static_cntr rx_64_bytes_frames_cnt;
+	union static_cntr rx_65_127_bytes_frames_cnt;
+	union static_cntr rx_128_255_bytes_frames_cnt;
+	union static_cntr rx_256_511_bytes_frames_cnt;
+	union static_cntr rx_512_1023_bytes_frames_cnt;
+	union static_cntr rx_1024_max_frames_cnt;
+	union static_cntr rx_oversize_frames_cnt;
+	union static_cntr tx_64_bytes_frames_cnt;
+	union static_cntr tx_65_127_bytes_frames_cnt;
+	union static_cntr tx_128_255_bytes_frames_cnt;
+	union static_cntr tx_256_511_bytes_frames_cnt;
+	union static_cntr tx_512_1023_bytes_frames_cnt;
+	union static_cntr tx_1024_max_frames_cnt;
+	union static_cntr tx_oversize_frames_cnt;
+	union static_cntr rx_good_frames_cnt;
+	union static_cntr rx_fcs_err_cnt;
+	union static_cntr rx_good_broadcast_frames_cnt;
+	union static_cntr rx_good_multicast_frames_cnt;
+	union static_cntr rx_good_control_frames_cnt;
+	union static_cntr rx_out_of_range_err_cnt;
+	union static_cntr rx_good_vlan_frames_cnt;
+	union static_cntr rx_good_pause_frames_cnt;
+	union static_cntr rx_bad_opcode_frames_cnt;
+	union static_cntr tx_good_frames_cnt;
+	union static_cntr tx_good_broadcast_frames_cnt;
+	union static_cntr tx_good_multicast_frames_cnt;
+	union static_cntr tx_underrun_err_cnt;
+	union static_cntr tx_good_control_frames_cnt;
+	union static_cntr tx_good_vlan_frames_cnt;
+	union static_cntr tx_good_pause_frames_cnt;
+};
+
+struct pmac_counters {
+	struct statistics_counters sts;
+	struct mac_merge_counters merge;
+};
+
+struct emac_pmac_stats {
+	u8 preemp_en;
+	struct statistics_counters emac;
+	struct pmac_counters pmac;
+};
+
+struct preempt_status {
+	u8 preemp_en;
+	u8 preemp_sup;
+	struct preempt_ctrl_sts ctrl;
+};
+#endif /* XILINX_TSN_PREEMPTION_H */
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_ptp.h b/drivers/staging/xilinx-tsn/xilinx_tsn_ptp.h
new file mode 100644
index 000000000..15172c0af
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_ptp.h
@@ -0,0 +1,89 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN PTP header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _TSN_PTP_H_
+#define _TSN_PTP_H_
+
+#define PTP_HW_TSTAMP_SIZE  8   /* 64 bit timestamp */
+#define PTP_RX_HWBUF_SIZE   256
+#define PTP_RX_FRAME_SIZE   252
+#define PTP_HW_TSTAMP_OFFSET (PTP_RX_HWBUF_SIZE - PTP_HW_TSTAMP_SIZE)
+
+#define PTP_MSG_TYPE_MASK				BIT(3)
+#define PTP_TYPE_SYNC                                   0x0
+#define PTP_TYPE_FOLLOW_UP                              0x8
+#define PTP_TYPE_PDELAYREQ                              0x2
+#define PTP_TYPE_PDELAYRESP                             0x3
+#define PTP_TYPE_PDELAYRESP_FOLLOW_UP                   0xA
+#define PTP_TYPE_ANNOUNCE                               0xB
+#define PTP_TYPE_SIGNALING                              0xC
+
+#define PTP_TX_CONTROL_OFFSET		0x00012000 /**< Tx PTP Control Reg */
+#define PTP_RX_CONTROL_OFFSET		0x00012004 /**< Rx PTP Control Reg */
+#define RX_FILTER_CONTROL		0x00012008 /**< Rx Filter Ctrl Reg */
+
+#define PTP_RX_BASE_OFFSET		0x00010000
+#define PTP_RX_CONTROL_OFFSET		0x00012004 /**< Rx PTP Control Reg */
+#define PTP_RX_PACKET_FIELD_MASK	0x00000F00
+#define PTP_RX_PACKET_CLEAR		0x00000001
+
+#define PTP_TX_BUFFER_OFFSET(index)	   (0x00011000 + (index) * 0x100)
+
+#define PTP_TX_CMD_FIELD_LEN			8
+#define PTP_TX_CMD_1STEP_SHIFT			BIT(16)
+#define PTP_TX_BUFFER_CMD2_FIELD		0x4
+
+#define PTP_TX_SYNC_OFFSET                 0x00011000
+#define PTP_TX_FOLLOW_UP_OFFSET            0x00011100
+#define PTP_TX_PDELAYREQ_OFFSET            0x00011200
+#define PTP_TX_PDELAYRESP_OFFSET           0x00011300
+#define PTP_TX_PDELAYRESP_FOLLOW_UP_OFFSET 0x00011400
+#define PTP_TX_ANNOUNCE_OFFSET             0x00011500
+#define PTP_TX_SIGNALING_OFFSET		   0x00011600
+#define PTP_TX_GENERIC_OFFSET		   0x00011700
+#define PTP_TX_SEND_SYNC_FRAME_MASK                     0x00000001
+#define PTP_TX_SEND_FOLLOWUP_FRAME_MASK                 0x00000002
+#define PTP_TX_SEND_PDELAYREQ_FRAME_MASK                0x00000004
+#define PTP_TX_SEND_PDELAYRESP_FRAME_MASK               0x00000008
+#define PTP_TX_SEND_PDELAYRESPFOLLOWUP_FRAME_MASK       0x00000010
+#define PTP_TX_SEND_ANNOUNCE_FRAME_MASK                 0x00000020
+#define PTP_TX_SEND_FRAME6_BIT_MASK                     0x00000040
+#define PTP_TX_SEND_FRAME7_BIT_MASK                     0x00000080
+#define PTP_TX_FRAME_WAITING_MASK			0x0000ff00
+#define PTP_TX_FRAME_WAITING_SHIFT			8
+#define PTP_TX_WAIT_SYNC_FRAME_MASK                     0x00000100
+#define PTP_TX_WAIT_FOLLOWUP_FRAME_MASK                 0x00000200
+#define PTP_TX_WAIT_PDELAYREQ_FRAME_MASK                0x00000400
+#define PTP_TX_WAIT_PDELAYRESP_FRAME_MASK               0x00000800
+#define PTP_TX_WAIT_PDELAYRESPFOLLOWUP_FRAME_MASK       0x00001000
+#define PTP_TX_WAIT_ANNOUNCE_FRAME_MASK                 0x00002000
+#define PTP_TX_WAIT_FRAME6_BIT_MASK                     0x00004000
+#define PTP_TX_WAIT_FRAME7_BIT_MASK                     0x00008000
+#define PTP_TX_WAIT_ALL_FRAMES_MASK                     0x0000FF00
+#define PTP_TX_PACKET_FIELD_MASK                        0x00070000
+#define PTP_TX_PACKET_FIELD_SHIFT                       16
+/* 1-step Correction Field offset 802.1 ASrev */
+#define PTP_CRCT_FIELD_OFFSET				22
+/* 1-step Time Of Day offset 1588-2008 */
+#define PTP_TOD_FIELD_OFFSET				48
+
+int axienet_ptp_xmit(struct sk_buff *skb, struct net_device *ndev);
+irqreturn_t axienet_ptp_rx_irq(int irq, void *_ndev);
+irqreturn_t axienet_ptp_tx_irq(int irq, void *_ndev);
+
+#endif
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_clock.c b/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_clock.c
new file mode 100644
index 000000000..b967451c6
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_clock.c
@@ -0,0 +1,311 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN PTP protocol clock Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/ptp_clock_kernel.h>
+#include <linux/platform_device.h>
+#include <linux/of_irq.h>
+#include "xilinx_tsn_timer.h"
+
+struct xlnx_ptp_timer {
+	struct                 device *dev;
+	void __iomem          *baseaddr;
+	struct ptp_clock      *ptp_clock;
+	struct ptp_clock_info  ptp_clock_info;
+	spinlock_t             reg_lock; /* ptp timer lock */
+	int                    irq;
+	int                    pps_enable;
+	int                    countpulse;
+	u32                    rtc_value;
+};
+
+static void xlnx_tod_read(struct xlnx_ptp_timer *timer, struct timespec64 *ts)
+{
+	u32 sec, nsec;
+
+	nsec = readl(timer->baseaddr + XTIMER1588_CURRENT_RTC_NS);
+	sec = readl(timer->baseaddr + XTIMER1588_CURRENT_RTC_SEC_L);
+
+	ts->tv_sec = sec;
+	ts->tv_nsec = nsec;
+}
+
+static void xlnx_rtc_offset_write(struct xlnx_ptp_timer *timer,
+				  const struct timespec64 *ts)
+{
+	pr_debug("%s: sec: %lld nsec: %ld\n", __func__, ts->tv_sec, ts->tv_nsec);
+
+	writel(0, (timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_H));
+	writel(ts->tv_sec, (timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_L));
+	writel(ts->tv_nsec, (timer->baseaddr + XTIMER1588_RTC_OFFSET_NS));
+}
+
+static void xlnx_rtc_offset_read(struct xlnx_ptp_timer *timer,
+				 struct timespec64 *ts)
+{
+	ts->tv_sec = readl(timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_L);
+	ts->tv_nsec = readl(timer->baseaddr + XTIMER1588_RTC_OFFSET_NS);
+}
+
+/* PTP clock operations
+ */
+static int xlnx_ptp_adjfine(struct ptp_clock_info *ptp, long scaled_ppm)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+
+	u32 incval;
+
+	incval = adjust_by_scaled_ppm(timer->rtc_value, scaled_ppm);
+	writel(incval, (timer->baseaddr + XTIMER1588_RTC_INCREMENT));
+	return 0;
+}
+
+static int xlnx_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)
+{
+	unsigned long flags;
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	struct timespec64 now, then = ns_to_timespec64(delta);
+
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	xlnx_rtc_offset_read(timer, &now);
+
+	now = timespec64_add(now, then);
+
+	xlnx_rtc_offset_write(timer, (const struct timespec64 *)&now);
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+
+	return 0;
+}
+
+static int xlnx_ptp_gettime(struct ptp_clock_info *ptp, struct timespec64 *ts)
+{
+	unsigned long flags;
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	xlnx_tod_read(timer, ts);
+
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+	return 0;
+}
+
+/**
+ * xlnx_ptp_settime - Set the current time on the hardware clock
+ * @ptp: ptp clock structure
+ * @ts: timespec64 containing the new time for the cycle counter
+ *
+ * Return: 0 in all cases.
+ *
+ * The seconds register is written first, then the nanosecond
+ * The hardware loads the entire new value when a nanosecond register
+ * is written
+ **/
+static int xlnx_ptp_settime(struct ptp_clock_info *ptp,
+			    const struct timespec64 *ts)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	struct timespec64 delta, tod;
+	struct timespec64 offset;
+	unsigned long flags;
+
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	/* First zero the offset */
+	offset.tv_sec = 0;
+	offset.tv_nsec = 0;
+	xlnx_rtc_offset_write(timer, &offset);
+
+	/* Get the current timer value */
+	xlnx_tod_read(timer, &tod);
+
+	/* Subtract the current reported time from our desired time */
+	delta = timespec64_sub(*ts, tod);
+
+	/* Don't write a negative offset */
+	if (delta.tv_sec <= 0) {
+		delta.tv_sec = 0;
+		if (delta.tv_nsec < 0)
+			delta.tv_nsec = 0;
+	}
+
+	xlnx_rtc_offset_write(timer, &delta);
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+	return 0;
+}
+
+static int xlnx_ptp_enable(struct ptp_clock_info *ptp,
+			   struct ptp_clock_request *rq, int on)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+
+	switch (rq->type) {
+	case PTP_CLK_REQ_PPS:
+		timer->pps_enable = 1;
+		return 0;
+	default:
+		break;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info xlnx_ptp_clock_info = {
+	.owner    = THIS_MODULE,
+	.name     = "Xilinx Timer",
+	.max_adj  = 999999999,
+	.n_ext_ts	= 0,
+	.pps      = 1,
+	.adjfine  = xlnx_ptp_adjfine,
+	.adjtime  = xlnx_ptp_adjtime,
+	.gettime64  = xlnx_ptp_gettime,
+	.settime64 = xlnx_ptp_settime,
+	.enable   = xlnx_ptp_enable,
+};
+
+/* module operations */
+
+/**
+ * xlnx_ptp_timer_isr - Interrupt Service Routine
+ * @irq:               IRQ number
+ * @priv:              pointer to the timer structure
+ *
+ * Returns: IRQ_HANDLED for all cases
+ *
+ * Handles the timer interrupt. The timer interrupt fires 128 times per
+ * secound. When our count reaches 128 emit a PTP_CLOCK_PPS event
+ */
+static irqreturn_t xlnx_ptp_timer_isr(int irq, void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_PPS;
+	++timer->countpulse;
+	if (timer->countpulse >= PULSESIN1PPS) {
+		timer->countpulse = 0;
+		if (timer->ptp_clock && timer->pps_enable)
+			ptp_clock_event(timer->ptp_clock, &event);
+	}
+	writel((1 << XTIMER1588_INT_SHIFT),
+	       (timer->baseaddr + XTIMER1588_INTERRUPT));
+
+	return IRQ_HANDLED;
+}
+
+int axienet_get_phc_index(void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+
+	if (timer->ptp_clock)
+		return ptp_clock_index(timer->ptp_clock);
+	else
+		return -1;
+}
+
+static void tsn_ptp_unregister(void *ptp)
+{
+	ptp_clock_unregister((struct ptp_clock *)ptp);
+}
+
+void *axienet_ptp_timer_probe(void __iomem *base, struct platform_device *pdev)
+{
+	struct xlnx_ptp_timer *timer;
+	struct timespec64 ts;
+	int err = 0;
+
+	timer = devm_kzalloc(&pdev->dev, sizeof(*timer), GFP_KERNEL);
+	if (!timer)
+		return ERR_PTR(-ENOMEM);
+
+	timer->baseaddr = base;
+
+	timer->irq = platform_get_irq_byname(pdev, "interrupt_ptp_timer");
+
+	if (timer->irq < 0) {
+		timer->irq = platform_get_irq_byname(pdev, "rtc_irq");
+		if (timer->irq > 0) {
+			pr_err("ptp timer interrupt name 'rtc_irq' is deprecated\n");
+		} else {
+			pr_err("ptp timer interrupt not found\n");
+			return ERR_PTR(-EINVAL);
+		}
+	}
+	spin_lock_init(&timer->reg_lock);
+
+	timer->ptp_clock_info = xlnx_ptp_clock_info;
+
+	timer->ptp_clock = ptp_clock_register(&timer->ptp_clock_info,
+					      &pdev->dev);
+	if (IS_ERR(timer->ptp_clock)) {
+		err = PTR_ERR(timer->ptp_clock);
+		pr_debug("Failed to register ptp clock\n");
+		goto out;
+	}
+	err = devm_add_action_or_reset(&pdev->dev,
+				       tsn_ptp_unregister,
+				       timer->ptp_clock);
+	if (err) {
+		pr_debug("Failed to add PTP clock unregister action\n");
+		goto out;
+	}
+
+	axienet_phc_index = ptp_clock_index(timer->ptp_clock);
+
+	ts = ktime_to_timespec64(ktime_get_real());
+
+	xlnx_ptp_settime(&timer->ptp_clock_info, &ts);
+	/* In the TSN IP Core, RTC clock is connected to gtx_clk which is
+	 * 125 MHz. This is specified in the TSN PG and is not configurable.
+	 *
+	 * Calculating the RTC Increment Value once and storing it in
+	 * timer->rtc_value to prevent recalculating it each time the PTP
+	 * frequency is adjusted in xlnx_ptp_adjfine()
+	 */
+	timer->rtc_value = (div_u64(NSEC_PER_SEC, XTIMER1588_GTX_CLK_FREQ) <<
+			    XTIMER1588_RTC_NS_SHIFT);
+	writel(timer->rtc_value, (timer->baseaddr + XTIMER1588_RTC_INCREMENT));
+
+	/* Enable interrupts */
+	err = devm_request_irq(&pdev->dev, timer->irq,
+			       xlnx_ptp_timer_isr,
+			       0,
+			       "ptp_rtc",
+			       (void *)timer);
+	if (err) {
+		pr_err("Failed to request IRQ: %d\n", err);
+		ptp_clock_unregister(timer->ptp_clock);
+		goto out;
+	}
+
+	return timer;
+out:
+	timer->ptp_clock = NULL;
+	return ERR_PTR(err);
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_xmit.c b/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_xmit.c
new file mode 100644
index 000000000..1810205f3
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_xmit.c
@@ -0,0 +1,369 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN PTP transfer protocol module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_ptp.h"
+#include "xilinx_tsn_timer.h"
+#include <linux/ptp_classify.h>
+
+#define PTP_ONE_SECOND            1000000000    /**< Value in ns */
+
+#define msg_type_string(type) \
+	((type) == PTP_TYPE_SYNC) ? "SYNC" : \
+	((type) == PTP_TYPE_FOLLOW_UP)		  ? "FOLLOW_UP" : \
+	((type) == PTP_TYPE_PDELAYREQ)		  ? "PDELAY_REQ" : \
+	((type) == PTP_TYPE_PDELAYRESP)		  ? "PDELAY_RESP" : \
+	((type) == PTP_TYPE_PDELAYRESP_FOLLOW_UP) ? "PDELAY_RESP_FOLLOW_UP" : \
+	((type) == PTP_TYPE_ANNOUNCE)		  ? "ANNOUNCE" : \
+	"UNKNOWN"
+
+/**
+ * memcpy_fromio_32 - copy ptp buffer from HW
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Offset in the PTP buffer
+ * @data:	Destination buffer
+ * @len:	Len to copy
+ *
+ * This functions copies the data from PTP buffer to destination data buffer
+ */
+static void memcpy_fromio_32(struct axienet_local *lp,
+			     unsigned long offset, u8 *data, size_t len)
+{
+	while (len >= 4) {
+		*(u32 *)data = axienet_ior(lp, offset);
+		len -= 4;
+		offset += 4;
+		data += 4;
+	}
+
+	if (len > 0) {
+		u32 leftover = axienet_ior(lp, offset);
+		u8 *src = (u8 *)&leftover;
+
+		while (len) {
+			*data++ = *src++;
+			len--;
+		}
+	}
+}
+
+/**
+ * memcpy_toio_32 - copy ptp buffer from HW
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Offset in the PTP buffer
+ * @data:	Source data
+ * @len:	Len to copy
+ *
+ * This functions copies the source data to destination ptp buffer
+ */
+static void memcpy_toio_32(struct axienet_local *lp,
+			   unsigned long offset, u8 *data, size_t len)
+{
+	while (len >= 4) {
+		axienet_iow(lp, offset, *(u32 *)data);
+		len -= 4;
+		offset += 4;
+		data += 4;
+	}
+
+	if (len > 0) {
+		u32 leftover = 0;
+		u8 *dest = (u8 *)&leftover;
+
+		while (len) {
+			*dest++ = *data++;
+			len--;
+		}
+		axienet_iow(lp, offset, leftover);
+	}
+}
+
+static int is_sync(struct sk_buff *skb)
+{
+	u8 *msg_type;
+
+	msg_type = (u8 *)skb->data + ETH_HLEN;
+
+	return (*msg_type & 0xf) == PTP_TYPE_SYNC;
+}
+
+/**
+ * axienet_ptp_xmit - xmit skb using PTP HW
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ *
+ * This function is called to transmit a PTP skb. The function uses
+ * the free PTP TX buffer entry and sends the frame
+ */
+int axienet_ptp_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	u8 msg_type;
+	struct axienet_local *lp = netdev_priv(ndev);
+	unsigned long flags;
+	u8 tx_frame_waiting;
+	u8 free_index;
+	u32 cmd1_field = 0;
+	u32 cmd2_field = 0;
+
+	msg_type  = *(u8 *)(skb->data + ETH_HLEN);
+
+	pr_debug("  -->XMIT: protocol: %x message: %s frame_len: %d\n",
+		 skb->protocol,
+		 msg_type_string(msg_type & 0xf), skb->len);
+
+	tx_frame_waiting =  (axienet_ior(lp, PTP_TX_CONTROL_OFFSET) &
+				PTP_TX_FRAME_WAITING_MASK) >>
+				PTP_TX_FRAME_WAITING_SHIFT;
+
+	/* we reached last frame */
+	if (tx_frame_waiting & (1 << 7)) {
+		netif_stop_subqueue(ndev, lp->num_tc);
+		pr_debug("tx_frame_waiting: %d\n", tx_frame_waiting);
+		return NETDEV_TX_BUSY;
+	}
+
+	/* go to next available slot */
+	free_index  = fls(tx_frame_waiting);
+
+	/* write the len */
+	if (lp->ptp_ts_type == HWTSTAMP_TX_ONESTEP_SYNC &&
+	    is_sync(skb)) {
+		/* enable 1STEP SYNC */
+		cmd1_field |= PTP_TX_CMD_1STEP_SHIFT;
+		cmd2_field |= PTP_TOD_FIELD_OFFSET;
+	}
+
+	cmd1_field |= skb->len;
+
+	axienet_iow(lp, PTP_TX_BUFFER_OFFSET(free_index), cmd1_field);
+	axienet_iow(lp, PTP_TX_BUFFER_OFFSET(free_index) +
+			PTP_TX_BUFFER_CMD2_FIELD, cmd2_field);
+	memcpy_toio_32(lp,
+		       (PTP_TX_BUFFER_OFFSET(free_index) +
+			PTP_TX_CMD_FIELD_LEN),
+		       skb->data, skb->len);
+
+	/* send the frame */
+	axienet_iow(lp, PTP_TX_CONTROL_OFFSET, (1 << free_index));
+
+	if (lp->ptp_ts_type != HWTSTAMP_TX_ONESTEP_SYNC ||
+	    (!is_sync(skb))) {
+		spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+		skb->cb[0] = free_index;
+		skb_queue_tail(&lp->ptp_txq, skb);
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+
+		skb_tx_timestamp(skb);
+		spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+	}
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_set_timestamp - timestamp skb with HW timestamp
+ * @lp:		Pointer to axienet local structure
+ * @hwtstamps:  Pointer to skb timestamp structure
+ * @offset:	offset of the timestamp in the PTP buffer
+ *
+ * Return:	None.
+ *
+ */
+static void axienet_set_timestamp(struct axienet_local *lp,
+				  struct skb_shared_hwtstamps *hwtstamps,
+				  unsigned int offset)
+{
+	u32 captured_ns;
+	u32 captured_sec;
+
+	captured_ns = axienet_ior(lp, offset + 4);
+	captured_sec = axienet_ior(lp, offset);
+
+	/* Upper 32 bits contain s, lower 32 bits contain ns. */
+	hwtstamps->hwtstamp = ktime_set(captured_sec,
+					captured_ns);
+}
+
+/**
+ * axienet_ptp_recv - receive ptp buffer in skb from HW
+ * @ndev:	Pointer to net_device structure.
+ *
+ * This function is called from the ptp rx isr. It allocates skb, and
+ * copies the ptp rx buffer data to it and calls netif_rx for further
+ * processing.
+ *
+ */
+static void axienet_ptp_recv(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	unsigned long ptp_frame_base_addr = 0;
+	struct sk_buff *skb;
+	u16 msg_len;
+	u8 msg_type;
+	u32 bytes = 0;
+	u32 packets = 0;
+
+	pr_debug("%s:\n ", __func__);
+
+	while (((lp->ptp_rx_hw_pointer & 0xf) !=
+		 (lp->ptp_rx_sw_pointer & 0xf))) {
+		skb = netdev_alloc_skb(ndev, PTP_RX_FRAME_SIZE);
+
+		lp->ptp_rx_sw_pointer += 1;
+
+		ptp_frame_base_addr = PTP_RX_BASE_OFFSET +
+				   ((lp->ptp_rx_sw_pointer & 0xf) *
+							PTP_RX_HWBUF_SIZE);
+
+		memset(skb->data, 0x0, PTP_RX_FRAME_SIZE);
+
+		memcpy_fromio_32(lp, ptp_frame_base_addr, skb->data,
+				 PTP_RX_FRAME_SIZE);
+
+		msg_type  = *(u8 *)(skb->data + ETH_HLEN) & 0xf;
+		msg_len  = *(u16 *)(skb->data + ETH_HLEN + 2);
+
+		skb_put(skb, ntohs(msg_len) + ETH_HLEN);
+
+		bytes += skb->len;
+		packets++;
+
+		skb->protocol = eth_type_trans(skb, ndev);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+		pr_debug("  -->RECV: protocol: %x message: %s frame_len: %d\n",
+			 skb->protocol, msg_type_string(msg_type & 0xf),
+			 skb->len);
+		/* timestamp only event messages */
+		if (!(msg_type & PTP_MSG_TYPE_MASK)) {
+			axienet_set_timestamp(lp, skb_hwtstamps(skb),
+					      (ptp_frame_base_addr +
+					      PTP_HW_TSTAMP_OFFSET));
+		}
+
+		netif_rx(skb);
+	}
+	ndev->stats.rx_packets += packets;
+	ndev->stats.rx_bytes += bytes;
+}
+
+/**
+ * axienet_ptp_rx_irq - PTP RX ISR handler
+ * @irq:		irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return:	IRQ_HANDLED for all cases.
+ */
+irqreturn_t axienet_ptp_rx_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	pr_debug("%s: received\n ", __func__);
+	lp->ptp_rx_hw_pointer = (axienet_ior(lp, PTP_RX_CONTROL_OFFSET)
+					& PTP_RX_PACKET_FIELD_MASK)  >> 8;
+
+	axienet_ptp_recv(ndev);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * axienet_tx_tstamp - timestamp skb on trasmit path
+ * @work:	Pointer to work_struct structure
+ *
+ * This adds TX timestamp to skb
+ */
+void axienet_tx_tstamp(struct work_struct *work)
+{
+	struct axienet_local *lp = container_of(work, struct axienet_local,
+			tx_tstamp_work);
+	struct net_device *ndev = lp->ndev;
+	struct skb_shared_hwtstamps hwtstamps;
+	struct sk_buff *skb;
+	unsigned long ts_reg_offset;
+	unsigned long flags;
+	u8 tx_packet;
+	u8 index;
+	u32 bytes = 0;
+	u32 packets = 0;
+
+	memset(&hwtstamps, 0, sizeof(struct skb_shared_hwtstamps));
+
+	spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+
+	tx_packet =  (axienet_ior(lp, PTP_TX_CONTROL_OFFSET) &
+				PTP_TX_PACKET_FIELD_MASK) >>
+				PTP_TX_PACKET_FIELD_SHIFT;
+
+	while ((skb = __skb_dequeue(&lp->ptp_txq)) != NULL) {
+		index = skb->cb[0];
+
+		/* dequeued packet yet to be xmited? */
+		if (index > tx_packet) {
+			/* enqueue it back and break */
+			skb_queue_tail(&lp->ptp_txq, skb);
+			break;
+		}
+		/* time stamp reg offset */
+		ts_reg_offset = PTP_TX_BUFFER_OFFSET(index) +
+					PTP_HW_TSTAMP_OFFSET;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) {
+			axienet_set_timestamp(lp, &hwtstamps, ts_reg_offset);
+			skb_tstamp_tx(skb, &hwtstamps);
+		}
+
+		bytes += skb->len;
+		packets++;
+		dev_kfree_skb_any(skb);
+	}
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += bytes;
+
+	spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+}
+
+/**
+ * axienet_ptp_tx_irq - PTP TX irq handler
+ * @irq:		irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return:	IRQ_HANDLED for all cases.
+ *
+ */
+irqreturn_t axienet_ptp_tx_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	pr_debug("%s: got tx interrupt\n", __func__);
+
+	/* read ctrl register to clear the interrupt */
+	axienet_ior(lp, PTP_TX_CONTROL_OFFSET);
+
+	schedule_work(&lp->tx_tstamp_work);
+	if (__netif_subqueue_stopped(ndev, lp->num_tc))
+		netif_wake_subqueue(ndev, lp->num_tc);
+
+	return IRQ_HANDLED;
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_qci.c b/drivers/staging/xilinx-tsn/xilinx_tsn_qci.c
new file mode 100644
index 000000000..3262330f2
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_qci.c
@@ -0,0 +1,154 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QCI Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_tsn_switch.h"
+
+#define SMC_MODE_SHIFT				28
+#define	SMC_CBR_MASK				0x00FFFFFF
+#define	SMC_EBR_MASK				0x00FFFFFF
+#define IN_PORTID_MASK				0x3
+#define IN_PORT_SHIFT				14
+#define MAX_FR_SIZE_MASK			0x00000FFF
+
+#define GATE_ID_SHIFT				24
+#define METER_ID_SHIFT				8
+#define EN_METER_SHIFT				6
+#define ALLOW_STREM_SHIFT			5
+#define EN_PSFP_SHIFT				4
+#define WR_OP_TYPE_MASK				0x3
+#define WR_OP_TYPE_SHIFT			2
+#define OP_TYPE_SHIFT				1
+#define PSFP_EN_CONTROL_MASK			0x1
+
+/**
+ * psfp_control - Configure thr control for PSFP
+ * @data:	Value to be programmed
+ */
+void psfp_control(struct psfp_config data)
+{
+	u32 mask;
+	u32 timeout = 20000;
+
+	mask = data.gate_id << GATE_ID_SHIFT;
+	mask |= data.meter_id << METER_ID_SHIFT;
+	mask |= data.en_meter << EN_METER_SHIFT;
+	mask |= data.allow_stream << ALLOW_STREM_SHIFT;
+	mask |= data.en_psfp << EN_PSFP_SHIFT;
+	mask |= (data.wr_op_type & WR_OP_TYPE_MASK) << WR_OP_TYPE_SHIFT;
+	mask |= data.op_type << OP_TYPE_SHIFT;
+	mask |= PSFP_EN_CONTROL_MASK;
+
+	axienet_iow(&lp, PSFP_CONTROL_OFFSET, mask);
+
+	/* wait for write to complete */
+	while ((axienet_ior(&lp, PSFP_CONTROL_OFFSET) &
+		PSFP_EN_CONTROL_MASK) && timeout)
+		timeout--;
+
+	if (!timeout)
+		pr_warn("PSFP control write took longer time!!");
+}
+
+/**
+ * get_stream_filter_config -  Get Stream Filter Configuration
+ * @data:	Value returned
+ */
+void get_stream_filter_config(struct stream_filter *data)
+{
+	u32 reg_val;
+
+	reg_val = axienet_ior(&lp, STREAM_FILTER_CONFIG_OFFSET);
+
+	data->max_fr_size = reg_val & MAX_FR_SIZE_MASK;
+	data->in_pid = (reg_val >> IN_PORT_SHIFT) & IN_PORTID_MASK;
+}
+
+/**
+ * config_stream_filter -  Configure Stream Filter Configuration
+ * @data:	Value to be programmed
+ */
+void config_stream_filter(struct stream_filter data)
+{
+	u32 mask;
+
+	mask = ((data.in_pid & IN_PORTID_MASK) << IN_PORT_SHIFT) |
+					(data.max_fr_size & MAX_FR_SIZE_MASK);
+	axienet_iow(&lp, STREAM_FILTER_CONFIG_OFFSET, mask);
+}
+
+/**
+ * get_meter_reg -  Read Stream Meter Configuration registers value
+ * @data:	Value returned
+ */
+void get_meter_reg(struct meter_config *data)
+{
+	u32 conf_r4;
+
+	data->cir = axienet_ior(&lp, STREAM_METER_CIR_OFFSET);
+	data->eir = axienet_ior(&lp, STREAM_METER_EIR_OFFSET);
+	data->cbr = axienet_ior(&lp, STREAM_METER_CBR_OFFSET) & SMC_CBR_MASK;
+	conf_r4 = axienet_ior(&lp, STREAM_METER_EBR_OFFSET);
+
+	data->ebr = conf_r4 & SMC_EBR_MASK;
+	data->mode = (conf_r4 & 0xF0000000) >> SMC_MODE_SHIFT;
+}
+
+/**
+ * program_meter_reg -  configure Stream Meter Configuration registers
+ * @data:	Value to be programmed
+ */
+void program_meter_reg(struct meter_config data)
+{
+	u32 conf_r4;
+
+	axienet_iow(&lp, STREAM_METER_CIR_OFFSET, data.cir);
+	axienet_iow(&lp, STREAM_METER_EIR_OFFSET, data.eir);
+	/* TODO: Check if this barrier is necessary */
+	wmb();
+	axienet_iow(&lp, STREAM_METER_CBR_OFFSET, data.cbr & SMC_CBR_MASK);
+
+	conf_r4 = (data.ebr & SMC_EBR_MASK) | (data.mode << SMC_MODE_SHIFT);
+	axienet_iow(&lp, STREAM_METER_EBR_OFFSET, conf_r4);
+}
+
+/**
+ * get_psfp_static_counter -  get memory static counters value
+ * @data  :	return value, containing counter value
+ */
+void get_psfp_static_counter(struct psfp_static_counter *data)
+{
+	int offset = (data->num) * 8;
+
+	data->psfp_fr_count.lsb = axienet_ior(&lp, TOTAL_PSFP_FRAMES_OFFSET +
+									offset);
+	data->psfp_fr_count.msb = axienet_ior(&lp, TOTAL_PSFP_FRAMES_OFFSET  +
+								offset + 0x4);
+
+	data->err_filter_ins_port.lsb = axienet_ior(&lp,
+						    FLTR_INGS_PORT_ERR_OFFSET + offset);
+	data->err_filter_ins_port.msb = axienet_ior(&lp,
+						    FLTR_INGS_PORT_ERR_OFFSET + offset + 0x4);
+
+	data->err_filtr_sdu.lsb = axienet_ior(&lp, FLTR_STDU_ERR_OFFSET +
+									offset);
+	data->err_filtr_sdu.msb = axienet_ior(&lp, FLTR_STDU_ERR_OFFSET +
+								offset + 0x4);
+
+	data->err_meter.lsb = axienet_ior(&lp, METER_ERR_OFFSET + offset);
+	data->err_meter.msb = axienet_ior(&lp, METER_ERR_OFFSET + offset + 0x4);
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.c b/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.c
new file mode 100644
index 000000000..96e9d3e51
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.c
@@ -0,0 +1,459 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QBV sheduler module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_shaper.h"
+#include <net/pkt_sched.h>
+
+/* Total number of TAS GCL entries */
+#define XLNX_TAPRIO_NUM_GCL			256
+
+/* Maximum supported cycle time in nanoseconds */
+#define XLNX_TAPRIO_MAX_CYCLE_TIME_NS		(BIT(30) - 1)
+
+static inline int axienet_map_gs_to_hw(struct axienet_local *lp, u32 gs)
+{
+	u8 be_queue = 0;
+	u8 re_queue = 1;
+	u8 st_queue = 2;
+	unsigned int acl_bit_map = 0;
+
+	if (lp->num_tc == XAE_MIN_LEGACY_TSN_TC)
+		st_queue = 1;
+
+	if (gs & GS_BE_OPEN)
+		acl_bit_map |= (1 << be_queue);
+	if (gs & GS_ST_OPEN)
+		acl_bit_map |= (1 << st_queue);
+	if (lp->num_tc == XAE_MAX_LEGACY_TSN_TC && (gs & GS_RE_OPEN))
+		acl_bit_map |= (1 << re_queue);
+
+	return acl_bit_map;
+}
+
+static int validate_taprio_qopt(struct net_device *ndev,
+				struct tc_taprio_qopt_offload *qopt)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 i = 0, max_tc = 0;
+	u64 total_time = 0;
+
+	if (qopt->cycle_time_extension)
+		return -EOPNOTSUPP;
+
+	if (qopt->num_entries > XLNX_TAPRIO_NUM_GCL)
+		return -EOPNOTSUPP;
+
+	if (!qopt->cycle_time || qopt->cycle_time > XLNX_TAPRIO_MAX_CYCLE_TIME_NS)
+		return -ERANGE;
+
+	for (i = 0; i < qopt->num_entries; ++i) {
+		struct tc_taprio_sched_entry *entry = &qopt->entries[i];
+
+		if (entry->interval > XLNX_TAPRIO_MAX_CYCLE_TIME_NS)
+			return -EOPNOTSUPP;
+
+		max_tc = fls(entry->gate_mask);
+		if (max_tc > lp->num_tc) {
+			netdev_err(ndev, "Invalid gate_mask 0x%x at off %d\n",
+				   entry->gate_mask, i);
+			return -EINVAL;
+		}
+
+		if (entry->command != TC_TAPRIO_CMD_SET_GATES)
+			return -EINVAL;
+
+		total_time += entry->interval;
+	}
+
+	if (total_time > XLNX_TAPRIO_MAX_CYCLE_TIME_NS)
+		return -EINVAL;
+
+	/* The cycle time to be at least as big as sum of each interval of gcl */
+	if (qopt->cycle_time < total_time)
+		return -EINVAL;
+
+	if (qopt->base_time <= 0) {
+		netdev_err(ndev, "Invalid base_time: must be greater than 0, got %lld\n",
+			   qopt->base_time);
+		return -ERANGE;
+	}
+
+	return 0;
+}
+
+static int xlnx_disable_queues(struct net_device *ndev,
+			       struct tc_taprio_qopt_offload *offload)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_local *master_lp;
+	struct net_device *master;
+	int i, j, err;
+
+	master = lp->master ? lp->master : ndev;
+	master_lp = netdev_priv(master);
+
+	lp->qbv_enabled = 0;
+	for (i = 0; i < offload->num_entries; i++)
+		lp->qbv_enabled |= offload->entries[i].gate_mask;
+
+	for (i = 0; i < lp->num_tc; i++) {
+		if (master_lp->txqs[i].is_tadma)
+			continue;
+
+		if (lp->qbv_enabled & BIT(i))
+			continue;
+
+		if (!master_lp->txqs[i].disable_cnt) {
+			err = axienet_mcdma_disable_tx_q(master, i);
+			if (err)
+				goto q_disable_err;
+		}
+
+		master_lp->txqs[i].disable_cnt++;
+	}
+
+	return 0;
+
+q_disable_err:
+	for (j = 0; j < i; j++) {
+		if (lp->qbv_enabled & BIT(j))
+			continue;
+
+		master_lp->txqs[j].disable_cnt--;
+		if (!master_lp->txqs[j].disable_cnt)
+			axienet_mcdma_enable_tx_q(master, i);
+	}
+
+	return err;
+}
+
+static int xlnx_taprio_replace(struct net_device *ndev,
+			       struct tc_taprio_qopt_offload *offload)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	unsigned int u_config_change = 0;
+	struct timespec64 ts;
+	u16 i, err = 0;
+
+	err = validate_taprio_qopt(ndev, offload);
+	if (err)
+		return err;
+
+	err = xlnx_disable_queues(ndev, offload);
+	if (err) {
+		dev_err(&ndev->dev, "Failed to disable unused queues\n");
+		return err;
+	}
+
+	/* write admin cycle time */
+	axienet_qbv_iow(lp, ADMIN_CYCLE_TIME_DENOMINATOR,
+			offload->cycle_time & CYCLE_TIME_DENOMINATOR_MASK);
+
+	/* write admin base time */
+	ts = ktime_to_timespec64(offload->base_time);
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SEC, lower_32_bits(ts.tv_sec));
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SECS, upper_32_bits(ts.tv_sec));
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_NS, ts.tv_nsec);
+
+	u_config_change = axienet_qbv_ior(lp, CONFIG_CHANGE);
+
+	u_config_change &= ~(CC_ADMIN_CTRL_LIST_LENGTH_MASK <<
+			     CC_ADMIN_CTRL_LIST_LENGTH_SHIFT);
+	u_config_change |= (offload->num_entries & CC_ADMIN_CTRL_LIST_LENGTH_MASK)
+			   << CC_ADMIN_CTRL_LIST_LENGTH_SHIFT;
+
+	/* program each list */
+	for (i = 0; i < offload->num_entries; i++) {
+		axienet_qbv_iow(lp,  ADMIN_CTRL_LIST(i),
+				(offload->entries[i].gate_mask &
+				ACL_GATE_STATE_MASK) << ACL_GATE_STATE_SHIFT);
+
+		/* set the time for each entry */
+		axienet_qbv_iow(lp, ADMIN_CTRL_LIST_TIME(i),
+				(offload->entries[i].interval / 8) &
+				CTRL_LIST_TIME_INTERVAL_MASK);
+	}
+
+	/* clear interrupt status */
+	axienet_qbv_iow(lp, INT_STATUS, 0);
+
+	/* kick in new config change */
+	u_config_change |= CC_ADMIN_CONFIG_CHANGE_BIT;
+
+	/* enable gate */
+	u_config_change |= CC_ADMIN_GATE_ENABLE_BIT;
+
+	/* start */
+	axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+
+	return 0;
+}
+
+static void xlnx_enable_queues(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_local *master_lp;
+	struct net_device *master;
+	int i;
+
+	master = lp->master ? lp->master : ndev;
+	master_lp = netdev_priv(master);
+
+	for (i = 0; i < lp->num_tc; i++) {
+		if (master_lp->txqs[i].is_tadma)
+			continue;
+
+		if (lp->qbv_enabled & BIT(i))
+			continue;
+
+		master_lp->txqs[i].disable_cnt--;
+		if (!master_lp->txqs[i].disable_cnt)
+			axienet_mcdma_enable_tx_q(master, i);
+	}
+
+	lp->qbv_enabled = 0;
+}
+
+static void xlnx_taprio_destroy(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 u_config_change = 0;
+
+	u_config_change &= ~CC_ADMIN_GATE_ENABLE_BIT;
+	/* open all the gates */
+	u_config_change |= CC_ADMIN_GATE_STATE_MASK;
+	axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+	xlnx_enable_queues(ndev);
+}
+
+static int tsn_setup_shaper_tc_taprio(struct net_device *ndev, void *type_data)
+{
+	struct tc_taprio_qopt_offload *offload = type_data;
+	int ret = 0;
+
+	switch (offload->cmd) {
+	case TAPRIO_CMD_REPLACE:
+		ret = xlnx_taprio_replace(ndev, offload);
+		break;
+	case TAPRIO_CMD_DESTROY:
+		xlnx_taprio_destroy(ndev);
+		break;
+	default:
+		ret = -EOPNOTSUPP;
+	}
+
+	return ret;
+}
+
+int axienet_tsn_shaper_tc(struct net_device *dev, enum tc_setup_type type, void *type_data)
+{
+	switch (type) {
+	case TC_SETUP_QDISC_TAPRIO:
+		return tsn_setup_shaper_tc_taprio(dev, type_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int __axienet_set_schedule(struct net_device *ndev, struct qbv_info *qbv)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u16 i;
+	unsigned int acl_bit_map = 0;
+	u32 u_config_change = 0;
+
+	if (qbv->cycle_time == 0) {
+		/* clear the gate enable bit */
+		u_config_change &= ~CC_ADMIN_GATE_ENABLE_BIT;
+		/* open all the gates */
+		u_config_change |= CC_ADMIN_GATE_STATE_MASK;
+
+		axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+
+		return 0;
+	}
+
+	if (axienet_qbv_ior(lp, PORT_STATUS) & 1) {
+		if (qbv->force) {
+			u_config_change &= ~CC_ADMIN_GATE_ENABLE_BIT;
+			axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+		} else {
+			return -EALREADY;
+		}
+	}
+	/* write admin time */
+	axienet_qbv_iow(lp, ADMIN_CYCLE_TIME_DENOMINATOR,
+			qbv->cycle_time & CYCLE_TIME_DENOMINATOR_MASK);
+
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_NS, qbv->ptp_time_ns);
+
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SEC,
+			qbv->ptp_time_sec & 0xFFFFFFFF);
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SECS,
+			(qbv->ptp_time_sec >> 32) & BASE_TIME_SECS_MASK);
+
+	u_config_change = axienet_qbv_ior(lp, CONFIG_CHANGE);
+
+	u_config_change &= ~(CC_ADMIN_CTRL_LIST_LENGTH_MASK <<
+				CC_ADMIN_CTRL_LIST_LENGTH_SHIFT);
+	u_config_change |= (qbv->list_length & CC_ADMIN_CTRL_LIST_LENGTH_MASK)
+					<< CC_ADMIN_CTRL_LIST_LENGTH_SHIFT;
+
+	/* program each list */
+	for (i = 0; i < qbv->list_length; i++) {
+		acl_bit_map = axienet_map_gs_to_hw(lp, qbv->acl_gate_state[i]);
+		axienet_qbv_iow(lp,  ADMIN_CTRL_LIST(i),
+				(acl_bit_map & (ACL_GATE_STATE_MASK)) <<
+				ACL_GATE_STATE_SHIFT);
+
+	    /* set the time for each entry */
+	    axienet_qbv_iow(lp, ADMIN_CTRL_LIST_TIME(i),
+			    qbv->acl_gate_time[i] &
+			    CTRL_LIST_TIME_INTERVAL_MASK);
+	}
+
+	/* clear interrupt status */
+	axienet_qbv_iow(lp, INT_STATUS, 0);
+
+	/* kick in new config change */
+	u_config_change |= CC_ADMIN_CONFIG_CHANGE_BIT;
+
+	/* enable gate */
+	u_config_change |= CC_ADMIN_GATE_ENABLE_BIT;
+
+	/* start */
+	axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+
+	return 0;
+}
+
+int axienet_set_schedule(struct net_device *ndev, void __user *useraddr)
+{
+	struct qbv_info *config;
+	int ret;
+
+	config = kmalloc(sizeof(*config), GFP_KERNEL);
+	if (!config)
+		return -ENOMEM;
+
+	if (copy_from_user(config, useraddr, sizeof(struct qbv_info))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	pr_debug("setting new schedule\n");
+
+	ret = __axienet_set_schedule(ndev, config);
+out:
+	kfree(config);
+	return ret;
+}
+
+static int __axienet_get_schedule(struct net_device *ndev, struct qbv_info *qbv)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u16 i = 0;
+	u32 u_value = 0;
+
+	if (!(axienet_qbv_ior(lp, CONFIG_CHANGE) &
+			CC_ADMIN_GATE_ENABLE_BIT)) {
+		qbv->cycle_time = 0;
+		return 0;
+	}
+
+	u_value = axienet_qbv_ior(lp, GATE_STATE);
+	qbv->list_length = (u_value >> CC_ADMIN_CTRL_LIST_LENGTH_SHIFT) &
+				CC_ADMIN_CTRL_LIST_LENGTH_MASK;
+
+	u_value = axienet_qbv_ior(lp, OPER_CYCLE_TIME_DENOMINATOR);
+	qbv->cycle_time = u_value & CYCLE_TIME_DENOMINATOR_MASK;
+
+	u_value = axienet_qbv_ior(lp, OPER_BASE_TIME_NS);
+	qbv->ptp_time_ns = u_value & OPER_BASE_TIME_NS_MASK;
+
+	qbv->ptp_time_sec = axienet_qbv_ior(lp, OPER_BASE_TIME_SEC);
+	u_value = axienet_qbv_ior(lp, OPER_BASE_TIME_SECS);
+	qbv->ptp_time_sec |= (u64)(u_value & BASE_TIME_SECS_MASK) << 32;
+
+	for (i = 0; i < qbv->list_length; i++) {
+		u_value = axienet_qbv_ior(lp, OPER_CTRL_LIST(i));
+		qbv->acl_gate_state[i] = (u_value >> ACL_GATE_STATE_SHIFT) &
+					ACL_GATE_STATE_MASK;
+		/**
+		 * In 2Q system, the actual ST Gate state value is 2,
+		 * for user the ST Gate state value is always 4.
+		 */
+		if (lp->num_tc == 2 && qbv->acl_gate_state[i] == 2)
+			qbv->acl_gate_state[i] = 4;
+
+		u_value = axienet_qbv_ior(lp, OPER_CTRL_LIST_TIME(i));
+		qbv->acl_gate_time[i] = u_value & CTRL_LIST_TIME_INTERVAL_MASK;
+	}
+	return 0;
+}
+
+int axienet_get_schedule(struct net_device *ndev, void __user *useraddr)
+{
+	struct qbv_info *qbv;
+	int ret = 0;
+
+	qbv = kmalloc(sizeof(*qbv), GFP_KERNEL);
+	if (!qbv)
+		return -ENOMEM;
+
+	if (copy_from_user(qbv, useraddr, sizeof(struct qbv_info))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	__axienet_get_schedule(ndev, qbv);
+
+	if (copy_to_user(useraddr, qbv, sizeof(struct qbv_info)))
+		ret = -EFAULT;
+out:
+	kfree(qbv);
+	return ret;
+}
+
+static irqreturn_t axienet_qbv_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	/* clear status */
+	axienet_qbv_iow(lp, INT_CLEAR, 0);
+
+	return IRQ_HANDLED;
+}
+
+int axienet_qbv_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int rc = 0;
+	static char irq_name[24];
+
+	if (lp->qbv_irq > 0) {
+		sprintf(irq_name, "%s_qbv", ndev->name);
+		rc = devm_request_irq(lp->dev, lp->qbv_irq, axienet_qbv_irq,
+				      0, irq_name, ndev);
+		if (rc)
+			dev_err(&ndev->dev, "Failed to request qbv_irq: %d\n", rc);
+	}
+	return rc;
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.h b/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.h
new file mode 100644
index 000000000..7afe470fb
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.h
@@ -0,0 +1,138 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN QBV scheduler header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_SHAPER_H
+#define XILINX_TSN_SHAPER_H
+
+/* 0x0		CONFIG_CHANGE
+ * 0x8		GATE_STATE
+ * 0x10		ADMIN_CTRL_LIST_LENGTH
+ * 0x18		ADMIN_CYCLE_TIME_DENOMINATOR
+ * 0x20         ADMIN_BASE_TIME_NS
+ * 0x24		ADMIN_BASE_TIME_SEC
+ * 0x28		ADMIN_BASE_TIME_SECS
+ * 0x30		INT_STAT
+ * 0x34		INT_EN
+ * 0x38		INT_CLR
+ * 0x3c		STATUS
+ * 0x40		CONFIG_CHANGE_TIME_NS
+ * 0x44		CONFIG_CHANGE_TIME_SEC
+ * 0x48		CONFIG_CHANGE_TIME_SECS
+ * 0x50		OPER_CTRL_LIST_LENGTH
+ * 0x58		OPER_CYCLE_TIME_DENOMINATOR
+ * 0x60		OPER_BASE_TIME_NS
+ * 0x64		OPER_BASE_TIME_SEC
+ * 0x68		OPER_BASE_TIME_SECS
+ * 0x6c		BE_XMIT_OVRRUN_CNT
+ * 0x74		RES_XMIT_OVRRUN_CNT
+ * 0x7c		ST_XMIT_OVRRUN_CNT
+ */
+
+#define CTRL_LIST_BASE			0x1000
+
+/* control list entries
+ * admin control list 0 : 31
+ * "Time interval between two gate entries" must be greater than
+ * "time required to transmit biggest supported frame" on that queue when
+ * the gate for the queue is going from open to close state.
+ */
+#define ADMIN_CTRL_LIST(n)		(CTRL_LIST_BASE + ((n) * 8))
+#define ACL_GATE_STATE_SHIFT		8
+#define ACL_GATE_STATE_MASK		GENMASK(7, 0)
+#define ADMIN_CTRL_LIST_TIME(n)		(ADMIN_CTRL_LIST(n) + 4)
+
+#define OPER_CTRL_LIST(n)		(CTRL_LIST_BASE + 0x800 + ((n) * 8))
+#define OPER_CTRL_LIST_TIME(n)		(OPER_CTRL_LIST(n) + 4)
+#define CTRL_LIST_TIME_INTERVAL_MASK	0xFFFFF
+
+#define CONFIG_CHANGE			0x0
+#define CC_ADMIN_GATE_STATE_MASK	GENMASK(7, 0)
+#define CC_ADMIN_CTRL_LIST_LENGTH_SHIFT	(8)
+#define CC_ADMIN_CTRL_LIST_LENGTH_MASK	(0x1FF)
+/* This request bit is set when all the related Admin* filelds are populated.
+ * This bit is set by S/W and clear by core when core start with new schedule.
+ * Once set it can only be cleared by core or hard/soft reset.
+ */
+#define CC_ADMIN_CONFIG_CHANGE_BIT	BIT(30)
+#define CC_ADMIN_GATE_ENABLE_BIT	BIT(31)
+
+#define GATE_STATE			0x8
+#define GS_OPER_GATE_STATE_SHIFT	(0)
+#define GS_OPER_GATE_STATE_MASK		(0x7)
+#define GS_OPER_CTRL_LIST_LENGTH_SHIFT	(8)
+#define GS_OPER_CTRL_LIST_LENGTH_MASK	(0x3F)
+#define GS_SUP_MAX_LIST_LENGTH_SHIFT	(16)
+#define GS_SUP_MAX_LIST_LENGTH_MASK	(0x3F)
+#define GS_TICK_GRANULARITY_SHIFT	(24)
+#define GS_TICK_GRANULARITY_MASK	(0x3F)
+
+#define ADMIN_CYCLE_TIME_DENOMINATOR	0x18
+#define ADMIN_BASE_TIME_NS		0x20
+#define ADMIN_BASE_TIME_SEC		0x24
+#define ADMIN_BASE_TIME_SECS		0x28
+
+#define INT_STATUS			0x30
+#define INT_ENABLE			0x34
+#define INT_CLEAR			0x38
+#define PORT_STATUS			0x3c
+#define CONFIG_PENDING_MASK		BIT(0)
+
+/* Config Change time is valid after Config Pending bit is set. */
+#define CONFIG_CHANGE_TIME_NS		0x40
+#define CONFIG_CHANGE_TIME_SEC		0x44
+#define CONFIG_CHANGE_TIME_SECS		0x48
+
+#define OPER_CONTROL_LIST_LENGTH	0x50
+#define OPER_CYCLE_TIME_DENOMINATOR	0x58
+#define CYCLE_TIME_DENOMINATOR_MASK	(0x3FFFFFFF)
+
+#define OPER_BASE_TIME_NS		0x60
+#define OPER_BASE_TIME_NS_MASK		(0x3FFFFFFF)
+#define OPER_BASE_TIME_SEC		0x64
+#define OPER_BASE_TIME_SECS		0x68
+#define BASE_TIME_SECS_MASK		(0xFFFF)
+
+#define BE_XMIT_OVERRUN_COUNT		0x6c
+#define RES_XMIT_OVERRUN_COUNT		0x74
+#define ST_XMIT_OVERRUN_COUNT		0x7c
+
+/* internally hw deals with queues only,
+ * in 3q system ST acl bitmap would be would 1 << 2
+ * in 2q system ST acl bitmap would be 1 << 1
+ * But this is confusing to users.
+ * so use the following fixed gate state and internally
+ * map them to hw
+ */
+#define GS_BE_OPEN			BIT(0)
+#define GS_RE_OPEN			BIT(1)
+#define GS_ST_OPEN			BIT(2)
+#define GS_ST_2TC_OPEN			BIT(1)
+#define QBV_MAX_ENTRIES			256
+
+struct qbv_info {
+	u8 port;
+	u8 force;
+	u32 cycle_time;
+	u64 ptp_time_sec;
+	u32 ptp_time_ns;
+	u32 list_length;
+	u32 acl_gate_state[QBV_MAX_ENTRIES];
+	u32 acl_gate_time[QBV_MAX_ENTRIES];
+};
+
+#endif /* XILINX_TSN_SHAPER_H */
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_switch.c b/drivers/staging/xilinx-tsn/xilinx_tsn_switch.c
new file mode 100644
index 000000000..aad652fe1
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_switch.c
@@ -0,0 +1,1940 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN Switch Controller driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/of_net.h>
+#include <linux/platform_device.h>
+#include "xilinx_tsn_switch.h"
+
+static u8 fp_map[XAE_MAX_TSN_TC];
+static uint fp_map_count = XAE_MAX_TSN_TC;
+module_param_array(fp_map, byte, &fp_map_count, 0644);
+MODULE_PARM_DESC(fp_map, "Array of queues mapped to EMAC/PMAC");
+
+static struct miscdevice switch_dev;
+static struct device_node *ep_node;
+struct axienet_local lp;
+static struct axienet_local *ep_lp;
+static u8 en_hw_addr_learning;
+static u8 sw_mac_addr[ETH_ALEN];
+
+#define DELAY_OF_FIVE_MILLISEC			(5 * DELAY_OF_ONE_MILLISEC)
+
+#define ADD					1
+#define DELETE					0
+
+#define PMAP_EGRESS_QUEUE_MASK			0x7
+#define PMAP_EGRESS_QUEUE0_SELECT		0x0
+#define PMAP_EGRESS_QUEUE1_SELECT		0x1
+#define PMAP_EGRESS_QUEUE2_SELECT		0x2
+#define SDL_EN_CAM_IPV_SHIFT			28
+#define SDL_CAM_IPV_SHIFT			29
+
+#define SDL_CAM_WR_ENABLE			BIT(0)
+#define SDL_CAM_ADD_ENTRY			0x3
+#define SDL_CAM_DELETE_ENTRY			0x5
+#define SDL_CAM_READ_KEY_ENTRY			0x1
+#define SDL_CAM_READ_ENTRY			GENMASK(2, 0)
+#define SDL_CAM_VLAN_SHIFT			16
+#define SDL_CAM_VLAN_MASK			0xFFF
+#define SDL_CAM_IPV_MASK			0x7
+#define SDL_CAM_PORT_LIST_SHIFT			8
+#define SDL_GATEID_SHIFT			16
+#define SDL_CAM_EP_MGMTQ_EN			BIT(15)
+#define SDL_CAM_FWD_TO_EP			BIT(0)
+#define SDL_CAM_FWD_TO_PORT_1			BIT(1)
+#define SDL_CAM_FWD_TO_PORT_2			BIT(2)
+#define SDL_CAM_EP_ACTION_LIST_SHIFT		0
+#define SDL_CAM_MAC_ACTION_LIST_SHIFT		4
+#define SDL_CAM_DEST_MAC_XLATION		BIT(0)
+#define SDL_CAM_VLAN_ID_XLATION			BIT(1)
+#define SDL_CAM_UNTAG_FRAME			BIT(2)
+#define SDL_CAM_TAG_FRAME			BIT(3)
+
+#define PORT_MAC_ADDR_LSB_MASK			(0xF)
+#define MAC2_PORT_MAC_ADDR_LSB_SHIFT		(20)
+#define PORT_STATUS_MASK                       (0x7)
+#define MAC2_PORT_STATUS_SHIFT                 (17)
+#define MAC2_PORT_STATUS_CHG_BIT               BIT(16)
+#define MAC1_PORT_MAC_ADDR_LSB_SHIFT		(12)
+#define MAC1_PORT_STATUS_SHIFT                 (9)
+#define MAC1_PORT_STATUS_CHG_BIT               BIT(8)
+#define EP_PORT_STATUS_SHIFT                   (1)
+#define EP_PORT_STATUS_CHG_BIT                 BIT(0)
+#define EP_PORT_STATUS_EP_STATE_SHIFT		1
+#define EP_PORT_STATUS_EP_MAC_ADDR_SHIFT	4
+#define EX_EP_PORT_STATUS_SHIFT			(25)
+#define EX_EP_PORT_STATUS_CHG_BIT		BIT(24)
+#define EP_EX_CTRL_REG_EP_MAC_ADDR_SHIFT	24
+
+#define SDL_CAM_LEARNT_ENT_MAC2_SHIFT		(20)
+#define SDL_CAM_LEARNT_ENT_MAC1_SHIFT		(8)
+#define SDL_CAM_LEARNT_ENT_MASK			GENMASK(11, 0)
+#define SDL_CAM_FOUND_BIT			BIT(7)
+#define SDL_CAM_READ_KEY_ADDR_SHIFT		(8)
+
+#define HW_ADDR_AGING_TIME_SHIFT		(8)
+#define HW_ADDR_AGING_TIME_MASK			GENMASK(19, 0)
+#define HW_ADDR_AGING_BIT			BIT(2)
+#define HW_ADDR_LEARN_UNTAG_BIT			BIT(1)
+#define HW_ADDR_LEARN_BIT			BIT(0)
+
+#define PORT_VLAN_ID_SHIFT			(16)
+#define PORT_VLAN_ID_MASK			(0xFFF)
+#define PORT_VLAN_IPV_SHIFT			(13)
+#define PORT_VLAN_EN_IPV_SHIFT			(12)
+#define PORT_VLAN_WRITE				(0x3)
+#define PORT_VLAN_READ				(0x1)
+#define PORT_VLAN_WRITE_READ_EN_BIT		BIT(0)
+#define PORT_VLAN_PORT_LIST_VALID_BIT		BIT(3)
+#define PORT_VLAN_HW_ADDR_LEARN_BIT		BIT(9)
+#define PORT_VLAN_HW_ADDR_AGING_BIT		BIT(11)
+#define PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT	(12)
+
+#define NATIVE_MAC1_PCP_SHIFT			(13)
+#define NATIVE_MAC2_VLAN_SHIFT			(16)
+#define NATIVE_MAC2_PCP_SHIFT			(29)
+
+#define DEFAULT_PVID		1
+#define DEFAULT_FWD_ALL		GENMASK(2, 0)
+#define FWD_TO_EP		0x1
+#define FWD_TO_MAC1		0x2
+#define FWD_TO_MAC2		0x4
+
+/* Match table for of_platform binding */
+static const struct of_device_id tsnswitch_of_match[] = {
+	{ .compatible = "xlnx,tsn-switch", },
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsnswitch_of_match);
+
+static void tsn_switch_set_fp_map(struct platform_device *pdev, u16 num_tc)
+{
+	u32 fp_value = 0;
+	int i;
+
+	if (num_tc <= XAE_MAX_LEGACY_TSN_TC)
+		return;
+
+	for (i = 0; i < fp_map_count; i++) {
+		if (fp_map[i])
+			fp_value |= BIT(i);
+	}
+
+	axienet_iow(&lp, XAS_PREEMPTION_QUEUE_MAP_OFFSET, fp_value);
+	dev_info(&pdev->dev, "Preemption queue map 0x%x\n", fp_value);
+}
+
+static int switch_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int switch_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+/* set_frame_filter_option Frame Filtering Type Field Options */
+static void set_frame_filter_opt(u16 type1, u16 type2)
+{
+	int type = axienet_ior(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET);
+
+	if (type1)
+		type = (type & 0x0000FFFF) | (type1 << 16);
+	if (type2)
+		type = (type & 0xFFFF0000) | type2;
+	axienet_iow(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET, type);
+}
+
+/* MAC Port-1 Management Queueing Options */
+static void set_mac1_mngmntq(u32 config)
+{
+	axienet_iow(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET, config);
+}
+
+/* MAC Port-2 Management Queueing Options */
+static void set_mac2_mngmntq(u32 config)
+{
+	axienet_iow(&lp, XAS_MNG_Q_CTRL_OFFSET, config);
+}
+
+static int set_pmap_config(u8 *pcpmap)
+{
+	u32 reg, err;
+	u32 pmap = 0;
+	u8 i = 0;
+
+	if (!pcpmap)
+		return -EINVAL;
+
+	/* wait for switch init done */
+	err = readl_poll_timeout(lp.regs + XAS_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				  DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	for (i = 0; i < XAE_MAX_TSN_TC; i++) {
+		pmap = pmap | ((pcpmap[i] & PMAP_EGRESS_QUEUE_MASK) <<
+			       (4 * i));
+	}
+
+	axienet_iow(&lp, XAS_PMAP_OFFSET, pmap);
+
+	/* wait for cam init done */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				  DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+	return 0;
+}
+
+/**
+ * set_switch_regs -  read the various status of switch
+ * @data:	Pointer which will be writen to switch
+ */
+static void set_switch_regs(struct switch_data *data)
+{
+	int tmp;
+	u8 mac_addr[6];
+
+	axienet_iow(&lp, XAS_CONTROL_OFFSET, data->switch_ctrl);
+	axienet_iow(&lp, XAS_PMAP_OFFSET, data->switch_prt);
+	mac_addr[0] = data->sw_mac_addr[0];
+	mac_addr[1] = data->sw_mac_addr[1];
+	mac_addr[2] = data->sw_mac_addr[2];
+	mac_addr[3] = data->sw_mac_addr[3];
+	mac_addr[4] = data->sw_mac_addr[4];
+	mac_addr[5] = data->sw_mac_addr[5];
+	axienet_iow(&lp, XAS_MAC_LSB_OFFSET,
+		    (mac_addr[0] << 24) | (mac_addr[1] << 16) |
+		    (mac_addr[2] << 8)  | (mac_addr[3]));
+	axienet_iow(&lp, XAS_MAC_MSB_OFFSET, (mac_addr[4] << 8) | mac_addr[5]);
+
+	/* Threshold */
+	tmp = (data->thld_ep_mac[0].t1 << 16) | data->thld_ep_mac[0].t2;
+	axienet_iow(&lp, XAS_EP2MAC_PRI7_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_ep_mac[1].t1 << 16) | data->thld_ep_mac[1].t2;
+	axienet_iow(&lp, XAS_EP2MAC_PRI6_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_ep_mac[2].t1 << 16) | data->thld_ep_mac[2].t2;
+	axienet_iow(&lp, XAS_EP2MAC_PRI5_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[0].t1 << 16) | data->thld_mac_mac[0].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_PRI7_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[1].t1 << 16) | data->thld_mac_mac[1].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_PRI6_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[2].t1 << 16) | data->thld_mac_mac[2].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_PRI5_FIFOT_OFFSET, tmp);
+
+	/* Port VLAN ID */
+	axienet_iow(&lp, XAS_EP_PORT_VLAN_OFFSET, data->ep_vlan);
+	axienet_iow(&lp, XAS_MAC_PORT_VLAN_OFFSET, data->mac_vlan);
+
+	/* max frame size */
+	axienet_iow(&lp, XAS_PRI7_MAX_FRAME_SIZE_OFFSET, data->max_frame_sc_que);
+	axienet_iow(&lp, XAS_PRI6_MAX_FRAME_SIZE_OFFSET, data->max_frame_res_que);
+	axienet_iow(&lp, XAS_PRI5_MAX_FRAME_SIZE_OFFSET, data->max_frame_be_que);
+}
+
+/**
+ * get_switch_regs -  read the various status of switch
+ * @data:	Pointer which will return the switch status
+ */
+static void get_switch_regs(struct switch_data *data)
+{
+	int tmp;
+
+	data->switch_status = axienet_ior(&lp, XAS_STATUS_OFFSET);
+	data->switch_ctrl = axienet_ior(&lp, XAS_CONTROL_OFFSET);
+	data->switch_prt = axienet_ior(&lp, XAS_PMAP_OFFSET);
+	tmp = axienet_ior(&lp, XAS_MAC_LSB_OFFSET);
+	data->sw_mac_addr[0] = (tmp & 0xFF000000) >> 24;
+	data->sw_mac_addr[1] = (tmp & 0xFF0000) >> 16;
+	data->sw_mac_addr[2] = (tmp & 0xFF00) >> 8;
+	data->sw_mac_addr[3] = (tmp & 0xFF);
+	tmp = axienet_ior(&lp, XAS_MAC_MSB_OFFSET);
+	data->sw_mac_addr[4] = (tmp & 0xFF00) >> 8;
+	data->sw_mac_addr[5] = (tmp & 0xFF);
+
+	/* Threshold */
+	tmp = axienet_ior(&lp, XAS_EP2MAC_PRI7_FIFOT_OFFSET);
+	data->thld_ep_mac[0].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[0].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_EP2MAC_PRI6_FIFOT_OFFSET);
+	data->thld_ep_mac[1].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[1].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_EP2MAC_PRI5_FIFOT_OFFSET);
+	data->thld_ep_mac[2].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[2].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_PRI7_FIFOT_OFFSET);
+	data->thld_mac_mac[0].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[0].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_PRI6_FIFOT_OFFSET);
+	data->thld_mac_mac[1].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[1].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_PRI5_FIFOT_OFFSET);
+	data->thld_mac_mac[2].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[2].t2 = tmp & (0xFFFF);
+
+	/* Port VLAN ID */
+	data->ep_vlan = axienet_ior(&lp, XAS_EP_PORT_VLAN_OFFSET);
+	data->mac_vlan = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+
+	/* max frame size */
+	data->max_frame_sc_que = (axienet_ior(&lp,
+				XAS_PRI7_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+	data->max_frame_res_que = (axienet_ior(&lp,
+				XAS_PRI6_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+	data->max_frame_be_que = (axienet_ior(&lp,
+				XAS_PRI5_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+
+	/* frame filter type options*/
+	tmp = axienet_ior(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET);
+	data->typefield.type2 = (tmp & 0xFFFF0000) >> 16;
+	data->typefield.type2 = tmp & 0x0000FFFF;
+
+	/* MAC Port 1 Management Q option*/
+	data->mac1_config = axienet_ior(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET);
+	/* MAC Port 2 Management Q option*/
+	data->mac2_config = axienet_ior(&lp, XAS_MNG_Q_CTRL_OFFSET);
+
+	/* Port VLAN Membership control*/
+	data->port_vlan_mem_ctrl = axienet_ior(&lp, XAS_VLAN_MEMB_CTRL_REG);
+	/* Port VLAN Membership read data*/
+	data->port_vlan_mem_data = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+}
+
+int tsn_switch_get_port_parent_id(struct net_device *dev, struct netdev_phys_item_id *ppid)
+{
+	u8 *switchid;
+
+	switchid = tsn_switch_get_id();
+	ppid->id_len = ETH_ALEN;
+	memcpy(&ppid->id, switchid, ppid->id_len);
+
+	return 0;
+}
+
+/**
+ * get_memory_static_counter -  get memory static counters value
+ * @data:	Value to be programmed
+ */
+static void get_memory_static_counter(struct switch_data *data)
+{
+	data->mem_arr_cnt.cam_lookup.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_CAM_LOOKUP);
+	data->mem_arr_cnt.cam_lookup.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_CAM_LOOKUP + 0x4);
+
+	data->mem_arr_cnt.multicast_fr.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_MULTCAST);
+	data->mem_arr_cnt.multicast_fr.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_MULTCAST + 0x4);
+
+	data->mem_arr_cnt.err_mac1.lsb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC1);
+	data->mem_arr_cnt.err_mac1.msb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC1 + 0x4);
+
+	data->mem_arr_cnt.err_mac2.lsb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC2);
+	data->mem_arr_cnt.err_mac2.msb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC2 + 0x4);
+
+	data->mem_arr_cnt.sc_mac1_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC1_EP);
+	data->mem_arr_cnt.sc_mac1_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC1_EP + 0x4);
+	data->mem_arr_cnt.res_mac1_ep.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC1_EP);
+	data->mem_arr_cnt.res_mac1_ep.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC1_EP + 0x4);
+	data->mem_arr_cnt.be_mac1_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC1_EP);
+	data->mem_arr_cnt.be_mac1_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_sc_mac1_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC1_EP);
+	data->mem_arr_cnt.err_sc_mac1_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_res_mac1_ep.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC1_EP);
+	data->mem_arr_cnt.err_res_mac1_ep.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_be_mac1_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC1_EP);
+	data->mem_arr_cnt.err_be_mac1_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC1_EP + 0x4);
+
+	data->mem_arr_cnt.sc_mac2_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC2_EP);
+	data->mem_arr_cnt.sc_mac2_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC2_EP + 0x4);
+	data->mem_arr_cnt.res_mac2_ep.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC2_EP);
+	data->mem_arr_cnt.res_mac2_ep.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC2_EP + 0x4);
+	data->mem_arr_cnt.be_mac2_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC2_EP);
+	data->mem_arr_cnt.be_mac2_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_sc_mac2_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC2_EP);
+	data->mem_arr_cnt.err_sc_mac2_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_res_mac2_ep.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC2_EP);
+	data->mem_arr_cnt.err_res_mac2_ep.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_be_mac2_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC2_EP);
+	data->mem_arr_cnt.err_be_mac2_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC2_EP + 0x4);
+
+	data->mem_arr_cnt.sc_ep_mac1.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC1);
+	data->mem_arr_cnt.sc_ep_mac1.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.res_ep_mac1.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC1);
+	data->mem_arr_cnt.res_ep_mac1.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.be_ep_mac1.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC1);
+	data->mem_arr_cnt.be_ep_mac1.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_sc_ep_mac1.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC1);
+	data->mem_arr_cnt.err_sc_ep_mac1.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_res_ep_mac1.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC1);
+	data->mem_arr_cnt.err_res_ep_mac1.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_be_ep_mac1.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC1);
+	data->mem_arr_cnt.err_be_ep_mac1.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC1 + 0x4);
+
+	data->mem_arr_cnt.sc_mac2_mac1.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC2_MAC1);
+	data->mem_arr_cnt.sc_mac2_mac1.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.res_mac2_mac1.lsb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC2_MAC1);
+	data->mem_arr_cnt.res_mac2_mac1.msb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.be_mac2_mac1.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC2_MAC1);
+	data->mem_arr_cnt.be_mac2_mac1.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_sc_mac2_mac1.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1);
+	data->mem_arr_cnt.err_sc_mac2_mac1.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_res_mac2_mac1.lsb = axienet_ior(&lp,
+							      XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1);
+	data->mem_arr_cnt.err_res_mac2_mac1.msb =
+	axienet_ior(&lp, XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_be_mac2_mac1.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1);
+	data->mem_arr_cnt.err_be_mac2_mac1.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1 + 0x4);
+
+	data->mem_arr_cnt.sc_ep_mac2.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC2);
+	data->mem_arr_cnt.sc_ep_mac2.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.res_ep_mac2.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC2);
+	data->mem_arr_cnt.res_ep_mac2.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.be_ep_mac2.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC2);
+	data->mem_arr_cnt.be_ep_mac2.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_sc_ep_mac2.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC2);
+	data->mem_arr_cnt.err_sc_ep_mac2.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_res_ep_mac2.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC2);
+	data->mem_arr_cnt.err_res_ep_mac2.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_be_ep_mac2.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC2);
+	data->mem_arr_cnt.err_be_ep_mac2.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC2 + 0x4);
+
+	data->mem_arr_cnt.sc_mac1_mac2.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC1_MAC2);
+	data->mem_arr_cnt.sc_mac1_mac2.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.res_mac1_mac2.lsb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC1_MAC2);
+	data->mem_arr_cnt.res_mac1_mac2.msb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.be_mac1_mac2.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC1_MAC2);
+	data->mem_arr_cnt.be_mac1_mac2.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_sc_mac1_mac2.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2);
+	data->mem_arr_cnt.err_sc_mac1_mac2.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_res_mac1_mac2.lsb = axienet_ior(&lp,
+							      XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2);
+	data->mem_arr_cnt.err_res_mac1_mac2.msb =
+	axienet_ior(&lp, XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_be_mac1_mac2.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2);
+	data->mem_arr_cnt.err_be_mac1_mac2.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2 + 0x4);
+}
+
+int tsn_switch_cam_set(struct cam_struct data, u8 add)
+{
+	u32 port_action = 0;
+	u32 tv2 = 0;
+	u32 reg, err;
+	u8 en_ipv = 0;
+
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+	if (add && ((data.fwd_port & PORT_EX_ONLY) || (data.fwd_port & PORT_EX_EP))) {
+		if (!(ep_lp->ex_ep)) {
+			pr_err("Endpoint extension support is not present in this design\n");
+			return -EINVAL;
+		} else if ((data.fwd_port & PORT_EX_ONLY) &&
+			    (data.fwd_port & PORT_EX_EP)) {
+			if (!(ep_lp->packet_switch)) {
+				pr_err("Support for forwarding packets from endpoint to extended endpoint or vice versa is not present in this design\n");
+				return -EINVAL;
+			}
+		}
+	}
+	/* mac and vlan */
+	axienet_iow(&lp, XAS_SDL_CAM_KEY1_OFFSET,
+		    (data.dest_addr[0] << 24) | (data.dest_addr[1] << 16) |
+		    (data.dest_addr[2] << 8)  | (data.dest_addr[3]));
+	axienet_iow(&lp, XAS_SDL_CAM_KEY2_OFFSET,
+		    ((data.dest_addr[4] << 8) | data.dest_addr[5]) |
+		    ((data.vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT));
+
+	/* Introduce wmb to preserve KEY2 and TV1 write order fix possible
+	 * HW hang when KEY2 and TV1 registers are accessed sequentially.
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+	/* TV 1 and TV 2 */
+	axienet_iow(&lp, XAS_SDL_CAM_TV1_OFFSET,
+		    (data.src_addr[0] << 24) | (data.src_addr[1] << 16) |
+		    (data.src_addr[2] << 8)  | (data.src_addr[3]));
+
+	tv2 = ((data.src_addr[4] << 8) | data.src_addr[5]) |
+	       ((data.tv_vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT);
+
+	if (data.flags & XAS_CAM_IPV_EN)
+		en_ipv = 1;
+
+	tv2 = tv2 | ((data.ipv & SDL_CAM_IPV_MASK) << SDL_CAM_IPV_SHIFT)
+				| (en_ipv << SDL_EN_CAM_IPV_SHIFT);
+
+	axienet_iow(&lp, XAS_SDL_CAM_TV2_OFFSET, tv2);
+	/* Force complete write to translation value registers
+	 * before writing to port action register
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+
+	if (data.fwd_port & PORT_EP)
+		port_action = data.ep_port_act << SDL_CAM_EP_ACTION_LIST_SHIFT;
+	if (data.fwd_port & PORT_MAC1 || data.fwd_port & PORT_MAC2)
+		port_action |= data.mac_port_act <<
+				SDL_CAM_MAC_ACTION_LIST_SHIFT;
+
+	if (data.flags & XAS_CAM_EP_MGMTQ_EN)
+		port_action |= SDL_CAM_EP_MGMTQ_EN;
+
+	port_action = port_action | (data.fwd_port << SDL_CAM_PORT_LIST_SHIFT);
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI) || IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	port_action = port_action | (data.gate_id << SDL_GATEID_SHIFT);
+#endif
+
+	/* port action */
+	axienet_iow(&lp, XAS_SDL_CAM_PORT_ACT_OFFSET, port_action);
+	/* Force complete writes to port action register before initiating
+	 * CAM write by setting CAM ADD bit in control register since this value
+	 * needs to be written to the CAM.
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+
+	if (add)
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_ADD_ENTRY);
+	else
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_DELETE_ENTRY);
+
+	/* wait for write to complete */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_CTRL_OFFSET, reg,
+				 (!(reg & SDL_CAM_WR_ENABLE)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static void port_vlan_mem_ctrl(u32 port_vlan_mem)
+{
+		axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, port_vlan_mem);
+}
+
+static int read_cam_entry(struct cam_struct data, void __user *arg)
+{
+	u32 u_value, reg, err;
+
+	/* wait for cam init done */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	/* mac and vlan */
+	axienet_iow(&lp, XAS_SDL_CAM_KEY1_OFFSET,
+		    (data.dest_addr[0] << 24) | (data.dest_addr[1] << 16) |
+		    (data.dest_addr[2] << 8)  | (data.dest_addr[3]));
+	axienet_iow(&lp, XAS_SDL_CAM_KEY2_OFFSET,
+		    ((data.dest_addr[4] << 8) | data.dest_addr[5]) |
+		    ((data.vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT));
+	/* Finish writing vlan id and mac address before triggering a read
+	 * from the CAM since read depends on these parameters
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+	axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_READ_ENTRY);
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_CTRL_OFFSET, reg,
+				 (!(reg & SDL_CAM_WR_ENABLE)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value = axienet_ior(&lp, XAS_SDL_CAM_CTRL_OFFSET);
+
+	if (u_value & SDL_CAM_FOUND_BIT) {
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_TV1_OFFSET);
+		data.src_addr[0] = (u_value >> 24) & 0xFF;
+		data.src_addr[1] = (u_value >> 16) & 0xFF;
+		data.src_addr[2] = (u_value >> 8) & 0xFF;
+		data.src_addr[3] = (u_value) & 0xFF;
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_TV2_OFFSET);
+		data.src_addr[4] = (u_value >> 8) & 0xFF;
+		data.src_addr[5] = (u_value) & 0xFF;
+		data.tv_vlanid   = (u_value >> SDL_CAM_VLAN_SHIFT)
+					& SDL_CAM_VLAN_MASK;
+		data.ipv = (u_value >> SDL_CAM_IPV_SHIFT) & SDL_CAM_IPV_MASK;
+		if ((u_value >> SDL_EN_CAM_IPV_SHIFT) & 0x1)
+			data.flags |= XAS_CAM_IPV_EN;
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_PORT_ACT_OFFSET);
+		if (ep_lp->ex_ep)
+			data.fwd_port = (u_value >> SDL_CAM_PORT_LIST_SHIFT) & 0x1F;
+		else
+			data.fwd_port = (u_value >> SDL_CAM_PORT_LIST_SHIFT) & 0x7;
+		data.ep_port_act = (u_value >> SDL_CAM_EP_ACTION_LIST_SHIFT)
+					& 0xF;
+		data.mac_port_act = (u_value >> SDL_CAM_MAC_ACTION_LIST_SHIFT)
+					& 0xF;
+		data.gate_id = (u_value >> SDL_GATEID_SHIFT) & 0xFF;
+		data.flags |= XAS_CAM_VALID;
+	} else {
+		data.flags &= ~XAS_CAM_VALID;
+	}
+	if (copy_to_user(arg, &data, sizeof(struct cam_struct)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int set_mac_addr_learn(void __user *arg)
+{
+	struct mac_addr_learn mac_learn;
+	u32 u_value;
+
+	if (copy_from_user(&mac_learn, arg, sizeof(struct mac_addr_learn)))
+		return -EFAULT;
+
+	u_value = axienet_ior(&lp, XAS_HW_ADDR_LEARN_CTRL_OFFSET);
+	if (mac_learn.aging_time) {
+		u_value &= ~(HW_ADDR_AGING_TIME_MASK <<
+				HW_ADDR_AGING_TIME_SHIFT);
+		u_value |= (mac_learn.aging_time << HW_ADDR_AGING_TIME_SHIFT);
+	}
+	if (mac_learn.is_age) {
+		if (!mac_learn.aging)
+			u_value |= HW_ADDR_AGING_BIT;
+		else
+			u_value &= ~HW_ADDR_AGING_BIT;
+	}
+	if (mac_learn.is_learn) {
+		if (!mac_learn.learning)
+			u_value |= HW_ADDR_LEARN_BIT;
+		else
+			u_value &= ~HW_ADDR_LEARN_BIT;
+	}
+	if (mac_learn.is_untag) {
+		if (mac_learn.learn_untag)
+			u_value |= HW_ADDR_LEARN_UNTAG_BIT;
+		else
+			u_value &= ~HW_ADDR_LEARN_UNTAG_BIT;
+	}
+	axienet_iow(&lp, XAS_HW_ADDR_LEARN_CTRL_OFFSET, u_value);
+
+	return 0;
+}
+
+static int get_mac_addr_learn(void __user *arg)
+{
+	struct mac_addr_learn mac_learn;
+	u32 u_value;
+
+	u_value = axienet_ior(&lp, XAS_HW_ADDR_LEARN_CTRL_OFFSET);
+	mac_learn.aging_time = (u_value >> HW_ADDR_AGING_TIME_SHIFT) &
+				HW_ADDR_AGING_TIME_MASK;
+	mac_learn.aging = u_value & HW_ADDR_AGING_BIT;
+	mac_learn.learning = u_value & HW_ADDR_LEARN_BIT;
+	mac_learn.learn_untag = u_value & HW_ADDR_LEARN_UNTAG_BIT;
+
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+
+	if (copy_to_user(arg, &mac_learn, sizeof(struct mac_addr_learn)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int get_mac_addr_learnt_list(void __user *arg)
+{
+	struct mac_addr_list *mac_list;
+	u32 i = 0;
+	u32 u_value, reg, err;
+	u16 read_key_addr = 0;
+	int ret = 0;
+
+	mac_list = kzalloc(sizeof(*mac_list), GFP_KERNEL);
+	if (!mac_list) {
+		ret = -ENOMEM;
+		goto ret_status;
+	}
+
+	if (copy_from_user(mac_list, arg, sizeof(u8))) {
+		ret = -EFAULT;
+		goto free_mac_list;
+	}
+	/* wait for cam init done */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		ret = -ETIMEDOUT;
+		goto free_mac_list;
+	}
+	u_value = axienet_ior(&lp, XAS_SDL_CAM_STATUS_OFFSET);
+
+	if (mac_list->port_num == PORT_MAC1) {
+		mac_list->num_list = (u_value >> SDL_CAM_LEARNT_ENT_MAC1_SHIFT)
+					& SDL_CAM_LEARNT_ENT_MASK;
+	} else {
+		mac_list->num_list = (u_value >> SDL_CAM_LEARNT_ENT_MAC2_SHIFT)
+					& SDL_CAM_LEARNT_ENT_MASK;
+		read_key_addr = 0x800;
+	}
+
+	for (i = 0; i < MAX_NUM_MAC_ENTRIES ; i++) {
+		err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+					 (reg & SDL_CAM_WR_ENABLE), 10,
+					 DELAY_OF_FIVE_MILLISEC);
+		if (err) {
+			pr_err("CAM init timed out\n");
+			ret = -ETIMEDOUT;
+			goto free_mac_list;
+		}
+
+		u_value = ((read_key_addr + i) << SDL_CAM_READ_KEY_ADDR_SHIFT)
+			  | SDL_CAM_READ_KEY_ENTRY;
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, u_value);
+
+		err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_CTRL_OFFSET, reg,
+					 (!(reg & SDL_CAM_WR_ENABLE)), 10,
+					 DELAY_OF_FIVE_MILLISEC);
+		if (err) {
+			pr_err("CAM write timed out\n");
+			ret = -ETIMEDOUT;
+			goto free_mac_list;
+		}
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_CTRL_OFFSET);
+
+		if (u_value & SDL_CAM_FOUND_BIT) {
+			u_value = axienet_ior(&lp, XAS_SDL_CAM_KEY1_OFFSET);
+			mac_list->list[i].mac_addr[0] = (u_value >> 24) & 0xFF;
+			mac_list->list[i].mac_addr[1] = (u_value >> 16) & 0xFF;
+			mac_list->list[i].mac_addr[2] = (u_value >> 8) & 0xFF;
+			mac_list->list[i].mac_addr[3] = (u_value) & 0xFF;
+			u_value = axienet_ior(&lp, XAS_SDL_CAM_KEY2_OFFSET);
+			mac_list->list[i].mac_addr[4] = (u_value >> 8) & 0xFF;
+			mac_list->list[i].mac_addr[5] = (u_value) & 0xFF;
+			mac_list->list[i].vlan_id     = (u_value >> 16) & 0xFFF;
+		}
+	}
+	if (copy_to_user(arg, mac_list, sizeof(struct mac_addr_list))) {
+		ret = -EFAULT;
+		goto free_mac_list;
+	}
+
+free_mac_list:
+	kfree(mac_list);
+ret_status:
+	return ret;
+}
+
+int tsn_switch_set_stp_state(struct port_status *port)
+{
+	u32 u_value, reg, err;
+	u32 en_port_sts_chg_bit = 1;
+
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+	switch (port->port_num) {
+	case PORT_EP:
+		if (!(u_value & EP_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK <<
+					EP_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status << EP_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = EP_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	case PORT_MAC1:
+		if (!(u_value & MAC1_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK <<
+					MAC1_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status <<
+					MAC1_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = MAC1_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	case PORT_MAC2:
+		if (!(u_value & MAC2_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK <<
+					MAC2_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status <<
+					MAC2_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = MAC2_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	case PORT_EX_ONLY:
+		if (!(ep_lp->ex_ep))
+			return -EINVAL;
+		if (!(u_value & EX_EP_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK << EX_EP_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status << EX_EP_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = EX_EP_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	}
+
+	u_value |= en_port_sts_chg_bit;
+	axienet_iow(&lp, XAS_PORT_STATE_CTRL_OFFSET, u_value);
+
+	/* wait for write to complete */
+	err = readl_poll_timeout(lp.regs + XAS_PORT_STATE_CTRL_OFFSET, reg,
+				 (!(reg & en_port_sts_chg_bit)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+
+	return 0;
+}
+
+static int set_port_status(void __user *arg)
+{
+	struct port_status port;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_status)))
+		return -EFAULT;
+
+	return tsn_switch_set_stp_state(&port);
+}
+
+static int get_port_status(void __user *arg)
+{
+	struct port_status port;
+	u32 u_value;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_status)))
+		return -EINVAL;
+
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+	switch (port.port_num) {
+	case PORT_EP:
+		port.port_status = (u_value >> EP_PORT_STATUS_SHIFT) &
+					PORT_STATUS_MASK;
+		break;
+	case PORT_MAC1:
+		port.port_status = (u_value >> MAC1_PORT_STATUS_SHIFT) &
+					PORT_STATUS_MASK;
+		break;
+	case PORT_MAC2:
+		port.port_status = (u_value >> MAC2_PORT_STATUS_SHIFT) &
+					PORT_STATUS_MASK;
+		break;
+	case PORT_EX_ONLY:
+		if (!(ep_lp->ex_ep))
+			return -EINVAL;
+		port.port_status = (u_value >> EX_EP_PORT_STATUS_SHIFT) & PORT_STATUS_MASK;
+		break;
+	}
+
+	if (copy_to_user(arg, &port, sizeof(struct port_status)))
+		return -EINVAL;
+
+	return 0;
+}
+
+u8 *tsn_switch_get_id(void)
+{
+	return sw_mac_addr;
+}
+
+int tsn_switch_vlan_add(struct port_vlan *port, int add)
+{
+	u32 u_value, u_value1, reg, err;
+	u8 learning = 0;
+
+	u_value1 = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+	learning = (u_value1 & PORT_VLAN_HW_ADDR_LEARN_BIT) ? 0 : 1;
+	if (learning) {
+		if (port->en_ipv == 1) {
+			pr_err("When hardware address learning is enabled for a VLAN, TSN streams cannot be mapped to a particular priority queue\n");
+			return -EPERM;
+		}
+	} else {
+		if (port->en_port_status == 1) {
+			pr_err("When hardware address learning is disabled for a VLAN, port status cannot be set per VLAN\n");
+			return -EPERM;
+		}
+	}
+	u_value = ((port->vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_READ;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	if (add)
+		u_value1 |= PORT_VLAN_PORT_LIST_VALID_BIT | port->port_num;
+	else
+		u_value1 &= ~(port->port_num);
+	axienet_iow(&lp, XAS_VLAN_MEMB_DATA_REG, u_value1);
+
+	u_value = ((port->vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_WRITE;
+	if (port->en_port_status == 1) {
+		u_value |= ((port->port_status & SDL_CAM_IPV_MASK) << PORT_VLAN_IPV_SHIFT)
+				| (port->en_port_status << PORT_VLAN_EN_IPV_SHIFT);
+	}
+	if (port->en_ipv == 1) {
+		u_value |= ((port->ipv & SDL_CAM_IPV_MASK) << PORT_VLAN_IPV_SHIFT)
+				| (port->en_ipv << PORT_VLAN_EN_IPV_SHIFT);
+	}
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static int add_del_port_vlan(void __user *arg, u8 add)
+{
+	struct port_vlan port;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	return tsn_switch_vlan_add(&port, add);
+}
+
+static int set_vlan_mac_addr_learn(void __user *arg)
+{
+	struct port_vlan port;
+	u32 u_value, u_value1, reg, err;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	u_value = ((port.vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_READ;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value = axienet_ior(&lp, XAS_VLAN_MEMB_CTRL_REG);
+
+	u_value1 = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+
+	if (port.aging_time) {
+		u_value1 &= ~(HW_ADDR_AGING_TIME_MASK <<
+				PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT);
+		u_value1 |= (port.aging_time <<
+				PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT);
+	}
+	if (port.is_age) {
+		if (port.aging)
+			u_value1 |= PORT_VLAN_HW_ADDR_AGING_BIT;
+		else
+			u_value1 &= ~PORT_VLAN_HW_ADDR_AGING_BIT;
+	}
+	if (port.is_learn) {
+		if (port.learning)
+			u_value1 &= ~PORT_VLAN_HW_ADDR_LEARN_BIT;
+		else
+			u_value1 |= PORT_VLAN_HW_ADDR_LEARN_BIT;
+	}
+	axienet_iow(&lp, XAS_VLAN_MEMB_DATA_REG, u_value1);
+
+	u_value |= PORT_VLAN_WRITE;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static int get_vlan_mac_addr_learn(void __user *arg)
+{
+	struct port_vlan port;
+	u32 u_value, u_value1, reg, err;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	u_value = ((port.vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_READ;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value1 = axienet_ior(&lp, XAS_VLAN_MEMB_CTRL_REG);
+
+	u_value = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+
+	port.aging_time = (u_value >> PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT) &
+				HW_ADDR_AGING_TIME_MASK;
+	port.aging = (u_value & PORT_VLAN_HW_ADDR_AGING_BIT) ? 1 : 0;
+	port.learning = (u_value & PORT_VLAN_HW_ADDR_LEARN_BIT) ? 0 : 1;
+	port.port_num = u_value & PORT_STATUS_MASK;
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	if (port.learning) {
+		port.port_status = (u_value1 >> PORT_VLAN_IPV_SHIFT) & SDL_CAM_IPV_MASK;
+		port.en_port_status = (u_value1 >> PORT_VLAN_EN_IPV_SHIFT) & 0x1;
+	} else {
+		port.ipv = (u_value1 >> PORT_VLAN_IPV_SHIFT) & SDL_CAM_IPV_MASK;
+		port.en_ipv = (u_value1 >> PORT_VLAN_EN_IPV_SHIFT) & 0x1;
+	}
+#endif
+	if (copy_to_user(arg, &port, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	return 0;
+}
+
+int tsn_switch_pvid_add(struct native_vlan *port)
+{
+	u32 u_value;
+
+	switch (port->port_num) {
+	case PORT_EP:
+		u_value = axienet_ior(&lp, XAS_EP_PORT_VLAN_OFFSET);
+		u_value &= ~PORT_VLAN_ID_MASK;
+		u_value |= port->vlan_id;
+		if (port->en_ipv) {
+			u_value &= ~(SDL_CAM_IPV_MASK << NATIVE_MAC1_PCP_SHIFT);
+			u_value |= (port->ipv << NATIVE_MAC1_PCP_SHIFT);
+		}
+		axienet_iow(&lp, XAS_EP_PORT_VLAN_OFFSET, u_value);
+		break;
+	case PORT_MAC1:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		u_value &= ~PORT_VLAN_ID_MASK;
+		u_value |= port->vlan_id;
+		if (port->en_ipv) {
+			u_value &= ~(SDL_CAM_IPV_MASK << NATIVE_MAC1_PCP_SHIFT);
+			u_value |= (port->ipv << NATIVE_MAC1_PCP_SHIFT);
+		}
+		axienet_iow(&lp, XAS_MAC_PORT_VLAN_OFFSET, u_value);
+		break;
+	case PORT_MAC2:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		u_value &= ~(PORT_VLAN_ID_MASK << NATIVE_MAC2_VLAN_SHIFT);
+		u_value |= (port->vlan_id << NATIVE_MAC2_VLAN_SHIFT);
+		if (port->en_ipv) {
+			u_value &= ~(SDL_CAM_IPV_MASK << NATIVE_MAC2_PCP_SHIFT);
+			u_value |= (port->ipv << NATIVE_MAC2_PCP_SHIFT);
+		}
+		axienet_iow(&lp, XAS_MAC_PORT_VLAN_OFFSET, u_value);
+		break;
+	}
+
+	return 0;
+}
+
+static int set_native_vlan(void __user *arg)
+{
+	struct native_vlan port;
+
+	if (copy_from_user(&port, arg, sizeof(struct native_vlan)))
+		return -EFAULT;
+
+	return tsn_switch_pvid_add(&port);
+}
+
+int tsn_switch_pvid_get(struct native_vlan *port)
+{
+	u32 u_value;
+
+	switch (port->port_num) {
+	case PORT_EP:
+		u_value = axienet_ior(&lp, XAS_EP_PORT_VLAN_OFFSET);
+		port->vlan_id = u_value & PORT_VLAN_ID_MASK;
+		port->ipv = (u_value >> NATIVE_MAC1_PCP_SHIFT) &
+				SDL_CAM_IPV_MASK;
+		break;
+	case PORT_MAC1:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		port->vlan_id = u_value & PORT_VLAN_ID_MASK;
+		port->ipv = (u_value >> NATIVE_MAC1_PCP_SHIFT) &
+				SDL_CAM_IPV_MASK;
+		break;
+	case PORT_MAC2:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		port->vlan_id = (u_value >> NATIVE_MAC2_VLAN_SHIFT) &
+				PORT_VLAN_ID_MASK;
+		port->ipv = (u_value >> NATIVE_MAC2_PCP_SHIFT) &
+				SDL_CAM_IPV_MASK;
+		break;
+	}
+
+	return 0;
+}
+
+static int get_native_vlan(void __user *arg)
+{
+	struct native_vlan port;
+
+	if (copy_from_user(&port, arg, sizeof(struct native_vlan)))
+		return -EFAULT;
+
+	tsn_switch_pvid_get(&port);
+	if (copy_to_user(arg, &port, sizeof(struct native_vlan)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static long switch_ioctl(struct file *file, unsigned int cmd,
+			 unsigned long arg)
+{
+	long retval = 0;
+	struct switch_data data;
+	struct pmap_data pri_info;
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	struct qci qci_data;
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	struct cb cb_data;
+#endif
+	switch (cmd) {
+	case GET_STATUS_SWITCH:
+		/* Switch configurations */
+		get_switch_regs(&data);
+
+		/* Memory static counter*/
+		get_memory_static_counter(&data);
+		if (copy_to_user((char __user *)arg, &data, sizeof(data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+			}
+		break;
+
+	case SET_STATUS_SWITCH:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_switch_regs(&data);
+		break;
+
+	case ADD_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		if (tsn_switch_cam_set(data.cam_data, ADD)) {
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case DELETE_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		if (tsn_switch_cam_set(data.cam_data, DELETE)) {
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case PORT_VLAN_MEM_CTRL:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		port_vlan_mem_ctrl(data.port_vlan_mem_ctrl);
+		break;
+
+	case READ_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			retval = -EFAULT;
+			goto end;
+		}
+		retval = read_cam_entry(data.cam_data, (void __user *)arg);
+		break;
+
+	case SET_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = set_mac_addr_learn((void __user *)arg);
+		goto end;
+
+	case GET_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_mac_addr_learn((void __user *)arg);
+		goto end;
+
+	case GET_MAC_ADDR_LEARNT_LIST:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_mac_addr_learnt_list((void __user *)arg);
+		goto end;
+
+	case SET_PORT_STATUS:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = set_port_status((void __user *)arg);
+		goto end;
+
+	case GET_PORT_STATUS:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_port_status((void __user *)arg);
+		goto end;
+
+	case ADD_PORT_VLAN:
+		retval = add_del_port_vlan((void __user *)arg, ADD);
+		break;
+
+	case DEL_PORT_VLAN:
+		retval = add_del_port_vlan((void __user *)arg, DELETE);
+		break;
+
+	case SET_VLAN_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = set_vlan_mac_addr_learn((void __user *)arg);
+		break;
+
+	case GET_VLAN_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_vlan_mac_addr_learn((void __user *)arg);
+		break;
+
+	case GET_VLAN_MAC_ADDR_LEARN_CONFIG_VLANM:
+		retval = get_vlan_mac_addr_learn((void __user *)arg);
+		break;
+
+	case SET_PORT_NATIVE_VLAN:
+		retval = set_native_vlan((void __user *)arg);
+		break;
+
+	case GET_PORT_NATIVE_VLAN:
+		retval = get_native_vlan((void __user *)arg);
+		break;
+
+	case SET_PMAP_CONFIG:
+		if (copy_from_user(&pri_info, (char __user *)arg, sizeof(pri_info))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+
+		if (lp.num_tc <= XAE_MAX_LEGACY_TSN_TC) {
+			axienet_set_pcpmap(ep_lp);
+			retval = set_pmap_config(ep_lp->pcpmap);
+		} else {
+			retval = -EOPNOTSUPP;
+		}
+		break;
+
+	case SET_FRAME_TYPE_FIELD:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_frame_filter_opt(data.typefield.type1,
+				     data.typefield.type2);
+		break;
+
+	case SET_MAC1_MNGMNT_Q_CONFIG:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_mac1_mngmntq(data.mac1_config);
+		break;
+
+	case SET_MAC2_MNGMNT_Q_CONFIG:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_mac2_mngmntq(data.mac2_config);
+		break;
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	case CONFIG_METER_MEM:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		program_meter_reg(qci_data.meter_config_data);
+		break;
+
+	case CONFIG_GATE_MEM:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		config_stream_filter(qci_data.stream_config_data);
+		break;
+
+	case PSFP_CONTROL:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			retval = -EINVAL;
+			pr_err("Copy from user failed\n");
+			goto end;
+		}
+		psfp_control(qci_data.psfp_config_data);
+		break;
+
+	case GET_STATIC_PSFP_COUNTER:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		get_psfp_static_counter(&qci_data.psfp_counter_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+	case GET_METER_REG:
+		get_meter_reg(&qci_data.meter_config_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+	case GET_STREAM_FLTR_CONFIG:
+		get_stream_filter_config(&qci_data.stream_config_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	case CONFIG_MEMBER_MEM:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		program_member_reg(cb_data);
+		break;
+
+	case CONFIG_INGRESS_FLTR:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		config_ingress_filter(cb_data);
+		break;
+
+	case FRER_CONTROL:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		frer_control(cb_data.frer_ctrl_data);
+		break;
+
+	case GET_STATIC_FRER_COUNTER:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		get_frer_static_counter(&cb_data.frer_counter_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case GET_MEMBER_REG:
+		get_member_reg(&cb_data.frer_memb_config_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case GET_INGRESS_FLTR:
+		get_ingress_filter_config(&cb_data.in_fltr_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+#endif
+	}
+end:
+	return retval;
+}
+
+static const struct file_operations switch_fops = {
+	.owner		=	THIS_MODULE,
+	.unlocked_ioctl	=	switch_ioctl,
+	.open		=	switch_open,
+	.release	=       switch_release,
+};
+
+static int tsn_switch_init(void)
+{
+	int ret;
+
+	switch_dev.minor = MISC_DYNAMIC_MINOR;
+	switch_dev.name = "switch";
+	switch_dev.fops = &switch_fops;
+	ret = misc_register(&switch_dev);
+	if (ret < 0) {
+		pr_err("Switch driver registration failed!\n");
+		return ret;
+	}
+
+	eth_random_addr((u8 *)&sw_mac_addr);
+
+	pr_debug("Xilinx TSN Switch driver initialized!\n");
+	return 0;
+}
+
+static inline void tsn_switch_set_src_mac_filter(const u8 *mac, int port)
+{
+	u32 val;
+	u32 shift = (port == 1) ? MAC1_PORT_MAC_ADDR_LSB_SHIFT :
+		MAC2_PORT_MAC_ADDR_LSB_SHIFT;
+
+	/* program the Source MAC address to filter */
+	val = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+
+	val &= ~(PORT_MAC_ADDR_LSB_MASK << shift);
+	val |= ((mac[5] & PORT_MAC_ADDR_LSB_MASK) << shift);
+
+	axienet_iow(&lp, XAS_PORT_STATE_CTRL_OFFSET, val);
+}
+
+/* initialize pre-configured fdbs in the system */
+static int tsn_switch_fdb_init(struct platform_device *pdev)
+{
+	u32 val, port;
+	u8 mac_addr[ETH_ALEN];
+	struct device_node *np;
+	u16 num_ports;
+	int ret;
+	struct cam_struct cam;
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-ports",
+				   &num_ports);
+	if (ret) {
+		dev_err(&pdev->dev, "could not read xlnx,num-ports\n");
+		return -EINVAL;
+	}
+
+	/* Check if the property 'xlnx,packet-switch' is present in the device tree */
+	lp.packet_switch = device_property_read_bool(&pdev->dev, "xlnx,packet-switch");
+
+	/* enable source mac based filtering */
+	val = axienet_ior(&lp, XAS_MNG_Q_CTRL_OFFSET);
+
+	/* compares Source MAC address to determine the Network
+	 * Port on which a frame needs to be forwarded for frames received
+	 * with Internal Endpoint interface.
+	 * [i.e. CPU generated  management frames]
+	 */
+	val |= (1 << XAS_MNG_Q_SRC_MAC_FIL_EN_SHIFT);
+
+	axienet_iow(&lp, XAS_MNG_Q_CTRL_OFFSET, val);
+
+	/* port0 == ep, port1 == temac1 port2 == temac2
+	 * temac1, temac2.. temacn are network ports
+	 */
+	/* network port mac addresses must differ in last lsb nibble
+	 * this is a pre-requisite;
+	 * we program the last nibble here per mac basis
+	 */
+	ep_node = of_parse_phandle(pdev->dev.of_node, "ports", 0);
+	for (port = 1; port < num_ports; port++) {
+		np = of_parse_phandle(pdev->dev.of_node, "ports", port);
+		if (!np) {
+			dev_err(&pdev->dev, "Failed to parse phandle for port %d\n", port);
+			return -EINVAL;
+		}
+		ret = of_get_mac_address(np, mac_addr);
+		if (ret) {
+			dev_err(&pdev->dev, "could not find MAC address\n");
+			of_node_put(np);
+			return -EINVAL;
+		}
+		tsn_switch_set_src_mac_filter(mac_addr, port);
+		of_node_put(np);
+	}
+
+	/* rest of the mac addr for all ports would be same
+	 * so use the last mac instance of the loop above
+	 * set the 32bit lsb of mac address
+	 */
+	axienet_iow(&lp, XAS_MAC_LSB_OFFSET,
+		    (mac_addr[2] << 24) | (mac_addr[3] << 16) |
+		    (mac_addr[4] << 8)  | (mac_addr[5]));
+
+	/* set rest of 16bit msb and 4bit filter(0xF) */
+	axienet_iow(&lp, XAS_MAC_MSB_OFFSET,
+		    (0xF << XAS_MAC_MSB_FF_MASK_SHIFT) |
+		    (mac_addr[0] << 8) | mac_addr[1]);
+
+	/* If 'xlnx,packet-switch' is enabled */
+	if (lp.packet_switch) {
+		/* Modify the Management Queue Options register of the
+		 * EP packet switch for multicast frames.
+		 */
+		val = axienet_ior(&lp, XAS_MNG_Q_CTRL_OFFSET);
+		axienet_iow(&lp, XAS_MNG_Q_CTRL_OFFSET,
+			    val | (1 << XAS_MNG_Q_EPPKSW_MULI_EN_SHIFT));
+		/* Retrieve the MAC address of EP from the device tree */
+		ret = of_get_mac_address(ep_node, mac_addr);
+		if (ret) {
+			dev_err(&pdev->dev, "could not find MAC address\n");
+			return -EINVAL;
+		}
+		/* Update Switch Port State Control register with lower 4 bits of MAC,
+		 * state change and port state bits.
+		 */
+		val = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+		val = val | (1 << EP_PORT_STATUS_EP_STATE_SHIFT) |
+			EP_PORT_STATUS_CHG_BIT |
+			(mac_addr[5] << EP_PORT_STATUS_EP_MAC_ADDR_SHIFT);
+		/* Update Endpoint Extension Control Register with lower 4 bits of EP MAC */
+		axienet_iow(&lp, XAS_PORT_STATE_CTRL_OFFSET, val);
+		val = axienet_ior(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET);
+		axienet_iow(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET,
+			    (val | ((mac_addr[5] & 0xF) <<
+			     EP_EX_CTRL_REG_EP_MAC_ADDR_SHIFT)));
+	}
+	/* now tell the switch which frames to consider as mgmt frames
+	 */
+	/*  DA list
+	 *  01-80-C2-00-00-00 STP 802.1d && LLDP
+	 */
+	memset(&cam, 0, sizeof(struct cam_struct));
+
+	cam.dest_addr[0] = 0x01;
+	cam.dest_addr[1] = 0x80;
+	cam.dest_addr[2] = 0xc2;
+	cam.dest_addr[3] = 0x00;
+	cam.dest_addr[4] = 0x00;
+	cam.dest_addr[5] = 0x00;
+
+	/*
+	 * Layer-2 point-to-point control frames, such as those with the destination
+	 * MAC address 01-80-C2-00-00-00 used by STP (Spanning Tree Protocol) 802.1d
+	 * and LLDP (Link Layer Discovery Protocol), should only be forwarded to the
+	 * Endpoint port. When these control frames are received from a Network Port
+	 * or Endpoint port, they are not to be broadcast or forwarded to other network
+	 * ports but are expected to be directed specifically to the Endpoint port
+	 * for appropriate handling.
+	 */
+	if (lp.packet_switch)
+		cam.fwd_port = FWD_TO_EP;
+	else
+	/* send it all, src mac filter will pick the port */
+		cam.fwd_port = DEFAULT_FWD_ALL;
+
+	cam.flags |= XAS_CAM_EP_MGMTQ_EN;
+	cam.vlanid = DEFAULT_PVID;
+	/* TODO if pvid changes on the port of switch,
+	 * these cam entries have to be updated
+	 */
+
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+
+	/* 01-80-c2-00-00-0e  LLDP */
+	cam.dest_addr[5] = 0x0e;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+	/* CDP */
+	cam.dest_addr[0] = 0x01;
+	cam.dest_addr[1] = 0x00;
+	cam.dest_addr[2] = 0x0c;
+	cam.dest_addr[3] = 0xcc;
+	cam.dest_addr[4] = 0xcc;
+	cam.dest_addr[5] = 0xcc;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+
+	/* send it all, src mac filter will pick the port */
+	cam.fwd_port = DEFAULT_FWD_ALL;
+	/* PTPv2 UDP Announce Messages */
+	cam.dest_addr[0] = 0x01;
+	cam.dest_addr[1] = 0x00;
+	cam.dest_addr[2] = 0x5e;
+	cam.dest_addr[3] = 0x00;
+	cam.dest_addr[4] = 0x01;
+	cam.dest_addr[5] = 0x81;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+	/* PTPv2 UDP P2P Mechanism Messages */
+	cam.dest_addr[4] = 0x00;
+	cam.dest_addr[5] = 0x6b;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+
+	/* on RX path enable sideband management on frames forwarded to ep
+	 * this only applicable if IP param EN_INBAND_MGMT_TAG is 0
+	 */
+	val = axienet_ior(&lp, XAS_MNG_Q_CTRL_OFFSET);
+	val |= (1 << XAS_MNG_Q_SIDEBAND_EN_SHIFT);
+	axienet_iow(&lp, XAS_MNG_Q_CTRL_OFFSET, val);
+
+	return 0;
+}
+
+static int tsnswitch_probe(struct platform_device *pdev)
+{
+	struct net_device *ndev;
+	struct resource *swt;
+	u8 inband_mgmt_tag;
+	int value;
+	u32 data;
+	int ret;
+
+	pr_info("TSN Switch probe\n");
+	/* Map device registers */
+	swt = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp.regs = devm_ioremap_resource(&pdev->dev, swt);
+	if (IS_ERR(lp.regs))
+		return PTR_ERR(lp.regs);
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &lp.num_tc);
+	if (ret || !axienet_tsn_num_tc_valid(lp.num_tc)) {
+		dev_err(&pdev->dev,
+			"xlnx,num-tc parameter not defined or valid\n");
+		return -EINVAL;
+	}
+
+	tsn_switch_set_fp_map(pdev, lp.num_tc);
+	en_hw_addr_learning = of_property_read_bool(pdev->dev.of_node,
+						    "xlnx,has-hwaddr-learning");
+
+	pr_info("TSN Switch Initializing ....\n");
+	pr_info("TSN Switch hw_addr_learning :%d\n", en_hw_addr_learning);
+
+	ret = tsn_switch_init();
+	if (ret)
+		return ret;
+	pr_info("TSN CAM Initializing ....\n");
+
+	inband_mgmt_tag = of_property_read_bool(pdev->dev.of_node,
+						"xlnx,has-inband-mgmt-tag");
+	/* only support switchdev in sideband management */
+	if (!inband_mgmt_tag) {
+		ret = tsn_switch_fdb_init(pdev);
+		xlnx_switchdev_init();
+	} else {
+		pr_info("TSN IP with inband mgmt: Linux SWITCHDEV turned off\n");
+	}
+
+	/* writing into endpoint extension control register for channel mapping as follows:
+	 *
+	 *	3 traffic classes & EP + switch + Extended EP
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *     ST------         |------BE
+	 *	  3   |         |   2
+	 *   mgmt------         |------RES
+	 *	  4   |         |
+	 *   RES-------         |
+	 *	  5   |         |
+	 * EX-BE-------         |
+	 *	  6   |         |
+	 * EX-ST-------         |
+	 *	  7   |         |
+	 * EX-RES------         |
+	 *	      +---------+
+	 *
+	 *	2 traffic classes & EP + switch + Extended EP
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *    ST-------         |------BE
+	 *	  3   |         |
+	 *   mgmt------         |
+	 *	  4   |         |
+	 * EX-BE-------         |
+	 *	  5   |         |
+	 * EX-ST-------         |
+	 *	      |         |
+	 *	      +---------+
+	 *	3 traffic classes & EP + switch
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *     ST------         |------BE
+	 *	  3   |         |   2
+	 *   mgmt------         |------RES
+	 *	  4   |         |
+	 *   RES-------         |
+	 *	      |         |
+	 *	      +---------+
+	 *
+	 *	2 traffic classes & EP + switch
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *     ST------         |------BE
+	 *	  3   |         |
+	 *   mgmt------         |
+	 *	      |         |
+	 *	      +---------+
+	 */
+	data = axienet_ior(&lp, XAE_EP_EXT_CTRL_OFFSET);
+	pr_info("Data in Endpoint Extension Control Register is %x\n", data);
+	if (lp.num_tc > XAE_MAX_LEGACY_TSN_TC) {
+		u32 ctrl1_data = axienet_ior(&lp, XAE_EP_EXT_CTRL1_OFFSET);
+
+		pr_info("Data in EP_EXT Control 1 Register %x\n", ctrl1_data);
+	}
+
+	ndev = of_find_net_device_by_node(ep_node);
+	if (!ndev) {
+		dev_err(&pdev->dev, "Defer Switch probe as EP is not probed\n");
+		ret = -EPROBE_DEFER;
+		goto err;
+	}
+	ep_lp = netdev_priv(ndev);
+
+	ret = set_pmap_config(ep_lp->pcpmap);
+	if (ret)
+		goto err;
+
+	if (ep_lp->ex_ep) {
+		if (lp.num_tc == XAE_MAX_LEGACY_TSN_TC) {
+			data = (data & XAE_EX_EP_EXT_CTRL_MASK) |
+					XAE_EX_EP_EXT_CTRL_DATA_TC_3;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+		if (lp.num_tc == XAE_MIN_LEGACY_TSN_TC) {
+			data = (data & XAE_EX_EP_EXT_CTRL_MASK) |
+					XAE_EX_EP_EXT_CTRL_DATA_TC_2;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+		/* Enabling endpoint packet switching extension for broadcast and
+		 * multicast packets received from endpoint
+		 */
+		value = axienet_ior(&lp, XAE_MGMT_QUEUING_OPTIONS_OFFSET);
+		value |= XAE_EX_EP_BROADCAST_PKT_SWITCH;
+		value |= XAE_EX_EP_MULTICAST_PKT_SWITCH;
+		axienet_iow(&lp, XAE_MGMT_QUEUING_OPTIONS_OFFSET, value);
+	} else {
+		if (lp.num_tc == XAE_MAX_LEGACY_TSN_TC) {
+			data = (data & XAE_EP_EXT_CTRL_MASK) |
+					XAE_EP_EXT_CTRL_DATA_TC_3;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+		if (lp.num_tc == XAE_MIN_LEGACY_TSN_TC) {
+			data = (data & XAE_EP_EXT_CTRL_MASK) |
+					XAE_EP_EXT_CTRL_DATA_TC_2;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+	}
+
+	of_node_put(ep_node);
+	return ret;
+err:
+	if (!inband_mgmt_tag)
+		xlnx_switchdev_remove();
+	misc_deregister(&switch_dev);
+	of_node_put(ep_node);
+	return ret;
+}
+
+static void tsnswitch_remove(struct platform_device *pdev)
+{
+	misc_deregister(&switch_dev);
+	xlnx_switchdev_remove();
+}
+
+struct platform_driver tsnswitch_driver = {
+	.probe = tsnswitch_probe,
+	.remove = tsnswitch_remove,
+	.driver = {
+		 .name = "xilinx_tsnswitch",
+		 .of_match_table = tsnswitch_of_match,
+	},
+};
+
+MODULE_DESCRIPTION("Xilinx TSN Switch driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL");
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_switch.h b/drivers/staging/xilinx-tsn/xilinx_tsn_switch.h
new file mode 100644
index 000000000..9b107eb1d
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_switch.h
@@ -0,0 +1,497 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN core switch header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_SWITCH_H
+#define XILINX_TSN_SWITCH_H
+
+#include "xilinx_axienet_tsn.h"
+
+/* ioctls */
+#define GET_STATUS_SWITCH			0x16
+#define SET_STATUS_SWITCH			0x17
+#define ADD_CAM_ENTRY				0x18
+#define DELETE_CAM_ENTRY			0x19
+#define PORT_VLAN_MEM_CTRL			0x20
+#define SET_FRAME_TYPE_FIELD			0x21
+#define SET_MAC1_MNGMNT_Q_CONFIG		0x22
+#define SET_MAC2_MNGMNT_Q_CONFIG		0x23
+#define CONFIG_METER_MEM			0x24
+#define CONFIG_GATE_MEM				0x25
+#define PSFP_CONTROL				0x26
+#define GET_STATIC_PSFP_COUNTER			0x27
+#define GET_METER_REG				0x28
+#define GET_STREAM_FLTR_CONFIG			0x29
+#define CONFIG_MEMBER_MEM			0x2A
+#define CONFIG_INGRESS_FLTR			0x2B
+#define FRER_CONTROL				0x2C
+#define GET_STATIC_FRER_COUNTER			0x2D
+#define GET_MEMBER_REG				0x2E
+#define GET_INGRESS_FLTR			0x2F
+#define SET_PORT_STATUS				0x33
+#define GET_PORT_STATUS				0x34
+#define SET_MAC_ADDR_LEARN_CONFIG		0x30
+#define GET_MAC_ADDR_LEARN_CONFIG		0x31
+#define GET_MAC_ADDR_LEARNT_LIST		0x32
+#define ADD_PORT_VLAN				0x35
+#define DEL_PORT_VLAN				0x36
+#define SET_VLAN_MAC_ADDR_LEARN_CONFIG		0x37
+#define GET_VLAN_MAC_ADDR_LEARN_CONFIG		0x38
+#define READ_CAM_ENTRY				0x39
+#define GET_VLAN_MAC_ADDR_LEARN_CONFIG_VLANM	0x3C
+#define SET_PORT_NATIVE_VLAN			0x3A
+#define GET_PORT_NATIVE_VLAN			0x3B
+#define SET_PMAP_CONFIG				0x3D
+
+/* Xilinx Axi Switch Offsets*/
+#define XAS_STATUS_OFFSET			0x00000
+#define XAS_CONTROL_OFFSET			0x00004
+#define XAS_PMAP_OFFSET				0x00008
+#define XAS_MAC_LSB_OFFSET			0x0000C
+#define XAS_MAC_MSB_OFFSET			0x00010
+#define XAS_MAC_MSB_FF_MASK_SHIFT		(16)
+#define XAS_PREEMPTION_QUEUE_MAP_OFFSET		0x00014
+
+#define XAS_EP2MAC_PRI7_FIFOT_OFFSET		0x00020
+#define XAS_EP2MAC_PRI6_FIFOT_OFFSET		0x00024
+#define XAS_EP2MAC_PRI5_FIFOT_OFFSET		0x00028
+#define XAS_EP2MAC_PRI4_FIFOT_OFFSET		0x00084
+#define XAS_EP2MAC_PRI3_FIFOT_OFFSET		0x0008C
+#define XAS_EP2MAC_PRI2_FIFOT_OFFSET		0x00090
+#define XAS_EP2MAC_PRI1_FIFOT_OFFSET		0x00094
+#define XAS_EP2MAC_PRI0_FIFOT_OFFSET		0x0009C
+#define XAS_MAC2MAC_PRI7_FIFOT_OFFSET		0x00030
+#define XAS_MAC2MAC_PRI6_FIFOT_OFFSET		0x00034
+#define XAS_MAC2MAC_PRI5_FIFOT_OFFSET		0x00038
+#define XAS_MAC2MAC_PRI4_FIFOT_OFFSET		0x000A4
+#define XAS_MAC2MAC_PRI3_FIFOT_OFFSET		0x000AC
+#define XAS_MAC2MAC_PRI2_FIFOT_OFFSET		0x000B0
+#define XAS_MAC2MAC_PRI1_FIFOT_OFFSET		0x000B4
+#define XAS_MAC2MAC_PRI0_FIFOT_OFFSET		0x000BC
+#define XAS_EP_PORT_VLAN_OFFSET			0x00040
+#define XAS_MAC_PORT_VLAN_OFFSET		0x00044
+#define XAS_HW_ADDR_LEARN_CTRL_OFFSET		0x00048
+#define XAS_PORT_STATE_CTRL_OFFSET		0x0004c
+#define XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET	0x00050
+
+#define XAS_MNG_Q_CTRL_OFFSET			0x00054
+#define XAS_MNG_Q_SRC_MAC_FIL_EN_SHIFT		(4)
+#define XAS_MNG_Q_SIDEBAND_EN_SHIFT		(3)
+#define XAS_MNG_Q_EPPKSW_MULI_EN_SHIFT		6
+
+#define XAS_MAC1_MNG_Q_OPTION_OFFSET		0x00058
+#define XAS_PRI7_MAX_FRAME_SIZE_OFFSET		0x00060
+#define XAS_PRI6_MAX_FRAME_SIZE_OFFSET		0x00064
+#define XAS_PRI5_MAX_FRAME_SIZE_OFFSET		0x00068
+#define XAS_PRI4_MAX_FRAME_SIZE_OFFSET		0x0006C
+#define XAS_PRI3_MAX_FRAME_SIZE_OFFSET		0x00070
+#define XAS_PRI2_MAX_FRAME_SIZE_OFFSET		0x00074
+#define XAS_PRI1_MAX_FRAME_SIZE_OFFSET		0x00078
+#define XAS_PRI0_MAX_FRAME_SIZE_OFFSET		0x0007C
+
+/* Memory static counters */
+#define XAS_MEM_STCNTR_CAM_LOOKUP		0x00400
+#define XAS_MEM_STCNTR_MULTCAST			0x00408
+#define XAS_MEM_STCNTR_ERR_MAC1			0x00410
+#define XAS_MEM_STCNTR_ERR_MAC2			0x00418
+#define XAS_MEM_STCNTR_SC_MAC1_EP		0x00420
+#define XAS_MEM_STCNTR_RES_MAC1_EP		0x00428
+#define XAS_MEM_STCNTR_BE_MAC1_EP		0x00430
+#define XAS_MEM_STCNTR_ERR_SC_MAC1_EP		0x00438
+#define XAS_MEM_STCNTR_ERR_RES_MAC1_EP		0x00440
+#define XAS_MEM_STCNTR_ERR_BE_MAC1_EP		0x00448
+#define XAS_MEM_STCNTR_SC_MAC2_EP		0x00458
+#define XAS_MEM_STCNTR_RES_MAC2_EP		0x00460
+#define XAS_MEM_STCNTR_BE_MAC2_EP		0x00468
+#define XAS_MEM_STCNTR_ERR_SC_MAC2_EP		0x00470
+#define XAS_MEM_STCNTR_ERR_RES_MAC2_EP		0x00478
+#define XAS_MEM_STCNTR_ERR_BE_MAC2_EP		0x00480
+#define XAS_MEM_STCNTR_SC_EP_MAC1		0x00490
+#define XAS_MEM_STCNTR_RES_EP_MAC1		0x00498
+#define XAS_MEM_STCNTR_BE_EP_MAC1		0x004A0
+#define XAS_MEM_STCNTR_ERR_SC_EP_MAC1		0x004A8
+#define XAS_MEM_STCNTR_ERR_RES_EP_MAC1		0x004B0
+#define XAS_MEM_STCNTR_ERR_BE_EP_MAC1		0x004B8
+#define XAS_MEM_STCNTR_SC_MAC2_MAC1		0x004C0
+#define XAS_MEM_STCNTR_RES_MAC2_MAC1		0x004C8
+#define XAS_MEM_STCNTR_BE_MAC2_MAC1		0x004D0
+#define XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1		0x004D8
+#define XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1	0x004E0
+#define XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1		0x004E8
+#define XAS_MEM_STCNTR_SC_EP_MAC2		0x004F0
+#define XAS_MEM_STCNTR_RES_EP_MAC2		0x004F8
+#define XAS_MEM_STCNTR_BE_EP_MAC2		0x00500
+#define XAS_MEM_STCNTR_ERR_SC_EP_MAC2		0x00508
+#define XAS_MEM_STCNTR_ERR_RES_EP_MAC2		0x00510
+#define XAS_MEM_STCNTR_ERR_BE_EP_MAC2		0x00518
+#define XAS_MEM_STCNTR_SC_MAC1_MAC2		0x00520
+#define XAS_MEM_STCNTR_RES_MAC1_MAC2		0x00528
+#define XAS_MEM_STCNTR_BE_MAC1_MAC2		0x00530
+#define XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2		0x00538
+#define XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2	0x00540
+#define XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2		0x00548
+
+/* Stream Destination Lookup CAM */
+#define XAS_SDL_CAM_CTRL_OFFSET			0x1000
+#define XAS_SDL_CAM_STATUS_OFFSET		0x1004
+#define XAS_SDL_CAM_KEY1_OFFSET			0x1008
+#define XAS_SDL_CAM_KEY2_OFFSET			0x100C
+#define XAS_SDL_CAM_TV1_OFFSET			0x1010
+#define XAS_SDL_CAM_TV2_OFFSET			0x1014
+#define XAS_SDL_CAM_PORT_ACT_OFFSET		0x1018
+
+/* Port VLAN Membership Memory */
+#define XAS_VLAN_MEMB_CTRL_REG			0x1100
+#define XAS_VLAN_MEMB_DATA_REG			0x1104
+
+/* QCI */
+#define PSFP_CONTROL_OFFSET			0x1200
+#define STREAM_FILTER_CONFIG_OFFSET		0x1204
+#define	STREAM_METER_CIR_OFFSET			0x1208
+#define	STREAM_METER_EIR_OFFSET			0x120C
+#define	STREAM_METER_CBR_OFFSET			0x1210
+#define	STREAM_METER_EBR_OFFSET			0x1214
+
+/* PSFP Statistics Counters */
+#define TOTAL_PSFP_FRAMES_OFFSET		0x2000
+#define FLTR_INGS_PORT_ERR_OFFSET		0x2800
+#define FLTR_STDU_ERR_OFFSET			0x3000
+#define METER_ERR_OFFSET			0x3800
+
+/* CB */
+#define FRER_CONTROL_OFFSET			0x1300
+#define INGRESS_FILTER_OFFSET			0x1304
+#define FRER_CONFIG_REG1			0x1308
+#define FRER_CONFIG_REG2			0x130C
+
+/* FRER Statistics Counters */
+#define TOTAL_FRER_FRAMES_OFFSET		0x4000
+#define FRER_DISCARD_INGS_FLTR_OFFSET		0x4800
+#define FRER_PASS_FRAMES_INDV_OFFSET		0x5000
+#define FRER_DISCARD_FRAMES_INDV_OFFSET		0x5800
+#define FRER_PASS_FRAMES_SEQ_OFFSET		0x6000
+#define FRER_DISCARD_FRAMES_SEQ_OFFSET		0x6800
+#define FRER_ROGUE_FRAMES_SEQ_OFFSET		0x7000
+#define SEQ_RECV_RESETS_OFFSET			0x7800
+
+/* endpoint extension control register */
+#define XAE_EP_EXT_CTRL_OFFSET			0x0058
+#define XAE_EP_EXT_CTRL1_OFFSET			0x005C
+#define XAE_MGMT_QUEUING_OPTIONS_OFFSET		0x0054
+#define XAE_EX_EP_BROADCAST_PKT_SWITCH		BIT(7)
+#define XAE_EX_EP_MULTICAST_PKT_SWITCH		BIT(6)
+#define XAE_EX_EP_EXT_CTRL_DATA_TC_3		0x00135419
+#define XAE_EX_EP_EXT_CTRL_DATA_TC_2		0x000DC401
+#define XAE_EP_EXT_CTRL_DATA_TC_3		0x00400419
+#define XAE_EP_EXT_CTRL_DATA_TC_2		0x00400409
+#define XAE_EX_EP_EXT_CTRL_MASK			0xFFE00000
+#define XAE_EP_EXT_CTRL_MASK			0xFF1FF000
+
+/* 64 bit counter*/
+struct static_cntr {
+	u32 msb;
+	u32 lsb;
+};
+
+/*********** QCI Structures **************/
+struct psfp_config {
+	u8 gate_id;
+	u8 meter_id;
+	bool en_meter;
+	bool allow_stream;
+	bool en_psfp;
+	u8 wr_op_type;
+	bool op_type;
+};
+
+struct meter_config {
+	u32 cir;
+	u32 eir;
+	u32 cbr;
+	u32 ebr;
+	u8 mode;
+};
+
+struct stream_filter {
+	u8 in_pid; /* ingress port id*/
+	u16 max_fr_size; /* max frame size*/
+};
+
+/* PSFP Static counter*/
+struct psfp_static_counter {
+	struct static_cntr psfp_fr_count;
+	struct static_cntr err_filter_ins_port;
+	struct static_cntr err_filtr_sdu;
+	struct static_cntr err_meter;
+	unsigned char num;
+};
+
+/* QCI Core stuctures */
+struct qci {
+	struct meter_config meter_config_data;
+	struct stream_filter stream_config_data;
+	struct psfp_config psfp_config_data;
+	struct psfp_static_counter psfp_counter_data;
+};
+
+/************* QCI Structures end *************/
+
+/*********** CB Structures **************/
+struct frer_ctrl {
+	u8 gate_id;
+	u8 memb_id;
+	bool seq_reset;
+	bool gate_state;
+	bool rcvry_tmout;
+	bool frer_valid;
+	u8 wr_op_type;
+	bool op_type;
+};
+
+struct in_fltr {
+	u8 in_port_id;
+	u16 max_seq_id;
+};
+
+struct frer_memb_config {
+	u8 seq_rec_hist_len;
+	u8 split_strm_egport_id;
+	u16 split_strm_vlan_id;
+	u32 rem_ticks;
+};
+
+/* FRER Static counter*/
+struct frer_static_counter {
+	struct static_cntr frer_fr_count;
+	struct static_cntr disc_frames_in_portid;
+	struct static_cntr pass_frames_seq_recv;
+	struct static_cntr disc_frames_seq_recv;
+	struct static_cntr rogue_frames_seq_recv;
+	struct static_cntr pass_frames_ind_recv;
+	struct static_cntr disc_frames_ind_recv;
+	struct static_cntr seq_recv_rst;
+	unsigned char num;
+};
+
+/* CB Core stuctures */
+struct cb {
+	struct frer_ctrl frer_ctrl_data;
+	struct in_fltr in_fltr_data;
+	struct frer_memb_config frer_memb_config_data;
+	struct frer_static_counter frer_counter_data;
+};
+
+/************* CB Structures end *************/
+
+/********* Switch Structures Starts ***********/
+struct thershold {
+	u16 t1;
+	u16 t2;
+};
+
+struct pmap_data {
+	int st_pcp_reg;
+	int res_pcp_reg;
+};
+
+/* memory static counters */
+struct mem_static_arr_cntr {
+	struct static_cntr cam_lookup;
+	struct static_cntr multicast_fr;
+	struct static_cntr err_mac1;
+	struct static_cntr err_mac2;
+	struct static_cntr sc_mac1_ep;
+	struct static_cntr res_mac1_ep;
+	struct static_cntr be_mac1_ep;
+	struct static_cntr err_sc_mac1_ep;
+	struct static_cntr err_res_mac1_ep;
+	struct static_cntr err_be_mac1_ep;
+	struct static_cntr sc_mac2_ep;
+	struct static_cntr res_mac2_ep;
+	struct static_cntr be_mac2_ep;
+	struct static_cntr err_sc_mac2_ep;
+	struct static_cntr err_res_mac2_ep;
+	struct static_cntr err_be_mac2_ep;
+	struct static_cntr sc_ep_mac1;
+	struct static_cntr res_ep_mac1;
+	struct static_cntr be_ep_mac1;
+	struct static_cntr err_sc_ep_mac1;
+	struct static_cntr err_res_ep_mac1;
+	struct static_cntr err_be_ep_mac1;
+	struct static_cntr sc_mac2_mac1;
+	struct static_cntr res_mac2_mac1;
+	struct static_cntr be_mac2_mac1;
+	struct static_cntr err_sc_mac2_mac1;
+	struct static_cntr err_res_mac2_mac1;
+	struct static_cntr err_be_mac2_mac1;
+	struct static_cntr sc_ep_mac2;
+	struct static_cntr res_ep_mac2;
+	struct static_cntr be_ep_mac2;
+	struct static_cntr err_sc_ep_mac2;
+	struct static_cntr err_res_ep_mac2;
+	struct static_cntr err_be_ep_mac2;
+	struct static_cntr sc_mac1_mac2;
+	struct static_cntr res_mac1_mac2;
+	struct static_cntr be_mac1_mac2;
+	struct static_cntr err_sc_mac1_mac2;
+	struct static_cntr err_res_mac1_mac2;
+	struct static_cntr err_be_mac1_mac2;
+};
+
+#define XAS_CAM_IPV_EN		BIT(0)
+#define XAS_CAM_EP_MGMTQ_EN	BIT(1)
+#define XAS_CAM_VALID		BIT(2)
+
+/* CAM structure */
+struct cam_struct {
+	u8 src_addr[6];
+	u8 dest_addr[6];
+	u16 vlanid;
+	u16 tv_vlanid;
+	u8 fwd_port;
+	u8 gate_id;
+	u8 ipv;
+	u32 flags;
+	u8 ep_port_act;
+	u8 mac_port_act;
+};
+
+/*Frame Filtering Type Field Option */
+struct ff_type {
+	u16 type1;
+	u16 type2;
+};
+
+/* TODO Fix holes in this structure and corresponding TSN switch_prog app */
+struct mac_addr_learn {
+	bool aging;
+	bool is_age;
+	bool learning;
+	bool is_learn;
+	bool learn_untag;
+	bool is_untag;
+	u32 aging_time;
+};
+
+struct mac_learnt {
+	u8 mac_addr[6];
+	u16 vlan_id;
+};
+
+#define MAX_NUM_MAC_ENTRIES	2048
+struct mac_addr_list {
+	u8 port_num;
+	u16 num_list;
+	struct mac_learnt list[MAX_NUM_MAC_ENTRIES];
+};
+
+struct port_status {
+	u8 port_num;
+	u8 port_status;
+};
+
+struct port_vlan {
+	bool aging;
+	bool is_age;
+	bool learning;
+	bool is_learn;
+	bool is_mgmtq;
+	bool en_ipv;
+	bool en_port_status;
+	u8 mgmt_ext_id;
+	u8 port_num;
+	u8 ipv;
+	u8 port_status;
+	u16 vlan_id;
+	u32 aging_time;
+};
+
+struct native_vlan {
+	bool en_ipv;
+	u8 port_num;
+	u8 ipv;
+	u16 vlan_id;
+};
+
+enum switch_port {
+	PORT_EP = 1,
+	PORT_MAC1 = 2,
+	PORT_MAC2 = 4,
+	PORT_EX_ONLY = 8,
+	PORT_EX_EP = 16,
+};
+
+/* Core switch structure*/
+/* TODO Fix holes in this structure and corresponding TSN switch_prog app */
+struct switch_data {
+	u32 switch_status;
+	u32 switch_ctrl;
+	u32 switch_prt;
+	u8 sw_mac_addr[6];
+	/*0 - schedule, 1 - reserved, 2 - best effort queue*/
+	struct thershold thld_ep_mac[3];
+	struct thershold thld_mac_mac[3];
+	u32 ep_vlan;
+	u32 mac_vlan;
+	u32 max_frame_sc_que;
+	u32 max_frame_res_que;
+	u32 max_frame_be_que;
+	/* Memory counters */
+	struct mem_static_arr_cntr mem_arr_cnt;
+	/* CAM */
+	struct cam_struct cam_data;
+/* Frame Filtering Type Field Option */
+	struct ff_type typefield;
+/* MAC Port-1 Management Queueing Options */
+	int mac1_config;
+/* MAC Port-2 Management Queueing Options */
+	int mac2_config;
+/* Port VLAN Membership Registers */
+	int port_vlan_mem_ctrl;
+	char port_vlan_mem_data;
+};
+
+/********* Switch Structures ends ***********/
+
+extern struct axienet_local lp;
+
+/********* qci function declararions ********/
+void psfp_control(struct psfp_config data);
+void config_stream_filter(struct stream_filter data);
+void program_meter_reg(struct meter_config data);
+void get_psfp_static_counter(struct psfp_static_counter *data);
+void get_meter_reg(struct meter_config *data);
+void get_stream_filter_config(struct stream_filter *data);
+
+/********* cb function declararions ********/
+void frer_control(struct frer_ctrl data);
+void get_ingress_filter_config(struct in_fltr *data);
+void config_ingress_filter(struct cb data);
+void get_member_reg(struct frer_memb_config *data);
+void program_member_reg(struct cb data);
+void get_frer_static_counter(struct frer_static_counter *data);
+int tsn_switch_cam_set(struct cam_struct data, u8 add);
+u8 *tsn_switch_get_id(void);
+int tsn_switch_set_stp_state(struct port_status *port);
+int tsn_switch_vlan_add(struct port_vlan *port, int add);
+int tsn_switch_pvid_get(struct native_vlan *port);
+int tsn_switch_pvid_add(struct native_vlan *port);
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+int xlnx_switchdev_init(void);
+void xlnx_switchdev_remove(void);
+#endif
+#endif /* XILINX_TSN_SWITCH_H */
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_switchdev.c b/drivers/staging/xilinx-tsn/xilinx_tsn_switchdev.c
new file mode 100644
index 000000000..6ac053ac5
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_switchdev.c
@@ -0,0 +1,392 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx TSN Switch Device support driver
+ *
+ * Copyright (c) 2022 Xilinx, Inc. All rights reserved.
+ */
+#if IS_ENABLED(CONFIG_XILINX_TSN_SWITCH)
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/if_bridge.h>
+#include <net/switchdev.h>
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+
+#define tsn_to_linux_sw_state(s) \
+	(((s) == BR_STATE_DISABLED)   ? TSN_SW_STATE_DISABLED : \
+	 ((s) == BR_STATE_BLOCKING)   ? TSN_SW_STATE_BLOCKING : \
+	 ((s) == BR_STATE_LISTENING)  ? TSN_SW_STATE_LISTENING : \
+	 ((s) == BR_STATE_LEARNING)   ? TSN_SW_STATE_LEARNING : \
+	 ((s) == BR_STATE_FORWARDING) ? TSN_SW_STATE_FORWARDING : \
+	 TSN_SW_STATE_DISABLED)
+
+#define stp_state_string(s) \
+	(((s) == BR_STATE_DISABLED)   ? "disabled" : \
+	 ((s) == BR_STATE_BLOCKING)   ? "blocking" : \
+	 ((s) == BR_STATE_LISTENING)  ? "listening" : \
+	 ((s) == BR_STATE_LEARNING)   ? "learning" : \
+	 ((s) == BR_STATE_FORWARDING) ? "forwarding" : \
+	 "und_blocked")
+
+#define TSN_SW_STATE_DISABLED		0
+#define TSN_SW_STATE_BLOCKING		1
+#define TSN_SW_STATE_LISTENING		2
+#define TSN_SW_STATE_LEARNING		3
+#define TSN_SW_STATE_FORWARDING		4
+#define TSN_SW_STATE_FLUSH		5
+
+static struct workqueue_struct *xlnx_sw_owq;
+static int
+xlnx_switchdev_port_attr_set_event(struct net_device *netdev,
+				   struct switchdev_notifier_port_attr_info *port_attr_info);
+
+static int xlnx_switch_fdb_set(struct axienet_local *lp,
+			       struct switchdev_notifier_fdb_info *fdb_info,
+			       bool adding)
+{
+	struct cam_struct data;
+
+	memset(&data, 0, sizeof(struct cam_struct));
+	data.fwd_port = lp->switch_prt;
+	ether_addr_copy((u8 *)&data.dest_addr, fdb_info->addr);
+	data.vlanid = fdb_info->vid;
+
+	return tsn_switch_cam_set(data, adding);
+}
+
+struct xlnx_switchdev_event_work {
+	struct work_struct work;
+	struct switchdev_notifier_fdb_info fdb_info;
+	struct axienet_local *lp;
+	unsigned long event;
+};
+
+static void xlnx_sw_fdb_offload_notify(struct axienet_local *lp,
+				       struct switchdev_notifier_fdb_info *recv_info)
+{
+	struct switchdev_notifier_fdb_info info;
+
+	info.addr = recv_info->addr;
+	info.vid = recv_info->vid;
+	call_switchdev_notifiers(SWITCHDEV_FDB_OFFLOADED,
+				 lp->ndev, &info.info, NULL);
+}
+
+static int xlnx_sw_port_obj_vlan_add(struct axienet_local *lp,
+				     const struct switchdev_obj_port_vlan *vlan)
+{
+	struct port_vlan pvl;
+	struct native_vlan nvl;
+	bool flag_pvid = vlan->flags & BRIDGE_VLAN_INFO_PVID;
+	u16 vid;
+	int err;
+
+	memset(&nvl, 0, sizeof(struct native_vlan));
+	memset(&pvl, 0, sizeof(struct port_vlan));
+
+	pvl.port_num = lp->switch_prt;
+	nvl.port_num = lp->switch_prt;
+
+	/* TODO deal with vlan->flags for PVID and untagged */
+	vid = vlan->vid;
+	if (flag_pvid) {
+		nvl.vlan_id = vid;
+		err = tsn_switch_pvid_add(&nvl);
+	} else {
+		pvl.vlan_id = vid;
+		err = tsn_switch_vlan_add(&pvl, true);
+	}
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int xlnx_sw_port_obj_vlan_del(struct axienet_local *lp,
+				     const struct switchdev_obj_port_vlan *vlan)
+{
+	struct port_vlan pvl;
+	struct native_vlan nvl;
+	u16 vid;
+	int err;
+
+	memset(&nvl, 0, sizeof(struct native_vlan));
+	memset(&pvl, 0, sizeof(struct port_vlan));
+
+	pvl.port_num = lp->switch_prt;
+	nvl.port_num = lp->switch_prt;
+
+	tsn_switch_pvid_get(&nvl);
+
+	vid = vlan->vid;
+	if (vid == nvl.vlan_id) {
+		nvl.vlan_id = 1;
+		err = tsn_switch_pvid_add(&nvl);
+	} else {
+		pvl.vlan_id = vid;
+		err = tsn_switch_vlan_add(&pvl, false);
+	}
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int xlnx_sw_obj_add(struct net_device *ndev,
+			   const struct switchdev_obj *obj)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int err = 0;
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = xlnx_sw_port_obj_vlan_add(lp,
+						SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static int xlnx_sw_obj_del(struct net_device *ndev,
+			   const struct switchdev_obj *obj)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int err = 0;
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = xlnx_sw_port_obj_vlan_del(lp,
+						SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static void xlnx_switchdev_event_work(struct work_struct *work)
+{
+	struct xlnx_switchdev_event_work *switchdev_work =
+		container_of(work, struct xlnx_switchdev_event_work, work);
+	struct axienet_local *lp = switchdev_work->lp;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	int err;
+
+	rtnl_lock();
+	switch (switchdev_work->event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		err = xlnx_switch_fdb_set(lp, fdb_info, true);
+		if (err) {
+			netdev_dbg(lp->ndev, "fdb add failed err=%d\n", err);
+			break;
+		}
+		xlnx_sw_fdb_offload_notify(lp, fdb_info);
+		break;
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		err = xlnx_switch_fdb_set(lp, fdb_info, false);
+		if (err) {
+			netdev_dbg(lp->ndev, "fdb add failed err=%d\n", err);
+			break;
+		}
+		break;
+	}
+	rtnl_unlock();
+
+	kfree(switchdev_work->fdb_info.addr);
+	kfree(switchdev_work);
+	dev_put(lp->ndev);
+}
+
+static bool xlnx_switch_is_dev_valid(const struct net_device *ndev)
+{
+	return xlnx_is_port_ep_netdev(ndev) ||
+		xlnx_is_port_ep_ex_netdev(ndev) ||
+		xlnx_is_port_temac_netdev(ndev);
+}
+
+static int xlnx_switchdev_event(struct notifier_block *unused,
+				unsigned long event, void *ptr)
+{
+	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+	struct switchdev_notifier_fdb_info *fdb_info = ptr;
+	struct xlnx_switchdev_event_work *switchdev_work;
+	struct axienet_local *lp;
+	struct net_device *upper;
+
+	upper = netdev_master_upper_dev_get_rcu(dev);
+	if (!upper)
+		return NOTIFY_DONE;
+
+	if (!netif_is_bridge_master(upper))
+		return NOTIFY_DONE;
+
+	if (!xlnx_switch_is_dev_valid(dev))
+		return NOTIFY_DONE;
+
+	lp = netdev_priv(dev);
+	switchdev_work = kzalloc(sizeof(*switchdev_work), GFP_ATOMIC);
+	if (!switchdev_work)
+		return NOTIFY_BAD;
+
+	if (event == SWITCHDEV_PORT_ATTR_SET)
+		return xlnx_switchdev_port_attr_set_event(dev, ptr);
+
+	INIT_WORK(&switchdev_work->work, xlnx_switchdev_event_work);
+	switchdev_work->lp = lp;
+	switchdev_work->event = event;
+
+	switch (event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		memcpy(&switchdev_work->fdb_info, ptr,
+		       sizeof(switchdev_work->fdb_info));
+		switchdev_work->fdb_info.addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+		ether_addr_copy((u8 *)switchdev_work->fdb_info.addr,
+				fdb_info->addr);
+		/* take a reference on the switch port dev */
+		dev_hold(dev);
+		break;
+	default:
+		kfree(switchdev_work);
+		return NOTIFY_DONE;
+	}
+
+	queue_work(xlnx_sw_owq, &switchdev_work->work);
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block xlnx_switchdev_notifier = {
+	.notifier_call = xlnx_switchdev_event,
+};
+
+static int
+xlnx_switchdev_port_obj_event(unsigned long event, struct net_device *netdev,
+			      struct switchdev_notifier_port_obj_info *port_obj_info)
+{
+	int err = -EOPNOTSUPP;
+
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+		err = xlnx_sw_obj_add(netdev, port_obj_info->obj);
+		break;
+	case SWITCHDEV_PORT_OBJ_DEL:
+		err = xlnx_sw_obj_del(netdev, port_obj_info->obj);
+		break;
+	}
+
+	port_obj_info->handled = true;
+
+	return notifier_from_errno(err);
+}
+
+static int xlnx_switchdev_blocking_event(struct notifier_block *unused, unsigned long event,
+					 void *ptr)
+{
+	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+	case SWITCHDEV_PORT_OBJ_DEL:
+		return xlnx_switchdev_port_obj_event(event, dev, ptr);
+	case SWITCHDEV_PORT_ATTR_SET:
+		return xlnx_switchdev_port_attr_set_event(dev, ptr);
+	}
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block xlnx_switchdev_blocking_notifier = {
+	.notifier_call = xlnx_switchdev_blocking_event,
+};
+
+static int xlnx_sw_port_attr_stp_state_set(struct axienet_local *lp, u8 state)
+{
+	struct port_status ps;
+
+	ps.port_num = lp->switch_prt;
+	ps.port_status = tsn_to_linux_sw_state(state);
+
+	return tsn_switch_set_stp_state(&ps);
+}
+
+static int xlnx_sw_port_attr_pre_bridge_flags_set(struct axienet_local *lp,
+						  struct switchdev_brport_flags brport_flags)
+{
+	if (brport_flags.mask & ~(BR_LEARNING | BR_FLOOD))
+		return -EINVAL;
+
+	return 0;
+}
+
+static int xlnx_sw_attr_set(struct net_device *ndev,
+			    const struct switchdev_attr *attr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int err = 0;
+
+	switch (attr->id) {
+	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
+		err = xlnx_sw_port_attr_stp_state_set(lp,
+						      attr->u.stp_state);
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+		pr_info("received request to SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS: %lu\n",
+			attr->u.brport_flags.val);
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
+		err = xlnx_sw_port_attr_pre_bridge_flags_set(lp, attr->u.brport_flags);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+		break;
+	default:
+		pr_info("%s: unhandled id: %d\n", __func__, attr->id);
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static int
+xlnx_switchdev_port_attr_set_event(struct net_device *netdev,
+				   struct switchdev_notifier_port_attr_info *port_attr_info)
+{
+	int err;
+
+	err = xlnx_sw_attr_set(netdev, port_attr_info->attr);
+
+	port_attr_info->handled = true;
+
+	return notifier_from_errno(err);
+}
+
+int xlnx_switchdev_init(void)
+{
+	xlnx_sw_owq = alloc_ordered_workqueue("%s_ordered", WQ_MEM_RECLAIM,
+					      "xlnx_sw");
+	if (!xlnx_sw_owq)
+		return -ENOMEM;
+	register_switchdev_notifier(&xlnx_switchdev_notifier);
+	register_switchdev_blocking_notifier(&xlnx_switchdev_blocking_notifier);
+
+	return 0;
+}
+
+void xlnx_switchdev_remove(void)
+{
+	destroy_workqueue(xlnx_sw_owq);
+	unregister_switchdev_notifier(&xlnx_switchdev_notifier);
+	unregister_switchdev_blocking_notifier(&xlnx_switchdev_blocking_notifier);
+}
+#endif
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.c b/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.c
new file mode 100644
index 000000000..c80bbd816
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.c
@@ -0,0 +1,854 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx FPGA Xilinx TADMA driver.
+ *
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * Author: Syed Syed <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/list.h>
+#include <linux/hash.h>
+#include <linux/platform_device.h>
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_tadma.h"
+
+/* max packets that can be sent in a time trigger */
+#define MAX_TRIG_COUNT 4
+
+/* This driver assumes the num_streams configured in HW is always 2^n.
+ * TADMA IP has three master AXI4-Stream Interfaces. The names of these three
+ * interfaces are mentioned in the TADMA PG as ST, RES and BE. Although these
+ * names might be misleading since TADMA is used exclusively for ST traffic, we
+ * maintain consistency with the PG by using the same identifiers in the
+ * software, such as qt_st, qt_res, and qt_be
+ */
+
+typedef u32 pm_entry_t;
+
+struct tadma_stream {
+	u8 dmac[6];
+	short vid;
+	u32 trigger;
+	u32 count;
+	u8 qno;
+};
+
+struct tadma_stream_entry {
+	u8 macvlan[8];
+	u32 tticks;
+	struct hlist_node hash_link;
+	int sid;
+	int count;
+	enum qtype qtype;
+};
+
+#define sfm_entry_offset(lp, sid) \
+	((lp->active_sfm == SFM_UPPER) ? \
+	(XTADMA_USFM_OFFSET) +  ((sid) * sizeof(struct sfm_entry)) : \
+	(XTADMA_LSFM_OFFSET) +  ((sid) * sizeof(struct sfm_entry)))
+
+static inline bool tadma_queue_enabled(struct axienet_local *lp, enum qtype qt)
+{
+	return lp->tadma_queues & BIT(qt);
+}
+
+static inline int tadma_qt_to_txq_idx(struct axienet_local *lp, enum qtype qt)
+{
+	int qno;
+
+	for (qno = 0; qno < lp->num_tc; qno++) {
+		if (lp->txqs[qno].is_tadma &&
+		    lp->txqs[qno].dmaq_idx == qt) {
+			return qno;
+		}
+	}
+
+	return -EINVAL;
+}
+
+static inline u32 tadma_macvlan_hash(struct axienet_local *lp,
+				     const unsigned char *addr)
+{
+	u64 value = get_unaligned((u64 *)addr);
+
+	return hash_64(value, lp->tadma_hash_bits);
+}
+
+static inline bool mac_vlan_equal(const u8 addr1[8],
+				  const u8 addr2[8])
+{
+	const u16 *a = (const u16 *)addr1;
+	const u16 *b = (const u16 *)addr2;
+	bool ret = 1;
+
+	if ((a[0] ^ b[0]) | (a[1] ^ b[1]) |
+		(a[2] ^ b[2]) | (a[3] ^ a[3])) {
+		ret = 0;
+	}
+
+	return ret;
+}
+
+static struct tadma_stream_entry *tadma_hash_lookup_stream(struct hlist_head *head,
+							   const unsigned char *mac_vlan)
+{
+	struct tadma_stream_entry *entry;
+
+	hlist_for_each_entry(entry, head, hash_link) {
+		if (mac_vlan_equal(entry->macvlan, mac_vlan))
+			return entry;
+	}
+
+	return NULL;
+}
+
+static u32 tadma_stream_alm_offset_irq(int sid, u32 tx_bd_rd, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 alm_offset;
+
+	alm_offset = XTADMA_ALM_OFFSET +
+		sid * sizeof(struct alm_entry) * lp->num_tadma_buffers;
+
+	return alm_offset + (tx_bd_rd * sizeof(struct alm_entry));
+}
+
+static void tadma_xmit_done(struct net_device *ndev, u8 sid, u32 cnt)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 size = 0, packets = 0;
+
+	spin_lock(&lp->tadma_tx_lock);
+
+	if (lp->tx_bd_head[sid] == lp->tx_bd_tail[sid]) {
+		spin_unlock(&lp->tadma_tx_lock);
+		return;
+	}
+
+	while ((lp->tx_bd_head[sid] != lp->tx_bd_tail[sid]) && cnt) {
+		if (lp->tx_bd[sid][lp->tx_bd_tail[sid]].tx_desc_mapping ==
+		    DESC_DMA_MAP_PAGE) {
+			dma_unmap_page(ndev->dev.parent,
+				       lp->tx_bd[sid][lp->tx_bd_tail[sid]].phys,
+				       lp->tx_bd[sid][lp->tx_bd_tail[sid]].len,
+				       DMA_TO_DEVICE);
+		} else {
+			dma_unmap_single(ndev->dev.parent,
+					 lp->tx_bd[sid][lp->tx_bd_tail[sid]].
+					 phys,
+					 lp->tx_bd[sid][lp->tx_bd_tail[sid]].
+					 len,
+					 DMA_TO_DEVICE);
+		}
+		if (lp->tx_bd[sid][lp->tx_bd_tail[sid]].tx_skb) {
+			dev_kfree_skb_irq(lp->tx_bd[sid][lp->tx_bd_tail[sid]].
+					  tx_skb);
+		}
+
+		size += lp->tx_bd[sid][lp->tx_bd_tail[sid]].len;
+		packets++;
+		lp->tx_bd_tail[sid]++;
+		lp->tx_bd_tail[sid] %= lp->num_tadma_buffers;
+		cnt--;
+	}
+
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += size;
+	spin_unlock(&lp->tadma_tx_lock);
+}
+
+static irqreturn_t tadma_irq(int irq, void *_ndev)
+{
+	struct axienet_local *lp = netdev_priv(_ndev);
+	struct alm_entry alm;
+	u32 status, sid = 0, alm_offset;
+	int cnt = 0;
+
+	status = tadma_ior(lp, XTADMA_INT_STA_OFFSET);
+
+	/* clear interrupt */
+	tadma_iow(lp, XTADMA_INT_CLR_OFFSET, status);
+
+	if (status & XTADMA_FFI_INT_EN) {
+		for (sid = 0; sid < lp->num_streams; sid++) {
+			cnt = 0;
+			spin_lock(&lp->tadma_tx_lock);
+			alm_offset = tadma_stream_alm_offset_irq(sid, lp->tx_bd_rd[sid], _ndev);
+			alm.cfg = tadma_ior(lp, alm_offset + 4);
+			while (((alm.cfg & XTADMA_ALM_UFF) == 0) &&
+			       (cnt < lp->num_tadma_buffers) &&
+			       (lp->tx_bd_rd[sid] != lp->tx_bd_head[sid])) {
+				lp->tx_bd_rd[sid]++;
+				lp->tx_bd_rd[sid] = lp->tx_bd_rd[sid] % lp->num_tadma_buffers;
+				alm_offset = tadma_stream_alm_offset_irq(sid,
+									 lp->tx_bd_rd[sid], _ndev);
+				alm.cfg = tadma_ior(lp, alm_offset + 4);
+				cnt++;
+			}
+			spin_unlock(&lp->tadma_tx_lock);
+			if (cnt) {
+				tadma_xmit_done(_ndev, sid, cnt);
+				if (__netif_subqueue_stopped(_ndev,
+							     lp->sid_txq_idx[sid])) {
+					netif_wake_subqueue(_ndev,
+							    lp->sid_txq_idx[sid]);
+				}
+			}
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+static int tadma_sfm_hash_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_cb *cb;
+	int i;
+
+	cb = kzalloc(sizeof(*cb), GFP_KERNEL);
+	if (!cb)
+		return -ENOMEM;
+
+	cb->stream_hash = kcalloc(lp->num_entries, sizeof(struct hlist_head *),
+				  GFP_KERNEL);
+
+	if (!cb->stream_hash) {
+		kfree(cb);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < lp->num_entries; i++)
+		INIT_HLIST_HEAD(&cb->stream_hash[i]);
+
+	lp->t_cb = cb;
+
+	return 0;
+}
+
+static int tadma_sfm_program(struct net_device *ndev, int sid,
+			     enum qtype qtype, u32 tticks, u32 count)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct sfm_entry sfm = {0, };
+	int txq_idx;
+	u32 offset;
+
+	pr_debug("%s sid: %d, qtype %d, count: %d\n", __func__, sid, qtype,
+		 count);
+	txq_idx = tadma_qt_to_txq_idx(lp, qtype);
+	if (txq_idx < 0) {
+		dev_err(&ndev->dev, "Failed to get txq for TADMA qtype %d\n",
+			qtype);
+		return -EINVAL;
+	}
+
+	lp->sid_txq_idx[sid] = txq_idx;
+
+	offset = sfm_entry_offset(lp, sid);
+
+	/* each tick is 8ns */
+	sfm.tticks = tticks / 8;
+
+	/*clear strid and queue type */
+	sfm.cfg &= ~(XTADMA_STR_ID_MASK | XTADMA_STR_QUE_TYPE_MASK);
+
+	sfm.cfg |= (sid << XTADMA_STR_ID_SHIFT) & XTADMA_STR_ID_MASK;
+	sfm.cfg |= (qtype << XTADMA_STR_QUE_TYPE_SHIFT) &
+		   XTADMA_STR_QUE_TYPE_MASK;
+	if (count != 0)
+		sfm.cfg &= ~XTADMA_STR_CONT_FETCH_EN;
+	else
+		sfm.cfg |= XTADMA_STR_CONT_FETCH_EN;
+	sfm.cfg |= XTADMA_STR_ENTRY_VALID;
+
+	count  = (count > 0) ? (count - 1) : count;
+	/* hw xmits 1 more than what is programmed, so use count */
+	sfm.cfg |= (count << XTADMA_STR_NUM_FRM_SHIFT) &
+			XTADMA_STR_NUM_FRM_MASK;
+	pr_debug("sfm cfg: %x\n", sfm.cfg);
+	tadma_iow(lp, offset, sfm.tticks);
+	tadma_iow(lp, offset + 4, sfm.cfg);
+	return 0;
+}
+
+static int tadma_set_contiguous_mode(struct net_device *ndev, enum qtype qtype)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int sid, ret;
+
+	if (lp->get_sid >= lp->num_streams - 1) {
+		dev_info(&ndev->dev, "Can't support more than %d streams\n",
+			 lp->get_sid + 1);
+		return -EINVAL;
+	}
+
+	if (lp->get_sfm >= lp->num_entries) {
+		dev_info(&ndev->dev, "Can't support more than %d SFM entries\n",
+			 lp->get_sfm);
+		return -EINVAL;
+	}
+
+	sid = lp->get_sid++;
+	lp->get_sfm++;
+	ret = tadma_sfm_program(ndev, sid, qtype, NSEC_PER_MSEC, 0);
+	if (ret)
+		return ret;
+
+	return sid;
+}
+
+static int tadma_set_contiguous_mode_all(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (tadma_queue_enabled(lp, qt_res)) {
+		lp->default_res_sid = tadma_set_contiguous_mode(ndev, qt_res);
+		if (lp->default_res_sid < 0)
+			return lp->default_res_sid;
+	}
+
+	if (tadma_queue_enabled(lp, qt_st)) {
+		lp->default_st_sid = tadma_set_contiguous_mode(ndev, qt_st);
+		if (lp->default_st_sid < 0)
+			return lp->default_st_sid;
+	}
+
+	return 0;
+}
+
+static int tadma_sfm_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	lp->active_sfm = SFM_UPPER;
+	lp->get_sid = 0;
+	lp->get_sfm = 0;
+	return tadma_set_contiguous_mode_all(ndev);
+}
+
+int axienet_tadma_stop(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u8 i = 0;
+
+	for (i = 0; i < lp->num_streams ; i++)
+		kfree(lp->tx_bd[i]);
+
+	kfree(lp->tx_bd);
+
+	free_irq(lp->tadma_irq, ndev);
+
+	return 0;
+}
+
+int axienet_tadma_open(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	static char irq_name[32];
+	u8 i = 0;
+	int ret;
+	u32 cr;
+
+	if (lp->tadma_irq) {
+		snprintf(irq_name, sizeof(irq_name), "%s_tadma_tx", ndev->name);
+		ret = request_irq(lp->tadma_irq, tadma_irq, IRQF_SHARED,
+				  irq_name, ndev);
+		if (ret)
+			return ret;
+	}
+	pr_debug("%s TADMA irq %d\n", __func__, lp->tadma_irq);
+
+	/* enable all interrupts */
+	tadma_iow(lp, XTADMA_INT_EN_OFFSET, XTADMA_FFI_INT_EN |
+		  XTADMA_IE_INT_EN);
+
+	tadma_sfm_init(ndev);
+	tadma_sfm_hash_init(ndev);
+	cr = XTADMA_CFG_DONE;
+	tadma_iow(lp, XTADMA_CR_OFFSET, cr);
+
+	lp->tx_bd = kmalloc_array(lp->num_streams, sizeof(struct axitadma_bd *),
+				  GFP_KERNEL);
+	if (!lp->tx_bd)
+		ret = -ENOMEM;
+
+	for (i = 0; i < lp->num_streams ; i++) {
+		lp->tx_bd_head[i] = 0;
+		lp->tx_bd_tail[i] = 0;
+		lp->tx_bd_rd[i] = 0;
+		lp->tx_bd[i] = kmalloc_array(lp->num_tadma_buffers,
+					     sizeof(*lp->tx_bd[i]), GFP_KERNEL);
+		if (!lp->tx_bd[i])
+			return -ENOMEM;
+	}
+
+	return ret;
+}
+
+/*TODO: Fix TADMA probe error handling path */
+int __maybe_unused axienet_tadma_probe(struct platform_device *pdev,
+				       struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct resource tadma_res;
+	struct device_node *np;
+	u8 count = 0;
+	u16 num_tc;
+	int ret;
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &num_tc);
+	if (ret) {
+		dev_err(&pdev->dev, "xlnx,num-tc parameter not defined\n");
+		return ret;
+	}
+
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected-tx",
+			      num_tc - 1);
+	if (!np) {
+		dev_err(&pdev->dev, "could not find TA-DMA node\n");
+		return -ENODEV;
+	}
+
+	ret = of_address_to_resource(np, 0, &tadma_res);
+	if (ret)
+		goto err_put_node;
+
+	lp->tadma_regs = devm_ioremap_resource(&pdev->dev, &tadma_res);
+	if (IS_ERR(lp->tadma_regs)) {
+		ret = PTR_ERR(lp->tadma_regs);
+		goto err_put_node;
+	}
+
+	lp->tadma_irq = irq_of_parse_and_map(np, 0);
+	if (!lp->tadma_irq) {
+		ret = -EINVAL;
+		goto err_put_node;
+	}
+
+	ret = of_property_read_u32(np, "xlnx,num-buffers-per-stream",
+				   &lp->num_tadma_buffers);
+	if (ret)
+		lp->num_tadma_buffers = 64;
+
+	ret = of_property_read_u32(np, "xlnx,num-streams", &lp->num_streams);
+	if (ret)
+		lp->num_streams = 8;
+	ret = of_property_read_u32(np, "xlnx,num-fetch-entries",
+				   &lp->num_entries);
+	if (ret)
+		lp->num_entries = 8;
+
+	while (!((lp->num_streams >> count) & 1))
+		count++;
+
+	lp->get_sid = 0;
+	lp->get_sfm = 0;
+	lp->default_res_sid = -1;
+	lp->default_st_sid = -1;
+	lp->tadma_hash_bits = count;
+	pr_debug("%s num_stream: %d hash_bits: %d\n", __func__, lp->num_streams,
+		 lp->tadma_hash_bits);
+	pr_info("TADMA probe done\n");
+	spin_lock_init(&lp->tadma_tx_lock);
+	of_node_put(np);
+
+	return 0;
+err_put_node:
+	of_node_put(np);
+	return ret;
+}
+
+static int axienet_check_pm_space(int sid, int num_frag,
+				  u32 wr, u32 rd, int total)
+{
+	int avail;
+
+	avail = rd - wr;
+
+	if (avail < 0)
+		avail = total + avail;
+
+	return (avail >= num_frag);
+}
+
+static int tadma_get_strid(struct sk_buff *skb,
+			   struct net_device *ndev,
+			   enum qtype qt)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_cb *cb = lp->t_cb;
+	struct tadma_stream_entry *entry;
+	struct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)skb->data;
+	int sid = -1; /* BE entry is always 0 */
+	u32 idx;
+	u16 vlan_tci;
+	u8 mac_vlan[8];
+
+	memcpy(mac_vlan, vhdr->h_dest, 6);
+
+	vlan_tci = ntohs(vhdr->h_vlan_TCI);
+
+	mac_vlan[6] = (vlan_tci >> 8) & 0x0f;
+	mac_vlan[7] = (vlan_tci & 0xff);
+	if (qt == qt_st && lp->default_st_sid >= 0)
+		return lp->default_st_sid;
+	else if (qt == qt_res && lp->default_res_sid >= 0)
+		return lp->default_res_sid;
+
+	idx = tadma_macvlan_hash(lp, mac_vlan);
+	entry = tadma_hash_lookup_stream(&cb->stream_hash[idx],
+					 mac_vlan);
+	if (entry)
+		return entry->sid;
+
+	return sid;
+}
+
+static u32 tadma_stream_alm_offset(int sid, u32 wr, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 alm_offset;
+
+	alm_offset = XTADMA_ALM_OFFSET +
+		sid * sizeof(struct alm_entry) * lp->num_tadma_buffers;
+
+	wr = (wr + lp->num_tadma_buffers - 1) & (lp->num_tadma_buffers - 1);
+
+	return alm_offset + (wr * sizeof(struct alm_entry));
+}
+
+int axienet_tadma_xmit(struct sk_buff *skb, struct net_device *ndev,
+		       u16 queue_type)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct alm_entry alm, alm_fframe = {0};
+	dma_addr_t phys_addr;
+	pm_entry_t pm;
+	u32 num_frag, len, tot_len, alm_offset, alm_offset_fframe, write_p, read_p;
+	unsigned long flags;
+	int sid, ii, tot_sz8;
+	static int chk_ptr;
+
+	/* fetch stream ID */
+	sid = tadma_get_strid(skb, ndev, lp->txqs[queue_type].dmaq_idx);
+
+	if (sid < 0) {
+		dev_kfree_skb_irq(skb);
+		return NETDEV_TX_OK;
+	}
+	num_frag = skb_shinfo(skb)->nr_frags;
+
+	spin_lock_irqsave(&lp->tadma_tx_lock, flags);
+	pm = tadma_ior(lp, XTADMA_PM_OFFSET + (sid * sizeof(pm_entry_t)));
+
+	read_p  = pm & XTADMA_PM_RD_MASK;
+	write_p = (pm & XTADMA_PM_WR_MASK) >> XTADMA_PM_WR_SHIFT;
+
+	if (!axienet_check_pm_space(sid, num_frag + 1, write_p, read_p,
+				    lp->num_tadma_buffers)) {
+		if (!chk_ptr) {
+			pr_err("%s NO SPACE rd: %x wd: %x\n", __func__, read_p,
+			       write_p);
+			chk_ptr = 1;
+		}
+
+		netif_stop_subqueue(ndev, queue_type);
+		spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+		return NETDEV_TX_BUSY;
+	}
+	if (((lp->tx_bd_head[sid] + (num_frag + 1)) % lp->num_tadma_buffers) ==
+	    lp->tx_bd_tail[sid]) {
+		netif_stop_subqueue(ndev, queue_type);
+		spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+		return NETDEV_TX_BUSY;
+	}
+
+	/* get current alm offset */
+	alm_offset_fframe = tadma_stream_alm_offset(sid, write_p, ndev);
+
+	pr_debug("%d: num_frag: %d len: %d\n", sid, num_frag,
+		 skb_headlen(skb));
+	pr_debug("w:%d r:%d\n", write_p, read_p);
+
+	tot_len = skb_headlen(skb);
+	len = skb_headlen(skb);
+	phys_addr = dma_map_single(ndev->dev.parent, skb->data,
+				   len, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(ndev->dev.parent, phys_addr))) {
+		dev_err(&ndev->dev, "tadma map error\n");
+		spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+		return NETDEV_TX_BUSY;
+	}
+
+	alm_fframe.addr = (u32)phys_addr;
+
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+	alm_fframe.cfg |= (u32)(phys_addr >> 32);
+#endif
+
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].num_frag = num_frag + 1;
+	if (num_frag == 0) {
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb = skb;
+		alm_fframe.cfg |= XTADMA_ALM_SOP | XTADMA_ALM_EOP;
+	} else {
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb = 0;
+		alm_fframe.cfg |= XTADMA_ALM_SOP;
+	}
+	alm_fframe.cfg &= ~XTADMA_ALM_FETCH_SZ_MASK;
+	alm_fframe.cfg |= ((len << XTADMA_ALM_FETCH_SZ_SHIFT) &
+			   XTADMA_ALM_FETCH_SZ_MASK);
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].phys = phys_addr;
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].len = len;
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_desc_mapping =
+							DESC_DMA_MAP_SINGLE;
+	lp->tx_bd_head[sid]++;
+	lp->tx_bd_head[sid] %= lp->num_tadma_buffers;
+
+	for (ii = 0; ii < num_frag; ii++) {
+		skb_frag_t *frag;
+
+		frag = &skb_shinfo(skb)->frags[ii];
+		len = skb_frag_size(frag);
+		tot_len += len;
+		phys_addr = skb_frag_dma_map(ndev->dev.parent, frag, 0,
+					     len, DMA_TO_DEVICE);
+		memset(&alm, 0, sizeof(struct alm_entry));
+		alm.addr = (u32)phys_addr;
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+		alm.cfg |= (u32)(phys_addr >> 32);
+#endif
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb = 0;
+		if (ii == (num_frag - 1)) {
+			alm.cfg |= XTADMA_ALM_EOP;
+			lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb = skb;
+		}
+		alm.cfg &= ~XTADMA_ALM_FETCH_SZ_MASK;
+		alm.cfg |= ((len << XTADMA_ALM_FETCH_SZ_SHIFT) &
+				XTADMA_ALM_FETCH_SZ_MASK);
+		alm.cfg |= XTADMA_ALM_UFF;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].num_frag = 0;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].phys = phys_addr;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].len = len;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_desc_mapping =
+							DESC_DMA_MAP_PAGE;
+		lp->tx_bd_head[sid]++;
+		lp->tx_bd_head[sid] %= lp->num_tadma_buffers;
+
+		/* increment write */
+		write_p = (write_p + 1) & (lp->num_tadma_buffers - 1);
+		/* get current alm offset */
+		alm_offset = tadma_stream_alm_offset(sid, write_p, ndev);
+
+		tadma_iow(lp, alm_offset, alm.addr);
+		tadma_iow(lp, alm_offset + 4, alm.cfg);
+	}
+	tot_sz8 = tot_len / 8 + 1;
+	alm_fframe.cfg &= ~XTADMA_ALM_TOT_PKT_SZ_BY8_MASK;
+	alm_fframe.cfg |= ((tot_sz8 << XTADMA_ALM_TOT_PKT_SZ_BY8_SHIFT) &
+			  XTADMA_ALM_TOT_PKT_SZ_BY8_MASK);
+	alm_fframe.cfg |= XTADMA_ALM_UFF;
+
+	tadma_iow(lp, alm_offset_fframe, alm_fframe.addr);
+	tadma_iow(lp, alm_offset_fframe + 4, alm_fframe.cfg);
+
+	/* increment write */
+	write_p = (write_p + 1) & (lp->num_tadma_buffers - 1);
+
+	pm &= ~XTADMA_PM_WR_MASK;
+	pm |= (write_p << XTADMA_PM_WR_SHIFT);
+
+	tadma_iow(lp, (XTADMA_PM_OFFSET + (sid * sizeof(pm_entry_t))), pm);
+	spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+
+	return NETDEV_TX_OK;
+}
+
+int axienet_tadma_program(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_stream_entry *entry;
+	struct tadma_cb *cb = lp->t_cb;
+	struct hlist_head *bucket;
+	bool res_enabled = false;
+	bool st_enabled = false;
+	struct hlist_node *tmp;
+	u32 cr, hash = 0;
+
+	for (hash = 0; hash < lp->num_entries; hash++) {
+		int ret;
+
+		bucket = &cb->stream_hash[hash];
+		hlist_for_each_entry_safe(entry, tmp, bucket, hash_link) {
+			ret = tadma_sfm_program(ndev, entry->sid, entry->qtype,
+						entry->tticks, entry->count);
+			if (ret)
+				return ret;
+
+			if (entry->qtype == qt_st) {
+				lp->default_st_sid = -1;
+				st_enabled = true;
+			} else if (entry->qtype == qt_res) {
+				lp->default_res_sid = -1;
+				res_enabled = true;
+			}
+		}
+	}
+
+	if (!res_enabled && tadma_queue_enabled(lp, qt_res))
+		lp->default_res_sid = tadma_set_contiguous_mode(ndev, qt_res);
+
+	if (!st_enabled && tadma_queue_enabled(lp, qt_st))
+		lp->default_st_sid = tadma_set_contiguous_mode(ndev, qt_st);
+
+	/* flip memory first so access other sfm bank
+	 * cr = tadma_ior(lp, XTADMA_CR_OFFSET);
+	 * cr |= XTADMA_FLIP_FETCH_MEM;
+	 * tadma_iow(lp, XTADMA_CR_OFFSET, cr);
+	 */
+
+	/* re-enable interrupts */
+	tadma_iow(lp, XTADMA_INT_EN_OFFSET, XTADMA_FFI_INT_EN |
+		  XTADMA_IE_INT_EN);
+	/* enable schedule */
+	cr = XTADMA_CFG_DONE | XTADMA_SCHED_ENABLE;
+	tadma_iow(lp, XTADMA_CR_OFFSET, cr);
+
+	return 0;
+}
+
+int axienet_tadma_off(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int ret;
+
+	tadma_iow(lp, XTADMA_INT_EN_OFFSET, XTADMA_FFI_INT_EN |
+		  XTADMA_IE_INT_EN);
+	ret = tadma_sfm_init(ndev);
+	tadma_iow(lp, XTADMA_CR_OFFSET, XTADMA_CFG_DONE);
+	return ret;
+}
+
+int axienet_tadma_flush_stream(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_cb *cb = lp->t_cb;
+	struct tadma_stream_entry *entry;
+	struct hlist_head *bucket;
+	struct hlist_node *tmp;
+	u32 offset;
+	int hash;
+
+	lp->get_sid = 0;
+	lp->get_sfm = 0;
+	/* set CFG_DONE to 0 */
+	tadma_iow(lp, XTADMA_CR_OFFSET, 0);
+
+	for (hash = 0; hash < lp->num_entries; hash++) {
+		offset = sfm_entry_offset(lp, hash);
+		tadma_iow(lp, offset, 0);
+		tadma_iow(lp, offset + 4, 0);
+
+		bucket = &cb->stream_hash[hash];
+		hlist_for_each_entry_safe(entry, tmp, bucket, hash_link) {
+			hlist_del(&entry->hash_link);
+			kfree(entry);
+		}
+	}
+
+	return 0;
+}
+
+int axienet_tadma_add_stream(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_stream_entry *entry;
+	struct tadma_cb *cb = lp->t_cb;
+	struct tadma_stream stream;
+	enum qtype qtype;
+	u8 mac_vlan[8];
+	u32 idx, sid;
+	u16 vlan_tci;
+
+	if (copy_from_user(&stream, useraddr, sizeof(struct tadma_stream)))
+		return -EFAULT;
+
+	if (stream.count > MAX_TRIG_COUNT)
+		return -EINVAL;
+
+	if (lp->num_tc <= XAE_MAX_LEGACY_TSN_TC) {
+		dev_dbg(&ndev->dev, "Legacy design with single ST queue\n");
+		qtype = qt_st;
+	} else {
+		if (stream.qno >= lp->num_tc)
+			return -EINVAL;
+
+		if (!lp->txqs[stream.qno].is_tadma) {
+			dev_err(&ndev->dev,
+				"Queue %d is not an ST traffic queue\n",
+				stream.qno);
+			return -EINVAL;
+		}
+
+		qtype = lp->txqs[stream.qno].dmaq_idx;
+	}
+
+	memcpy(mac_vlan, stream.dmac, 6);
+
+	vlan_tci = stream.vid & VLAN_VID_MASK;
+	mac_vlan[6] = (vlan_tci >> 8) & 0x0f;
+	mac_vlan[7] = (vlan_tci & 0xff);
+
+	idx = tadma_macvlan_hash(lp, mac_vlan);
+
+	entry = tadma_hash_lookup_stream(&cb->stream_hash[idx], mac_vlan);
+	if (entry && entry->count == stream.count &&
+	    entry->tticks == stream.trigger) {
+		return -EEXIST;	/*same entry*/
+	}
+
+	if (entry)
+		sid = entry->sid;	/*same sid diff entry*/
+	else
+		sid = lp->get_sid++;
+
+	if (sid >= lp->num_streams) {
+		pr_err("More no. of streams %d\n", sid);
+		return -EINVAL;
+	}
+
+	if (lp->get_sfm >= lp->num_entries) {
+		pr_err("\nMore no. of entries %d\n", lp->get_sfm + 1);
+		return -EINVAL;
+	}
+
+	lp->get_sfm++;
+	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
+
+	entry->tticks = stream.trigger;
+	entry->count = stream.count;
+	entry->sid = sid;
+	entry->qtype = qtype;
+	memcpy(entry->macvlan, mac_vlan, 8);
+
+	pr_debug("%s sid: %d, qtype %d\n", __func__, sid, entry->qtype);
+	hlist_add_head(&entry->hash_link, &cb->stream_hash[idx]);
+
+	return 0;
+}
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.h b/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.h
new file mode 100644
index 000000000..35fca9354
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.h
@@ -0,0 +1,132 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Definitions for TSN TADMA implementation
+ */
+#ifndef _XTADMA_TSN_H
+#define _XTADMA_TSN_H
+
+/* upper&lower stream fetch memory offsets */
+#define XTADMA_USFM_OFFSET	0x1000
+#define XTADMA_LSFM_OFFSET	0x2000
+
+/* pointers memory offset */
+#define XTADMA_PM_OFFSET	0x3000
+#define XTADMA_PM_RD_MASK	0xFF
+#define XTADMA_PM_WR_MASK	0xFF0000
+#define XTADMA_PM_WR_SHIFT	16
+
+/* Address length memory offset */
+#define XTADMA_ALM_OFFSET	0x40000
+
+#define XTADMA_CR_OFFSET		0x0
+#define XTADMA_TO_OFFSET		0x4
+#define XTADMA_FF_THRE_OFFSET		0x8
+#define XTADMA_STR_ID_OFFSET		0xC
+#define XTADMA_INT_EN_OFFSET		0x10
+#define XTADMA_INT_STA_OFFSET		0x14
+#define XTADMA_INT_CLR_OFFSET		0x18
+#define XTADMA_EDI_FFI_STAT_OFFSET	0x20
+#define XTADMA_NRDFI_FNDI_STAT_OFFSET	0x24
+#define XTADMA_BEI_STNSI_STAT_OFFSET	0x28
+#define XTADMA_BENSI_RESNSI_STAT_OFFSET	0x2C
+#define XTADMA_SEI_DEI_STAT_OFFSET	0x30
+#define XTADMA_IEI_STAT_OFFSET		0x34
+
+#define XTADMA_HALTED		BIT(5)
+#define XTADMA_SCHED_ENABLE	BIT(4)
+#define XTADMA_FLIP_FETCH_MEM	BIT(3)
+#define XTADMA_SKIP_DEL_ENTRY	BIT(2)
+#define XTADMA_SOFT_RST		BIT(1)
+#define XTADMA_CFG_DONE		BIT(0)
+
+#define XTADMA_OFFSET_TIME_SHIFT	16
+#define XTADMA_OFFSET_TIME_MASK		(0xFFFF)
+
+#define XTADMA_ENT_NUM_SEC_INTR_SHIFT	16
+#define XTADMA_ENT_NUM_SEC_INTR_MASK	(0xFF)
+#define XTADMA_FRAME_THRES_SHIFT	8
+#define XTADMA_FRAME_THRES_MASK		(0xFF)
+
+#define XTADMA_FIX_RES_QUEUE_ID_SHIFT	16
+#define XTADMA_FIX_RES_QUEUE_ID_MASK	(0xFF0000)
+#define XTADMA_FIX_BE_QUEUE_ID_SHIFT	0
+#define XTADMA_FIX_BE_QUEUE_ID_MASK	(0xFF)
+
+#define XTADMA_SEC_COMP_INT_EN		BIT(12)
+#define XTADMA_IE_INT_EN		BIT(11)
+#define XTADMA_SEI_INT_EN		BIT(10)
+#define XTADMA_DEI_INT_EN		BIT(9)
+#define XTADMA_BENSI_INT_EN		BIT(8)
+#define XTADMA_RESNSI_INT_EN		BIT(7)
+#define XTADMA_STNSI_INT_EN		BIT(6)
+#define XTADMA_BEI_INT_EN		BIT(5)
+#define XTADMA_NRDFI_INT_EN		BIT(4)
+#define XTADMA_FNDI_INT_EN		BIT(3)
+#define XTADMA_CDI_INT_EN		BIT(2)
+#define XTADMA_EDI_INT_EN		BIT(1)
+#define XTADMA_FFI_INT_EN		BIT(0)
+#define XTADMA_INT_EN_ALL_MASK		(0x1FFF)
+
+#define XTADMA_STR_FETCH_ENTRY_SIZE	64
+#define XTADMA_STR_TIME_TICKS_SHIFT	0
+#define XTADMA_STR_TIME_TICKS_MASK	(0x7FFFFFF)
+
+#define XTADMA_STR_ID_SHIFT		0
+#define XTADMA_STR_ID_MASK		0xFF
+#define XTADMA_STR_NUM_FRM_SHIFT	16
+#define XTADMA_STR_NUM_FRM_MASK		0x30000
+#define XTADMA_STR_QUE_TYPE_SHIFT	20
+#define XTADMA_STR_QUE_TYPE_MASK	0x300000
+#define XTADMA_STR_CONT_FETCH_EN	BIT(22)
+#define XTADMA_STR_ENTRY_VALID		BIT(31)
+
+#define XTADMA_ALM_ADDR_MSB_SHIFT	0
+#define XTADMA_ALM_ADDR_MSB_MASK	0xFF
+#define XTADMA_ALM_TOT_PKT_SZ_BY8_SHIFT	8
+#define XTADMA_ALM_TOT_PKT_SZ_BY8_MASK	0xFF00
+#define XTADMA_ALM_FETCH_SZ_SHIFT	16
+#define XTADMA_ALM_FETCH_SZ_MASK	0xFFF0000
+#define XTADMA_ALM_UFF			BIT(28)
+#define XTADMA_ALM_SOP			BIT(30)
+#define XTADMA_ALM_EOP			BIT(31)
+
+#define SFM_UPPER 0
+#define SFM_LOWER 1
+
+
+enum qtype {
+	qt_st = 0,
+	qt_res,
+	qt_be,
+	qt_resbe
+};
+
+/* address/length memory entry */
+struct alm_entry {
+	u32 addr;
+	u32 cfg;
+};
+
+/* stream fetch entry */
+struct sfm_entry {
+	u32 tticks;
+	u32 cfg;
+};
+
+struct tadma_cb {
+	struct hlist_head *stream_hash;
+	int streams;
+	u32 be_trigger;
+};
+
+static inline u32 tadma_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->tadma_regs + offset);
+}
+
+static inline void tadma_iow(struct axienet_local *lp, off_t offset,
+			     u32 value)
+{
+	iowrite32(value, (lp->tadma_regs + offset));
+}
+#endif /* _XTADMA_TSN_H */
diff --git a/drivers/staging/xilinx-tsn/xilinx_tsn_timer.h b/drivers/staging/xilinx-tsn/xilinx_tsn_timer.h
new file mode 100644
index 000000000..86ded83c1
--- /dev/null
+++ b/drivers/staging/xilinx-tsn/xilinx_tsn_timer.h
@@ -0,0 +1,67 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx FPGA Xilinx TSN timer module header.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _XILINX_TSN_H_
+#define _XILINX_TSN_H_
+
+#include <linux/platform_device.h>
+#include <linux/units.h>
+
+#define XAE_RTC_OFFSET			0x12800
+/* RTC Nanoseconds Field Offset Register */
+#define XTIMER1588_RTC_OFFSET_NS	0x00000
+/* RTC Seconds Field Offset Register - Low */
+#define XTIMER1588_RTC_OFFSET_SEC_L	0x00008
+/* RTC Seconds Field Offset Register - High */
+#define XTIMER1588_RTC_OFFSET_SEC_H	0x0000C
+/* RTC Increment */
+#define XTIMER1588_RTC_INCREMENT	0x00010
+/* Current TOD Nanoseconds - RO */
+#define XTIMER1588_CURRENT_RTC_NS	0x00014
+/* Current TOD Seconds -Low RO  */
+#define XTIMER1588_CURRENT_RTC_SEC_L	0x00018
+/* Current TOD Seconds -High RO */
+#define XTIMER1588_CURRENT_RTC_SEC_H	0x0001C
+#define XTIMER1588_SYNTONIZED_NS	0x0002C
+#define XTIMER1588_SYNTONIZED_SEC_L	0x00030
+#define XTIMER1588_SYNTONIZED_SEC_H	0x00034
+/* Write to Bit 0 to clear the interrupt */
+#define XTIMER1588_INTERRUPT		0x00020
+/* 8kHz Pulse Offset Register */
+#define XTIMER1588_8KPULSE		0x00024
+/* Correction Field - Low */
+#define XTIMER1588_CF_L			0x0002C
+/* Correction Field - Low */
+#define XTIMER1588_CF_H			0x00030
+
+#define XTIMER1588_RTC_MASK  ((1 << 26) - 1)
+#define XTIMER1588_INT_SHIFT 0
+#define NANOSECOND_BITS 20
+#define NANOSECOND_MASK ((1 << NANOSECOND_BITS) - 1)
+#define SECOND_MASK ((1 << (32 - NANOSECOND_BITS)) - 1)
+#define XTIMER1588_RTC_NS_SHIFT 20
+#define PULSESIN1PPS 128
+#define XTIMER1588_GTX_CLK_FREQ (125 * HZ_PER_MHZ)
+
+/* The tsn ptp module will set this variable */
+extern int axienet_phc_index;
+
+void *axienet_ptp_timer_probe(void __iomem *base,
+			      struct platform_device *pdev);
+int axienet_get_phc_index(void *priv);
+#endif
diff --git a/drivers/staging/xilinx_hdcp/Kconfig b/drivers/staging/xilinx_hdcp/Kconfig
new file mode 100644
index 000000000..acc9efa29
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/Kconfig
@@ -0,0 +1,8 @@
+config XILINX_HDCP_COMMON
+	bool
+	help
+	  This driver supports HDCP 1x/2x common functionalities for
+	  Xilinx devices
+
+	  This code will be compiled by selection from the interface
+	  drivers (DisplayPort/HDMI) of Xilinx.
diff --git a/drivers/staging/xilinx_hdcp/MAINTAINERS b/drivers/staging/xilinx_hdcp/MAINTAINERS
new file mode 100644
index 000000000..779f9574a
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/MAINTAINERS
@@ -0,0 +1,4 @@
+XILINX HDCP COMMON DRIVER
+M:	Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@xilinx.com>
+S:	Maintained
+F:	drivers/staging/xilinx_hdcp
diff --git a/drivers/staging/xilinx_hdcp/Makefile b/drivers/staging/xilinx_hdcp/Makefile
new file mode 100644
index 000000000..1f6d80c1d
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/Makefile
@@ -0,0 +1,5 @@
+obj-$(CONFIG_XILINX_HDCP_COMMON) += xlnx_hdcp_rng.o \
+	xlnx_timer.o \
+	xlnx_hdcp_bigdigits.o \
+	xlnx_hdcp2x_cipher.o \
+	xlnx_hdcp2x_mmult.o \
diff --git a/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_cipher.c b/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_cipher.c
new file mode 100644
index 000000000..7c5b1f75c
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_cipher.c
@@ -0,0 +1,97 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx HDCP2X Cipher driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ *
+ * This driver initializes the Cipher engine to implement AES-128
+ * standard for encrypting and decrypting the audiovisual content.
+ * The Cipher is required to be programmed with the Lc128, random number Riv,
+ * and session key Ks before encryption is enabled.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/io.h>
+#include <linux/xlnx/xlnx_hdcp2x_cipher.h>
+
+#define swap_bytes(i, buf, ptr, len) \
+do {\
+	typeof(i) (y) = (i); \
+	typeof(len) (x) = (len); \
+	for (y = 0; y < (x); y++) {\
+		buf[(((x) - 1) - y)] = ptr[y]; \
+	} \
+} while (0)
+
+void  xlnx_hdcp2x_tx_cipher_update_encryption(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+					      u8 enable)
+{
+	if (enable)
+		xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+					 XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET,
+					 XHDCP2X_CIPHER_REG_CTRL_ENCRYPT_MASK);
+	else
+		xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+					 XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET,
+					 XHDCP2X_CIPHER_REG_CTRL_ENCRYPT_MASK);
+}
+
+void xlnx_hdcp2x_cipher_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg)
+{
+	xlnx_hdcp2x_cipher_enable(cipher_cfg->cipher_coreaddress);
+	xlnx_hdcp2x_cipher_set_txmode(cipher_cfg->cipher_coreaddress);
+	xlnx_hdcp2x_tx_cipher_update_encryption(cipher_cfg, 0);
+	xlnx_hdcp2x_cipher_disable(cipher_cfg->cipher_coreaddress);
+}
+
+void xlnx_hdcp2x_rx_cipher_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg)
+{
+	xlnx_hdcp2x_cipher_enable(cipher_cfg->cipher_coreaddress);
+	xlnx_hdcp2x_cipher_set_rxmode(cipher_cfg->cipher_coreaddress);
+	xlnx_hdcp2x_cipher_disable(cipher_cfg->cipher_coreaddress);
+}
+
+int xlnx_hdcp2x_cipher_cfg_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg)
+{
+	u32 reg_read;
+
+	reg_read = xlnx_hdcp2x_cipher_read(cipher_cfg->cipher_coreaddress,
+					   XHDCP2X_CIPHER_VER_ID_OFFSET);
+	reg_read = FIELD_GET(XHDCP2X_CIPHER_MASK_16, reg_read);
+	if (reg_read != XHDCP2X_CIPHER_VER_ID)
+		return (-EINVAL);
+
+	return reg_read;
+}
+
+void xlnx_hdcp2x_cipher_set_keys(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				 const u8 *cipherkey, u32 offset, u16 len)
+{
+	u8 buf[XHDCP2X_CIPHER_KEY_LENGTH];
+	u32 *bufptr;
+	u8 i = 0;
+
+	swap_bytes(i, buf, cipherkey, len);
+
+	for (i = 0; i < len; i += 4) {
+		bufptr = (u32 *)&buf[i];
+		xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+					 offset, *bufptr);
+		/* Increase offset to the next register */
+		offset += 4;
+	}
+}
+
+void xlnx_hdcp2x_cipher_set_lanecount(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				      u8 lanecount)
+{
+	xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+				 XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET,
+				 XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_MASK);
+
+	xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+				 XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET,
+				 lanecount << XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_BIT_POS);
+}
diff --git a/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_mmult.c b/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_mmult.c
new file mode 100644
index 000000000..5b52e5a06
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_mmult.c
@@ -0,0 +1,108 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx HDCP2X Random Number Generator driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Kunal Vasant Rane <kunal.rane@amd.com>
+ *
+ * This driver initializes Montogomery Multiplier IP, which is used to
+ * exchange of the master key during authentication and key exchange is performed using
+ * the public key cryptography system which is based on the RSA algorithm.
+ * reference: https://docs.xilinx.com/v/u/en-US/pg249-hdcp22
+ */
+
+#include <linux/bitfield.h>
+#include <linux/xlnx/xlnx_hdcp2x_mmult.h>
+
+int arr[XHDCP2X_MMULT_MAX_TYPES][XHDCP2X_MMULT_ADDR] = {
+	{XHDCP2X_MMULT_A_BASE, XHDCP2X_MMULT_A_HIGH},
+	{XHDCP2X_MMULT_B_BASE, XHDCP2X_MMULT_B_HIGH},
+	{XHDCP2X_MMULT_N_BASE, XHDCP2X_MMULT_N_HIGH},
+	{XHDCP2X_MMULT_NPRIME_BASE, XHDCP2X_MMULT_NPRIME_HIGH},
+};
+
+static int xlnx_hdcp2x_mmult_read(void __iomem *mmult_coreaddress, int reg_offset)
+{
+	return readl(mmult_coreaddress + reg_offset);
+}
+
+static void xlnx_hdcp2x_mmult_write(void __iomem *mmult_coreaddress, int reg_offset, u32 data)
+{
+	writel(data, mmult_coreaddress + reg_offset);
+}
+
+int xlnx_hdcp2x_mmult_cfginit(struct xlnx_hdcp2x_mmult_hw *mmult_cfg)
+{
+	int reg_read;
+
+	reg_read = xlnx_hdcp2x_mmult_read(mmult_cfg->mmult_coreaddress,
+					  XHDCP2X_MMULT_ADDR_AP);
+
+	return reg_read;
+}
+
+void xlnx_hdcp2x_mmult_enable(struct xlnx_hdcp2x_mmult_hw *mmult_cfg)
+{
+	u32 data;
+
+	data = xlnx_hdcp2x_mmult_read(mmult_cfg->mmult_coreaddress,
+				      XHDCP2X_MMULT_ADDR_AP) & XHDCP2X_MMULT_ADDR_AP_RD;
+	xlnx_hdcp2x_mmult_write(mmult_cfg->mmult_coreaddress,
+				XHDCP2X_MMULT_ADDR_AP, data | XHDCP2X_MMULT_ADDR_AP_WR);
+}
+
+u32 xlnx_hdcp2x_mmult_is_done(struct xlnx_hdcp2x_mmult_hw *mmult_cfg)
+{
+	u32 data;
+
+	data = xlnx_hdcp2x_mmult_read(mmult_cfg->mmult_coreaddress,
+				      XHDCP2X_MMULT_ADDR_AP);
+
+	return (data & XHDCP2X_MMULT_DONE) ? 1 : 0;
+}
+
+u32 xlnx_hdcp2x_mmult_is_ready(struct xlnx_hdcp2x_mmult_hw *mmult_cfg)
+{
+	u32 data;
+
+	data = xlnx_hdcp2x_mmult_read(mmult_cfg->mmult_coreaddress,
+				      XHDCP2X_MMULT_ADDR_AP);
+
+	return !(data & XHDCP2X_MMULT_READY);
+}
+
+u32 xlnx_hdcp2x_mmult_read_u_words(struct xlnx_hdcp2x_mmult_hw *mmult_cfg,
+				   int offset, int *data, int length)
+{
+	int i;
+
+	if ((offset + length) * XHDCP2X_MMULT_OFFSET_MULT
+			> (XHDCP2X_MMULT_ADDR_U_HIGH - XHDCP2X_MMULT_ADDR_U_BASE + 1))
+		return 0;
+
+	for (i = 0; i < length; i++)
+		*(data + i) =
+			xlnx_hdcp2x_mmult_read(mmult_cfg->mmult_coreaddress,
+					       XHDCP2X_MMULT_ADDR_U_BASE
+					       + (offset + i) * XHDCP2X_MMULT_OFFSET_MULT);
+
+	return length;
+}
+
+int xlnx_hdcp2x_mmult_write_type(struct xlnx_hdcp2x_mmult_hw *mmult_cfg, int offset, int *data,
+				 int length, int type)
+{
+	int i, base, high;
+
+	base = arr[type][0];
+	high = arr[type][1];
+
+	if ((offset +  length) * XHDCP2X_MMULT_OFFSET_MULT > (high - base + 1))
+		return 0;
+
+	for (i = 0; i < length; i++)
+		xlnx_hdcp2x_mmult_write(mmult_cfg->mmult_coreaddress, base +
+			(offset + i) * XHDCP2X_MMULT_OFFSET_MULT, *(data + i));
+	return length;
+}
diff --git a/drivers/staging/xilinx_hdcp/xlnx_hdcp_bigdigits.c b/drivers/staging/xilinx_hdcp/xlnx_hdcp_bigdigits.c
new file mode 100644
index 000000000..f3ff57cd7
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/xlnx_hdcp_bigdigits.c
@@ -0,0 +1,1149 @@
+// SPDX-License-Identifier: GPL-2.0
+// Id: bigdigits.c
+
+/***** BEGIN LICENSE BLOCK *****
+ *
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2001-15 David Ireland, D.I. Management Services Pty Limited
+ * <http://www.di-mgt.com.au/bigdigits.html>. All rights reserved.
+ *
+ ***** END LICENSE BLOCK *****/
+/*
+ * Last updated:
+ * Date: 2015-10-22 10:23:00 $
+ * Revision: 2.5.0
+ * Author: dai
+ */
+
+/* Core code for BigDigits library "mp" functions */
+
+/* Some part of Code for BigDigits is modified according to Xilinx standards */
+
+#include <linux/slab.h>
+#include <linux/xlnx/xlnx_hdcp_common.h>
+
+#define MAX_DIGIT 0xFFFFFFFFUL
+#define MAX_HALF_DIGIT 0xFFFFUL	/* NB 'L' */
+#define XBITS_PER_DIGIT 32
+#define XMP_HI_BIT_MASK 0x80000000UL
+#define XBITS_PER_HALF_DIGIT (XBITS_PER_DIGIT / 2)
+#define XBIGDIG_LOHALF(x) ((unsigned int)((x) & MAX_HALF_DIGIT))
+#define XBIGDIG_HIHALF(x) ((unsigned int)((x) >> XBITS_PER_HALF_DIGIT & MAX_HALF_DIGIT))
+#define XBIGDIG_TOHALF(x) ((unsigned int)((x) << XBITS_PER_HALF_DIGIT))
+
+#define IS_NONZERO_DIGIT(x) \
+({\
+	typeof(x) _x = x;\
+	((_x) | ((~(_x)) + 1)) >> (XBITS_PER_DIGIT - 1); \
+})
+
+#define IS_ZERO_DIGIT(x) (1 ^ IS_NONZERO_DIGIT((x)))
+
+static void mp_next_bit_mask(unsigned int *mask, unsigned int *n)
+{
+	if ((*mask) == 1) {
+		*mask = XMP_HI_BIT_MASK; (*n)--;
+	} else {
+		*mask >>= 1;
+	}
+}
+
+static int sp_multiply(unsigned int p[2], unsigned int x, unsigned int y)
+{
+	unsigned int x0, y0, x1, y1;
+	unsigned int t, u, carry;
+
+	/*
+	 *	Split each x,y into two halves
+	 *	x = x0 + B*x1
+	 *	y = y0 + B*y1
+	 *	where B = 2^16, half the digit size
+	 *	Product is
+	 *	xy = x0y0 + B(x0y1 + x1y0) + B^2(x1y1)
+	 */
+	x0 = XBIGDIG_LOHALF(x);
+	x1 = XBIGDIG_HIHALF(x);
+	y0 = XBIGDIG_LOHALF(y);
+	y1 = XBIGDIG_HIHALF(y);
+
+	/* Calc low part - no carry */
+	p[0] = x0 * y0;
+
+	/* Calc middle part */
+	t = x0 * y1;
+	u = x1 * y0;
+	t += u;
+	if (t < u)
+		carry = 1;
+	else
+		carry = 0;
+
+	/*
+	 *	This carry will go to high half of p[1]
+	 *	+ high half of t into low half of p[1]
+	 */
+	carry = XBIGDIG_TOHALF(carry) + XBIGDIG_HIHALF(t);
+
+	/* Add low half of t to high half of p[0] */
+	t = XBIGDIG_TOHALF(t);
+	p[0] += t;
+	if (p[0] < t)
+		carry++;
+
+	p[1] = x1 * y1;
+	p[1] += carry;
+
+	return 0;
+}
+
+#define B (MAX_HALF_DIGIT + 1)
+
+static void sp_mult_sub(unsigned int uu[2], unsigned int qhat,
+			unsigned int v1, unsigned int v0)
+{
+	/*
+	 *  Compute uu = uu - q(v1v0)
+	 *  where uu = u3u2u1u0, u3 = 0
+	 *  and u_n, v_n are all half-digits
+	 *  even though v1, v2 are passed as full digits.
+	 */
+	unsigned int p0, p1, t;
+
+	p0 = qhat * v0;
+	p1 = qhat * v1;
+	t = p0 + XBIGDIG_TOHALF(XBIGDIG_LOHALF(p1));
+	uu[0] -= t;
+	if (uu[0] > MAX_DIGIT - t)
+		uu[1]--;	/* Borrow */
+	uu[1] -= XBIGDIG_HIHALF(p1);
+}
+
+static unsigned int sp_divide(unsigned int *q, unsigned int *r,
+			      const unsigned int u[2], unsigned int v)
+{
+	unsigned int qhat, rhat, t, v0, v1, u0, u1, u2, u3;
+	unsigned int uu[2], q2;
+
+	/* Check for normalisation */
+	if (!(v & XMP_HI_BIT_MASK)) {	/* Stop if assert is working, else return error */
+		/* assert(v & XMP_HI_BIT_MASK); */
+		*q = *r = 0;
+		return MAX_DIGIT;
+	}
+
+	/* Split up into half-digits */
+	v0 = XBIGDIG_LOHALF(v);
+	v1 = XBIGDIG_HIHALF(v);
+	u0 = XBIGDIG_LOHALF(u[0]);
+	u1 = XBIGDIG_HIHALF(u[0]);
+	u2 = XBIGDIG_LOHALF(u[1]);
+	u3 = XBIGDIG_HIHALF(u[1]);
+
+	qhat = (u3 < v1 ? 0 : 1);
+	if (qhat > 0) {	/* qhat is one, so no need to mult */
+		rhat = u3 - v1;
+		/* t = r.b + u2 */
+		t = XBIGDIG_TOHALF(rhat) | u2;
+		if (v0 > t)
+			qhat--;
+	}
+
+	uu[1] = 0;		/* (u4) */
+	uu[0] = u[1];	/* (u3u2) */
+	if (qhat > 0) {
+		/* (u4u3u2) -= qhat(v1v0) where u4 = 0 */
+		sp_mult_sub(uu, qhat, v1, v0);
+		if (XBIGDIG_HIHALF(uu[1]) != 0)	{	/* Add back */
+			qhat--;
+			uu[0] += v;
+			uu[1] = 0;
+		}
+	}
+	q2 = qhat;
+
+	/* ROUND 2. Set j = 1 and calculate q1 */
+
+	/*
+	 * Estimate qhat = (u3u2) / v1
+	 * then set (u3u2u1) -= qhat(v1v0)
+	 */
+	t = uu[0];
+	qhat = t / v1;
+	rhat = t - qhat * v1;
+	/* Test on v0 */
+	t = XBIGDIG_TOHALF(rhat) | u1;
+	if (qhat == B || (qhat * v0 > t)) {
+		qhat--;
+		rhat += v1;
+		t = XBIGDIG_TOHALF(rhat) | u1;
+		if (rhat < B && (qhat * v0 > t))
+			qhat--;
+	}
+
+	/*
+	 * Multiply and subtract
+	 * (u3u2u1)' = (u3u2u1) - qhat(v1v0)
+	 */
+	uu[1] = XBIGDIG_HIHALF(uu[0]);	/* (0u3) */
+	uu[0] = XBIGDIG_TOHALF(XBIGDIG_LOHALF(uu[0])) | u1;	/* (u2u1) */
+	sp_mult_sub(uu, qhat, v1, v0);
+	if (XBIGDIG_HIHALF(uu[1]) != 0) {	/* Add back */
+		qhat--;
+		uu[0] += v;
+		uu[1] = 0;
+	}
+
+	/* q1 = qhat */
+	*q = XBIGDIG_TOHALF(qhat);
+
+	/* ROUND 3. Set j = 0 and calculate q0 */
+	/*
+	 *	Estimate qhat = (u2u1) / v1
+	 *	then set (u2u1u0) -= qhat(v1v0)
+	 */
+	t = uu[0];
+	qhat = t / v1;
+	rhat = t - qhat * v1;
+	/* Test on v0 */
+	t = XBIGDIG_TOHALF(rhat) | u0;
+	if (qhat == B || (qhat * v0 > t)) {
+		qhat--;
+		rhat += v1;
+		t = XBIGDIG_TOHALF(rhat) | u0;
+		if (rhat < B && (qhat * v0 > t))
+			qhat--;
+	}
+
+	/*
+	 *	Multiply and subtract
+	 *	(u2u1u0)" = (u2u1u0)' - qhat(v1v0)
+	 */
+	uu[1] = XBIGDIG_HIHALF(uu[0]);	/* (0u2) */
+	uu[0] = XBIGDIG_TOHALF(XBIGDIG_LOHALF(uu[0])) | u0;	/* (u1u0) */
+	sp_mult_sub(uu, qhat, v1, v0);
+	if (XBIGDIG_HIHALF(uu[1]) != 0) {	/* Add back */
+		qhat--;
+		uu[0] += v;
+		uu[1] = 0;
+	}
+
+	/* q0 = qhat */
+	*q |= XBIGDIG_LOHALF(qhat);
+
+	/* Remainder is in (u1u0) i.e. uu[0] */
+	*r = uu[0];
+	return q2;
+}
+
+unsigned int mp_add(unsigned int w[], const unsigned int u[], const unsigned int v[],
+		    size_t ndigits)
+{
+	/*
+	 *	Calculates w = u + v
+	 *	where w, u, v are multiprecision integers of ndigits each
+	 *	Returns carry if overflow. Carry = 0 or 1.
+	 *	Ref: Knuth Vol 2 Ch 4.3.1 p 266 Algorithm A.
+	 */
+
+	unsigned int k;
+	size_t j;
+
+	k = 0;
+
+	for (j = 0; j < ndigits; j++) {
+		w[j] = u[j] + k;
+		if (w[j] < k)
+			k = 1;
+		else
+			k = 0;
+
+		w[j] += v[j];
+		if (w[j] < v[j])
+			k++;
+	}
+
+	return k;
+}
+
+int mp_multiply(unsigned int w[], const unsigned int u[], const unsigned int v[], size_t ndigits)
+{
+	/*
+	 *	Computes product w = u * v
+	 *	where u, v are multiprecision integers of ndigits each
+	 *	and w is a multiprecision integer of 2*ndigits
+	 *	Ref: Knuth Vol 2 Ch 4.3.1 p 268 Algorithm M.
+	 */
+
+	unsigned int k, t[2];
+	size_t i, j, m, n;
+
+	m = ndigits;
+	n = ndigits;
+
+	/* Step M1. Initialise */
+	for (i = 0; i < 2 * m; i++)
+		w[i] = 0;
+
+	for (j = 0; j < n; j++) {
+		/* Step M2. Zero multiplier? */
+		if (v[j] == 0) {
+			w[j + m] = 0;
+		} else {
+			/* Step M3. Initialise i */
+			k = 0;
+			for (i = 0; i < m; i++) {
+				/* Step M4. Multiply and add */
+				/* t = u_i * v_j + w_(i+j) + k */
+				sp_multiply(t, u[i], v[j]);
+
+				t[0] += k;
+				if (t[0] < k)
+					t[1]++;
+				t[0] += w[i + j];
+				if (t[0] < w[i + j])
+					t[1]++;
+
+				w[i + j] = t[0];
+				k = t[1];
+			}
+			/* Step M5. Loop on i, set w_(j+m) = k */
+			w[j + m] = k;
+		}
+	}	/* Step M6. Loop on j */
+
+	return 0;
+}
+
+static unsigned int mp_mult_sub(unsigned int wn, unsigned int w[],
+				const unsigned int v[],
+				unsigned int q, size_t n)
+{
+	/*
+	 *	Compute w = w - qv
+	 *	where w = (WnW[n-1]...W[0])
+	 *	return modified Wn.
+	 */
+	unsigned int k, t[2];
+	size_t i;
+
+	if (q == 0)	/* No change */
+		return wn;
+
+	k = 0;
+
+	for (i = 0; i < n; i++) {
+		sp_multiply(t, q, v[i]);
+		w[i] -= k;
+		if (w[i] > MAX_DIGIT - k)
+			k = 1;
+		else
+			k = 0;
+		w[i] -= t[0];
+		if (w[i] > MAX_DIGIT - t[0])
+			k++;
+		k += t[1];
+	}
+
+	/* Cope with Wn not stored in array w[0..n-1] */
+	wn -= k;
+
+	return wn;
+}
+
+static int qhat_too_big(unsigned int qhat, unsigned int rhat,
+			unsigned int vn2, unsigned int ujn2)
+{
+	/*
+	 *	Returns true if Qhat is too big
+	 *	i.e. if (Qhat * Vn-2) > (b.Rhat + Uj+n-2)
+	 */
+	unsigned int t[2];
+
+	sp_multiply(t, qhat, vn2);
+	if (t[1] < rhat)
+		return 0;
+	else if (t[1] > rhat)
+		return 1;
+	else if (t[0] > ujn2)
+		return 1;
+
+	return 0;
+}
+
+/** Returns 1 if a == 0, else 0 (constant-time) */
+static int mp_is_zero(const u32 a[], size_t ndigits)
+{
+	u32 dif = 0;
+	const u32 ZERO = 0;
+
+	while (ndigits--)
+		dif |= a[ndigits] ^ ZERO;
+
+	return IS_ZERO_DIGIT(dif);
+}
+
+static int mp_compare(const unsigned int a[], const unsigned int b[],
+		      size_t ndigits)
+{
+	/* All these vars are either 0 or 1 */
+	unsigned int gt = 0;
+	unsigned int lt = 0;
+	unsigned int mask = 1;	/* Set to zero once first inequality found */
+	unsigned int c;
+
+	while (ndigits--) {
+		gt |= (a[ndigits] > b[ndigits]) & mask;
+		lt |= (a[ndigits] < b[ndigits]) & mask;
+		c = (gt | lt);
+		mask &= (c - 1);	/* Unchanged if c==0 or mask==0, else mask=0 */
+	}
+
+	return (int)gt - (int)lt;	/* EQ=0 GT=+1 LT=-1 */
+}
+
+static size_t mp_sizeof(const unsigned int a[], size_t ndigits)
+{
+	while (ndigits--) {
+		if (a[ndigits] != 0)
+			return (++ndigits);
+	}
+	return 0;
+}
+
+static void mp_set_equal(unsigned int a[], const unsigned int b[], size_t ndigits)
+{
+	/* Sets a = b */
+	size_t i;
+
+	for (i = 0; i < ndigits; i++)
+		a[i] = b[i];
+}
+
+static unsigned int mp_set_zero(unsigned int a[], size_t ndigits)
+{
+	/* Sets a = 0 */
+
+	/* Prevent optimiser ignoring this */
+	unsigned int optdummy;
+	unsigned int *p = a;
+
+	while (ndigits--)
+		a[ndigits] = 0;
+
+	optdummy = *p;
+	return optdummy;
+}
+
+static void mp_set_digit(unsigned int a[], unsigned int d, size_t ndigits)
+{
+	/* Sets a = d where d is a single digit */
+	size_t i;
+
+	for (i = 1; i < ndigits; i++)
+		a[i] = 0;
+	a[0] = d;
+}
+
+static unsigned int mp_short_div(unsigned int q[], const unsigned int u[], unsigned int v,
+				 size_t ndigits)
+{
+	size_t j;
+	unsigned int t[2], r;
+	size_t shift;
+	unsigned int bitmask, overflow, *uu;
+
+	if (ndigits == 0)
+		return 0;
+	if (v == 0)
+		return 0;
+
+	bitmask = XMP_HI_BIT_MASK;
+	for (shift = 0; shift < XBITS_PER_DIGIT; shift++) {
+		if (v & bitmask)
+			break;
+		bitmask >>= 1;
+	}
+
+	v <<= shift;
+	overflow = mp_shift_left(q, u, shift, ndigits);
+	uu = q;
+
+	/* Step S1 - modified for extra digit. */
+	r = overflow;	/* New digit Un */
+	j = ndigits;
+	while (j--) {
+		/* Step S2. */
+		t[1] = r;
+		t[0] = uu[j];
+		overflow = sp_divide(&q[j], &r, t, v);
+	}
+	r >>= shift;
+
+	return r;
+}
+
+static unsigned int mp_shift_right(unsigned int a[], const unsigned int b[], size_t shift,
+				   size_t ndigits)
+{
+	/* Computes a = b >> shift */
+	/* [v2.1] Modified to cope with shift > BITS_PERDIGIT */
+	size_t i, y, nw, bits;
+	unsigned int mask, carry, nextcarry;
+	u8 shift_bit = 0;
+
+	/* Do we shift whole digits? */
+	if (shift >= XBITS_PER_DIGIT) {
+		nw = shift / XBITS_PER_DIGIT;
+		for (i = 0; i < ndigits; i++) {
+			if ((i + nw) < ndigits)
+				a[i] = b[i + nw];
+			else
+				a[i] = 0;
+		}
+		/* Call again to shift bits inside digits */
+		bits = shift % XBITS_PER_DIGIT;
+		carry = b[nw - 1] >> bits;
+		if (bits)
+			carry |= mp_shift_right(a, a, bits, ndigits);
+		return carry;
+	}
+	bits = shift;
+	/* Construct mask to set low bits */
+	mask = ~(~(unsigned int)shift_bit << bits);
+
+	y = XBITS_PER_DIGIT - bits;
+	carry = 0;
+	i = ndigits;
+	while (i--) {
+		nextcarry = (b[i] & mask) << y;
+		a[i] = b[i] >> bits | carry;
+		carry = nextcarry;
+	}
+
+	return carry;
+}
+
+int mp_divide(unsigned int q[], unsigned int r[], const unsigned int u[],
+	      size_t udigits, unsigned int v[], size_t vdigits)
+{
+	size_t shift;
+	int n, m, j;
+	unsigned int bitmask, overflow;
+	unsigned int qhat, rhat, t[2];
+	unsigned int *uu, *ww;
+	int qhat_ok, cmp;
+
+	/* Clear q and r */
+	mp_set_zero(q, udigits);
+	mp_set_zero(r, udigits);
+
+	/* Work out exact sizes of u and v */
+	n = (int)mp_sizeof(v, vdigits);
+	m = (int)mp_sizeof(u, udigits);
+	m -= n;
+
+	/* Catch special cases */
+	if (n == 0)
+		return -1;	/* Error: divide by zero */
+
+	if (n == 1) {	/* Use short division instead */
+		r[0] = mp_short_div(q, u, v[0], udigits);
+		return 0;
+	}
+
+	if (m < 0) {	/* v > u, so just set q = 0 and r = u */
+		mp_set_equal(r, u, udigits);
+		return 0;
+	}
+
+	if (m == 0) {	/* u and v are the same length */
+		cmp = mp_compare(u, v, (size_t)n);
+		if (cmp < 0) {	/* v > u, as above */
+			mp_set_equal(r, u, udigits);
+			return 0;
+		} else if (cmp == 0) {	/* v == u, so set q = 1 and r = 0 */
+			mp_set_digit(q, 1, udigits);
+			return 0;
+		}
+	}
+
+	bitmask = XMP_HI_BIT_MASK;
+	for (shift = 0; shift < XBITS_PER_DIGIT; shift++) {
+		if (v[n - 1] & bitmask)
+			break;
+		bitmask >>= 1;
+	}
+
+	/* Normalise v in situ - NB only shift non-zero digits */
+	overflow = mp_shift_left(v, v, shift, n);
+
+	/* Copy normalised dividend u*d into r */
+	overflow = mp_shift_left(r, u, shift, n + m);
+	uu = r;	/* Use ptr to keep notation constant */
+
+	t[0] = overflow;	/* Extra digit Um+n */
+
+	/* Step D2. Initialise j. Set j = m */
+	for (j = m; j >= 0; j--) {
+		/*
+		 * Step D3. Set Qhat = [(b.Uj+n + Uj+n-1)/Vn-1]
+		 * and Rhat = remainder
+		 */
+		qhat_ok = 0;
+		t[1] = t[0];	/* This is Uj+n */
+		t[0] = uu[j + n - 1];
+		overflow = sp_divide(&qhat, &rhat, t, v[n - 1]);
+
+		/* Test Qhat */
+		if (overflow) {	/* Qhat == b so set Qhat = b - 1 */
+			qhat = MAX_DIGIT;
+			rhat = uu[j + n - 1];
+			rhat += v[n - 1];
+			if (rhat < v[n - 1])	/* Rhat >= b, so no re-test */
+				qhat_ok = 1;
+		}
+		/* [VERSION 2: Added extra test "qhat && "] */
+		if (qhat && !qhat_ok && qhat_too_big(qhat, rhat,
+						     v[n - 2], uu[j + n - 2])) {
+			/*
+			 * If Qhat.Vn-2 > b.Rhat + Uj+n-2
+			 * decrease Qhat by one, increase Rhat by Vn-1
+			 */
+			qhat--;
+			rhat += v[n - 1];
+			/* Repeat this test if Rhat < b */
+			if (!(rhat < v[n - 1]))
+				if (qhat_too_big(qhat, rhat, v[n - 2], uu[j + n - 2]))
+					qhat--;
+		}
+
+		/* Step D4. Multiply and subtract */
+		ww = &uu[j];
+		overflow = mp_mult_sub(t[1], ww, v, qhat, (size_t)n);
+
+		/* Step D5. Test remainder. Set Qj = Qhat */
+		q[j] = qhat;
+		if (overflow) {	/* Step D6. Add back if D4 was negative */
+			q[j]--;
+			overflow = mp_add(ww, ww, v, (size_t)n);
+		}
+
+		t[0] = uu[j + n - 1];	/* Uj+n on next round */
+
+	}	/* Step D7. Loop on j */
+
+	/* Clear high digits in uu */
+	for (j = n; j < m + n; j++)
+		uu[j] = 0;
+
+	/* Step D8. Unnormalise. */
+
+	mp_shift_right(r, r, shift, n);
+	mp_shift_right(v, v, shift, n);
+
+	return 0;
+}
+
+int mp_mod_inv(u32 inv[], const u32 u[], const u32 v[], size_t ndigits)
+{
+	/* Computes inv = u^(-1) mod v */
+
+	/*
+	 * Ref: Knuth Algorithm X Vol 2 p 342
+	 * ignoring u2, v2, t2
+	 * and avoiding negative numbers.
+	 * Returns non-zero if inverse undefined.
+	 */
+	int b_iter;
+	int result;
+
+	/* 1 * ndigits each, except w = 2 * ndigits */
+	u32 *u1, *u3, *v1, *v3, *t1, *t3, *q, *w;
+	/* allocate 9 * ndigits */
+	u1 = kzalloc(9 * ndigits * sizeof(u32), GFP_KERNEL);
+	if (!u1)
+		return -ENOMEM;
+
+	u3 = &u1[1 * ndigits];
+	v1 = &u1[2 * ndigits];
+	v3 = &u1[3 * ndigits];
+	t1 = &u1[4 * ndigits];
+	t3 = &u1[5 * ndigits];
+	q = &u1[6 * ndigits];
+	/* 2 * ndigits each */
+	w = &u1[7 * ndigits];
+
+	/* Step X1. Initialise */
+	mp_set_digit(u1, 1, ndigits);
+	mp_set_equal(u3, u, ndigits);
+	mp_set_zero(v1, ndigits);
+	mp_set_equal(v3, v, ndigits);
+
+	b_iter = 1;
+	while (!mp_is_zero(v3, ndigits)) {
+		mp_divide(q, t3, u3, ndigits, v3, ndigits);
+		mp_multiply(w, q, v1, ndigits);
+		mp_add(t1, u1, w, ndigits);
+		/* Swap u1 = v1; v1 = t1; u3 = v3; v3 = t3 */
+		mp_set_equal(u1, v1, ndigits);
+		mp_set_equal(v1, t1, ndigits);
+		mp_set_equal(u3, v3, ndigits);
+		mp_set_equal(v3, t3, ndigits);
+
+		b_iter = -b_iter;
+	}
+
+	if (b_iter < 0)
+		mp_subtract(inv, v, u1, ndigits);/* inv = v - u1 */
+	else
+		mp_set_equal(inv, u1, ndigits);	/* inv = u1 */
+
+	/* Make sure u3 = gcd(u,v) == 1 */
+	if (mp_short_cmp(u3, 1, ndigits) != 0) {
+		result = 1;
+		mp_set_zero(inv, ndigits);
+	} else {
+		result = 0;
+	}
+	/* Clear up */
+	kfree(u1);
+
+	return result;
+}
+
+int mp_modulo(u32 r[], const u32 u[], size_t udigits,
+	      u32 v[], size_t vdigits)
+{
+	/*
+	 * Computes r = u mod v
+	 * where r, v are multiprecision integers of length vdigits
+	 * and u is a multiprecision integer of length udigits.
+	 * r may overlap v.
+	 * Note that r here is only vdigits long,
+	 * whereas in mpDivide it is udigits long.
+	 * Use remainder from mpDivide function.
+	 */
+	u32 *qq, *rr;
+	size_t nn = max(udigits, vdigits);
+
+	qq = kcalloc(udigits, udigits * sizeof(u32), GFP_KERNEL);
+	if (!qq)
+		return -ENOMEM;
+
+	rr = kcalloc(nn, nn * sizeof(u32), GFP_KERNEL);
+	if (!rr) {
+		kfree(qq);
+		return -ENOMEM;
+	}
+	/* rr[nn] = u mod v */
+	mp_divide(qq, rr, u, udigits, v, vdigits);
+
+	/* Final r is only vdigits long */
+	mp_set_equal(r, rr, vdigits);
+
+	kfree(rr);
+	kfree(qq);
+
+	return 0;
+}
+
+int mp_get_bit(u32 a[], size_t ndigits, size_t ibit)
+{
+	/* Returns value 1 or 0 of bit n (0..nbits-1); or -1 if out of range */
+
+	size_t idigit, bit_to_get;
+	u32 mask;
+
+	/* Which digit? (0-based) */
+	idigit = ibit / XBITS_PER_DIGIT;
+	if (idigit >= ndigits)
+		return -1;
+
+	/* Set mask */
+	bit_to_get = ibit % XBITS_PER_DIGIT;
+	mask = 0x01 << bit_to_get;
+
+	return ((a[idigit] & mask) ? 1 : 0);
+}
+
+int mp_mod_mult(u32 a[], const u32 x[], const u32 y[], u32 m[], size_t ndigits)
+{
+	/* Computes a = (x * y) mod m */
+	/* Double-length temp variable p */
+	u32 *p;
+
+	p = kzalloc(ndigits * 2 * sizeof(u32), GFP_KERNEL);
+	if (!p)
+		return -ENOMEM;
+
+	/* Calc p[2n] = x * y */
+	mp_multiply(p, x, y, ndigits);
+
+	/* Then modulo (NOTE: a is OK at only ndigits long) */
+	mp_modulo(a, p, ndigits * 2, m, ndigits);
+
+	kfree(p);
+
+	return 0;
+}
+
+/** Returns 1 if a == b, else 0 (constant-time) */
+int mp_equal(const u32 a[], const u32 b[], size_t ndigits)
+{
+	u32 dif = 0;
+
+	while (ndigits--)
+		dif |= a[ndigits] ^ b[ndigits];
+
+	return IS_ZERO_DIGIT(dif);
+}
+
+u32 mp_subtract(u32 w[], const u32 u[], const u32 v[], size_t ndigits)
+{
+	/*
+	 * Calculates w = u - v where u >= v
+	 * w, u, v are multiprecision integers of ndigits each
+	 * Returns 0 if OK, or 1 if v > u.
+	 * Ref: Knuth Vol 2 Ch 4.3.1 p 267 Algorithm S.
+	 */
+
+	u32 k;
+	size_t j;
+
+	/* Step S1. Initialise */
+	k = 0;
+
+	for (j = 0; j < ndigits; j++) {
+		/*
+		 * Step S2. Subtract digits w_j = (u_j - v_j - k)
+		 * Set k = 1 if borrow occurs.
+		 */
+
+		w[j] = u[j] - k;
+		if (w[j] > MAX_DIGIT - k)
+			k = 1;
+		else
+			k = 0;
+
+		w[j] -= v[j];
+		if (w[j] > MAX_DIGIT - v[j])
+			k++;
+	}
+	/* Step S3. Loop on j */
+	/* Should be zero if u >= v */
+	return k;
+}
+
+/** Returns sign of (a - d) where d is a single digit */
+int mp_short_cmp(const u32 a[], u32 d, size_t ndigits)
+{
+	size_t i;
+	int gt = 0;
+	int lt = 0;
+
+	/* Zero-length a => a is zero */
+	if (ndigits == 0)
+		return (d ? -1 : 0);
+
+	/* If |a| > 1 then a > d */
+	for (i = 1; i < ndigits; i++) {
+		if (a[i] != 0)
+			return 1;	/* GT */
+	}
+
+	lt = (a[0] < d);
+	gt = (a[0] > d);
+
+	return gt - lt;	/* EQ=0 GT=+1 LT=-1 */
+}
+
+unsigned int mp_shift_left(unsigned int a[], const unsigned int *b, size_t shift, size_t ndigits)
+{
+	/* Computes a = b << shift */
+	/* [v2.1] Modified to cope with shift > BITS_PERDIGIT */
+	size_t i, y, nw, bits;
+	unsigned int mask, carry, nextcarry;
+	u8 shift_bit = 0;
+
+	/* Do we shift whole digits? */
+	if (shift >= XBITS_PER_DIGIT) {
+		nw = shift / XBITS_PER_DIGIT;
+		i = ndigits;
+		while (i--) {
+			if (i >= nw)
+				a[i] = b[i - nw];
+			else
+				a[i] = 0;
+		}
+		/* Call again to shift bits inside digits */
+		bits = shift % XBITS_PER_DIGIT;
+		carry = b[ndigits - nw] << bits;
+		if (bits)
+			carry |= mp_shift_left(a, a, bits, ndigits);
+		return carry;
+	}
+	bits = shift;
+	/* Construct mask = high bits set */
+	mask = ~(~(unsigned int)shift_bit >> bits);
+
+	y = XBITS_PER_DIGIT - bits;
+	carry = 0;
+	for (i = 0; i < ndigits; i++) {
+		nextcarry = (b[i] & mask) >> y;
+		a[i] = b[i] << bits | carry;
+		carry = nextcarry;
+	}
+
+	return carry;
+}
+
+static int mp_square(unsigned int w[], const unsigned int x[], size_t ndigits)
+{
+	unsigned int k, p[2], u[2], cbit, carry;
+	size_t i, j, t, i2, cpos;
+
+	t = ndigits;
+	/* 1. For i from 0 to (2t-1) do: w_i = 0 */
+	i2 = t << 1;
+	for (i = 0; i < i2; i++)
+		w[i] = 0;
+
+	carry = 0;
+	cpos = i2 - 1;
+	/* 2. For i from 0 to (t-1) do: */
+	for (i = 0; i < t; i++) {
+	/*
+	 * 2.1 (uv) = w_2i + x_i * x_i, w_2i = v, c = u
+	 *  Careful, w_2i may be double-prec
+	 */
+		i2 = i << 1; /* 2*i */
+		sp_multiply(p, x[i], x[i]);
+		p[0] += w[i2];
+		if (p[0] < w[i2])
+			p[1]++;
+		k = 0;	/* p[1] < b, so no overflow here */
+		if (i2 == cpos && carry) {
+			p[1] += carry;
+			if (p[1] < carry)
+				k++;
+			carry = 0;
+		}
+		w[i2] = p[0];
+		u[0] = p[1];
+		u[1] = k;
+
+		/*
+		 * 2.2 for j from (i+1) to (t-1) do:
+		 * (uv) = w_{i+j} + 2x_j * x_i + c,
+		 * w_{i+j} = v, c = u,
+		 * u is double-prec
+		 * w_{i+j} is dbl if [i+j] == cpos
+		 */
+		k = 0;
+		for (j = i + 1; j < t; j++) {
+			/* p = x_j * x_i */
+			sp_multiply(p, x[j], x[i]);
+			/* p = 2p <=> p <<= 1 */
+			cbit = (p[0] & XMP_HI_BIT_MASK) != 0;
+			k =  (p[1] & XMP_HI_BIT_MASK) != 0;
+			p[0] <<= 1;
+			p[1] <<= 1;
+			p[1] |= cbit;
+			/* p = p + c */
+			p[0] += u[0];
+			if (p[0] < u[0]) {
+				p[1]++;
+				if (p[1] == 0)
+					k++;
+			}
+			p[1] += u[1];
+			if (p[1] < u[1])
+				k++;
+			/* p = p + w_{i+j} */
+			p[0] += w[i + j];
+			if (p[0] < w[i + j]) {
+				p[1]++;
+				if (p[1] == 0)
+					k++;
+			}
+			if ((i + j) == cpos && carry) {/* catch overflow from last round */
+				p[1] += carry;
+				if (p[1] < carry)
+					k++;
+				carry = 0;
+			}
+			/* w_{i+j} = v, c = u */
+			w[i + j] = p[0];
+			u[0] = p[1];
+			u[1] = k;
+		}
+		/* 2.3 w_{i+t} = u */
+		w[i + t] = u[0];
+		/* remember overflow in w_{i+t} */
+		carry = u[1];
+		cpos = i + t;
+	}
+
+	return 0;
+}
+
+size_t mp_conv_from_octets(unsigned int a[], size_t ndigits, const unsigned char *c,
+			   size_t nbytes)
+{
+	/*
+	 *	Converts nbytes octets into big digit a of max size ndigits
+	 *	Returns actual number of digits set (may be larger than mp_sizeof)
+	 */
+
+	size_t i;
+	int j, k = 0;
+	unsigned int t;
+
+	mp_set_zero(a, ndigits);
+	/* Read in octets, least significant first */
+	/* i counts into big_d, j along c, and k is # bits to shift */
+	for (i = 0, j = (int)nbytes - 1; i < ndigits && j >= 0; i++) {
+		t = 0;
+		for (k = 0; (j >= 0) && (k < XBITS_PER_DIGIT) ; j--, k = k + 8)
+			t |= ((unsigned int)c[j]) << k;
+		a[i] = t;
+	}
+	return i;
+}
+
+static size_t mp_bit_length(const unsigned int d[], size_t ndigits)
+{
+	/* Returns no of significant bits in d */
+	size_t n, i, bits;
+	unsigned int mask;
+
+	if (!d || ndigits == 0)
+		return 0;
+
+	n = mp_sizeof(d, ndigits);
+	if (n == 0)
+		return 0;
+
+	for (i = 0, mask = XMP_HI_BIT_MASK; mask > 0; mask >>= 1, i++) {
+		if (d[n - 1] & mask)
+			break;
+	}
+
+	bits = n * XBITS_PER_DIGIT - i;
+
+	return bits;
+}
+
+size_t mp_conv_to_octets(const unsigned int a[], size_t ndigits, unsigned char *c,
+			 size_t nbytes)
+{
+	/*
+	 *	Convert big digit a into string of octets, in big-endian order,
+	 *	padding on the left to nbytes or truncating if necessary.
+	 *	Return number of octets required excluding leading zero bytes.
+	 */
+
+	int j, k, len;
+	unsigned int t;
+	size_t i, noctets, nbits;
+
+	nbits = mp_bit_length(a, ndigits);
+	noctets = (nbits + 7) / 8;
+
+	len = (int)nbytes;
+
+	for (i = 0, j = len - 1; i < ndigits && j >= 0; i++) {
+		t = a[i];
+		for (k = 0; j >= 0 && k < XBITS_PER_DIGIT; j--, k += 8)
+			c[j] = (unsigned char)(t >> k);
+	}
+
+	for ( ; j >= 0; j--)
+		c[j] = 0;
+
+	return (size_t)noctets;
+}
+
+static void mp_mod_square_temp(unsigned int y[], unsigned int m[], size_t ndigits,
+			       unsigned int t1[], unsigned int t2[])
+{
+	mp_square(t1, y, ndigits);
+	mp_divide(t2, y, t1, ndigits * 2, m, ndigits);
+}
+
+static void mp_mod_mult_temp(unsigned int y[], const unsigned int x[], unsigned int m[],
+			     size_t ndigits, unsigned int t1[], unsigned int t2[])
+{
+	mp_multiply(t1, x, y, ndigits);
+	mp_divide(t2, y, t1, ndigits * 2, m, ndigits);
+}
+
+static int mp_mod_exp_1(unsigned int yout[], const unsigned int x[],
+			const unsigned int e[], unsigned int m[], size_t ndigits)
+{
+	/* Computes y = x^e mod m */
+	/* Classic binary left-to-right method */
+	unsigned int mask;
+	size_t n;
+	size_t nn = ndigits * 2;
+	u32 *t1, *t2, *y;
+
+	/*
+	 * [v2.2] removed const restriction on m[] to avoid using an extra alloc'd
+	 * var (m is changed in-situ during the divide operation then restored)
+	 */
+
+	t1 = kzalloc(nn * 3 * sizeof(u32), GFP_KERNEL);
+	t2 = &t1[nn * 1];
+	y  = &t1[nn * 2];
+
+	n = mp_sizeof(e, ndigits);
+	/* Catch e==0 => x^0=1 */
+	if (n == 0) {
+		mp_set_digit(yout, 1, ndigits);
+		goto done;
+	}
+	/* Find second-most significant bit in e */
+	for (mask = XMP_HI_BIT_MASK; mask > 0; mask >>= 1) {
+		if (e[n - 1] & mask)
+			break;
+	}
+	mp_next_bit_mask(&mask, (unsigned int *)&n);
+
+	/* Set y = x */
+	mp_set_equal(y, x, ndigits);
+
+	/* For bit j = k-2 downto 0 */
+	while (n) {
+		/* Square y = y * y mod n */
+		mp_mod_square_temp(y, m, ndigits, t1, t2);
+		if (e[n - 1] & mask) {
+			/*	if e(j) == 1 then multiply
+			 *	y = y * x mod
+			 */
+			mp_mod_mult_temp(y, x, m, ndigits, t1, t2);
+		}
+
+		/* Move to next bit */
+		mp_next_bit_mask(&mask, (unsigned int *)&n);
+	}
+
+	/* Return y */
+	mp_set_equal(yout, y, ndigits);
+
+done:
+	kfree(t1);
+
+	return 0;
+}
+
+int mp_mod_exp(unsigned int y[], const unsigned int x[], const unsigned int n[],
+	       unsigned int d[], size_t ndigits)
+{
+	/* Computes y = x^n mod d */
+
+	return mp_mod_exp_1(y, x, n, d, ndigits);
+}
diff --git a/drivers/staging/xilinx_hdcp/xlnx_hdcp_rng.c b/drivers/staging/xilinx_hdcp/xlnx_hdcp_rng.c
new file mode 100644
index 000000000..ec71edb10
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/xlnx_hdcp_rng.c
@@ -0,0 +1,72 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx HDCP2X Random Number Generator driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ *
+ * This driver initializes Random Number Generator-RNG, which is used to
+ * produce random numbers during the HDCP authentication and Key exchange.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/io.h>
+#include <linux/xlnx/xlnx_hdcp_rng.h>
+
+static int xlnx_hdcp2x_rng_read(void __iomem *rng_coreaddress, int reg_offset)
+{
+	return readl(rng_coreaddress + reg_offset);
+}
+
+static void xlnx_hdcp2x_rng_write(void __iomem *rng_coreaddress, int reg_offset,
+				  u32 data)
+{
+	writel(data, rng_coreaddress + reg_offset);
+}
+
+int xlnx_hdcp2x_rng_cfg_init(struct xlnx_hdcp2x_rng_hw *rng_cfg)
+{
+	u32 reg_read;
+
+	reg_read = xlnx_hdcp2x_rng_read(rng_cfg->rng_coreaddress,
+					XHDCP2X_RNG_VER_ID_OFFSET);
+	reg_read = FIELD_GET(XHDCP2X_RNG_MASK_16, reg_read);
+	if (reg_read != XHDCP2X_RNG_VER_ID)
+		return (-EINVAL);
+
+	return 0;
+}
+
+void xlnx_hdcp2x_rng_get_random_number(struct xlnx_hdcp2x_rng_hw *rng_cfg,
+				       u8 *writeptr, u16 length, u16 randomlength)
+{
+	u32 i, j;
+	u32 offset = 0;
+	u32 random_word;
+	u8 *read_ptr = (u8 *)&random_word;
+
+	/*
+	 * randomlength is the requested length of the random number in bytes.
+	 * The length must be a multiple of 4
+	 */
+	for (i = 0; i < randomlength; i += 4) {
+		random_word = xlnx_hdcp2x_rng_read(rng_cfg->rng_coreaddress,
+						   XHDCP2X_RNG_REG_RN_1_OFFSET + offset);
+		for (j = 0; j < 4; j++)
+			writeptr[i + j] = read_ptr[j];
+		offset = (offset + 4) % 16;
+	}
+}
+
+void xlnx_hdcp2x_rng_enable(struct xlnx_hdcp2x_rng_hw *rng_cfg)
+{
+	xlnx_hdcp2x_rng_write(rng_cfg->rng_coreaddress, XHDCP2X_RNG_REG_CTRL_SET_OFFSET,
+			      XHDCP2X_RNG_REG_CTRL_RUN_MASK);
+}
+
+void xlnx_hdcp2x_rng_disable(struct xlnx_hdcp2x_rng_hw *rng_cfg)
+{
+	xlnx_hdcp2x_rng_write(rng_cfg->rng_coreaddress, XHDCP2X_RNG_REG_CTRL_CLR_OFFSET,
+			      XHDCP2X_RNG_REG_CTRL_RUN_MASK);
+}
diff --git a/drivers/staging/xilinx_hdcp/xlnx_timer.c b/drivers/staging/xilinx_hdcp/xlnx_timer.c
new file mode 100644
index 000000000..4f12a5e83
--- /dev/null
+++ b/drivers/staging/xilinx_hdcp/xlnx_timer.c
@@ -0,0 +1,250 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AXI Timer driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ *
+ * This driver initializes Xilinx timer/counter component.
+ */
+
+#include <linux/io.h>
+#include <linux/xlnx/xlnx_timer.h>
+
+/*
+ * The following data type maps an option to a register mask such that getting
+ * and setting the options may be table driven.
+ */
+struct mapping {
+	u32 option;
+	u32 mask;
+};
+
+/*
+ * Create the table which contains options which are to be processed to get/set
+ * the options. These options are table driven to allow easy maintenance and
+ * expansion of the options.
+ */
+static struct mapping options_table[] = {
+	{XTC_CASCADE_MODE_OPTION, XTC_CSR_CASC_MASK},
+	{XTC_ENABLE_ALL_OPTION, XTC_CSR_ENABLE_ALL_MASK},
+	{XTC_DOWN_COUNT_OPTION, XTC_CSR_DOWN_COUNT_MASK},
+	{XTC_CAPTURE_MODE_OPTION, XTC_CSR_CAPTURE_MODE_MASK |
+	 XTC_CSR_EXT_CAPTURE_MASK},
+	{XTC_INT_MODE_OPTION, XTC_CSR_ENABLE_INT_MASK},
+	{XTC_AUTO_RELOAD_OPTION, XTC_CSR_AUTO_RELOAD_MASK},
+	{XTC_EXT_COMPARE_OPTION, XTC_CSR_EXT_GENERATE_MASK}
+};
+
+#define XTC_NUM_OPTIONS   ARRAY_SIZE(options_table)
+
+static const u8 xtmrctr_offset[] = { 0, XTC_TIMER_COUNTER_OFFSET };
+
+static void xlnx_hdcp_tmrcntr_write_reg(void __iomem *coreaddress,
+					u32 tmrctr_number, u32 offset, u32 value)
+{
+	writel(value, coreaddress + xtmrctr_offset[tmrctr_number] + offset);
+}
+
+static u32 xlnx_hdcp_tmrcntr_read_reg(void __iomem *coreaddress, u32 tmrctr_number,
+				      u32 offset)
+{
+	return readl(coreaddress + xtmrctr_offset[tmrctr_number] + offset);
+}
+
+void xlnx_hdcp_tmrcntr_set_handler(struct xlnx_hdcp_timer_config *xtimercntr,
+				   xlnx_timer_cntr_handler funcptr,
+				   void *callbackref)
+{
+	xtimercntr->handler = funcptr;
+	xtimercntr->callbackref = callbackref;
+}
+EXPORT_SYMBOL_GPL(xlnx_hdcp_tmrcntr_set_handler);
+
+void xlnx_hdcp_tmrcntr_cfg_init(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	xtimercntr->callbackref = xtimercntr;
+}
+
+static int xlnx_hdcp_timer_init(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 tmr_cntr_started[XTC_DEVICE_TIMER_COUNT];
+	int status = -EINVAL;
+	u8 tmr_index;
+
+	tmr_cntr_started[0] = xtimercntr->is_tmrcntr0_started;
+	tmr_cntr_started[1] = xtimercntr->is_tmrcntr1_started;
+
+	for (tmr_index = 0; tmr_index < XTC_DEVICE_TIMER_COUNT; tmr_index++) {
+		if (tmr_cntr_started[tmr_index] == XTC_COMPONENT_IS_STARTED)
+			continue;
+		xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_index,
+					    XTC_TLR_OFFSET, 0);
+		xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_index,
+					    XTC_TCSR_OFFSET,
+					    XTC_CSR_INT_OCCURED_MASK | XTC_CSR_LOAD_MASK);
+		xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_index,
+					    XTC_TCSR_OFFSET, 0);
+		status = 0;
+	}
+
+	return status;
+}
+
+int xlnx_hdcp_tmrcntr_init(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	if (xtimercntr->is_tmrcntr0_started == XTC_COMPONENT_IS_STARTED &&
+	    xtimercntr->is_tmrcntr1_started == XTC_COMPONENT_IS_STARTED)
+		return 0;
+
+	xlnx_hdcp_tmrcntr_cfg_init(xtimercntr);
+
+	return xlnx_hdcp_timer_init(xtimercntr);
+}
+
+void xlnx_hdcp_tmrcntr_start(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 cntrl_statusreg;
+
+	cntrl_statusreg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						     tmr_cntr_number,
+						     XTC_TCSR_OFFSET);
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET,
+				    XTC_CSR_LOAD_MASK);
+
+	if (tmr_cntr_number == XTC_TIMER_0)
+		xtimercntr->is_tmrcntr0_started = XTC_COMPONENT_IS_STARTED;
+	else
+		xtimercntr->is_tmrcntr1_started = XTC_COMPONENT_IS_STARTED;
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET,
+				    cntrl_statusreg | XTC_CSR_ENABLE_TMR_MASK);
+	cntrl_statusreg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						     tmr_cntr_number,
+						     XTC_TCSR_OFFSET);
+}
+
+void xlnx_hdcp_tmrcntr_stop(struct xlnx_hdcp_timer_config *xtimercntr,
+			    u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 cntrl_statusreg;
+
+	cntrl_statusreg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						     tmr_cntr_number, XTC_TCSR_OFFSET);
+
+	cntrl_statusreg &= (u32)~(XTC_CSR_ENABLE_TMR_MASK);
+
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET, cntrl_statusreg);
+
+	if (tmr_cntr_number == XTC_TIMER_0)
+		xtimercntr->is_tmrcntr0_started = 0;
+	else
+		xtimercntr->is_tmrcntr1_started = 0;
+}
+
+u32 xlnx_hdcp_tmrcntr_get_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+
+	return xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+					  tmr_cntr_number,
+					  XTC_TCR_OFFSET);
+}
+
+void xlnx_hdcp_tmrcntr_set_reset_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				       u8 tmr_cntr_number, u32 reset_value)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TLR_OFFSET, reset_value);
+}
+
+void xlnx_hdcp_tmrcntr_reset(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 counter_cntrl_reg;
+
+	counter_cntrl_reg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						       tmr_cntr_number,
+						       XTC_TCSR_OFFSET);
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET,
+				    counter_cntrl_reg | XTC_CSR_LOAD_MASK);
+
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET, counter_cntrl_reg);
+}
+
+void xlnx_hdcp_tmrcntr_set_options(struct xlnx_hdcp_timer_config *xtimercntr,
+				   u8 tmr_cntr_number, u32 options)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 counter_cntrl_reg = 0;
+	u32 index;
+
+	for (index = 0; index < XTC_NUM_OPTIONS; index++) {
+		if (options & options_table[index].option)
+			counter_cntrl_reg |= options_table[index].mask;
+		else
+			counter_cntrl_reg &= ~options_table[index].mask;
+	}
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET, counter_cntrl_reg);
+}
+
+/**
+ * xlnx_hdcp_tmrcntr_interrupt_handler - HDCP timercntr interrupt handler
+ * @xtimercntr: timercounter configuration structure
+ */
+void xlnx_hdcp_tmrcntr_interrupt_handler(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 control_status_reg;
+	u8 tmr_cntr_number;
+
+	for (tmr_cntr_number = 0;
+		tmr_cntr_number < XTC_DEVICE_TIMER_COUNT; tmr_cntr_number++) {
+		control_status_reg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+								tmr_cntr_number,
+								XTC_TCSR_OFFSET);
+		if (control_status_reg & XTC_CSR_ENABLE_INT_MASK) {
+			if (control_status_reg & XTC_CSR_INT_OCCURED_MASK) {
+				xtimercntr->handler(xtimercntr->callbackref, tmr_cntr_number);
+				control_status_reg =
+					xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+								   tmr_cntr_number,
+								   XTC_TCSR_OFFSET);
+				if (((control_status_reg & XTC_CSR_AUTO_RELOAD_MASK) == 0) &&
+				    ((control_status_reg & XTC_CSR_CAPTURE_MODE_MASK)
+				    == 0)) {
+					control_status_reg &=
+						(u32)~XTC_CSR_ENABLE_TMR_MASK;
+					xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress,
+								    tmr_cntr_number,
+								    XTC_TCSR_OFFSET,
+								    control_status_reg |
+								    XTC_CSR_LOAD_MASK);
+					xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress,
+								    tmr_cntr_number,
+								    XTC_TCSR_OFFSET,
+								    control_status_reg);
+				}
+				xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress,
+							    tmr_cntr_number, XTC_TCSR_OFFSET,
+							    control_status_reg |
+							    XTC_CSR_INT_OCCURED_MASK);
+			}
+		}
+	}
+}
+EXPORT_SYMBOL_GPL(xlnx_hdcp_tmrcntr_interrupt_handler);
diff --git a/drivers/staging/xlnx-mmi-dptx/Kconfig b/drivers/staging/xlnx-mmi-dptx/Kconfig
new file mode 100644
index 000000000..eaa9aab1e
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/Kconfig
@@ -0,0 +1,12 @@
+config DRM_AMD_MMI_DPTX
+	tristate "AMD Multimedia Integrated DisplayPort Tx driver"
+	depends on DRM && OF
+	select DRM_DISPLAY_DP_HELPER
+	select DRM_DISPLAY_HELPER
+	select DRM_KMS_HELPER
+	help
+	  Enable driver support for AMD Multimedia Integrated DisplayPort Tx.
+	  It is implemented as a bridge driver to be connected to AMD
+	  Multimedia Integrated Display Controller CRTC driver.
+	  Use this option if you plan to use the built in DisplayPort
+	  in Versal Gen 2 devices.
diff --git a/drivers/staging/xlnx-mmi-dptx/Makefile b/drivers/staging/xlnx-mmi-dptx/Makefile
new file mode 100644
index 000000000..f262cf7ed
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/Makefile
@@ -0,0 +1,2 @@
+mmi_dptx-objs += mmi_dp_intr.o mmi_dp_config.o mmi_dp_link.o mmi_dp.o
+obj-$(CONFIG_DRM_AMD_MMI_DPTX) += mmi_dptx.o
diff --git a/drivers/staging/xlnx-mmi-dptx/TODO b/drivers/staging/xlnx-mmi-dptx/TODO
new file mode 100644
index 000000000..3387d8642
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/TODO
@@ -0,0 +1,12 @@
+TODO list
+* Use drm helper APIs to read and parse Rx Capabilities.
+* Use drm helper APIs to handle the link training.
+* Add support for power domain and suspend/resume.
+* Add reset functionality.
+* Add clock support.
+* Add support for audio.
+* Add support for MST mode.
+* Add support for change color format.
+* Add support for eDP.
+* Add support for HDCP.
+* Compliance test support.
diff --git a/drivers/staging/xlnx-mmi-dptx/mmi_dp.c b/drivers/staging/xlnx-mmi-dptx/mmi_dp.c
new file mode 100644
index 000000000..a8b50464c
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/mmi_dp.c
@@ -0,0 +1,1202 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Multimedia Integrated DisplayPort Tx driver
+ *
+ * Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
+ */
+
+#include <drm/display/drm_dp_helper.h>
+#include <drm/drm_atomic_helper.h>
+#include <drm/drm_bridge.h>
+#include <drm/drm_connector.h>
+#include <drm/drm_device.h>
+#include <drm/drm_drv.h>
+#include <drm/drm_edid.h>
+#include <drm/drm_fixed.h>
+
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/kernel.h>
+#include <linux/media-bus-format.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/regmap.h>
+
+#include "mmi_dp.h"
+#include "mmi_dp_reg.h"
+
+#define MMI_DPTX_MAX_AUX_RETRIES	(80)
+#define MMI_DPTX_MAX_AUX_MSG_LEN	(16)
+
+/**
+ * mmi_dp_tx_first_bit_set - Find first (least significant) bit set
+ * @data: word to search
+ * Return: bit position or 32 if none is set
+ */
+static u32 mmi_dp_tx_first_bit_set(u32 data)
+{
+	u32 n = 0;
+
+	if (data != 0)
+		n = __ffs(data);
+
+	return n;
+}
+
+/**
+ * mmi_dp_set_field - Set bit field
+ * @data: raw data
+ * @mask: bit field mask
+ * @value: new value
+ * Return: new raw data
+ */
+u32 mmi_dp_set_field(u32 data, u32 mask, u32 value)
+{
+	return (((value << mmi_dp_tx_first_bit_set(mask)) & mask) | (data & ~mask));
+}
+
+/**
+ * mmi_dp_set8_field - Set bit field
+ * @data: raw data
+ * @mask: bit field mask
+ * @value: new value
+ * Return: new raw data
+ */
+u8 mmi_dp_set8_field(u8 data, u8 mask, u8 value)
+{
+	return (((value << mmi_dp_tx_first_bit_set(mask)) & mask) | (data & ~mask));
+}
+
+void mmi_dp_write_mask(struct dptx *dptx, u32 addr, u32 mask, u32 data)
+{
+	u32 temp;
+
+	temp = mmi_dp_set_field(mmi_dp_read(dptx->base, addr), mask, data);
+	mmi_dp_write(dptx->base, addr, temp);
+}
+
+/* Aux Related Api's */
+static int mmi_dp_handle_aux_reply(struct dptx *dptx)
+{
+	u32 auxsts;
+	u32 status;
+
+	while (1) {
+		if (!mmi_dp_read_regfield(dptx->base,
+					  AUX_STATUS, AUX_REPLY_MASK)) {
+			break;
+		}
+
+		if (mmi_dp_read_regfield(dptx->base,
+					 AUX_STATUS, AUX_TIMEOUT_MASK)) {
+			return -ETIMEDOUT;
+		}
+
+		fsleep(1);
+	}
+	auxsts = mmi_dp_read(dptx->base, AUX_STATUS);
+
+	status = mmi_dp_read_regfield(dptx->base, AUX_STATUS,
+				      AUX_STATUS_MASK) >> DPTX_AUX_STS_STATUS_SHIFT;
+
+	switch (status) {
+	case DPTX_AUX_STS_STATUS_ACK:
+	case DPTX_AUX_STS_STATUS_NACK:
+	case DPTX_AUX_STS_STATUS_DEFER:
+	case DPTX_AUX_STS_STATUS_I2C_NACK:
+	case DPTX_AUX_STS_STATUS_I2C_DEFER:
+		break;
+	default:
+		dptx_err(dptx, "Invalid AUX status 0x%x\n", status);
+		break;
+	}
+
+	dptx->aux.data[0] = mmi_dp_read(dptx->base, AUX_DATA0);
+	dptx->aux.data[1] = mmi_dp_read(dptx->base, AUX_DATA1);
+	dptx->aux.data[2] = mmi_dp_read(dptx->base, AUX_DATA2);
+	dptx->aux.data[3] = mmi_dp_read(dptx->base, AUX_DATA3);
+	dptx->aux.sts = auxsts;
+
+	return 0;
+}
+
+static void mmi_dp_aux_clear_data(struct dptx *dptx)
+{
+	mmi_dp_write(dptx->base, AUX_DATA0, 0);
+	mmi_dp_write(dptx->base, AUX_DATA1, 0);
+	mmi_dp_write(dptx->base, AUX_DATA2, 0);
+	mmi_dp_write(dptx->base, AUX_DATA3, 0);
+}
+
+static int mmi_dp_aux_read_data(struct dptx *dptx, u8 *bytes, unsigned int len)
+{
+	const u32 *data = dptx->aux.data;
+
+	memcpy(bytes, data, len);
+
+	return len;
+}
+
+static int mmi_dp_aux_write_data(struct dptx *dptx, u8 const *bytes,
+				 unsigned int len)
+{
+	unsigned int i;
+	u32 data[4] = { 0 };
+
+	for (i = 0; i < len; i++)
+		data[i / 4] |= (bytes[i] << ((i % 4) * 8));
+
+	mmi_dp_write(dptx->base, AUX_DATA0, data[0]);
+	mmi_dp_write(dptx->base, AUX_DATA1, data[1]);
+	mmi_dp_write(dptx->base, AUX_DATA2, data[2]);
+	mmi_dp_write(dptx->base, AUX_DATA3, data[3]);
+
+	return len;
+}
+
+static int mmi_dp_aux_rw(struct dptx *dptx, bool rw, bool i2c, bool mot,
+			 bool addr_only, u32 addr, u8 *bytes, unsigned int len)
+{
+	int retval, tries = 0;
+	u32 auxcmd, type;
+	unsigned int status, br;
+
+again:
+	tries++;
+	if (tries > MMI_DPTX_MAX_AUX_RETRIES)
+		return -ENODATA;
+
+	dptx_dbg(dptx, "%s: addr=0x%08x, len=%d, try=%d\n",
+		 __func__, addr, len, tries);
+
+	if (len > MMI_DPTX_MAX_AUX_MSG_LEN || len == 0) {
+		dptx_warn(dptx, "AUX read/write len must be 1-15, len=%d\n", len);
+		return -EINVAL;
+	}
+
+	type = rw ? DPTX_AUX_CMD_TYPE_READ : DPTX_AUX_CMD_TYPE_WRITE;
+
+	if (!i2c)
+		type |= DPTX_AUX_CMD_TYPE_NATIVE;
+
+	if (i2c && mot)
+		type |= DPTX_AUX_CMD_TYPE_MOT;
+
+	mdelay(1);
+	mmi_dp_aux_clear_data(dptx);
+
+	if (!rw)
+		mmi_dp_aux_write_data(dptx, bytes, len);
+
+	auxcmd = (type << DPTX_AUX_CMD_TYPE_SHIFT |
+		  addr << DPTX_AUX_CMD_ADDR_SHIFT |
+		  (len - 1) << DPTX_AUX_CMD_REQ_LEN_SHIFT);
+
+	if (addr_only)
+		auxcmd |= DPTX_AUX_CMD_I2C_ADDR_ONLY;
+
+	dptx_dbg(dptx, "%s - AUX_CMD: 0x%04X\n", __func__, auxcmd);
+	mmi_dp_write(dptx->base, AUX_CMD, auxcmd);
+
+	retval = mmi_dp_handle_aux_reply(dptx);
+
+	if (retval == -ETIMEDOUT) {
+		dptx_err(dptx, "AUX timed out\n");
+		goto again;
+	}
+
+	if (retval == -ESHUTDOWN) {
+		dptx_err(dptx, "AUX aborted on driver shutdown\n");
+		return retval;
+	}
+
+	if (atomic_read(&dptx->aux.abort) && !(atomic_read(&dptx->aux.serving))) {
+		dptx_err(dptx, "AUX aborted\n");
+		return -ETIMEDOUT;
+	}
+
+	if (retval) {
+		dptx_err(dptx, "new error\n");
+		return retval;
+	}
+
+	status = mmi_dp_read_regfield(dptx->base, AUX_STATUS,
+				      AUX_STATUS_MASK) >> DPTX_AUX_STS_STATUS_SHIFT;
+
+	br = mmi_dp_read_regfield(dptx->base, AUX_STATUS, AUX_BYTES_READ);
+
+	switch (status) {
+	case DPTX_AUX_STS_STATUS_ACK:
+		dptx_dbg(dptx, "AUX Success\n");
+		if (!br) {
+			dptx_err(dptx, "BR=0, Retry\n");
+			mmi_dp_soft_reset(dptx, DPTX_SRST_CTRL_AUX);
+			goto again;
+		}
+		break;
+	case DPTX_AUX_STS_STATUS_NACK:
+	case DPTX_AUX_STS_STATUS_I2C_NACK:
+		dptx_err(dptx, "AUX Nack\n");
+		return -EINVAL;
+	case DPTX_AUX_STS_STATUS_I2C_DEFER:
+	case DPTX_AUX_STS_STATUS_DEFER:
+		dptx_dbg(dptx, "AUX Defer\n");
+		goto again;
+	default:
+		dptx_err(dptx, "AUX Status Invalid\n");
+		mmi_dp_soft_reset(dptx, DPTX_SRST_CTRL_AUX);
+		goto again;
+	}
+
+	if (rw)
+		mmi_dp_aux_read_data(dptx, bytes, len);
+
+	return 0;
+}
+
+static int mmi_dp_aux_rw_bytes(struct dptx *dptx, bool rw, bool i2c,
+			       u32 addr, u8 *bytes, unsigned int len)
+{
+	int retval;
+	unsigned int i;
+	u32 addr_v = addr;
+
+	for (i = 0; i < len;) {
+		unsigned int curlen;
+
+		curlen = min_t(unsigned int, len - i, MMI_DPTX_MAX_AUX_MSG_LEN);
+		/* In case of i2c address will be handled by i2c protocol */
+		if (!i2c)
+			addr_v = addr + i;
+		retval = mmi_dp_aux_rw(dptx, rw, i2c, true, false, addr_v, &bytes[i], curlen);
+		if (retval)
+			return retval;
+
+		i += curlen;
+	}
+
+	return 0;
+}
+
+int __mmi_dp_read_bytes_from_dpcd(struct dptx *dptx,
+				  u32 reg_addr,
+				  u8 *bytes,
+				  u32 len)
+{
+	return mmi_dp_aux_rw_bytes(dptx, true, false, reg_addr, bytes, len);
+}
+
+int __mmi_dp_write_bytes_to_dpcd(struct dptx *dptx,
+				 u32 reg_addr,
+				 u8 *bytes,
+				 u32 len)
+{
+	return mmi_dp_aux_rw_bytes(dptx, false, false, reg_addr, bytes, len);
+}
+
+int __mmi_dp_read_dpcd(struct dptx *dptx, u32 addr, u8 *byte)
+{
+	return __mmi_dp_read_bytes_from_dpcd(dptx, addr, byte, 1);
+}
+
+int __mmi_dp_write_dpcd(struct dptx *dptx, u32 addr, u8 byte)
+{
+	return __mmi_dp_write_bytes_to_dpcd(dptx, addr, &byte, 1);
+}
+
+/* Core Related api's */
+/*
+ * Core Access Layer
+ *
+ * Provides low-level register access to the DPTX core.
+ */
+
+/**
+ * mmi_dp_intr_en() - Enables interrupts
+ * @dptx: The dptx struct
+ * @bits: The interrupts to enable
+ *
+ * This function enables (unmasks) all interrupts in the INTERRUPT
+ * register specified by @bits.
+ */
+static void mmi_dp_intr_en(struct dptx *dptx, u32 bits)
+{
+	u32 ien;
+
+	ien = mmi_dp_read(dptx->base, GENERAL_INTERRUPT_ENABLE);
+	ien |= bits;
+	mmi_dp_write(dptx->base, GENERAL_INTERRUPT_ENABLE, ien);
+}
+
+/**
+ * mmi_dp_intr_dis() - Disables interrupts
+ * @dptx: The dptx struct
+ * @bits: The interrupts to disable
+ *
+ * This function disables (masks) all interrupts in the INTERRUPT
+ * register specified by @bits.
+ */
+static void mmi_dp_intr_dis(struct dptx *dptx, u32 bits)
+{
+	u32 ien;
+
+	ien = mmi_dp_read(dptx->base, GENERAL_INTERRUPT_ENABLE);
+	ien &= ~bits;
+	mmi_dp_write(dptx->base, GENERAL_INTERRUPT_ENABLE, ien);
+}
+
+/**
+ * mmi_dp_global_intr_en() - Enables top-level interrupts
+ * @dptx: The dptx struct
+ *
+ * Enables (unmasks) all top-level interrupts.
+ */
+
+void mmi_dp_global_intr_en(struct dptx *dptx)
+{
+	mmi_dp_intr_en(dptx, DPTX_IEN_ALL_INTR &
+		       ~(DPTX_ISTS_AUX_REPLY | DPTX_ISTS_AUX_CMD_INVALID));
+}
+
+/**
+ * mmi_dp_global_intr_dis() - Disables top-level interrupts
+ * @dptx: The dptx struct
+ *
+ * Disables (masks) all top-level interrupts.
+ */
+void mmi_dp_global_intr_dis(struct dptx *dptx)
+{
+	mmi_dp_intr_dis(dptx, DPTX_IEN_ALL_INTR);
+}
+
+/**
+ * mmi_dp_video_intr_dis() - Disables video interrupts
+ * @dptx: The dptx struct
+ *
+ * Disables (masks) all video interrupts.
+ */
+void mmi_dp_video_intr_dis(struct dptx *dptx)
+{
+	mmi_dp_intr_dis(dptx, DPTX_IEN_VIDEO_FIFO_OVERFLOW |
+			DPTX_IEN_VIDEO_FIFO_UNDERFLOW);
+}
+
+/**
+ * mmi_dp_enable_hpd_intr() - Enables HPD interrupts
+ * @dptx: The dptx struct
+ *
+ * Enables (unmasks) HPD interrupts.
+ */
+void mmi_dp_enable_hpd_intr(struct dptx *dptx)
+{
+	mmi_dp_intr_en(dptx, DPTX_ISTS_HPD);
+}
+
+void mmi_dp_clean_interrupts(struct dptx *dptx)
+{
+	u32 intr;
+
+	/* Set reset bits */
+	intr = mmi_dp_read(dptx->base, GENERAL_INTERRUPT);
+	intr |= GEN_INTR_AUX_REPLY_EVENT;
+	intr |= GEN_INTR_AUDIO_FIFO_OVERFLOW_STREAM0;
+	intr |= GEN_INTR_VIDEO_FIFO_OVERFLOW_STREAM0;
+	intr |= GEN_INTR_VIDEO_FIFO_UNDERFLOW_STREAM0;
+	mmi_dp_write(dptx->base, GENERAL_INTERRUPT, intr);
+}
+
+/**
+ * mmi_dp_soft_reset() - Performs a core soft reset
+ * @dptx: The dptx struct
+ * @bits: The components to reset
+ *
+ * Resets specified parts of the core by writing @bits into the core
+ * soft reset control register and clearing them 10-20 microseconds
+ * later.
+ */
+void mmi_dp_soft_reset(struct dptx *dptx, u32 bits)
+{
+	u32 rst;
+
+	bits &= DPTX_SRST_CTRL_ALL;
+
+	/* Set reset bits */
+	rst = mmi_dp_read(dptx->base, SOFT_RESET_CTRL);
+	rst |= bits;
+	mmi_dp_write(dptx->base, SOFT_RESET_CTRL, rst);
+
+	usleep_range(10, 20);
+
+	/* Clear reset bits */
+	rst = mmi_dp_read(dptx->base, SOFT_RESET_CTRL);
+	rst &= ~bits;
+	mmi_dp_write(dptx->base, SOFT_RESET_CTRL, rst);
+}
+
+/**
+ * mmi_dp_soft_reset_all() - Reset all core modules
+ * @dptx: The dptx struct
+ */
+static void mmi_dp_soft_reset_all(struct dptx *dptx)
+{
+	mmi_dp_soft_reset(dptx, DPTX_SRST_CTRL_ALL);
+}
+
+/**
+ * mmi_dp_core_init_phy() - Initializes the DP TX PHY module
+ * @dptx: The dptx struct
+ *
+ * Initializes the PHY layer of the core. This needs to be called
+ * whenever the PHY layer is reset.
+ */
+void mmi_dp_core_init_phy(struct dptx *dptx)
+{
+	mmi_dp_set(dptx->base, PHYIF_CTRL, PHYIF_PHY_WIDTH);
+}
+
+/**
+ * mmi_dp_check_dptx_id_n_ver() - Check value of DPTX_ID register
+ * @dptx: The dptx struct
+ *
+ * Return: True if DPTX core correctly identified.
+ */
+static bool mmi_dp_check_dptx_id_n_ver(struct dptx *dptx)
+{
+	u32 dptx_id, version;
+
+	dptx_id = mmi_dp_read(dptx->base, DPTX_ID);
+	version = mmi_dp_read(dptx->base, DPTX_VERSION_NUMBER);
+	if ((dptx_id != ((DPTX_ID_DEVICE_ID << DPTX_ID_DEVICE_ID_SHIFT) |
+			DPTX_ID_VENDOR_ID)) || version != DPTX_VERSION)
+		return false;
+
+	return true;
+}
+
+static void mmi_dp_init_hwparams(struct dptx *dptx)
+{
+	/* Combo PHY */
+	dptx->hwparams.gen2_phy = mmi_dp_read_regfield(dptx->base,
+						       DPTX_CONFIG_REG1,
+						       DPTX_GEN2_PHY_MASK);
+
+	/* Forward Error Correction (FEC) */
+	dptx->hwparams.fec = mmi_dp_read_regfield(dptx->base,
+						  DPTX_CONFIG_REG1,
+						  DPTX_FEC_EN_MASK);
+
+	/* Embedded DisplayPort (eDP) */
+	dptx->hwparams.edp = mmi_dp_read_regfield(dptx->base,
+						  DPTX_CONFIG_REG1,
+						  DPTX_EDP_EN_MASK);
+
+	/* Display Stream Compression (DSC) */
+	dptx->hwparams.dsc = mmi_dp_read_regfield(dptx->base,
+						  DPTX_CONFIG_REG1,
+						  DPTX_DSC_EN_MASK);
+
+	/* Multi pixel mode */
+	dptx->hwparams.mp_mode = mmi_dp_read_regfield(dptx->base,
+						      DPTX_CONFIG_REG1,
+						      DPTX_MP_MODE_MASK);
+
+	/* Max Number MST streams */
+	dptx->hwparams.num_streams = mmi_dp_read_regfield(dptx->base,
+							  DPTX_CONFIG_REG1,
+							  DPTX_NUM_STREAMS_MASK);
+
+	/* Sync Depth - 2 or 3 Stages */
+	dptx->hwparams.sync_depth = mmi_dp_read_regfield(dptx->base,
+							 DPTX_CONFIG_REG1,
+							 DPTX_SYNC_DEPTH_MASK);
+
+	/* FPGA - Internal Video and Audio Generators Instantiation */
+	dptx->hwparams.fpga = mmi_dp_read_regfield(dptx->base,
+						   DPTX_CONFIG_REG1,
+						   DPTX_FPGA_EN_MASK);
+
+	/* SDP Register Bank Size */
+	dptx->hwparams.sdp_reg_bank_size = mmi_dp_read_regfield(dptx->base,
+								DPTX_CONFIG_REG1,
+								DPTX_SDP_REG_BANK_SZ_MASK);
+
+	/* Audio Selected */
+	dptx->hwparams.audio_select = mmi_dp_read_regfield(dptx->base,
+							   DPTX_CONFIG_REG1,
+							   DPTX_AUDIO_SELECT_MASK);
+
+	/* HDCP */
+	dptx->hwparams.hdcp = mmi_dp_read_regfield(dptx->base,
+						   DPTX_CONFIG_REG1,
+						   DPTX_HDCP_SELECT_MASK);
+
+	/* Panel Self Refresh (PSR) Version */
+	dptx->hwparams.psr_version = mmi_dp_read_regfield(dptx->base,
+							  DPTX_CONFIG_REG3,
+							  DPTX_PSR_VER_MASK);
+
+	/* Adaptive Sync */
+	dptx->hwparams.adsync = mmi_dp_read_regfield(dptx->base,
+						     DPTX_CONFIG_REG3,
+						     DPTX_ADSYNC_EN_MASK);
+
+	/* PHY Type */
+	dptx->hwparams.phy_type = mmi_dp_read_regfield(dptx->base,
+						       DPTX_CONFIG_REG3,
+						       DPTX_PHY_TYPE_MASK);
+}
+
+/**
+ * mmi_dp_core_init() - Initializes the DP TX core
+ * @dptx: The dptx struct
+ *
+ * Initialize the DP TX core and put it in a known state.
+ * Return: returns 0 on success
+ */
+static void mmi_dp_core_init(struct dptx *dptx)
+{
+	u32 hpd_ien;
+
+	/* Reset the core */
+	mmi_dp_soft_reset_all(dptx);
+
+	/* Enable MST */
+	mmi_dp_write(dptx->base, CCTL,
+		     (dptx->mst ? DPTX_CCTL_ENABLE_MST_MODE : 0));
+
+	mmi_dp_core_init_phy(dptx);
+
+	/* Enable all HPD interrupts */
+	hpd_ien = mmi_dp_read(dptx->base, HPD_INTERRUPT_ENABLE);
+	hpd_ien |= (DPTX_HPD_IEN_IRQ_EN |
+		    DPTX_HPD_IEN_HOT_PLUG_EN |
+		    DPTX_HPD_IEN_HOT_UNPLUG_EN);
+	mmi_dp_write(dptx->base, HPD_INTERRUPT_ENABLE, hpd_ien);
+}
+
+/**
+ * mmi_dp_core_deinit() - Deinitialize the core
+ * @dptx: The dptx struct
+ *
+ * Disable the core in preparation for module shutdown.
+ */
+static void mmi_dp_core_deinit(struct dptx *dptx)
+{
+	mmi_dp_global_intr_dis(dptx);
+	mmi_dp_soft_reset_all(dptx);
+}
+
+void mmi_dp_phy_set_lanes(struct dptx *dptx, unsigned int lanes)
+{
+	u32 val;
+
+	dptx_dbg(dptx, "%s: lanes=%d\n", __func__, lanes);
+
+	switch (lanes) {
+	case 1:
+		val = 0;
+		break;
+	case 2:
+		val = 1;
+		break;
+	case 4:
+		val = 2;
+		break;
+	default:
+		dptx_warn(dptx, "Invalid number of lanes %d\n - will set to 4", lanes);
+		val = 2;
+		break;
+	}
+
+	mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_LANES, val);
+}
+
+void mmi_dp_phy_set_rate(struct dptx *dptx, unsigned int rate)
+{
+	dptx_dbg(dptx, "%s: rate=%d\n", __func__, rate);
+
+	switch (rate) {
+	case DPTX_PHYIF_CTRL_RATE_RBR:
+	case DPTX_PHYIF_CTRL_RATE_HBR:
+		/* Set 20-bit PHY width */
+		mmi_dp_clr(dptx->base, PHYIF_CTRL, PHYIF_PHY_WIDTH);
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR2:
+	case DPTX_PHYIF_CTRL_RATE_HBR3:
+		/* Set 40-bit PHY width */
+		mmi_dp_set(dptx->base, PHYIF_CTRL, PHYIF_PHY_WIDTH);
+		break;
+	default:
+		dptx_warn(dptx, "Invalid PHY rate %d\n", rate);
+		break;
+	}
+
+	mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_RATE, rate);
+}
+
+void mmi_dp_phy_set_pre_emphasis(struct dptx *dptx,
+				 unsigned int lane,
+				 unsigned int level)
+{
+	u32 phytxeq;
+
+	dptx_dbg(dptx, "%s: lane=%d, level=0x%x\n", __func__, lane, level);
+
+	if (lane > 3) {
+		dptx_warn(dptx, "Invalid Lane %d", lane);
+		return;
+	}
+
+	if (level > 3) {
+		dptx_warn(dptx, "Invalid pre-emphasis level %d, using 3", level);
+		level = 3;
+	}
+
+	phytxeq = mmi_dp_read(dptx->base, PHY_TX_EQ);
+	phytxeq &= ~DPTX_PHY_TX_EQ_PREEMP_MASK(lane);
+	phytxeq |= (level << DPTX_PHY_TX_EQ_PREEMP_SHIFT(lane)) &
+		   DPTX_PHY_TX_EQ_PREEMP_MASK(lane);
+
+	mmi_dp_write(dptx->base, PHY_TX_EQ, phytxeq);
+}
+
+void mmi_dp_phy_set_vswing(struct dptx *dptx,
+			   unsigned int lane,
+			   unsigned int level)
+{
+	u32 phytxeq;
+
+	dptx_dbg(dptx, "%s: lane=%d, level=0x%x\n", __func__, lane, level);
+
+	if (lane > 3) {
+		dptx_warn(dptx, "Invalid Lane %d", lane);
+		return;
+	}
+
+	if (level > DPTX_PHY_TX_EQ_VSWING_LVL_3) {
+		dptx_warn(dptx, "Invalid pre-emphasis level %d, using 3", level);
+		level = DPTX_PHY_TX_EQ_VSWING_LVL_3;
+	}
+
+	phytxeq = mmi_dp_read(dptx->base, PHY_TX_EQ);
+	phytxeq &= ~DPTX_PHY_TX_EQ_VSWING_MASK(lane);
+	phytxeq |= (level << DPTX_PHY_TX_EQ_VSWING_SHIFT(lane)) &
+		   DPTX_PHY_TX_EQ_VSWING_MASK(lane);
+
+	mmi_dp_write(dptx->base, PHY_TX_EQ, phytxeq);
+}
+
+void mmi_dp_phy_set_pattern(struct dptx *dptx, unsigned int pattern)
+{
+	mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_TPS_SEL, pattern);
+}
+
+void mmi_dp_phy_enable_xmit(struct dptx *dptx, unsigned int lanes, bool enable)
+{
+	u32 phyifctrl;
+	u32 mask = 0;
+
+	phyifctrl = mmi_dp_read(dptx->base, PHYIF_CTRL);
+
+	switch (lanes) {
+	case 4:
+		mask |= DPTX_PHYIF_CTRL_XMIT_EN(3);
+		mask |= DPTX_PHYIF_CTRL_XMIT_EN(2);
+		fallthrough;
+	case 2:
+		mask |= DPTX_PHYIF_CTRL_XMIT_EN(1);
+		fallthrough;
+	case 1:
+		mask |= DPTX_PHYIF_CTRL_XMIT_EN(0);
+		break;
+	default:
+		dptx_warn(dptx, "Invalid number of lanes %d\n", lanes);
+		break;
+	}
+
+	if (enable)
+		phyifctrl |= mask;
+	else
+		phyifctrl &= ~mask;
+
+	mmi_dp_write(dptx->base, PHYIF_CTRL, phyifctrl);
+}
+
+int mmi_dp_phy_rate_to_bw(unsigned int rate)
+{
+	switch (rate) {
+	case DPTX_PHYIF_CTRL_RATE_RBR:
+		return DP_LINK_BW_1_62;
+	case DPTX_PHYIF_CTRL_RATE_HBR:
+		return DP_LINK_BW_2_7;
+	case DPTX_PHYIF_CTRL_RATE_HBR2:
+		return DP_LINK_BW_5_4;
+	case DPTX_PHYIF_CTRL_RATE_HBR3:
+		return DP_LINK_BW_8_1;
+	default:
+		return -EINVAL;
+	}
+}
+
+int mmi_dp_bw_to_phy_rate(unsigned int bw)
+{
+	switch (bw) {
+	case DP_LINK_BW_1_62:
+		return DPTX_PHYIF_CTRL_RATE_RBR;
+	case DP_LINK_BW_2_7:
+		return DPTX_PHYIF_CTRL_RATE_HBR;
+	case DP_LINK_BW_5_4:
+		return DPTX_PHYIF_CTRL_RATE_HBR2;
+	case DP_LINK_BW_8_1:
+		return DPTX_PHYIF_CTRL_RATE_HBR3;
+	default:
+		return DPTX_MAX_LINK_RATE;
+	}
+}
+
+static __maybe_unused struct dptx *to_dptx(struct drm_bridge *bridge)
+{
+	return container_of(bridge, struct dptx, bridge);
+}
+
+void mmi_dp_notify(struct dptx *dptx)
+{
+	wake_up_interruptible(&dptx->waitq);
+}
+
+static void mmi_dp_notify_shutdown(struct dptx *dptx)
+{
+	atomic_set(&dptx->shutdown, 1);
+	mmi_dp_notify(dptx);
+}
+
+/**
+ * mmi_dp_max_rate - Calculate and return available max pixel clock
+ * @link_rate: Link rate
+ * @lane_num: Number of lanes
+ * @bpp: bits per pixel
+ *
+ * Return: max pixel clock (Khz) supported by current link training
+ */
+static inline int mmi_dp_max_rate(int link_rate, u8 lane_num, u8 bpp)
+{
+	return link_rate * lane_num * 8 / bpp;
+}
+
+static ssize_t mmi_dp_aux_transfer(struct drm_dp_aux *aux, struct drm_dp_aux_msg *msg)
+{
+	struct dptx *dptx = container_of(aux, struct dptx, dp_aux);
+
+	/* check if the aux is connected */
+	if (dptx->conn_status == connector_status_connected) {
+		if (msg->request & DPTX_AUX_CMD_TYPE_READ) {
+			mmi_dp_aux_rw_bytes(dptx, true, true, (u32)msg->address,
+					    (u8 *)msg->buffer, msg->size);
+		} else {
+			mmi_dp_aux_rw_bytes(dptx, false, true, msg->address,
+					    msg->buffer, msg->size);
+		}
+	} else {
+		dptx_err(dptx, "%s: Aux channel no connected\n", __func__);
+	}
+
+	return msg->size;
+}
+
+static int mmi_dp_aux_init(struct dptx *dptx)
+{
+	int ret = 0;
+
+	dptx->dp_aux.name = "MMI DPTx aux";
+	dptx->dp_aux.dev = dptx->dev;
+	dptx->dp_aux.drm_dev = dptx->bridge.dev;
+	dptx->dp_aux.transfer = mmi_dp_aux_transfer;
+
+	ret = drm_dp_aux_register(&dptx->dp_aux);
+	if (ret) {
+		dptx_err(dptx, "%s: Failed to register drm_dp_aux %d\n",
+			 __func__, ret);
+		return ret;
+	}
+
+	return ret;
+}
+
+static int mmi_dp_bridge_attach(struct drm_bridge *bridge,
+				enum drm_bridge_attach_flags flags)
+{
+	struct dptx *dptx = (struct dptx *)bridge->driver_private;
+	int ret;
+
+	if (flags == DRM_BRIDGE_ATTACH_NO_CONNECTOR)
+		dptx_err(dptx, "%s : DRM_BRIDGE_ATTACH_NO_CONNECTOR\n", __func__);
+
+	/* Initialize and Register Aux */
+	ret = mmi_dp_aux_init(dptx);
+	if (ret) {
+		dptx_err(dptx, "%s: Failed to initialize Dp aux\n", __func__);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void mmi_dp_bridge_detach(struct drm_bridge *bridge)
+{
+	struct dptx *dptx = (struct dptx *)bridge->driver_private;
+
+	if (!dptx)
+		return;
+
+	/* Unregister the aux */
+	drm_dp_aux_unregister(&dptx->dp_aux);
+}
+
+static enum drm_connector_status mmi_dp_bridge_detect(struct drm_bridge *bridge)
+{
+	const struct dptx *dptx = (struct dptx *)bridge->driver_private;
+
+	return dptx->conn_status;
+}
+
+static u32 *mmi_dp_bridge_get_output_bus_fmts(struct drm_bridge *bridge,
+					      struct drm_bridge_state *bridge_state,
+					      struct drm_crtc_state *crtc_state,
+					      struct drm_connector_state *conn_state,
+					      unsigned int *num_output_formats)
+{
+	u32 *out_bus_formats;
+
+	out_bus_formats = kmalloc(sizeof(*out_bus_formats), GFP_KERNEL);
+	if (!out_bus_formats) {
+		*num_output_formats = 0;
+		return NULL;
+	}
+
+	/* TODO - Add more formats */
+	*num_output_formats = 1;
+	out_bus_formats[0] = MEDIA_BUS_FMT_FIXED;
+
+	return out_bus_formats;
+}
+
+static u32 *mmi_dp_bridge_get_input_bus_fmts(struct drm_bridge *bridge,
+					     struct drm_bridge_state *bridge_state,
+					     struct drm_crtc_state *crtc_state,
+					     struct drm_connector_state *conn_state,
+					     u32 output_format,
+					     unsigned int *num_input_formats)
+{
+	u32 *in_bus_formats;
+
+	in_bus_formats = kmalloc(sizeof(*in_bus_formats), GFP_KERNEL);
+	if (!in_bus_formats) {
+		*num_input_formats = 0;
+		return NULL;
+	}
+
+	/* TODO - Add more formats */
+	*num_input_formats = 1;
+	in_bus_formats[0] = MEDIA_BUS_FMT_FIXED;
+
+	return in_bus_formats;
+}
+
+static const struct drm_edid *mmi_dp_bridge_edid_read(struct drm_bridge *bridge,
+						      struct drm_connector *connector)
+{
+	struct dptx *dptx = container_of(bridge, struct dptx, bridge);
+
+	return drm_edid_read_ddc(connector, &dptx->dp_aux.ddc);
+}
+
+static enum drm_mode_status
+mmi_dp_bridge_mode_valid(struct drm_bridge *bridge,
+			 const struct drm_display_info *info,
+			 const struct drm_display_mode *mode)
+{
+	struct dptx *dptx = container_of(bridge, struct dptx, bridge);
+	u32 max_pxl_clk = 0;
+	u32 link_rate = 0;
+
+	dptx->bpp = mmi_dp_get_color_depth_bpp(dptx->vparams[0].bpc,
+					       dptx->vparams[0].pix_enc);
+
+	link_rate = mmi_dp_get_link_rate(dptx->max_rate);
+	link_rate *= 1000;
+
+	max_pxl_clk = mmi_dp_max_rate(link_rate, dptx->max_lanes, dptx->bpp);
+
+	dptx_dbg(dptx, "%s Bpp %d, link_rate %d pixel clock set %d\n", __func__,
+		 dptx->bpp, link_rate, max_pxl_clk);
+
+	if (mode->clock > max_pxl_clk) {
+		dptx_dbg(dptx, "filtered mode %s for high pixel rate\n",
+			 mode->name);
+		return MODE_CLOCK_HIGH;
+	}
+
+	return MODE_OK;
+}
+
+/* Function to fill the dptx display mode */
+static void mmi_dp_configure_params(struct drm_display_mode *mode,
+				    struct display_mode_t *cmode)
+
+{
+	/* FPS */
+	cmode->refresh_rate = drm_mode_vrefresh(mode) * 1000;
+	/* Pixel Clock */
+	cmode->dtd.m_pixel_clock = mode->clock;
+	/* Interlaced */
+	cmode->dtd.m_interlaced = mode->flags & DRM_MODE_FLAG_INTERLACE;
+
+	/* Horizontal data */
+	cmode->dtd.m_h_active = mode->hdisplay;
+	cmode->dtd.m_h_blanking = mode->htotal - mode->hdisplay;
+	cmode->dtd.m_h_border = 0;
+	cmode->dtd.m_h_image_size = mode->hdisplay * mode->width_mm;
+	cmode->dtd.m_h_sync_pulse_width = mode->hsync_end - mode->hsync_start;
+	cmode->dtd.m_h_sync_offset = mode->hsync_start - mode->hdisplay;
+
+	/* Vertical data */
+	cmode->dtd.m_v_active = mode->vdisplay;
+	cmode->dtd.m_v_blanking = mode->vtotal - mode->vdisplay;
+	cmode->dtd.m_v_border = 0;
+	cmode->dtd.m_v_image_size = mode->vdisplay * mode->height_mm;
+	cmode->dtd.m_v_sync_pulse_width = mode->vsync_end - mode->vsync_start;
+	cmode->dtd.m_v_sync_offset = mode->vsync_start - mode->vdisplay;
+}
+
+/* Configure the video mode */
+static int mmi_dp_configure_video(struct dptx *dptx,
+				  struct drm_display_mode *mode)
+{
+	struct video_params *vparams;
+	struct display_mode_t current_vmode;
+	struct dtd mdtd;
+	u32 pixel_clk;
+	u16 rate, peak_stream_bw, link_bw;
+	u8 bpp;
+	s64 fixp;
+	int retval;
+
+	vparams = &dptx->vparams[0];
+
+	/* Reset the dtd structure and fill it */
+	mmi_dp_configure_params(mode, &current_vmode);
+	mmi_dp_dtd_fill(&mdtd, &current_vmode);
+
+	vparams->mdtd = mdtd;
+	dptx->selected_pixel_clock = mode->clock;
+
+	/* check if the link is enough for the payload requested */
+	bpp = mmi_dp_get_color_depth_bpp(vparams->bpc, vparams->pix_enc);
+	rate = mmi_dp_get_link_rate(dptx->link.rate);
+	pixel_clk = vparams->mdtd.pixel_clock;
+	fixp = drm_fixp_div(drm_int2fixp(bpp), drm_int2fixp(8));
+	fixp = drm_fixp_mul(fixp, drm_int2fixp(pixel_clk));
+	fixp = drm_fixp_div(fixp, drm_int2fixp(1000));
+	peak_stream_bw = drm_fixp2int(fixp);
+	link_bw = rate * dptx->link.lanes;
+
+	if (peak_stream_bw > link_bw) {
+		dptx_err(dptx, "ERROR: Mode chosen isn't suitable for Link Rate running\n");
+		return -EINVAL;
+	}
+
+	/* Disable video stream */
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(0), VIDEO_STREAM_ENABLE_MASK, 0);
+	/* As of now do sst configuration */
+	retval = mmi_dp_sst_configuration(dptx);
+	if (retval < 0) {
+		dptx_err(dptx, "Failed sst configuration\n");
+		return retval;
+	}
+	mmi_dp_clean_interrupts(dptx);
+	return 0;
+}
+
+/* Atomic enable function */
+static void mmi_dp_bridge_atomic_enable(struct drm_bridge *bridge,
+					struct drm_bridge_state *old_bridge_state)
+{
+	struct dptx *dptx = container_of(bridge, struct dptx, bridge);
+	struct drm_atomic_state *state = old_bridge_state->base.state;
+	struct drm_crtc_state *crtc_state;
+	struct drm_display_mode *adjusted_mode;
+	struct drm_connector *connector;
+	struct drm_crtc *crtc;
+	int retval;
+
+	connector = drm_atomic_get_new_connector_for_encoder(state,
+							     bridge->encoder);
+
+	crtc = drm_atomic_get_new_connector_state(state, connector)->crtc;
+	crtc_state = drm_atomic_get_new_crtc_state(state, crtc);
+	adjusted_mode = &crtc_state->adjusted_mode;
+
+	/* Configure the video */
+	retval = mmi_dp_configure_video(dptx, adjusted_mode);
+	if (retval < 0) {
+		dptx_err(dptx, "Failed to configure video mode\n");
+		return;
+	}
+	mmi_dp_intr_en(dptx, DPTX_IEN_VIDEO_FIFO_UNDERFLOW |
+		       DPTX_IEN_VIDEO_FIFO_OVERFLOW |
+		       DPTX_IEN_AUDIO_FIFO_OVERFLOW);
+}
+
+static void mmi_dp_bridge_atomic_disable(struct drm_bridge *bridge,
+					 struct drm_bridge_state *old_bridge_state)
+{
+	struct dptx *dptx = container_of(bridge, struct dptx, bridge);
+
+	mmi_dp_intr_dis(dptx, DPTX_IEN_VIDEO_FIFO_UNDERFLOW |
+			DPTX_IEN_VIDEO_FIFO_OVERFLOW |
+			DPTX_IEN_AUDIO_FIFO_OVERFLOW);
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(0), VIDEO_STREAM_ENABLE_MASK, 0);
+}
+
+static const struct drm_bridge_funcs mmi_dp_bridge_funcs = {
+	.attach = mmi_dp_bridge_attach,
+	.detach = mmi_dp_bridge_detach,
+	.detect = mmi_dp_bridge_detect,
+	.atomic_get_output_bus_fmts = mmi_dp_bridge_get_output_bus_fmts,
+	.atomic_get_input_bus_fmts = mmi_dp_bridge_get_input_bus_fmts,
+
+	.atomic_duplicate_state = drm_atomic_helper_bridge_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_bridge_destroy_state,
+	.atomic_reset = drm_atomic_helper_bridge_reset,
+
+	.edid_read = mmi_dp_bridge_edid_read,
+	.mode_valid = mmi_dp_bridge_mode_valid,
+	.atomic_enable = mmi_dp_bridge_atomic_enable,
+	.atomic_disable = mmi_dp_bridge_atomic_disable,
+};
+
+static int mmi_dp_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	struct device *dev;
+	struct dptx *dptx;
+	int retval;
+
+	dev = &pdev->dev;
+
+	dptx = devm_kzalloc(dev, sizeof(*dptx), GFP_KERNEL);
+	if (!dptx)
+		return -ENOMEM;
+
+	/* Update the device node */
+	dptx->dev = dev;
+
+	/* Get MEM resources */
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "dp");
+	dptx->base = devm_ioremap_resource(dptx->dev, res);
+	if (IS_ERR(dptx->base)) {
+		dev_err(dev, "Failed to get and map memory resource\n");
+		return PTR_ERR(dptx->base);
+	}
+
+	if (!mmi_dp_check_dptx_id_n_ver(dptx)) {
+		dev_err(dev, "DPTX_ID or DPTX_VERSION_NUMBER not match to 0x%04x:0x%04x & 0x%08x\n",
+			DPTX_ID_DEVICE_ID, DPTX_ID_VENDOR_ID, DPTX_VERSION);
+		return -ENODEV;
+	}
+
+	/* Get IRQ numbers from device */
+	dptx->irq = platform_get_irq_byname(pdev, "dptx");
+	if (dptx->irq < 0)
+		return dptx->irq;
+
+	dev_info(dev, "IRQ number %d.\n", dptx->irq);
+
+	dptx->cr_fail = false;
+	dptx->mst = false; /* Should be disabled for HDCP. */
+	dptx->ssc_en = false;
+	dptx->fec = false;
+	dptx->streams = 1;
+	dptx->multipixel = DPTX_MP_QUAD_PIXEL;
+
+	mutex_init(&dptx->mutex);
+	init_waitqueue_head(&dptx->waitq);
+	atomic_set(&dptx->sink_request, 0);
+	atomic_set(&dptx->shutdown, 0);
+	atomic_set(&dptx->c_connect, 0);
+
+	dptx->max_rate = DPTX_DEFAULT_LINK_RATE;
+	dptx->max_lanes = DPTX_DEFAULT_LINK_LANES;
+
+	platform_set_drvdata(pdev, dptx);
+
+	/* update connector status */
+	dptx->bridge.driver_private = dptx;
+	dptx->bridge.ops = DRM_BRIDGE_OP_DETECT | DRM_BRIDGE_OP_EDID;
+	dptx->bridge.interlace_allowed = true;
+	dptx->bridge.type = DRM_MODE_CONNECTOR_DisplayPort;
+	dptx->bridge.of_node = pdev->dev.of_node;
+	dptx->bridge.funcs = &mmi_dp_bridge_funcs;
+	dptx->conn_status = connector_status_disconnected;
+
+	/* Get next bridge in chain using drm_of_find_panel_or_bridge */
+	devm_drm_bridge_add(dev, &dptx->bridge);
+
+	mmi_dp_global_intr_dis(dptx);
+
+	mmi_dp_core_init(dptx);
+
+	mmi_dp_init_hwparams(dptx);
+
+	retval = devm_request_threaded_irq(dptx->dev,
+					   dptx->irq,
+					   mmi_dp_irq,
+					   mmi_dp_threaded_irq,
+					   IRQF_SHARED | IRQ_LEVEL,
+					   "dptx_main_handler",
+					   dptx);
+	if (retval) {
+		dev_err(dev, "Request for irq %d failed\n", dptx->irq);
+		return retval;
+	}
+
+	/* Enable HPD Interrupt */
+	mmi_dp_enable_hpd_intr(dptx);
+
+	dev_dbg(dev, "MMI DP Tx Driver probed\n");
+	return 0;
+}
+
+static void mmi_dp_remove(struct platform_device *plat)
+{
+	struct dptx *dptx = platform_get_drvdata(plat);
+
+	mmi_dp_notify_shutdown(dptx);
+
+	/* wait for completing outstanding transmission on phy */
+	msleep(20);
+	mmi_dp_core_deinit(dptx);
+
+	dev_dbg(dptx->dev, "MMI DP Tx Driver removed\n");
+}
+
+static const struct of_device_id mmi_dptx_of_match[] = {
+	{ .compatible = "amd,mmi-dptx-1.0", },
+	{ /* end of table */ },
+};
+MODULE_DEVICE_TABLE(of, mmi_dptx_of_match);
+
+static struct platform_driver mmi_dptx_driver = {
+	.probe		= mmi_dp_probe,
+	.remove		= mmi_dp_remove,
+	.driver		= {
+		.name	= "mmi_dptx",
+		.of_match_table	= mmi_dptx_of_match,
+	},
+};
+
+module_platform_driver(mmi_dptx_driver);
+
+MODULE_AUTHOR("Advanced Micro Devices, Inc.");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("AMD MMI DisplayPort TX Driver");
diff --git a/drivers/staging/xlnx-mmi-dptx/mmi_dp.h b/drivers/staging/xlnx-mmi-dptx/mmi_dp.h
new file mode 100644
index 000000000..71ce9245c
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/mmi_dp.h
@@ -0,0 +1,465 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Multimedia Integrated DisplayPort Tx driver
+ *
+ * Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
+ */
+
+#ifndef __MMI_DP_H__
+#define __MMI_DP_H__
+
+#include <drm/display/drm_dp_helper.h>
+#include <drm/drm_bridge.h>
+
+#include "mmi_dp_config.h"
+#include "mmi_dp_reg.h"
+
+#define DPTX_RECEIVER_CAP_SIZE		0x100
+#define DPTX_SDP_NUM			0x10
+#define DPTX_SDP_LEN			0x9
+#define DPTX_SDP_SIZE			(9 * 4)
+
+/* ALPM */
+#define RECEIVER_ALPM_CAPABILITIES	0x0002E
+#define RECEIVER_ALPM_CONFIGURATIONS	0x00116
+
+struct dptx;
+
+/* The max rate and lanes supported by the core */
+#define DPTX_MAX_LINK_RATE		DPTX_PHYIF_CTRL_RATE_HBR
+#define DPTX_MAX_LINK_LANES		2
+
+/* The default rate and lanes to use for link training */
+#define DPTX_DEFAULT_LINK_RATE		DPTX_MAX_LINK_RATE
+#define DPTX_DEFAULT_LINK_LANES		DPTX_MAX_LINK_LANES
+
+/* The max number of streams supported */
+#define DPTX_MAX_STREAM_NUMBER		4
+
+/**
+ * struct dptx_link - The link state.
+ * @status: Holds the sink status register values.
+ * @trained: True if the link is successfully trained.
+ * @rate: The current rate that the link is trained at.
+ * @lanes: The current number of lanes that the link is trained at.
+ * @preemp_level: The pre-emphasis level used for each lane.
+ * @vswing_level: The vswing level used for each lane.
+ */
+struct dptx_link {
+	u8 status[DP_LINK_STATUS_SIZE];
+	bool trained;
+	u8 rate;
+	u8 lanes;
+	u8 preemp_level[4];
+	u8 vswing_level[4];
+};
+
+enum established_timings {
+	DMT_640x480_60hz,
+	DMT_800x600_60hz,
+	DMT_1024x768_60hz,
+	NONE
+};
+
+struct dptx_aux {
+	u32 sts;
+	u32 data[4];
+	atomic_t abort;
+	atomic_t serving;
+};
+
+struct sdp_header {
+	u8 HB0;
+	u8 HB1;
+	u8 HB2;
+	u8 HB3;
+} __packed;
+
+struct sdp_full_data {
+	u8 en;
+	u32 payload[DPTX_SDP_LEN];
+	u8 blanking;
+	u8 cont;
+} __packed;
+
+enum ALPM_Status {
+	NOT_AVAILABLE = -1,
+	DISABLED = 0,
+	ENABLED = 1
+};
+
+enum ALPM_State {
+	POWER_ON = 0,
+	POWER_OFF = 1
+};
+
+struct edp_alpm {
+	enum ALPM_Status status;
+	enum ALPM_State state;
+};
+
+enum hdcp_enum {
+	HDCP_OFF = 0,
+	HDCP_13,
+	HDCP_22,
+	HDCP_MAX
+};
+
+struct hdcp_aksv {
+	u32 lsb;
+	u32 msb;
+};
+
+struct hdcp_dpk {
+	u32 lsb;
+	u32 msb;
+};
+
+struct hdcp_params {
+	struct hdcp_aksv aksv;
+	struct hdcp_dpk dpk[40];
+	u32 enc_key;
+	u32 crc32;
+	u8 auth_fail_count;
+	enum hdcp_enum hdcp_en;
+};
+
+/* Interrupt Resources */
+enum irq_res_enum {
+	MAIN_IRQ = 0,
+	MAX_IRQ_IDX
+};
+
+/* Phy */
+enum rate_enum {
+	RBR = 0,
+	HBR1,
+	HBR2,
+	HBR3,
+	EDP0,
+	EDP1,
+	EDP2,
+	EDP3,
+	MAX_RATE
+};
+
+/* Dpcd Configuration */
+#define DPCD_REV			0x00000
+#define MAX_LINK_RATE			0x00001
+#define MAX_LANE_COUNT			0x00002
+#define MAX_DOWNSPREAD			0x00003
+#define NORP_DP_PWR_VOLTAGE_CAP		0x00004
+#define DOWN_STREAM_PORT_PRESENT	0x00005
+#define MAIN_LINK_CHANNEL_CODING	0x00006
+#define DOWN_STREAM_PORT_COUNT		0x00007
+#define RECEIVE_PORT0_CAP_0		0x00008
+#define RECEIVE_PORT0_CAP_1		0x00009
+#define RECEIVE_PORT1_CAP_0		0x0000A
+#define RECEIVE_PORT1_CAP_1		0x0000B
+#define I2C_SPEED_CONTROL		0x0000C
+#define TRAINING_AUX_RD_INTERVAL	0x0000E
+#define ADAPTER_CAP			0x0000F
+
+/* Link Configuration */
+#define MAX_PHY_BUSY_WAIT_ITER	20
+#define DEFAULT_STREAM		0
+#define LT_DONE			0
+#define LT_CR_FAIL		1
+#define LT_CH_EQ_FAIL		2
+#define CR_DONE			3
+#define CR_FAIL			4
+#define CH_EQ_DONE		5
+#define CH_EQ_FAIL		6
+#define ELOWESTRATE		7
+#define ELOWESTLANENR		8
+#define LANE_REDUCTION		9
+#define RATE_REDUCTION		10
+#define PATTERN_MASK		0x0F
+#define SCRAMBLING_DIS_MASK	0x20
+#define VSWING_MASK		0x03
+#define MAX_VSWING_MASK		0x04
+#define PREEMPH_MASK		0x18
+#define MAX_PREEMPH_MASK	0x20
+
+struct rx_capabilities {
+	u8 minor_rev_num;
+	u8 major_rev_num;
+	u8 max_link_rate;
+	u8 max_lane_count;
+	bool post_lt_adj_req_supported;
+	bool tps3_supported;
+	bool enhanced_frame_cap;
+	bool max_downspread;
+	bool no_aux_transaction_link_training;
+	bool tps4_supported;
+	bool norp;
+	bool crc_3d_option_supported;
+	bool dp_pwer_cap_5v;
+	bool dp_pwer_cap_12v;
+	bool dp_pwer_cap_18v;
+	bool dfp_present;
+	u8 dfp_type;
+	bool format_conversion;
+	bool detailed_cap_info_available;
+	bool channel_coding_8b10b_supported;
+	u8 dfp_count;
+	bool msa_timing_par_ignored;
+	bool oui_support;
+	bool port0_local_edid_present;
+	bool port0_associated_to_preceding_port;
+	bool port0_hblank_expansion_capable;
+	bool port0_buffer_size_unit;
+	bool port0_buffer_size_per_port;
+	u8 port0_buffer_size;
+	bool port1_local_edid_present;
+	bool port1_associated_to_preceding_port;
+	bool port1_hblank_expansion_capable;
+	bool port1_buffer_size_unit;
+	bool port1_buffer_size_per_port;
+	u8 port1_buffer_size;
+	u8 i2c_speed;
+	u8 training_aux_rd_interval;
+	bool extended_receiver_cap_present;
+	bool force_load_sense_cap;
+	bool alternate_i2c_pattern_cap;
+};
+
+/**
+ * struct dptx - The representation of the DP TX core
+ * @mutex: dptx mutex
+ * @hwparams: HW config parameters
+ * @base: Base address of the registers
+ * @irq: IRQ number
+ * @max_rate: The maximum rate that the controller supports
+ * @max_lanes: The maximum lane count that the controller supports
+ * @bpp: Bits per pixel
+ * @ycbcr420: sending YUV 420 data flag
+ * @streams: Number of streams
+ * @hdcp_en: Type of hdcp enabled
+ * @selected_pixel_clock: Selected pixel clock
+ * @mst: Flag for MST mode or not
+ * @cr_fail: Clock recovery fail flag
+ * @ssc_en: Spread Spectrum clocking enabled flag
+ * @enhanced_frame_cap: Enhanced frame capabilities flag
+ * @fec: FEC flag
+ * @edp: EDP flag
+ * @dev: The struct device
+ * @bridge: DRM Bridge
+ * @conn_status: connection status
+ * @dp_aux: DRM DP Aux
+ * @vparams: The video params to use
+ * @hparams: The HDCP params to use
+ * @waitq: The waitq
+ * @shutdown: Signals that the driver is shutting down and that all
+ *	    operations should be aborted.
+ * @c_connect: Signals that a HOT_PLUG or HOT_UNPLUG has occurred.
+ * @sink_request: Signals the a HPD_IRQ has occurred.
+ * @alpm: ALPM state and status
+ * @rx_caps: The sink's receiver capabilities.
+ * @sdp_list: The array of SDP elements
+ * @aux: AUX channel state for performing an AUX transfer.
+ * @link: The current link state.
+ * @multipixel: Controls multipixel configuration. 0-Single, 1-Dual, 2-Quad.
+ */
+struct dptx {
+	struct mutex mutex; /* generic mutex for dptx */
+
+	struct {
+		u8 sdp_reg_bank_size;
+		u8 audio_select;
+		u8 num_streams;
+		u8 psr_version;
+		u8 sync_depth;
+		u8 phy_type;
+		u8 mp_mode;
+		bool gen2_phy;
+		bool adsync;
+		bool fpga;
+		bool hdcp;
+		bool edp;
+		bool fec;
+		bool dsc;
+	} hwparams;
+
+	void __iomem *base;
+	int irq;
+
+	u8 max_rate;
+	u8 max_lanes;
+	u8 bpp;
+	bool ycbcr420;
+	u8 streams;
+	enum hdcp_enum hdcp_en;
+	u32 selected_pixel_clock;
+	bool mst;
+	bool cr_fail;
+	u8 multipixel;
+	bool ssc_en;
+	bool enhanced_frame_cap;
+	bool fec;
+	bool edp;
+
+	struct device *dev;
+	struct drm_bridge bridge;
+	/* struct drm_bridge *next_bridge; */
+	enum drm_connector_status conn_status;
+	struct drm_dp_aux dp_aux;
+
+	struct video_params vparams[DPTX_MAX_STREAM_NUMBER];
+	struct hdcp_params hparams;
+
+	wait_queue_head_t waitq;
+
+	atomic_t shutdown;
+	atomic_t c_connect;
+	atomic_t sink_request;
+
+	struct edp_alpm alpm;
+	struct rx_capabilities rx_caps;
+
+	struct sdp_full_data sdp_list[DPTX_SDP_NUM];
+	struct dptx_aux aux;
+	struct dptx_link link;
+};
+
+/* DP Registers Accessors */
+
+/**
+ * mmi_dp_read - Read mmi_dp register
+ * @base: base register address
+ * @offset: register offset
+ *
+ * Return: value stored in the hardware register
+ */
+static inline u32 mmi_dp_read(void __iomem *base, u32 offset)
+{
+	return readl(base + offset);
+}
+
+/**
+ * mmi_dp_write - Write the value into mmi_dp register
+ * @base: base register address
+ * @offset: register offset
+ * @val: value to write
+ */
+static inline void mmi_dp_write(void __iomem *base, u32 offset, u32 val)
+{
+	writel(val, base + offset);
+}
+
+/**
+ * mmi_dp_clr - Clear bits in mmi_dp register
+ * @base: base register address
+ * @offset: register offset
+ * @clr: bits to clear
+ */
+static inline void mmi_dp_clr(void __iomem *base, u32 offset, u32 clr)
+{
+	mmi_dp_write(base, offset, mmi_dp_read(base, offset) & ~clr);
+}
+
+/**
+ * mmi_dp_set - Set bits in mmi_dp register
+ * @base: base register address
+ * @offset: register offset
+ * @set: bits to set
+ */
+static inline void mmi_dp_set(void __iomem *base, u32 offset, u32 set)
+{
+	mmi_dp_write(base, offset, mmi_dp_read(base, offset) | set);
+}
+
+/*
+ * Core interface functions
+ */
+void mmi_dp_core_init_phy(struct dptx *dptx);
+void mmi_dp_soft_reset(struct dptx *dptx, u32 bits);
+
+irqreturn_t mmi_dp_irq(int irq, void *dev);
+irqreturn_t mmi_dp_threaded_irq(int irq, void *dev);
+
+void mmi_dp_global_intr_en(struct dptx *dp);
+void mmi_dp_global_intr_dis(struct dptx *dp);
+void mmi_dp_enable_hpd_intr(struct dptx *dp);
+void mmi_dp_video_intr_dis(struct dptx *dp);
+void mmi_dp_clean_interrupts(struct dptx *dp);
+
+/*
+ * PHY IF Control
+ */
+void mmi_dp_phy_set_lanes(struct dptx *dptx, unsigned int num);
+void mmi_dp_phy_set_rate(struct dptx *dptx, unsigned int rate);
+void mmi_dp_phy_set_pre_emphasis(struct dptx *dptx,
+				 unsigned int lane, unsigned int level);
+void mmi_dp_phy_set_vswing(struct dptx *dptx, unsigned int lane,
+			   unsigned int level);
+void mmi_dp_phy_set_pattern(struct dptx *dptx, unsigned int pattern);
+void mmi_dp_phy_enable_xmit(struct dptx *dptx, unsigned int lane, bool enable);
+
+int mmi_dp_phy_rate_to_bw(unsigned int rate);
+int mmi_dp_bw_to_phy_rate(unsigned int bw);
+
+/*
+ * AUX Channel
+ */
+
+int __mmi_dp_read_dpcd(struct dptx *dptx, u32 addr, u8 *byte);
+int __mmi_dp_write_dpcd(struct dptx *dptx, u32 addr, u8 byte);
+
+int __mmi_dp_read_bytes_from_dpcd(struct dptx *dptx, unsigned int reg_addr,
+				  u8 *bytes, u32 len);
+
+int __mmi_dp_write_bytes_to_dpcd(struct dptx *dptx, unsigned int reg_addr,
+				 u8 *bytes, u32 len);
+
+static inline int mmi_dp_read_dpcd(struct dptx *dptx, u32 addr, u8 *byte)
+{
+	return __mmi_dp_read_dpcd(dptx, addr, byte);
+}
+
+static inline int mmi_dp_write_dpcd(struct dptx *dptx, u32 addr, u8 byte)
+{
+	return __mmi_dp_write_dpcd(dptx, addr, byte);
+}
+
+static inline int mmi_dp_read_bytes_from_dpcd(struct dptx *dptx,
+					      unsigned int reg_addr,
+					      u8 *bytes, u32 len)
+{
+	return __mmi_dp_read_bytes_from_dpcd(dptx, reg_addr, bytes, len);
+}
+
+static inline int mmi_dp_write_bytes_to_dpcd(struct dptx *dptx,
+					     unsigned int reg_addr,
+					     u8 *bytes, u32 len)
+{
+	return __mmi_dp_write_bytes_to_dpcd(dptx, reg_addr, bytes, len);
+}
+
+#define mmi_dp_read_regfield(_base, _offset, _bit_mask) ({ \
+	FIELD_GET(_bit_mask, mmi_dp_read(_base, _offset)); \
+})
+
+/* Link training */
+int mmi_dp_full_link_training(struct dptx *dptx);
+int mmi_dp_fast_link_training(struct dptx *dptx);
+
+int mmi_dp_adjust_vswing_and_preemphasis(struct dptx *dptx);
+void mmi_dp_notify(struct dptx *dptx);
+
+/* Phy */
+int mmi_dp_power_state_change_phy(struct dptx *dptx, u8 power_state);
+int mmi_dp_disable_datapath_phy(struct dptx *dptx);
+
+/* Dptx Util utils */
+u32 mmi_dp_set_field(u32 data, u32 mask, u32 value);
+u8 mmi_dp_set8_field(u8 data, u8 mask, u8 value);
+void mmi_dp_write_mask(struct dptx *dptx, u32 addr, u32 mask, u32 data);
+
+/* Debug */
+#define dptx_dbg(_dp, _fmt...) dev_dbg((_dp)->dev, _fmt)
+#define dptx_info(_dp, _fmt...) dev_info((_dp)->dev, _fmt)
+#define dptx_warn(_dp, _fmt...) dev_warn((_dp)->dev, _fmt)
+#define dptx_err(_dp, _fmt...) dev_err((_dp)->dev, _fmt)
+
+#endif
diff --git a/drivers/staging/xlnx-mmi-dptx/mmi_dp_config.c b/drivers/staging/xlnx-mmi-dptx/mmi_dp_config.c
new file mode 100644
index 000000000..d5fb28283
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/mmi_dp_config.c
@@ -0,0 +1,799 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Multimedia Integrated DisplayPort Tx driver
+ *
+ * Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
+ */
+
+#include <drm/drm_fixed.h>
+#include <linux/kernel.h>
+
+#include "mmi_dp.h"
+#include "mmi_dp_config.h"
+
+/* Configuration Api's */
+
+u8 mmi_dp_bit_field(const u16 data, u8 shift, u8 width)
+{
+	return ((data >> shift) & ((((u16)1) << width) - 1));
+}
+
+u16 mmi_dp_concat_bits(u8 bhi, u8 ohi, u8 nhi, u8 blo, u8 olo, u8 nlo)
+{
+	return (mmi_dp_bit_field(bhi, ohi, nhi) << nlo) |
+	       mmi_dp_bit_field(blo, olo, nlo);
+}
+
+u16 mmi_dp_byte_to_word(const u8 hi, const u8 lo)
+{
+	return mmi_dp_concat_bits(hi, 0, 8, lo, 0, 8);
+}
+
+static u8 mmi_dp_get_bpc_mapping(u8 pix_enc, u8 bpc)
+{
+	u8 bpc_mapping = 0;
+
+	switch (pix_enc) {
+	case RGB:
+
+		if (bpc == COLOR_DEPTH_6)
+			bpc_mapping = 0;
+		else if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case YCBCR444:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case YCBCR422:
+
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case YONLY:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case RAW:
+		if (bpc == COLOR_DEPTH_6)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 3;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 4;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 5;
+		else if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 7;
+		break;
+	default:
+		bpc_mapping = 0;
+	}
+
+	return bpc_mapping;
+}
+
+static u8 mmi_dp_get_color_mapping(u8 pix_enc, u8 dynamic_range, u8 colorimetry)
+{
+	u8 col_mapping = 0;
+
+	/* According to Table 2-94 of DisplayPort spec 1.4 */
+	switch (pix_enc) {
+	case RGB:
+		if (dynamic_range == CEA)
+			col_mapping = 4;
+		else if (dynamic_range == VESA)
+			col_mapping = 0;
+		break;
+	case YCBCR422:
+		if (colorimetry == ITU601)
+			col_mapping = 5;
+		else if (colorimetry == ITU709)
+			col_mapping = 13;
+		break;
+	case YCBCR444:
+		if (colorimetry == ITU601)
+			col_mapping = 6;
+		else if (colorimetry == ITU709)
+			col_mapping = 14;
+		break;
+	case RAW:
+		col_mapping = 1;
+		break;
+	case YCBCR420:
+	case YONLY:
+		col_mapping = 0;
+		break;
+	}
+
+	return col_mapping;
+}
+
+static u8 get_video_mapping(u8 bpc, u8 pixel_encoding)
+{
+	u8 bpc_mapping = 1;
+
+	switch (pixel_encoding) {
+	case RGB:
+		if (bpc == COLOR_DEPTH_6)
+			bpc_mapping = 0;
+		else if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case YCBCR444:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 5;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 6;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 7;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 8;
+		break;
+	case YCBCR422:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 9;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 10;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 11;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 12;
+		break;
+	case YCBCR420:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 13;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 14;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 15;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 16;
+		break;
+	case YONLY:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 17;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 18;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 19;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 20;
+		break;
+	case RAW:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 23;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 24;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 25;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 27;
+		break;
+	}
+
+	return bpc_mapping;
+}
+
+static void mmi_dp_disable_sdp(struct dptx *dptx, u32 *payload)
+{
+	int i;
+
+	for (i = 0; i < DPTX_SDP_NUM; i++)
+		if (!memcmp(dptx->sdp_list[i].payload, payload, DPTX_SDP_LEN))
+			memset(dptx->sdp_list[i].payload, 0, DPTX_SDP_SIZE);
+}
+
+static void mmi_dp_enable_sdp(struct dptx *dptx, struct sdp_full_data *data)
+{
+	int i, reg_num, sdp_offset;
+	u32 reg, header;
+
+	header = (__force u32)cpu_to_be32(data->payload[0]);
+	for (i = 0; i < DPTX_SDP_NUM; i++)
+		if (dptx->sdp_list[i].payload[0] == 0) {
+			dptx->sdp_list[i].payload[0] = header;
+			sdp_offset = i * DPTX_SDP_SIZE;
+			reg_num = 0;
+			while (reg_num < DPTX_SDP_LEN) {
+				mmi_dp_write(dptx->base, SDP_REGISTER_BANK_0 +
+				sdp_offset + reg_num * 4,
+				(__force u32)cpu_to_be32(data->payload[reg_num]));
+				reg_num++;
+			}
+			switch (data->blanking) {
+			case 0:
+				reg = mmi_dp_read(dptx->base, SDP_VERTICAL_CTRL);
+				reg |= (1 << (2 + i));
+				mmi_dp_write(dptx->base, SDP_VERTICAL_CTRL, reg);
+				break;
+			case 1:
+				reg = mmi_dp_read(dptx->base, SDP_HORIZONTAL_CTRL);
+				reg |= (1 << (2 + i));
+				mmi_dp_write(dptx->base, SDP_HORIZONTAL_CTRL, reg);
+				break;
+			case 2:
+				reg = mmi_dp_read(dptx->base, SDP_VERTICAL_CTRL);
+				reg |= (1 << (2 + i));
+				mmi_dp_write(dptx->base, SDP_VERTICAL_CTRL, reg);
+				reg = mmi_dp_read(dptx->base, SDP_HORIZONTAL_CTRL);
+				reg |= (1 << (2 + i));
+				mmi_dp_write(dptx->base, SDP_HORIZONTAL_CTRL, reg);
+				break;
+			}
+			break;
+		}
+}
+
+static void mmi_dp_fill_sdp(struct dptx *dptx, struct sdp_full_data *data)
+{
+	if (data->en == 1)
+		mmi_dp_enable_sdp(dptx, data);
+	else
+		mmi_dp_disable_sdp(dptx, data->payload);
+}
+
+void mmi_dp_video_config1(struct dptx *dptx, u8 stream)
+{
+	struct dtd *mdtd;
+	u16 h_blank;
+
+	mdtd = &dptx->vparams[stream].mdtd;
+
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG1_N(stream),
+			  H_ACTIVE_MASK, mdtd->h_active);
+	h_blank = mdtd->h_blanking + mdtd->h_border;
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG1_N(stream), H_BLANK_MASK, h_blank);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG1_N(stream), I_P_MASK, mdtd->interlaced);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG1_N(stream), R_V_BLANK_IN_OSC_MASK, 0);
+}
+
+void mmi_dp_video_config2(struct dptx *dptx, u8 stream)
+{
+	struct dtd *mdtd;
+	u16 v_blank;
+
+	mdtd = &dptx->vparams[stream].mdtd;
+
+	v_blank = mdtd->v_blanking + mdtd->v_border;
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG2_N(stream), V_BLANK_MASK, v_blank);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG2_N(stream), V_ACTIVE_MASK, mdtd->v_active);
+}
+
+void mmi_dp_video_config3(struct dptx *dptx, u8 stream)
+{
+	struct dtd *mdtd;
+
+	mdtd = &dptx->vparams[stream].mdtd;
+
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG3_N(stream),
+			  H_SYNC_WIDTH_MASK, mdtd->h_sync_pulse_width);
+}
+
+void mmi_dp_video_config4(struct dptx *dptx, u8 stream)
+{
+	struct dtd *mdtd;
+
+	mdtd = &dptx->vparams[stream].mdtd;
+
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_CONFIG4_N(stream),
+			  V_SYNC_WIDTH_MASK, mdtd->v_sync_pulse_width);
+}
+
+void mmi_dp_video_msa1(struct dptx *dptx, u8 stream)
+{
+	struct dtd *mdtd;
+	u16 v_start, h_start;
+
+	mdtd = &dptx->vparams[stream].mdtd;
+
+	v_start = mdtd->v_blanking - mdtd->v_sync_offset + (mdtd->v_border / 2);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_MSA1_N(stream), MSA_V_START_MASK, v_start);
+
+	h_start = mdtd->h_blanking - mdtd->h_sync_offset + (mdtd->h_border / 2);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_MSA1_N(stream), MSA_H_START_MASK, h_start);
+}
+
+void mmi_dp_video_msa2(struct dptx *dptx, u8 stream)
+{
+	u8 bpc_mapping, col_mapping, bpc, dynamic_range, colorimetry;
+	struct video_params *vparams;
+	enum pixel_enc_type pix_enc;
+
+	vparams = &dptx->vparams[stream];
+	pix_enc = vparams->pix_enc;
+	bpc = vparams->bpc;
+	dynamic_range = vparams->dynamic_range;
+	colorimetry = vparams->colorimetry;
+
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_MSA2_N(stream), MSA_MISC0_SYNC_MODE_MASK, 0);
+	col_mapping = mmi_dp_get_color_mapping(pix_enc, dynamic_range, colorimetry);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_MSA2_N(stream),
+			  MSA_MISC0_COLOR_MAP_MASK, col_mapping);
+	bpc_mapping = mmi_dp_get_bpc_mapping(pix_enc, bpc);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_MSA2_N(stream),
+			  MSA_MISC0_BPC_MAP_MASK, bpc_mapping);
+}
+
+void mmi_dp_video_msa3(struct dptx *dptx, u8 stream)
+{
+	enum pixel_enc_type pix_enc;
+	u8 pix_enc_map;
+
+	pix_enc = dptx->vparams[stream].pix_enc;
+
+	switch (pix_enc) {
+	case YCBCR420:
+		pix_enc_map = 1;
+		break;
+	case YONLY:
+	case RAW:
+		pix_enc_map = 2;
+		break;
+	default:
+		pix_enc_map = 0;
+		break;
+	}
+
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_MSA3_N(stream),
+			  MSA_MISC1_PIX_ENC_MASK, pix_enc_map);
+}
+
+static int mmi_dp_calculate_hblank_interval(struct dptx *dptx, u8 stream)
+{
+	struct video_params *vparams;
+	int hblank_interval;
+	int pixel_clk;
+	u32 link_clk;
+	u16 h_blank;
+	u8 rate;
+	s64 fixp;
+
+	vparams = &dptx->vparams[stream];
+	pixel_clk = vparams->mdtd.pixel_clock;
+	h_blank = vparams->mdtd.h_blanking;
+	rate = dptx->link.rate;
+
+	switch (rate) {
+	case DPTX_PHYIF_CTRL_RATE_RBR:
+		link_clk = 40500;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR:
+		link_clk = 67500;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR2:
+		link_clk = 135000;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR3:
+		link_clk = 202500;
+		break;
+	default:
+		dptx_warn(dptx, "Invalid rate 0x%x\n", rate);
+		return -EINVAL;
+	}
+
+	fixp = drm_fixp_mul(drm_int2fixp(h_blank), drm_int2fixp(link_clk));
+	fixp = drm_fixp_div(fixp, drm_int2fixp(pixel_clk));
+	hblank_interval = drm_fixp2int(fixp);
+
+	return hblank_interval;
+}
+
+void mmi_dp_video_hblank_interval(struct dptx *dptx, u8 stream)
+{
+	u16 hblank_interval;
+
+	hblank_interval = mmi_dp_calculate_hblank_interval(dptx, stream);
+	dptx_dbg(dptx, "HBLANK INTERVAL: %d", hblank_interval);
+	mmi_dp_write_mask(dptx, DPTX_VIDEO_HBLANK_INTERVAL_N(stream),
+			  H_BLANK_INTERVAL_MASK, hblank_interval);
+}
+
+void mmi_dp_vinput_polarity_ctrl(struct dptx *dptx, u8 stream)
+{
+	struct dtd *mdtd;
+
+	mdtd = &dptx->vparams[stream].mdtd;
+
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_POLARITY_CTRL_N(stream),
+			  H_SYNC_IN_POLARITY_MASK, mdtd->h_sync_polarity);
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_POLARITY_CTRL_N(stream),
+			  V_SYNC_IN_POLARITY_MASK, mdtd->v_sync_polarity);
+}
+
+void mmi_dp_vsample_ctrl(struct dptx *dptx, u8 stream)
+{
+	u8 video_mapping;
+
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(stream),
+			  VIDEO_MAPPING_IPI_EN_MASK, 0);
+	video_mapping = get_video_mapping(dptx->vparams[stream].bpc,
+					  dptx->vparams[stream].pix_enc);
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(stream),
+			  VIDEO_MAPPING_MASK, video_mapping);
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(stream),
+			  PIXEL_MODE_SELECT_MASK, dptx->multipixel);
+}
+
+void mmi_dp_disable_video_stream(struct dptx *dptx, u8 stream)
+{
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(stream), VIDEO_STREAM_ENABLE_MASK, 0);
+}
+
+void mmi_dp_vsd_ycbcr420_send(struct dptx *dptx, u8 enable)
+{
+	struct sdp_full_data vsc_data;
+	int i;
+
+	struct video_params *vparams;
+
+	vparams = &dptx->vparams[0];
+
+	vsc_data.en = enable;
+	for (i = 0; i < 9; i++) {
+		if (i == 0)
+			vsc_data.payload[i] = 0x00070513;
+		else if (i == 5)
+			switch (vparams->bpc) {
+			case COLOR_DEPTH_8:
+				vsc_data.payload[i] = 0x30010000;
+				break;
+			case COLOR_DEPTH_10:
+				vsc_data.payload[i] = 0x30020000;
+				break;
+			case COLOR_DEPTH_12:
+				vsc_data.payload[i] = 0x30030000;
+				break;
+			case COLOR_DEPTH_16:
+				vsc_data.payload[i] = 0x30040000;
+				break;
+			}
+		else
+			vsc_data.payload[i] = 0x0;
+	}
+	vsc_data.blanking = 0;
+	vsc_data.cont = 1;
+
+	mmi_dp_fill_sdp(dptx, &vsc_data);
+}
+
+void mmi_dp_video_ts_change(struct dptx *dptx, int stream)
+{
+	u32 reg;
+	struct video_params *vparams;
+
+	vparams = &dptx->vparams[0];
+
+	reg = mmi_dp_read(dptx->base, DPTX_VIDEO_CONFIG5_N(stream));
+	reg = reg & (~DPTX_VIDEO_CONFIG5_TU_MASK);
+	reg = reg | (vparams->aver_bytes_per_tu <<
+			DPTX_VIDEO_CONFIG5_TU_SHIFT);
+	reg = reg & (~DPTX_VIDEO_CONFIG5_TU_FRAC_MASK_SST);
+	reg = reg | (vparams->aver_bytes_per_tu_frac <<
+			     DPTX_VIDEO_CONFIG5_TU_FRAC_SHIFT_SST);
+	reg = reg & (~DPTX_VIDEO_CONFIG5_INIT_THRESHOLD_MASK);
+	reg = reg | (vparams->init_threshold <<
+		      DPTX_VIDEO_CONFIG5_INIT_THRESHOLD_SHIFT);
+	mmi_dp_write(dptx->base, DPTX_VIDEO_CONFIG5_N(stream), reg);
+}
+
+static void mmi_dp_video_set_core_bpc(struct dptx *dptx, int stream)
+{
+	u32 reg;
+	u8 bpc_mapping = 0, bpc = 0;
+	enum pixel_enc_type pix_enc;
+	struct video_params *vparams;
+
+	vparams = &dptx->vparams[0];
+	bpc = vparams->bpc;
+	pix_enc = vparams->pix_enc;
+
+	reg = mmi_dp_read(dptx->base, DPTX_VSAMPLE_CTRL_N(stream));
+	reg &= ~DPTX_VSAMPLE_CTRL_VMAP_BPC_MASK;
+
+	bpc_mapping = get_video_mapping(bpc, pix_enc);
+	reg |= (bpc_mapping << DPTX_VSAMPLE_CTRL_VMAP_BPC_SHIFT);
+	mmi_dp_write(dptx->base, DPTX_VSAMPLE_CTRL_N(stream), reg);
+}
+
+static void mmi_dp_video_set_sink_col(struct dptx *dptx, int stream)
+{
+	u32 reg_msa2;
+	u8 col_mapping;
+	u8 colorimetry;
+	u8 dynamic_range;
+	struct video_params *vparams;
+	enum pixel_enc_type pix_enc;
+
+	vparams = &dptx->vparams[0];
+	pix_enc = vparams->pix_enc;
+	colorimetry = vparams->colorimetry;
+	dynamic_range = vparams->dynamic_range;
+
+	reg_msa2 = mmi_dp_read(dptx->base, DPTX_VIDEO_MSA2_N(stream));
+	reg_msa2 &= ~DPTX_VIDEO_VMSA2_COL_MASK;
+
+	col_mapping = 0;
+
+	/* According to Table 2-94 of DisplayPort spec 1.3 */
+	switch (pix_enc) {
+	case RGB:
+		if (dynamic_range == CEA)
+			col_mapping = 4;
+		else if (dynamic_range == VESA)
+			col_mapping = 0;
+		break;
+	case YCBCR422:
+		if (colorimetry == ITU601)
+			col_mapping = 5;
+		else if (colorimetry == ITU709)
+			col_mapping = 13;
+		break;
+	case YCBCR444:
+		if (colorimetry == ITU601)
+			col_mapping = 6;
+		else if (colorimetry == ITU709)
+			col_mapping = 14;
+		break;
+	case RAW:
+		col_mapping = 1;
+		break;
+	case YCBCR420:
+	case YONLY:
+		break;
+	}
+
+	reg_msa2 |= (col_mapping << DPTX_VIDEO_VMSA2_COL_SHIFT);
+	mmi_dp_write(dptx->base, DPTX_VIDEO_MSA2_N(stream), reg_msa2);
+}
+
+static void mmi_dp_video_set_sink_bpc(struct dptx *dptx, int stream)
+{
+	u32 reg_msa2, reg_msa3;
+	u8 bpc_mapping = 0, bpc = 0;
+	struct video_params *vparams;
+	enum pixel_enc_type pix_enc;
+
+	vparams = &dptx->vparams[0];
+	pix_enc = vparams->pix_enc;
+	bpc = vparams->bpc;
+
+	reg_msa2 = mmi_dp_read(dptx->base, DPTX_VIDEO_MSA2_N(stream));
+	reg_msa3 = mmi_dp_read(dptx->base, DPTX_VIDEO_MSA3_N(stream));
+
+	reg_msa2 &= ~DPTX_VIDEO_VMSA2_BPC_MASK;
+	reg_msa3 &= ~DPTX_VIDEO_VMSA3_PIX_ENC_MASK;
+
+	switch (pix_enc) {
+	case RGB:
+		if (bpc == COLOR_DEPTH_6)
+			bpc_mapping = 0;
+		else if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case YCBCR444:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case YCBCR422:
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case YCBCR420:
+		reg_msa3 |= DPTX_VIDEO_VMSA3_PIX_ENC_YCBCR420;
+		break;
+	case YONLY:
+		/* According to Table 2-94 of DisplayPort spec 1.3 */
+		reg_msa3 |= DPTX_VIDEO_VMSA3_PIX_ENC;
+
+		if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 2;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 3;
+		if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 4;
+		break;
+	case RAW:
+		/* According to Table 2-94 of DisplayPort spec 1.3 */
+		reg_msa3 |= DPTX_VIDEO_VMSA3_PIX_ENC;
+
+		if (bpc == COLOR_DEPTH_6)
+			bpc_mapping = 1;
+		else if (bpc == COLOR_DEPTH_8)
+			bpc_mapping = 3;
+		else if (bpc == COLOR_DEPTH_10)
+			bpc_mapping = 4;
+		else if (bpc == COLOR_DEPTH_12)
+			bpc_mapping = 5;
+		else if (bpc == COLOR_DEPTH_16)
+			bpc_mapping = 7;
+		break;
+	}
+
+	reg_msa2 |= (bpc_mapping << DPTX_VIDEO_VMSA2_BPC_SHIFT);
+
+	mmi_dp_write(dptx->base, DPTX_VIDEO_MSA2_N(stream), reg_msa2);
+	mmi_dp_write(dptx->base, DPTX_VIDEO_MSA3_N(stream), reg_msa3);
+
+	mmi_dp_video_set_sink_col(dptx, stream);
+}
+
+void mmi_dp_video_bpc_change(struct dptx *dptx, int stream)
+{
+	mmi_dp_video_set_core_bpc(dptx, stream);
+	mmi_dp_video_set_sink_bpc(dptx, stream);
+}
+
+void mmi_dp_disable_default_video_stream(struct dptx *dptx, int stream)
+{
+	u32 vsamplectrl;
+
+	vsamplectrl = mmi_dp_read(dptx->base, DPTX_VSAMPLE_CTRL_N(stream));
+	vsamplectrl &= ~DPTX_VSAMPLE_CTRL_STREAM_EN;
+	mmi_dp_write(dptx->base, DPTX_VSAMPLE_CTRL_N(stream), vsamplectrl);
+}
+
+void mmi_dp_enable_default_video_stream(struct dptx *dptx, int stream)
+{
+	u32 vsamplectrl;
+
+	vsamplectrl = mmi_dp_read(dptx->base, DPTX_VSAMPLE_CTRL_N(stream));
+	vsamplectrl |= DPTX_VSAMPLE_CTRL_STREAM_EN;
+	mmi_dp_write(dptx->base, DPTX_VSAMPLE_CTRL_N(stream), vsamplectrl);
+}
+
+/*
+ * DTD
+ */
+void mmi_dp_dtd_reset(struct dtd *mdtd)
+{
+	mdtd->pixel_repetition_input = 0;
+	mdtd->pixel_clock = 0;
+	mdtd->h_active = 0;
+	mdtd->h_border = 0;
+	mdtd->h_blanking = 0;
+	mdtd->h_sync_offset = 0;
+	mdtd->h_sync_pulse_width = 0;
+	mdtd->h_image_size = 0;
+	mdtd->v_active = 0;
+	mdtd->v_border = 0;
+	mdtd->v_blanking = 0;
+	mdtd->v_sync_offset = 0;
+	mdtd->v_sync_pulse_width = 0;
+	mdtd->v_image_size = 0;
+	mdtd->interlaced = 0;
+	mdtd->v_sync_polarity = 0;
+	mdtd->h_sync_polarity = 0;
+}
+
+u8 mmi_dp_get_color_depth_bpp(u8 bpc, u8 encoding)
+{
+	u8 color_dep;
+
+	switch (bpc) {
+	case COLOR_DEPTH_6:
+		color_dep = 18;
+		break;
+	case COLOR_DEPTH_8:
+		if (encoding == YCBCR420)
+			color_dep = 12;
+		else if (encoding == YCBCR422)
+			color_dep = 16;
+		else if (encoding == YONLY)
+			color_dep = 8;
+		else
+			color_dep = 24;
+		break;
+	case COLOR_DEPTH_10:
+		if (encoding == YCBCR420)
+			color_dep = 15;
+		else if (encoding == YCBCR422)
+			color_dep = 20;
+		else if (encoding == YONLY)
+			color_dep = 10;
+		else
+			color_dep = 30;
+		break;
+
+	case COLOR_DEPTH_12:
+		if (encoding == YCBCR420)
+			color_dep = 18;
+		else if (encoding == YCBCR422)
+			color_dep = 24;
+		else if (encoding == YONLY)
+			color_dep = 12;
+		else
+			color_dep = 36;
+		break;
+
+	case COLOR_DEPTH_16:
+		if (encoding == YCBCR420)
+			color_dep = 24;
+		else if (encoding == YCBCR422)
+			color_dep = 32;
+		else if (encoding == YONLY)
+			color_dep = 16;
+		else
+			color_dep = 48;
+		break;
+	default:
+		color_dep = 18;
+		break;
+	}
+
+	return color_dep;
+}
+
+u16 mmi_dp_get_link_rate(u8 rate)
+{
+	u16 link_rate;
+
+	switch (rate) {
+	case DPTX_PHYIF_CTRL_RATE_RBR:
+		link_rate = 162;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR:
+		link_rate = 270;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR2:
+		link_rate = 540;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR3:
+		link_rate = 810;
+		break;
+	default:
+		link_rate = 162;
+	}
+
+	return link_rate;
+}
diff --git a/drivers/staging/xlnx-mmi-dptx/mmi_dp_config.h b/drivers/staging/xlnx-mmi-dptx/mmi_dp_config.h
new file mode 100644
index 000000000..3c1e50e19
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/mmi_dp_config.h
@@ -0,0 +1,222 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Multimedia Integrated DisplayPort Tx driver
+ *
+ * Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
+ */
+
+#ifndef __MMI_DP_CONFIG_H__
+#define __MMI_DP_CONFIG_H__
+
+struct dptx;
+
+struct short_audio_desc_t {
+	u8 m_format;
+	u8 m_max_channels;
+	u8 m_sample_rates;
+	u8 m_byte3;
+};
+
+struct speaker_allocation_data_block_t {
+	u8 m_byte1;
+	u8 m_byte2;
+	u8 m_byte3;
+	int m_valid;
+};
+
+struct room_config_data_block_t {
+	u8 m_speaker_count;
+	u8 m_sld;
+	u8 m_speaker;
+	u8 m_display;
+	u8 m_spm1;
+	u8 m_spm2;
+	u8 m_spm3;
+	int m_valid;
+};
+
+enum pixel_enc_type {
+	RGB = 0,
+	YCBCR420 = 1,
+	YCBCR422 = 2,
+	YCBCR444 = 3,
+	YONLY = 4,
+	RAW = 5
+};
+
+enum color_depth {
+	COLOR_DEPTH_INVALID = 0,
+	COLOR_DEPTH_6 = 6,
+	COLOR_DEPTH_8 = 8,
+	COLOR_DEPTH_10 = 10,
+	COLOR_DEPTH_12 = 12,
+	COLOR_DEPTH_16 = 16
+};
+
+enum dynamic_range_type {
+	CEA = 1,
+	VESA = 2
+};
+
+enum colorimetry_type {
+	ITU601 = 1,
+	ITU709 = 2
+};
+
+enum video_format_type {
+	VCEA = 0,
+	CVT = 1,
+	DMT = 2
+};
+
+struct dtd {
+	u16 pixel_repetition_input;
+	int pixel_clock;
+	u8 interlaced; /* 1 for interlaced, 0 progressive */
+	u16 h_active;
+	u16 h_border;
+	u16 h_blanking;
+	u16 h_image_size;
+	u16 h_sync_offset;
+	u16 h_sync_pulse_width;
+	u8 h_sync_polarity;
+	u16 v_active;
+	u16 v_border;
+	u16 v_blanking;
+	u16 v_image_size;
+	u16 v_sync_offset;
+	u16 v_sync_pulse_width;
+	u8 v_sync_polarity;
+};
+
+struct dtd_t {
+	/* VIC code */
+	u32 m_code;
+
+	/* Identifies modes that ONLY can be displayed in YCC 4:2:0 */
+	u8 m_limited_to_ycc420;
+
+	/* Identifies modes that can also be displayed in YCC 4:2:0 */
+	u8 m_ycc420;
+
+	u16 m_pixel_repetition_factor;
+
+	/* In units of 1kHz */
+	u32 m_pixel_clock;
+
+	/* 1 for interlaced, 0 progressive */
+	u8 m_interlaced;
+
+	u16 m_h_active;
+
+	u16 m_h_blanking;
+
+	u16 m_h_border;
+
+	u16 m_h_image_size;
+
+	u16 m_h_sync_offset;
+
+	u16 m_h_sync_pulse_width;
+
+	/* 0 for Active low, 1 active high */
+	u8 m_h_sync_polarity;
+
+	u16 m_v_active;
+
+	u16 m_v_blanking;
+
+	u16 m_v_border;
+
+	u16 m_v_image_size;
+
+	u16 m_v_sync_offset;
+
+	u16 m_v_sync_pulse_width;
+
+	/* 0 for Active low, 1 active high */
+	u8 m_v_sync_polarity;
+};
+
+struct display_mode_t {
+	u32 refresh_rate;
+	struct dtd_t dtd;
+};
+
+struct video_params {
+	u8 pix_enc;
+	u8 pattern_mode;
+	struct dtd mdtd;
+	u8 mode;
+	u8 bpc;
+	u8 colorimetry;
+	u8 dynamic_range;
+	u8 vc_payload;
+	u16 pbn;
+	u8 aver_bytes_per_tu;
+	u8 aver_bytes_per_tu_frac;
+	u8 init_threshold;
+	u32 refresh_rate;
+	u8 video_format;
+};
+
+struct short_video_desc_t {
+	int m_native;
+	unsigned int m_code;
+	unsigned int m_limited_to_ycc420;
+	unsigned int m_ycc420;
+};
+
+struct monitor_range_limits_t {
+	u8 m_min_vertical_rate;
+	u8 m_max_vertical_rate;
+	u8 m_min_horizontal_rate;
+	u8 m_max_horizontal_rate;
+	u8 m_max_pixel_clock;
+	int m_valid;
+};
+
+struct video_capability_data_block_t {
+	int m_quantization_range_selectable;
+	u8 m_preferred_timing_scan_info;
+	u8 m_it_scan_info;
+	u8 m_ce_scan_info;
+	int m_valid;
+};
+
+struct colorimetry_data_block_t {
+	u8 m_byte3;
+	u8 m_byte4;
+	int m_valid;
+};
+
+u8 mmi_dp_bit_field(const u16 data, u8 shift, u8 width);
+u16 mmi_dp_concat_bits(u8 bhi, u8 ohi, u8 nhi, u8 blo, u8 olo, u8 nlo);
+u16 mmi_dp_byte_to_word(const u8 hi, const u8 lo);
+int mmi_dp_dtd_fill(struct dtd *mdtd, struct display_mode_t *display_mode);
+void mmi_dp_dtd_reset(struct dtd *mdtd);
+void mmi_dp_video_bpc_change(struct dptx *dptx, int stream);
+void mmi_dp_video_ts_change(struct dptx *dptx, int stream);
+void mmi_dp_video_ts_calculate(struct dptx *dptx, int lane_num, int rate,
+			       int bpc, int encoding, int pixel_clock);
+void mmi_dp_enable_default_video_stream(struct dptx *dptx, int stream);
+void mmi_dp_disable_default_video_stream(struct dptx *dptx, int stream);
+void mmi_dp_vsd_ycbcr420_send(struct dptx *dptx, u8 enable);
+u16 mmi_dp_get_link_rate(u8 rate);
+u8 mmi_dp_get_color_depth_bpp(u8 bpc, u8 encoding);
+int mmi_dp_sst_configuration(struct dptx *dptx);
+
+/* MST-Configuration */
+void mmi_dp_video_config1(struct dptx *dptx, u8 stream);
+void mmi_dp_video_config2(struct dptx *dptx, u8 stream);
+void mmi_dp_video_config3(struct dptx *dptx, u8 stream);
+void mmi_dp_video_config4(struct dptx *dptx, u8 stream);
+void mmi_dp_video_msa1(struct dptx *dptx, u8 stream);
+void mmi_dp_video_msa2(struct dptx *dptx, u8 stream);
+void mmi_dp_video_msa3(struct dptx *dptx, u8 stream);
+void mmi_dp_video_hblank_interval(struct dptx *dptx, u8 stream);
+void mmi_dp_vinput_polarity_ctrl(struct dptx *dptx, u8 stream);
+void mmi_dp_vsample_ctrl(struct dptx *dptx, u8 stream);
+void mmi_dp_disable_video_stream(struct dptx *dptx, u8 stream);
+
+#endif
diff --git a/drivers/staging/xlnx-mmi-dptx/mmi_dp_intr.c b/drivers/staging/xlnx-mmi-dptx/mmi_dp_intr.c
new file mode 100644
index 000000000..82e3e0cc9
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/mmi_dp_intr.c
@@ -0,0 +1,427 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Multimedia Integrated DisplayPort Tx driver
+ *
+ * Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
+ */
+
+#include <drm/display/drm_dp_helper.h>
+#include <drm/drm_bridge.h>
+
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/kernel.h>
+
+#include "mmi_dp.h"
+
+static void mmi_dp_parse_rx_capabilities(struct dptx *dptx, const u8 *rx_caps)
+{
+	/* DPCD_REV - 00000h */
+	dptx->rx_caps.minor_rev_num = rx_caps[DPCD_REV] & 0x0F;
+	dptx->rx_caps.major_rev_num = (rx_caps[DPCD_REV] & 0xF0) >> 4;
+
+	/* MAX_LINK_RATE - 00001h */
+	dptx->rx_caps.max_link_rate = rx_caps[MAX_LINK_RATE];
+
+	/* MAX_LANE_COUNT - 00002h */
+	dptx->rx_caps.max_lane_count = rx_caps[MAX_LANE_COUNT] & 0x0F;
+	dptx->rx_caps.post_lt_adj_req_supported = (rx_caps[MAX_LANE_COUNT] & BIT(5)) >> 5;
+	dptx->rx_caps.tps3_supported = (rx_caps[MAX_LANE_COUNT] & BIT(6)) >> 6;
+	dptx->rx_caps.enhanced_frame_cap = (rx_caps[MAX_LANE_COUNT] & BIT(7)) >> 7;
+
+	/* MAX_DOWNSPREAD - 00003h */
+	dptx->rx_caps.max_downspread = rx_caps[MAX_DOWNSPREAD] & BIT(0);
+	dptx->rx_caps.no_aux_transaction_link_training = (rx_caps[MAX_DOWNSPREAD] & BIT(6)) >> 6;
+	dptx->rx_caps.tps4_supported = (rx_caps[MAX_DOWNSPREAD] & BIT(7)) >> 7;
+
+	/* NORP & DP_PWR_VOLTAGE_CAP - 00004h */
+	dptx->rx_caps.norp = rx_caps[NORP_DP_PWR_VOLTAGE_CAP] & BIT(0);
+	dptx->rx_caps.crc_3d_option_supported = (rx_caps[MAX_DOWNSPREAD] & BIT(1)) >> 1;
+	dptx->rx_caps.dp_pwer_cap_5v = (rx_caps[NORP_DP_PWR_VOLTAGE_CAP] & BIT(5)) >> 5;
+	dptx->rx_caps.dp_pwer_cap_12v = (rx_caps[NORP_DP_PWR_VOLTAGE_CAP] & BIT(6)) >> 6;
+	dptx->rx_caps.dp_pwer_cap_18v = (rx_caps[NORP_DP_PWR_VOLTAGE_CAP] & BIT(7)) >> 7;
+
+	/* DOWN_STREAM_PORT_PRESENT - 00005h */
+	dptx->rx_caps.dfp_present = rx_caps[DOWN_STREAM_PORT_PRESENT] & BIT(0);
+	dptx->rx_caps.dfp_type = (rx_caps[DOWN_STREAM_PORT_PRESENT] & 0x06) >> 1;
+	dptx->rx_caps.format_conversion = (rx_caps[DOWN_STREAM_PORT_PRESENT] & BIT(3)) >> 3;
+	dptx->rx_caps.detailed_cap_info_available =
+	(rx_caps[DOWN_STREAM_PORT_PRESENT] & BIT(4)) >> 4;
+
+	/* MAIN_LINK_CHANNEL_CODING - 00006h */
+	dptx->rx_caps.channel_coding_8b10b_supported = rx_caps[MAIN_LINK_CHANNEL_CODING] & BIT(0);
+
+	/* DOWN_STREAM_PORT_COUNT - 00007h */
+	dptx->rx_caps.dfp_count = rx_caps[DOWN_STREAM_PORT_COUNT] & 0x0F;
+	dptx->rx_caps.msa_timing_par_ignored = (rx_caps[DOWN_STREAM_PORT_COUNT] & BIT(6)) >> 6;
+	dptx->rx_caps.oui_support = (rx_caps[DOWN_STREAM_PORT_COUNT] & BIT(7)) >> 7;
+
+	/* RECEIVE_PORT0_CAP_0 - 00008h */
+	dptx->rx_caps.port0_local_edid_present = (rx_caps[RECEIVE_PORT0_CAP_0] & BIT(1)) >> 1;
+	dptx->rx_caps.port0_associated_to_preceding_port = (rx_caps[RECEIVE_PORT0_CAP_0] &
+	BIT(2)) >> 2;
+	dptx->rx_caps.port0_hblank_expansion_capable = (rx_caps[RECEIVE_PORT0_CAP_0] & BIT(3)) >> 3;
+	dptx->rx_caps.port0_buffer_size_unit = (rx_caps[RECEIVE_PORT0_CAP_0] & BIT(4)) >> 4;
+	dptx->rx_caps.port0_buffer_size_per_port = (rx_caps[RECEIVE_PORT0_CAP_0] & BIT(5)) >> 5;
+
+	/* RECEIVE_PORT0_CAP_1 - 00009h */
+	dptx->rx_caps.port0_buffer_size = rx_caps[RECEIVE_PORT0_CAP_1];
+
+	/* RECEIVE_PORT1_CAP_0 - 0000Ah */
+	dptx->rx_caps.port1_local_edid_present = (rx_caps[RECEIVE_PORT1_CAP_0] & BIT(1)) >> 1;
+	dptx->rx_caps.port1_associated_to_preceding_port =
+	(rx_caps[RECEIVE_PORT1_CAP_0] & BIT(2)) >> 2;
+	dptx->rx_caps.port1_hblank_expansion_capable = (rx_caps[RECEIVE_PORT1_CAP_0] & BIT(3)) >> 3;
+	dptx->rx_caps.port1_buffer_size_unit = (rx_caps[RECEIVE_PORT1_CAP_0] & BIT(4)) >> 4;
+	dptx->rx_caps.port1_buffer_size_per_port = (rx_caps[RECEIVE_PORT1_CAP_0] & BIT(5)) >> 5;
+
+	/* RECEIVE_PORT1_CAP_1 - 0000Bh */
+	dptx->rx_caps.port1_buffer_size = rx_caps[RECEIVE_PORT1_CAP_1];
+
+	/* I2C_SPEED_CONTROL - 0000Ch */
+	dptx->rx_caps.i2c_speed = rx_caps[I2C_SPEED_CONTROL];
+
+	/* TRAINING_AUX_RD_INTERVAL - 0000Eh */
+	dptx->rx_caps.training_aux_rd_interval = rx_caps[TRAINING_AUX_RD_INTERVAL] & 0x7F;
+	dptx->rx_caps.extended_receiver_cap_present = (rx_caps[TRAINING_AUX_RD_INTERVAL] &
+	BIT(7)) >> 7;
+
+	/* ADAPTER_CAP - 0000Fh */
+	dptx->rx_caps.force_load_sense_cap = rx_caps[ADAPTER_CAP] & BIT(0);
+	dptx->rx_caps.alternate_i2c_pattern_cap = (rx_caps[ADAPTER_CAP] & BIT(1)) >> 1;
+}
+
+int mmi_dp_adjust_vswing_and_preemphasis(struct dptx *dptx)
+{
+	u8 lane_01 = 0, lane_23 = 0;
+	int retval, i;
+
+	retval = mmi_dp_read_dpcd(dptx, DP_ADJUST_REQUEST_LANE0_1, &lane_01);
+	if (retval)
+		return retval;
+
+	retval = mmi_dp_read_dpcd(dptx, DP_ADJUST_REQUEST_LANE2_3, &lane_23);
+	if (retval)
+		return retval;
+
+	for (i = 0; i < dptx->link.lanes; i++) {
+		u8 pe = 0, vs = 0;
+
+		switch (i) {
+		case 0:
+			pe = (lane_01 & DP_ADJUST_PRE_EMPHASIS_LANE0_MASK)
+				>> DP_ADJUST_PRE_EMPHASIS_LANE0_SHIFT;
+			vs = (lane_01 & DP_ADJUST_VOLTAGE_SWING_LANE0_MASK)
+				>> DP_ADJUST_VOLTAGE_SWING_LANE0_SHIFT;
+			break;
+		case 1:
+			pe = (lane_01 & DP_ADJUST_PRE_EMPHASIS_LANE1_MASK)
+				>> DP_ADJUST_PRE_EMPHASIS_LANE1_SHIFT;
+			vs = (lane_01 & DP_ADJUST_VOLTAGE_SWING_LANE1_MASK)
+				>> DP_ADJUST_VOLTAGE_SWING_LANE1_SHIFT;
+			break;
+		case 2:
+			pe = (lane_23 & DP_ADJUST_PRE_EMPHASIS_LANE0_MASK)
+				>> DP_ADJUST_PRE_EMPHASIS_LANE0_SHIFT;
+			vs = (lane_23 & DP_ADJUST_VOLTAGE_SWING_LANE0_MASK)
+				>> DP_ADJUST_VOLTAGE_SWING_LANE0_SHIFT;
+			break;
+		case 3:
+			pe = (lane_23 & DP_ADJUST_PRE_EMPHASIS_LANE1_MASK)
+				>> DP_ADJUST_PRE_EMPHASIS_LANE1_SHIFT;
+			vs = (lane_23 & DP_ADJUST_VOLTAGE_SWING_LANE1_MASK)
+				>> DP_ADJUST_VOLTAGE_SWING_LANE1_SHIFT;
+			break;
+		default:
+			break;
+		}
+
+		mmi_dp_phy_set_pre_emphasis(dptx, i, pe);
+		mmi_dp_phy_set_vswing(dptx, i, vs);
+	}
+
+	return 0;
+}
+
+static int mmi_dp_handle_hotunplug(struct dptx *dptx)
+{
+	u32 hpd_ien;
+
+	dptx_info(dptx, "DPTX - Hotunplug Detected");
+
+	atomic_set(&dptx->sink_request, 0);
+	dptx->link.trained = false;
+
+	/* PHY Standby */
+	mmi_dp_disable_datapath_phy(dptx);
+	mmi_dp_power_state_change_phy(dptx, DPTX_PHY_POWER_DOWN);
+
+	hpd_ien = mmi_dp_read(dptx->base, HPD_INTERRUPT_ENABLE);
+	hpd_ien |= (DPTX_HPD_IEN_IRQ_EN |
+		    DPTX_HPD_IEN_HOT_PLUG_EN |
+		    DPTX_HPD_IEN_HOT_UNPLUG_EN);
+	mmi_dp_write(dptx->base, HPD_INTERRUPT_ENABLE, hpd_ien);
+
+	dptx->conn_status = connector_status_disconnected;
+
+	return 0;
+}
+
+static int mmi_dp_alpm_is_available(struct dptx *dptx)
+{
+	u8 alpm_cap = 0;
+	int retval;
+
+	retval = mmi_dp_read_dpcd(dptx, RECEIVER_ALPM_CAPABILITIES, &alpm_cap);
+	if (retval)
+		return retval;
+	dptx_dbg(dptx, "ALPM Availability: %lu\n", alpm_cap & BIT(0));
+	return (alpm_cap & BIT(0));
+}
+
+static int mmi_dp_handle_hotplug(struct dptx *dptx)
+{
+	u8 rx_caps[DPTX_RECEIVER_CAP_SIZE];
+	int alpm_availability, retval;
+	struct edp_alpm *alpm;
+	u32 hpd_ien;
+	u8 sink_cnt;
+	u8 byte;
+
+	dptx_info(dptx, "DPTX - Hotplug Detected");
+
+	mmi_dp_video_intr_dis(dptx);
+	hpd_ien = mmi_dp_read(dptx->base, HPD_INTERRUPT_ENABLE);
+	hpd_ien |= (DPTX_HPD_IEN_IRQ_EN |
+		    DPTX_HPD_IEN_HOT_UNPLUG_EN);
+	mmi_dp_write(dptx->base, HPD_INTERRUPT_ENABLE, hpd_ien);
+	mmi_dp_enable_hpd_intr(dptx);
+
+	mmi_dp_core_init_phy(dptx);
+	mmi_dp_clr(dptx->base, CCTL, CCTL_DEFAULT_FAST_LINK_TRAIN_EN);
+
+	/* HDCP Soft Reset */
+	mmi_dp_set(dptx->base, SOFT_RESET_CTRL, HDCP_MODULE_RESET);
+	usleep_range(10, 20);
+	mmi_dp_clr(dptx->base, SOFT_RESET_CTRL, HDCP_MODULE_RESET);
+	msleep(100);
+
+	/* Read Sink DPCD registers - Receiver Capability */
+	memset(rx_caps, 0, DPTX_RECEIVER_CAP_SIZE);
+	retval = mmi_dp_read_bytes_from_dpcd(dptx, DP_DPCD_REV,
+					     rx_caps, DPTX_RECEIVER_CAP_SIZE);
+	if (retval) {
+		dptx_err(dptx, "DPCD Sink Capabilities: Unable to retrieve. retval:%d\n", retval);
+		return retval;
+	}
+	mmi_dp_parse_rx_capabilities(dptx, rx_caps);
+	dptx_dbg(dptx, "DP Revision %x.%x\n", dptx->rx_caps.major_rev_num,
+		 dptx->rx_caps.minor_rev_num);
+
+	/* Read Sink DPCD registers - Extended Receiver Capability */
+	if (dptx->rx_caps.extended_receiver_cap_present) {
+		retval = mmi_dp_read_bytes_from_dpcd(dptx, 0x2200, rx_caps,
+						     DPTX_RECEIVER_CAP_SIZE);
+		if (retval) {
+			dptx_err(dptx, "DPCD Extended Sink Capabilities: Unable to retrieve\n");
+			return retval;
+		}
+
+		mmi_dp_parse_rx_capabilities(dptx, rx_caps);
+		dptx_dbg(dptx, "Extended DP Revision %x.%x\n", dptx->rx_caps.major_rev_num,
+			 dptx->rx_caps.minor_rev_num);
+	}
+
+	mmi_dp_write_dpcd(dptx, DP_SET_POWER, 0);
+	msleep(100);
+	mmi_dp_write_dpcd(dptx, DP_SET_POWER, 1);
+	msleep(50);
+
+	if (dptx->rx_caps.enhanced_frame_cap) {
+		u8 val = 0;
+
+		mmi_dp_read_dpcd(dptx, 0x00101, &val);
+		val |= BIT(7);
+		mmi_dp_write_dpcd(dptx, 0x00101, val);
+		dptx_dbg(dptx, "ENHANCED FRAME CAPABILITY ACTIVATED");
+	}
+
+	mmi_dp_read_dpcd(dptx, DP_SINK_COUNT, &sink_cnt);
+	sink_cnt &= 0x3F;
+	if (sink_cnt == 0) {
+		dptx_dbg(dptx, "ZERO SINKS CONNECTED");
+		return 0;
+	}
+
+	/* Initialize ALPM variables */
+	alpm = &dptx->alpm;
+	alpm_availability = mmi_dp_alpm_is_available(dptx);
+	if (alpm_availability)
+		alpm->status = DISABLED;
+	else
+		alpm->status = NOT_AVAILABLE;
+
+	/* Define Stream Mode */
+	mmi_dp_write_mask(dptx, CCTL, CCTL_ENABLE_MST_MODE, dptx->mst);
+	mmi_dp_read_dpcd(dptx, DP_MSTM_CAP, &byte);
+	if (dptx->mst && byte) {
+		mmi_dp_write_dpcd(dptx, DP_MSTM_CTRL, 0x7);
+		dptx_dbg(dptx, "ENABLING MST ON SINK");
+		mmi_dp_write_dpcd(dptx, DP_BRANCH_DEVICE_CTRL, 0x1);
+	} else {
+		mmi_dp_write_dpcd(dptx, DP_MSTM_CTRL, 0x0);
+	}
+
+	dptx->link.rate = dptx->max_rate;
+	dptx->link.lanes = dptx->max_lanes;
+
+	/* Initiate link training */
+	if (dptx->fec) {
+		mmi_dp_set(dptx->base, CCTL, CCTL_ENHANCE_FRAMING_WITH_FEC_EN);
+
+		/* Set FEC_READY on the sink side */
+		retval = mmi_dp_write_dpcd(dptx, DP_FEC_CONFIGURATION, DP_FEC_READY);
+		if (retval)
+			return retval;
+	}
+
+	if (dptx->rx_caps.no_aux_transaction_link_training) {
+		mmi_dp_fast_link_training(dptx);
+	} else {
+		retval = mmi_dp_full_link_training(dptx);
+		if (retval)
+			return retval;
+	}
+
+	dptx->conn_status = connector_status_connected;
+
+	/* Clean interrupts */
+	mmi_dp_clean_interrupts(dptx);
+
+	return 0;
+}
+
+irqreturn_t mmi_dp_threaded_irq(int irq, void *dev)
+{
+	u32 hpdsts, hpd_ien;
+	struct dptx *dptx = dev;
+
+	mutex_lock(&dptx->mutex);
+
+	/*
+	 * TODO this should be set after all AUX transactions that are
+	 * queued are aborted. Currently we don't queue AUX and AUX is
+	 * only started from this function.
+	 */
+	atomic_set(&dptx->aux.abort, 0);
+	atomic_set(&dptx->aux.serving, 1);
+
+	if (atomic_read(&dptx->c_connect)) {
+		atomic_set(&dptx->c_connect, 0);
+
+		if (mmi_dp_read_regfield(dptx->base, HPD_STATUS, HPD_STATUS_MASK))
+			mmi_dp_handle_hotplug(dptx);
+		else
+			mmi_dp_handle_hotunplug(dptx);
+		hpd_ien = mmi_dp_read(dptx->base, HPD_INTERRUPT_ENABLE);
+		hpd_ien |= DPTX_HPD_IEN_IRQ_EN;
+		mmi_dp_write(dptx->base, HPD_INTERRUPT_ENABLE, hpd_ien);
+		mmi_dp_global_intr_en(dptx);
+	}
+
+	if (atomic_read(&dptx->sink_request)) {
+		atomic_set(&dptx->sink_request, 0);
+		hpdsts = 0x1;
+		mmi_dp_write(dptx->base, HPD_STATUS, hpdsts);
+		mmi_dp_global_intr_en(dptx);
+	}
+
+	atomic_set(&dptx->aux.serving, 0);
+
+	mutex_unlock(&dptx->mutex);
+
+	return IRQ_HANDLED;
+}
+
+static void mmi_dp_handle_hpd_irq(struct dptx *dptx)
+{
+	dptx_dbg(dptx, "%s: HPD_IRQ\n", __func__);
+	mmi_dp_notify(dptx);
+}
+
+irqreturn_t mmi_dp_irq(int irq, void *dev)
+{
+	irqreturn_t retval = IRQ_HANDLED;
+	struct dptx *dptx = dev;
+	u32 ists;
+
+	ists = mmi_dp_read(dptx->base, GENERAL_INTERRUPT);
+
+	if (!(ists & DPTX_ISTS_ALL_INTR)) {
+		dptx_dbg(dptx, "%s: IRQ_NONE\n", __func__);
+		return IRQ_NONE;
+	}
+
+	if (FIELD_GET(GEN_INTR_SDP_EVENT_STREAM0, ists)) {
+		dptx_dbg(dptx, "%s: DPTX_ISTS_SDP\n", __func__);
+		/* TODO Handle and clear */
+	}
+
+	if (FIELD_GET(GEN_INTR_AUDIO_FIFO_OVERFLOW_STREAM0, ists)) {
+		dptx_dbg(dptx, "%s: DPTX_ISTS_AUDIO_FIFO_OVERFLOW\n", __func__);
+		mmi_dp_set(dptx->base, GENERAL_INTERRUPT,
+			   GEN_INTR_AUDIO_FIFO_OVERFLOW_STREAM0);
+	}
+
+	if (FIELD_GET(GEN_INTR_VIDEO_FIFO_OVERFLOW_STREAM0, ists)) {
+		dptx_dbg(dptx, "%s: DPTX_ISTS_VIDEO_FIFO_OVERFLOW\n", __func__);
+		ists |= BIT(6);
+		mmi_dp_write(dptx->base, GENERAL_INTERRUPT, ists);
+	}
+
+	if (FIELD_GET(GEN_INTR_VIDEO_FIFO_UNDERFLOW_STREAM0, ists)) {
+		dptx_dbg(dptx, "%s: DPTX_ISTS_VIDEO_FIFO_UNDERFLOW\n", __func__);
+		ists |= BIT(8);
+		mmi_dp_write(dptx->base, GENERAL_INTERRUPT, ists);
+	}
+
+	if (FIELD_GET(GEN_INTR_HPD_EVENT, ists)) {
+		u32 hpdsts;
+
+		mmi_dp_global_intr_dis(dptx);
+
+		if (mmi_dp_read_regfield(dptx->base, HPD_STATUS, HPD_IRQ)) {
+			hpdsts = mmi_dp_read(dptx->base, HPD_STATUS);
+			hpdsts |= BIT(0);
+			mmi_dp_write(dptx->base, HPD_STATUS, hpdsts);
+			mmi_dp_handle_hpd_irq(dptx);
+			retval = IRQ_WAKE_THREAD;
+		}
+
+		if (mmi_dp_read_regfield(dptx->base, HPD_STATUS, HPD_HOT_PLUG)) {
+			hpdsts = mmi_dp_read(dptx->base, HPD_STATUS);
+
+			hpdsts |= BIT(1);
+			mmi_dp_write(dptx->base, HPD_STATUS, hpdsts);
+
+			atomic_set(&dptx->aux.abort, 1);
+			atomic_set(&dptx->c_connect, 1);
+			mmi_dp_notify(dptx);
+			retval = IRQ_WAKE_THREAD;
+		}
+
+		if (mmi_dp_read_regfield(dptx->base, HPD_STATUS, HPD_HOT_UNPLUG)) {
+			hpdsts = mmi_dp_read(dptx->base, HPD_STATUS);
+
+			hpdsts |= BIT(2);
+			mmi_dp_write(dptx->base, HPD_STATUS, hpdsts);
+
+			atomic_set(&dptx->aux.abort, 1);
+			atomic_set(&dptx->c_connect, 1);
+			mmi_dp_notify(dptx);
+			retval = IRQ_WAKE_THREAD;
+		}
+	}
+
+	return retval;
+}
diff --git a/drivers/staging/xlnx-mmi-dptx/mmi_dp_link.c b/drivers/staging/xlnx-mmi-dptx/mmi_dp_link.c
new file mode 100644
index 000000000..acd4d1dca
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/mmi_dp_link.c
@@ -0,0 +1,1159 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Multimedia Integrated DisplayPort Tx driver
+ *
+ * Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
+ */
+
+#include <drm/display/drm_dp_helper.h>
+#include <drm/drm_fixed.h>
+
+#include "mmi_dp.h"
+
+/**
+ * mmi_dp_set_video_format() - Set video format
+ * @dptx: The dptx struct
+ * @video_format: video format
+ * Possible options: 0 - CEA, 1 - CVT, 2 - DMT
+ *
+ * Return: Returns 0 on success otherwise negative errno.
+ */
+static int mmi_dp_set_video_format(struct dptx *dptx, u8 video_format)
+{
+	struct video_params *vparams;
+
+	if (video_format > DMT) {
+		dptx_dbg(dptx, "%s: Invalid video format value %d\n",
+			 __func__, video_format);
+		return -EINVAL;
+	}
+
+	vparams = &dptx->vparams[0];
+	vparams->video_format = video_format;
+	return 0;
+}
+
+/**
+ * mmi_dp_set_video_dynamic_range() - Set video dynamic range
+ * @dptx: The dptx struct
+ * @dynamic_range: video dynamic range
+ * Possible options: 1 - CEA, 2 - VESA
+ *
+ * Return: Returns 0 on success otherwise negative errno.
+ */
+static int mmi_dp_set_video_dynamic_range(struct dptx *dptx, u8 dynamic_range)
+{
+	struct video_params *vparams;
+
+	if (dynamic_range > VESA) {
+		dptx_dbg(dptx, "%s: Invalid dynamic range value %d\n",
+			 __func__, dynamic_range);
+		return -EINVAL;
+	}
+
+	vparams = &dptx->vparams[0];
+	vparams->dynamic_range = dynamic_range;
+
+	return 0;
+}
+
+/**
+ * mmi_dp_set_video_colorimetry() - Set video colorimetry
+ * @dptx: The dptx struct
+ * @video_col: Video colorimetry
+ * Possible options: 1 - ITU-R BT.601, 2 - ITU-R BT.709
+ *
+ * Return: Returns 0 on success otherwise negative errno.
+ */
+static int mmi_dp_set_video_colorimetry(struct dptx *dptx, u8 video_col)
+{
+	struct video_params *vparams;
+
+	if (video_col > ITU709) {
+		dptx_dbg(dptx, "%s: Invalid video colorimetry value %d\n",
+			 __func__, video_col);
+		return -EINVAL;
+	}
+
+	vparams = &dptx->vparams[0];
+	vparams->colorimetry = video_col;
+
+	return 0;
+}
+
+/**
+ * mmi_dp_set_pixel_enc() - Set pixel encoding
+ * @dptx: The dptx struct
+ * @pix_enc: Video pixel encoding
+ * Possible options: RGB - 0, YCbCR420 - 1, YCbCR422 - 2
+ *		     YCbCR444 - 3, YOnly - 4, RAW -5
+ *
+ * Return: Returns 0 on success otherwise negative errno.
+ */
+static int mmi_dp_set_pixel_enc(struct dptx *dptx, u8 pix_enc)
+{
+	struct video_params *vparams;
+	struct dtd *mdtd;
+	u32 hpdsts;
+
+	hpdsts = mmi_dp_read_regfield(dptx->base, HPD_STATUS, HPD_STATUS_MASK);
+	if (!hpdsts) {
+		dptx_dbg(dptx, "%s: Not connected\n", __func__);
+		return -ENODEV;
+	}
+	if (pix_enc > RAW) {
+		dptx_dbg(dptx, "%s: Invalid pixel encoding value %d\n",
+			 __func__, pix_enc);
+		return -EINVAL;
+	}
+	vparams = &dptx->vparams[0];
+	mdtd = &vparams->mdtd;
+	mmi_dp_video_ts_calculate(dptx, dptx->link.lanes, dptx->link.rate,
+				  vparams->bpc, pix_enc, mdtd->pixel_clock);
+
+	vparams->pix_enc = pix_enc;
+
+	mmi_dp_disable_default_video_stream(dptx, DEFAULT_STREAM);
+	mmi_dp_video_bpc_change(dptx, DEFAULT_STREAM);
+	mmi_dp_video_ts_change(dptx, DEFAULT_STREAM);
+	mmi_dp_enable_default_video_stream(dptx, DEFAULT_STREAM);
+
+	if (pix_enc == YCBCR420) {
+		mmi_dp_vsd_ycbcr420_send(dptx, 1);
+		dptx->ycbcr420 = true;
+	} else {
+		mmi_dp_vsd_ycbcr420_send(dptx, 0);
+		dptx->ycbcr420 = false;
+	}
+
+	return 0;
+}
+
+void mmi_dp_video_ts_calculate(struct dptx *dptx, int lane_num, int rate,
+			       int bpc, int encoding, int pixel_clock)
+{
+	struct video_params *vparams;
+	struct dtd *mdtd;
+	int link_rate, link_clk, tu, tu_frac, color_dep, numerator, denominator;
+	int T1 = 0, T2 = 0;
+	s64 fixp;
+
+	vparams = &dptx->vparams[0];
+	mdtd = &vparams->mdtd;
+	link_rate = mmi_dp_get_link_rate(rate);
+	color_dep = mmi_dp_get_color_depth_bpp(bpc, encoding);
+
+	switch (rate) {
+	case DPTX_PHYIF_CTRL_RATE_RBR:
+		link_clk = 40500;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR:
+		link_clk = 67500;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR2:
+		link_clk = 135000;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR3:
+		link_clk = 202500;
+		break;
+	default:
+		link_clk = 40500;
+	}
+
+	numerator = dptx->selected_pixel_clock * (color_dep / 8);
+	denominator = (link_rate) * 10 * lane_num * 100;
+	fixp = drm_fixp_from_fraction(numerator * 64, denominator);
+	tu = drm_fixp2int(fixp);
+
+	fixp &= DRM_FIXED_DECIMAL_MASK;
+	if (dptx->mst)
+		fixp *= 64;
+	else
+		fixp *= 10;
+
+	tu_frac = drm_fixp2int(fixp);
+
+	/* Calculate init_threshold for non DSC mode */
+	if (dptx->multipixel == DPTX_MP_SINGLE_PIXEL) {
+		/* Single Pixel Mode */
+		if (tu <= 16)
+			vparams->init_threshold = 32;
+		else if (mdtd->h_blanking <= 40 && encoding == YCBCR420)
+			vparams->init_threshold = 3;
+		else if (mdtd->h_blanking <= 80 && encoding != YCBCR420)
+			vparams->init_threshold = 12;
+		else
+			vparams->init_threshold = 16;
+	} else {
+		/* Multiple Pixel Mode */
+		int init_thrshld;
+
+		switch (bpc) {
+		case COLOR_DEPTH_6:
+			T1 = (4 * 1000 / 9) * lane_num;
+			break;
+		case COLOR_DEPTH_8:
+			if (encoding == YCBCR422)
+				T1 = (1000 / 2) * lane_num;
+			else if (encoding == YONLY)
+				T1 = lane_num * 1000;
+			else if (dptx->multipixel == DPTX_MP_DUAL_PIXEL)
+				T1 = (1000 / 3) * lane_num;
+			else
+				T1 = (3000 / 16) * lane_num;
+			break;
+		case COLOR_DEPTH_10:
+			if (encoding == YCBCR422)
+				T1 = (2000 / 5) * lane_num;
+			else if (encoding == YONLY)
+				T1 = (4000 / 5) * lane_num;
+			else
+				T1 = (4000 / 15) * lane_num;
+			break;
+		case COLOR_DEPTH_12:
+			if (encoding == YCBCR422) /* Nothing happens here */
+				if (dptx->multipixel == DPTX_MP_DUAL_PIXEL)
+					T1 = (1000 / 6) * lane_num;
+				else
+					T1 = (1000 / 3) * lane_num;
+			else if (encoding == YONLY)
+				T1 = (2000 / 3) * lane_num;
+			else
+				T1 = (2000 / 9) * lane_num;
+			break;
+		case COLOR_DEPTH_16:
+			if (encoding == YONLY)
+				T1 = (1000 / 2) * lane_num;
+			else if ((encoding != YCBCR422) &&
+				 (dptx->multipixel == DPTX_MP_DUAL_PIXEL))
+				T1 = (1000 / 6) * lane_num;
+			else
+				T1 = (1000 / 4) * lane_num;
+			break;
+		default:
+			dptx_dbg(dptx, "Invalid param BPC = %d\n", bpc);
+		}
+
+		T2 = (link_clk * 1000 / dptx->selected_pixel_clock);
+
+		init_thrshld = T1 * T2 * tu / (1000 * 1000);
+		if (init_thrshld <= 16 || tu < 10)
+			vparams->init_threshold = 40;
+		else
+			vparams->init_threshold = init_thrshld;
+	}
+
+	vparams->aver_bytes_per_tu = tu;
+	vparams->aver_bytes_per_tu_frac = tu_frac;
+}
+
+static int mmi_dp_config_ctrl_video_mode(struct dptx *dptx)
+{
+	struct video_params *vparams;
+	struct dtd *mdtd;
+
+	vparams = &dptx->vparams[0];
+	mdtd = &vparams->mdtd;
+
+	mmi_dp_disable_video_stream(dptx, 0);
+	mmi_dp_vinput_polarity_ctrl(dptx, 0);
+	mmi_dp_vsample_ctrl(dptx, 0);
+	mmi_dp_video_config1(dptx, 0);
+	mmi_dp_video_config2(dptx, 0);
+	mmi_dp_video_config3(dptx, 0);
+	mmi_dp_video_config4(dptx, 0);
+
+	mmi_dp_video_ts_calculate(dptx, dptx->link.lanes, dptx->link.rate, vparams->bpc,
+				  vparams->pix_enc, mdtd->pixel_clock);
+	mmi_dp_write_mask(dptx, VIDEO_CONFIG5, AVERAGE_BYTES_PER_TU_MASK,
+			  vparams->aver_bytes_per_tu);
+
+	if (!dptx->mst)
+		mmi_dp_write_mask(dptx, VIDEO_CONFIG5, AVERAGE_BYTES_PER_TU_FRAC_MASK,
+				  vparams->aver_bytes_per_tu_frac << 2);
+	else
+		mmi_dp_write_mask(dptx, VIDEO_CONFIG5, AVERAGE_BYTES_PER_TU_FRAC_MASK,
+				  vparams->aver_bytes_per_tu_frac);
+
+	mmi_dp_write_mask(dptx, VIDEO_CONFIG5, INIT_THRESHOLD_MASK,
+			  vparams->init_threshold);
+
+	if (dptx->rx_caps.enhanced_frame_cap)
+		mmi_dp_write_mask(dptx, CCTL, CCTL_ENHANCE_FRAMING_EN, 1);
+
+	mmi_dp_video_msa1(dptx, 0);
+	mmi_dp_video_msa2(dptx, 0);
+	mmi_dp_video_msa3(dptx, 0);
+	mmi_dp_video_hblank_interval(dptx, 0);
+
+	return 0;
+}
+
+int mmi_dp_sst_configuration(struct dptx *dptx)
+{
+	struct video_params *vparams;
+
+	dptx_dbg(dptx, "Making SST Configuration");
+	dptx->streams = 1;
+	vparams = &dptx->vparams[0];
+
+	/* Configure CTRL for Timing requested */
+	mmi_dp_config_ctrl_video_mode(dptx);
+
+	/* Enable Video Stream */
+	mmi_dp_set(dptx->base, VSAMPLE_CTRL, VIDEO_STREAM_ENABLE_MASK);
+
+	dptx_info(dptx, "Video Transmission: %dx%d @ %dHz", vparams->mdtd.h_active,
+		  vparams->mdtd.v_active, ((vparams->refresh_rate + 500) / 1000));
+
+	return 0;
+}
+
+int mmi_dp_dtd_fill(struct dtd *mdtd, struct display_mode_t *display_mode)
+{
+	struct dtd_t dtd;
+
+	dtd = display_mode->dtd;
+	mmi_dp_dtd_reset(mdtd);
+
+	mdtd->h_image_size = dtd.m_h_image_size;
+	mdtd->v_image_size = dtd.m_v_image_size;
+	mdtd->h_active = dtd.m_h_active;
+	mdtd->v_active = dtd.m_v_active;
+	mdtd->h_border = dtd.m_h_border;
+	mdtd->v_border = dtd.m_v_border;
+	mdtd->h_blanking = dtd.m_h_blanking;
+	mdtd->v_blanking = dtd.m_v_blanking;
+	mdtd->h_sync_offset = dtd.m_h_sync_offset;
+	mdtd->v_sync_offset = dtd.m_v_sync_offset;
+	mdtd->h_sync_pulse_width = dtd.m_h_sync_pulse_width;
+	mdtd->v_sync_pulse_width = dtd.m_v_sync_pulse_width;
+	mdtd->interlaced = dtd.m_interlaced; /* (progressive_nI) */
+	mdtd->pixel_clock = dtd.m_pixel_clock;
+	mdtd->h_sync_polarity = 1; /* dtd.m_h_sync_polarity; */
+	mdtd->v_sync_polarity = 1; /* dtd.m_v_sync_polarity; */
+
+	return 0;
+}
+
+static int mmi_dp_fill_current_mode_1080(struct display_mode_t *cmode)
+{
+	cmode->refresh_rate = 60000;
+	/* Pixel Clock */
+	cmode->dtd.m_pixel_clock = 148500;
+	/* Interlaced */
+	cmode->dtd.m_interlaced = 0;
+
+	/* Horizontal data */
+	cmode->dtd.m_h_active = 1920;
+	cmode->dtd.m_h_blanking = 280;
+	cmode->dtd.m_h_border = 0;
+	cmode->dtd.m_h_image_size = 16;
+	cmode->dtd.m_h_sync_pulse_width = 44;
+	cmode->dtd.m_h_sync_offset = 88;
+
+	/* Vertical data */
+	cmode->dtd.m_v_active = 1080;
+	cmode->dtd.m_v_blanking = 45;
+	cmode->dtd.m_v_border = 0;
+	cmode->dtd.m_v_image_size = 9;
+	cmode->dtd.m_v_sync_pulse_width = 5;
+	cmode->dtd.m_v_sync_offset = 4;
+	return 0;
+}
+
+static int mmi_dp_video_mode_change(struct dptx *dptx)
+{
+	struct video_params *vparams;
+	struct display_mode_t current_mode;
+	u16 peak_stream_bw, link_bw;
+	struct dtd mdtd;
+	u32 pixel_clk;
+	int retval;
+	s64 fixp;
+	u16 rate;
+	u8 bpp;
+
+	vparams = &dptx->vparams[0];
+	retval = 0;
+
+	mmi_dp_fill_current_mode_1080(&current_mode);
+	mmi_dp_dtd_fill(&mdtd, &current_mode);
+
+	vparams->mdtd = mdtd;
+
+	dptx->selected_pixel_clock = mdtd.pixel_clock;
+	/* Check if the stablished link is enough for the payload requested */
+	bpp = mmi_dp_get_color_depth_bpp(vparams->bpc, vparams->pix_enc);
+	rate = mmi_dp_get_link_rate(dptx->link.rate);
+	pixel_clk = mdtd.pixel_clock;
+	fixp = drm_fixp_div(drm_int2fixp(bpp), drm_int2fixp(8));
+	fixp = drm_fixp_mul(fixp, drm_int2fixp(pixel_clk));
+	fixp = drm_fixp_div(fixp, drm_int2fixp(1000));
+	peak_stream_bw = drm_fixp2int(fixp);
+	link_bw = rate * dptx->link.lanes;
+
+	if (peak_stream_bw > link_bw) {
+		dptx_err(dptx, "ERROR: VIC chosen isn't suitable for Link Rate running\n");
+		dptx_err(dptx, "refresh_rate: %d BPC: %d PixelClock: %d",
+			 vparams->refresh_rate, vparams->bpc, mdtd.pixel_clock);
+		dptx_err(dptx, "Rate: %d Lanes: %d", dptx->link.rate, dptx->link.lanes);
+		return -EINVAL;
+	}
+
+	/* Disable Video Stream and Generator */
+	mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(0), VIDEO_STREAM_ENABLE_MASK, 0);
+
+	mmi_dp_sst_configuration(dptx);
+
+	mmi_dp_clean_interrupts(dptx);
+
+	return retval;
+}
+
+/**
+ * mmi_dp_set_video_mode() - Set current video mode
+ * @dptx: The dptx struct
+ *
+ * Return: Returns 0 on success otherwise negative errno.
+ */
+static int mmi_dp_set_video_mode(struct dptx *dptx)
+{
+	u32 hpdsts;
+	int retval;
+
+	retval = 0;
+
+	hpdsts = mmi_dp_read_regfield(dptx->base, HPD_STATUS, HPD_STATUS_MASK);
+	if (!hpdsts) {
+		dptx_dbg(dptx, "%s: Not connected\n", __func__);
+		return -ENODEV;
+	}
+	retval = mmi_dp_video_mode_change(dptx);
+	if (retval < 0) {
+		mmi_dp_write_mask(dptx, DPTX_VSAMPLE_CTRL_N(DEFAULT_STREAM),
+				  VIDEO_STREAM_ENABLE_MASK, 0);
+		mmi_dp_soft_reset(dptx, DPTX_SRST_VIDEO_RESET_ALL);
+	}
+
+	return retval;
+}
+
+/**
+ * mmi_dp_set_bpc() - Set bits per component
+ * @dptx: The dptx struct
+ * @bpc: Bits per component value
+ * Possible options: 6, 8, 10, 12, 16
+ *
+ * Return: Returns 0 on success otherwise negative errno.
+ */
+static int mmi_dp_set_bpc(struct dptx *dptx, u8 bpc)
+{
+	u32 hpdsts;
+	struct video_params *vparams;
+
+	hpdsts = mmi_dp_read_regfield(dptx->base, HPD_STATUS, HPD_STATUS_MASK);
+	if (!hpdsts) {
+		dptx_dbg(dptx, "%s: Not connected\n", __func__);
+		return -ENODEV;
+	}
+
+	switch (bpc) {
+	case (COLOR_DEPTH_6):
+	case (COLOR_DEPTH_8):
+	case (COLOR_DEPTH_10):
+	case (COLOR_DEPTH_12):
+	case (COLOR_DEPTH_16):
+		break;
+	default:
+		dptx_dbg(dptx, "%s: Invalid bits per component value %d\n",
+			 __func__, bpc);
+		return -EINVAL;
+	}
+
+	vparams = &dptx->vparams[0];
+	vparams->bpc = bpc;
+	mmi_dp_disable_default_video_stream(dptx, DEFAULT_STREAM);
+	mmi_dp_config_ctrl_video_mode(dptx);
+	mmi_dp_enable_default_video_stream(dptx, DEFAULT_STREAM);
+
+	return 0;
+}
+
+static int mmi_dp_check_phy_busy(struct dptx *dptx, u32 timeout)
+{
+	u32 count = 0, phy_busy;
+
+	phy_busy = mmi_dp_read_regfield(dptx->base, PHYIF_CTRL, PHYIF_PHY_BUSY);
+	while (phy_busy) {
+		count++;
+		if (count > timeout) {
+			dptx_err(dptx, "%s: TIMEOUT - PHY BUSY", __func__);
+			return -EBUSY;
+		}
+		msleep(20);
+		phy_busy = mmi_dp_read_regfield(dptx->base, PHYIF_CTRL, PHYIF_PHY_BUSY);
+	}
+	return 0;
+}
+
+static int mmi_dp_power_up_phy(struct dptx *dptx)
+{
+	dptx_dbg(dptx, "PHY: Power Up Sequence\n");
+
+	/* Set the initial input values */
+	mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_POWER_DOWN, DPTX_PHY_POWER_DOWN);
+	mmi_dp_clr(dptx->base, PHYIF_CTRL, DPTX_PHYIF_CTRL_XMIT_EN_ALL);
+
+	if (mmi_dp_check_phy_busy(dptx, MAX_PHY_BUSY_WAIT_ITER))
+		return -EAGAIN;
+
+	return 0;
+}
+
+int mmi_dp_power_state_change_phy(struct dptx *dptx, u8 power_state)
+{
+	if (!(power_state == DPTX_PHY_POWER_ON ||
+	      power_state == DPTX_PHY_INTER_P2_POWER ||
+	      power_state == DPTX_PHY_POWER_DOWN ||
+	      power_state == DPTX_PHY_P4_POWER_STATE))
+		return -EINVAL;
+
+	/* Select the power state to change into */
+	mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_POWER_DOWN, power_state);
+
+	if (mmi_dp_check_phy_busy(dptx, MAX_PHY_BUSY_WAIT_ITER))
+		return -EAGAIN;
+
+	return 0;
+}
+
+int mmi_dp_disable_datapath_phy(struct dptx *dptx)
+{
+	u32 phyifctrl;
+	u32 mask;
+
+	mask = 0xF00; /* XMIT_ENABLE bits (11-8) */
+	phyifctrl = mmi_dp_read(dptx->base, PHYIF_CTRL);
+	phyifctrl &= ~mask;
+	mmi_dp_write(dptx->base, PHYIF_CTRL, phyifctrl);
+
+	return 0;
+}
+
+static int mmi_dp_link_training_lanes_set(struct dptx *dptx)
+{
+	int retval;
+	unsigned int i;
+	u8 bytes[4] = { 0xff, 0xff, 0xff, 0xff };
+
+	for (i = 0; i < dptx->link.lanes; i++) {
+		u8 byte = 0;
+
+		byte |= ((dptx->link.vswing_level[i] <<
+			  DP_TRAIN_VOLTAGE_SWING_SHIFT) &
+			 DP_TRAIN_VOLTAGE_SWING_MASK);
+
+		if (dptx->link.vswing_level[i] == 3)
+			byte |= DP_TRAIN_MAX_SWING_REACHED;
+
+		byte |= ((dptx->link.preemp_level[i] <<
+			  DP_TRAIN_PRE_EMPHASIS_SHIFT) &
+			 DP_TRAIN_PRE_EMPHASIS_MASK);
+
+		if (dptx->link.preemp_level[i] == 3)
+			byte |= DP_TRAIN_MAX_PRE_EMPHASIS_REACHED;
+
+		bytes[i] = byte;
+	}
+
+	retval = mmi_dp_write_bytes_to_dpcd(dptx, DP_TRAINING_LANE0_SET, bytes,
+					    dptx->link.lanes);
+	if (retval)
+		return retval;
+
+	return 0;
+}
+
+int mmi_dp_fast_link_training(struct dptx *dptx)
+{
+	int nr_lanes, link_rate;
+
+	nr_lanes = dptx->max_lanes;
+	link_rate = dptx->max_rate;
+	mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_POWER_DOWN, DPTX_PHY_POWER_ON);
+	mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_RATE, link_rate);
+
+	switch (nr_lanes) {
+	case (1):
+		mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_LANES, 0);
+		break;
+	case (2):
+		mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_LANES, 2);
+		break;
+	case (4):
+		mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_LANES, 4);
+		break;
+	default:
+		mmi_dp_write_mask(dptx, PHYIF_CTRL, PHYIF_PHY_LANES, 0);
+	}
+
+	if (mmi_dp_check_phy_busy(dptx, 1000))
+		return -EBUSY;
+
+	mmi_dp_phy_set_pattern(dptx, 1);
+	mmi_dp_phy_enable_xmit(dptx, nr_lanes, true);
+
+	/* Wait for 500us as per DP Tx controller programming guide */
+	usleep_range(500, 600);
+
+	switch (link_rate) {
+	case (DPTX_PHYIF_CTRL_RATE_HBR):
+		mmi_dp_phy_set_pattern(dptx, 2);
+		break;
+	case (DPTX_PHYIF_CTRL_RATE_HBR2):
+		mmi_dp_phy_set_pattern(dptx, 3);
+		break;
+	case (DPTX_PHYIF_CTRL_RATE_HBR3):
+		mmi_dp_phy_set_pattern(dptx, 4);
+		break;
+	default:
+		mmi_dp_phy_set_pattern(dptx, 2);
+		break;
+	}
+
+	/* Wait for 500us as per DP Tx controller programming guide */
+	usleep_range(500, 600);
+
+	mmi_dp_phy_set_pattern(dptx, 0);
+
+	return 0;
+}
+
+static int mmi_dp_dpcd_link_configuration(struct dptx *dptx)
+{
+	u8 lanes, rate, byte;
+
+	/* LINK_BW_SET - rate */
+	rate = mmi_dp_phy_rate_to_bw(dptx->link.rate);
+	mmi_dp_write_dpcd(dptx, DP_LINK_BW_SET, rate);
+
+	/* LANE_COUNT_SET */
+	lanes = dptx->link.lanes | DP_LANE_COUNT_ENHANCED_FRAME_EN;
+	mmi_dp_write_dpcd(dptx, DP_LANE_COUNT_SET, lanes);
+
+	/* DOWNSPREAD_CTRL */
+	byte = 0x00; /* SPREAD_AMP must be set to 0 */
+	mmi_dp_write_dpcd(dptx, DP_DOWNSPREAD_CTRL, byte);
+
+	/* MAIN_LINK_CHANNEL_CODING_SET */
+	byte = 0x01; /* 8b/10b encoding selected */
+	mmi_dp_write_dpcd(dptx, DP_MAIN_LINK_CHANNEL_CODING_SET, byte);
+
+	return 0;
+}
+
+static int mmi_dp_transmit_TPS1(struct dptx *dptx)
+{
+	int ret;
+
+	mmi_dp_disable_datapath_phy(dptx);
+
+	/* Configure PHY Link Rate */
+	ret = mmi_dp_power_state_change_phy(dptx, DPTX_PHY_INTER_P2_POWER);
+	if (ret)
+		return ret;
+
+	/* Configure PHY Lanes */
+	mmi_dp_phy_set_lanes(dptx, dptx->link.lanes);
+	mmi_dp_phy_set_rate(dptx, dptx->link.rate);
+
+	/* wait for phy busy */
+	if (mmi_dp_check_phy_busy(dptx, 1000))
+		return -EBUSY;
+
+	/* Force no Transmitted Pattern */
+	mmi_dp_phy_set_pattern(dptx, DPTX_PHYIF_CTRL_TPS_NONE);
+
+	/* PHY Power On */
+	ret = mmi_dp_power_state_change_phy(dptx, DPTX_PHY_POWER_ON);
+	if (ret)
+		return ret;
+
+	/* Configure TPS1 transmission */
+	mmi_dp_phy_set_pattern(dptx, DPTX_PHYIF_CTRL_TPS_1);
+
+	mmi_dp_phy_enable_xmit(dptx, dptx->link.lanes, true);
+
+	return 0;
+}
+
+static int mmi_dp_set_training_set_regs(struct dptx *dptx, u8 pattern)
+{
+	u8 reg[5] = { 0 };
+	u8 i;
+
+	/* TRAINING_PATTERN_SET - DPCD 102h */
+	reg[0] = mmi_dp_set8_field(reg[0], PATTERN_MASK, pattern);
+	if (pattern == DP_TRAINING_PATTERN_4)
+		reg[0] = mmi_dp_set8_field(reg[0], SCRAMBLING_DIS_MASK, 0);
+	else
+		reg[0] = mmi_dp_set8_field(reg[0], SCRAMBLING_DIS_MASK, 1);
+
+	/* TRAINING_LANEx_SET */
+	for (i = 0; i < dptx->link.lanes; i++) {
+		reg[1 + i] = mmi_dp_set8_field(reg[1 + i], VSWING_MASK, dptx->link.vswing_level[i]);
+		if (dptx->link.vswing_level[i] == 3)
+			reg[1 + i] = mmi_dp_set8_field(reg[1 + i], MAX_VSWING_MASK, 1);
+		else
+			reg[1 + i] = mmi_dp_set8_field(reg[1 + i], MAX_VSWING_MASK, 0);
+
+		reg[1 + i] = mmi_dp_set8_field(reg[1 + i], PREEMPH_MASK,
+					       dptx->link.preemp_level[i]);
+		if (dptx->link.preemp_level[i] == 3)
+			reg[1 + i] = mmi_dp_set8_field(reg[1 + i], MAX_PREEMPH_MASK, 1);
+		else
+			reg[1 + i] = mmi_dp_set8_field(reg[1 + i], MAX_PREEMPH_MASK, 0);
+	}
+
+	mmi_dp_write_bytes_to_dpcd(dptx, DP_TRAINING_PATTERN_SET, reg, 5);
+
+	return 0;
+}
+
+static int mmi_dp_adjust_drive_settings(struct dptx *dptx, bool *settings_changed)
+{
+	u8 bytes[2] = { 0 }, adj[4] = { 0 };
+	u8 lanes;
+	u8 i;
+
+	*settings_changed = FALSE;
+	lanes = dptx->link.lanes;
+	bytes[0] = dptx->link.status[4];
+	bytes[1] = dptx->link.status[5];
+
+	switch (lanes) {
+	case 4:
+		adj[2] = bytes[1] & 0x0f;
+		adj[3] = (bytes[1] & 0xf0) >> 4;
+		fallthrough;
+	case 2:
+		adj[1] = (bytes[0] & 0xf0) >> 4;
+		fallthrough;
+	case 1:
+		adj[0] = bytes[0] & 0x0f;
+		break;
+	default:
+		dptx_warn(dptx, "Invalid number of lanes %d\n", lanes);
+		return -EINVAL;
+	}
+
+	/* Save the drive settings */
+	for (i = 0; i < lanes; i++) {
+		u8 vs = adj[i] & 0x3;
+		u8 pe = (adj[i] & 0xc) >> 2;
+
+		if (dptx->link.vswing_level[i] != vs)
+			*settings_changed = TRUE;
+
+		dptx->link.vswing_level[i] = vs;
+		dptx->link.preemp_level[i] = pe;
+		dptx_dbg(dptx, "%s - SET PREEMP/VSWING VALUES [Lane %d]: vswing - %X preemp - %X",
+			 __func__, i, vs, pe);
+	}
+
+	mmi_dp_adjust_vswing_and_preemphasis(dptx);
+
+	return 0;
+}
+
+static int mmi_dp_cr_done_seq(struct dptx *dptx)
+{
+	bool settings_changed = FALSE;
+	bool cr_fallback_required;
+	bool max_vswing_achieved;
+	u8 adj_req_ack_cnt = 1;
+	u8 main_ack_cnt = 0;
+	int ret = 1;
+
+	/* Transmit TPS1 */
+	if (mmi_dp_transmit_TPS1(dptx)) {
+		/* Reset PHY */
+		mmi_dp_power_up_phy(dptx);
+		mmi_dp_power_state_change_phy(dptx, DPTX_PHY_INTER_P2_POWER);
+
+		mmi_dp_transmit_TPS1(dptx);
+	}
+
+	/* Set TRAINING_PATTERN_SET and TRAINING_LANEx_SET registers */
+	mmi_dp_set_training_set_regs(dptx, DP_TRAINING_PATTERN_1);
+
+	do {
+		/* Wait for 100us between training pattern set and reading Lane status */
+		fsleep(100);
+
+		/* Read LANEx_CR_DONE bits and ADJUST_REQUEST_LANEx_y regs */
+		mmi_dp_read_bytes_from_dpcd(dptx, DP_LANE0_1_STATUS, dptx->link.status, 6);
+		main_ack_cnt++;
+
+		if (drm_dp_clock_recovery_ok(dptx->link.status, dptx->link.lanes)) {
+			ret = CR_DONE;
+		} else {
+			dptx_err(dptx, "If Clock recovery is not ok");
+			/* If one of the conditions achieved threshold */
+			max_vswing_achieved = (dptx->link.vswing_level[0] == 3);
+			cr_fallback_required = (max_vswing_achieved ||
+						(adj_req_ack_cnt >= 5) ||
+						(main_ack_cnt >= 10));
+
+			if (cr_fallback_required) {
+				ret = -CR_FAIL;
+				break;
+			}
+			/* Adjust driver settings */
+			mmi_dp_adjust_drive_settings(dptx, &settings_changed);
+			/* Update TRAINING_LANE_SET regs */
+			mmi_dp_link_training_lanes_set(dptx);
+			settings_changed ? adj_req_ack_cnt = 0 : adj_req_ack_cnt++;
+		}
+	} while (ret != CR_DONE);
+
+	return ret;
+}
+
+static int mmi_dp_reduce_link_rate(struct dptx *dptx)
+{
+	u8 rate = dptx->link.rate;
+
+	switch (rate) {
+	case DPTX_PHYIF_CTRL_RATE_RBR:
+		return -ELOWESTRATE;
+	case DPTX_PHYIF_CTRL_RATE_HBR:
+		rate = DPTX_PHYIF_CTRL_RATE_RBR;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR2:
+		rate = DPTX_PHYIF_CTRL_RATE_HBR;
+		break;
+	case DPTX_PHYIF_CTRL_RATE_HBR3:
+		rate = DPTX_PHYIF_CTRL_RATE_HBR2;
+		break;
+	}
+
+	dptx->link.rate = rate;
+
+	return RATE_REDUCTION;
+}
+
+static u8 dp_link_status(const u8 link_status[DP_LINK_STATUS_SIZE], int r)
+{
+	return link_status[r - DP_LANE0_1_STATUS];
+}
+
+static u8 dp_get_lane_status(const u8 link_status[DP_LINK_STATUS_SIZE],
+			     int lane)
+{
+	int i = DP_LANE0_1_STATUS + (lane >> 1);
+	int s = (lane & 1) * 4;
+	u8 l = dp_link_status(link_status, i);
+
+	return (l >> s) & 0xf;
+}
+
+static bool mmi_dp_lane_cr_done(struct dptx *dptx, u8 lane)
+{
+	u8 lane_status;
+
+	lane_status = dp_get_lane_status(dptx->link.status, lane);
+	if ((lane_status & DP_LANE_CR_DONE) == 0)
+		return FALSE;
+	return TRUE;
+}
+
+static int reduce_link_lanes(struct dptx *dptx)
+{
+	u8 lanes;
+
+	switch (dptx->link.lanes) {
+	case 4:
+		if (mmi_dp_lane_cr_done(dptx, 1) == FALSE) {
+			fallthrough;
+		} else {
+			lanes = 2;
+			break;
+		}
+	case 2:
+		if (mmi_dp_lane_cr_done(dptx, 0) == FALSE) {
+			fallthrough;
+		} else {
+			lanes = 1;
+			break;
+		}
+	case 1:
+	default:
+		return -ELOWESTLANENR;
+	}
+
+	dptx->link.lanes = lanes;
+
+	return 0;
+}
+
+static int mmi_dp_transmit_ch_eq_TPS(struct dptx *dptx)
+{
+	struct rx_capabilities rx_caps;
+	u8 dp_pattern;
+	u8 pattern;
+
+	rx_caps = dptx->rx_caps;
+
+	switch (dptx->max_rate) {
+	case DPTX_PHYIF_CTRL_RATE_HBR3:
+		if (rx_caps.tps4_supported) {
+			pattern = DPTX_PHYIF_CTRL_TPS_4;
+			dp_pattern = DP_TRAINING_PATTERN_4;
+			break;
+		}
+		fallthrough;
+	case DPTX_PHYIF_CTRL_RATE_HBR2:
+		if (rx_caps.tps3_supported) {
+			pattern = DPTX_PHYIF_CTRL_TPS_3;
+			dp_pattern = DP_TRAINING_PATTERN_3;
+			break;
+		}
+		fallthrough;
+	case DPTX_PHYIF_CTRL_RATE_HBR:
+	case DPTX_PHYIF_CTRL_RATE_RBR:
+		if (rx_caps.tps4_supported) {
+			pattern = DPTX_PHYIF_CTRL_TPS_4;
+			dp_pattern = DP_TRAINING_PATTERN_4;
+		} else if (rx_caps.tps3_supported) {
+			pattern = DPTX_PHYIF_CTRL_TPS_3;
+			dp_pattern = DP_TRAINING_PATTERN_3;
+		} else {
+			pattern = DPTX_PHYIF_CTRL_TPS_2;
+			dp_pattern = DP_TRAINING_PATTERN_2;
+		}
+		break;
+	default:
+		dptx_warn(dptx, "Invalid rate %d\n", dptx->link.rate);
+		return -EINVAL;
+	}
+
+	mmi_dp_phy_set_pattern(dptx, pattern);
+
+	/* Set TRAINING_PATTERN_SET and TRAINING_LANEx_SET registers */
+	mmi_dp_set_training_set_regs(dptx, dp_pattern);
+
+	return 0;
+}
+
+static int mmi_dp_wait_aux_rd_interval(struct dptx *dptx)
+{
+	u32 reg;
+	u8 byte = 0;
+
+	mmi_dp_read_dpcd(dptx, DP_TRAINING_AUX_RD_INTERVAL, &byte);
+
+	/*
+	 * DP_TRAINING_AUX_RD_INTERVAL contains the timeout values which can be
+	 * 4ms, 8ms, 12ms, 16ms or 400 us as per DP 1.4 DPCD register
+	 * TRAINING_AUX_RD_INTERVAL spec.
+	 */
+	reg = min_t(u32, (byte & 0x7f), 4);
+	reg *= 4000;
+	if (!reg)
+		reg = 400;
+
+	usleep_range(reg, reg + 100);
+
+	return 0;
+}
+
+static bool mmi_dp_any_lane_cr_bit_done(struct dptx *dptx)
+{
+	u8 i;
+
+	for (i = 0; i < dptx->link.lanes; i++) {
+		if (mmi_dp_lane_cr_done(dptx, i))
+			return TRUE;
+	}
+
+	return FALSE;
+}
+
+static int mmi_dp_ch_eq_done_seq(struct dptx *dptx)
+{
+	bool settings_changed;
+	u8 main_ack_cnt = 0;
+	int ret = 1;
+
+	/* Transmit CH_EQ Pattern */
+	dptx_dbg(dptx, "Transmit CH_EQ Pattern");
+	mmi_dp_transmit_ch_eq_TPS(dptx);
+
+	do {
+		/* Wait specified Interval */
+		dptx_dbg(dptx, "Wait specified Interval");
+		mmi_dp_wait_aux_rd_interval(dptx);
+
+		/* Read CR_DONE, CH_EQ_DONE, SYMBOL_LOCKED and ADJ_REQ */
+		dptx_dbg(dptx, "Read CR_DONE, CH_EQ_DONE, SYMBOL_LOCKED and ADJ_REQ");
+		mmi_dp_read_bytes_from_dpcd(dptx, DP_LANE0_1_STATUS, dptx->link.status, 6);
+
+		/* Check CR Done remains */
+		dptx_dbg(dptx, "Check if Clock Recovery is OK");
+		if (!drm_dp_clock_recovery_ok(dptx->link.status, dptx->link.lanes)) {
+			ret = -CH_EQ_FAIL;
+			break;
+		}
+
+		dptx_dbg(dptx, "Check if Channel Equalization is OK");
+		if (drm_dp_channel_eq_ok(dptx->link.status, dptx->link.lanes)) {
+			ret = CH_EQ_DONE;
+			break;
+		}
+
+		dptx_err(dptx, "Channel EQ bits not OK");
+		main_ack_cnt++;
+		if (main_ack_cnt > 5) {
+			ret = -CH_EQ_FAIL;
+			break;
+		}
+
+		mmi_dp_adjust_drive_settings(dptx, &settings_changed);
+		mmi_dp_link_training_lanes_set(dptx);
+
+		dptx_dbg(dptx, "Driver settings adjusted");
+	} while (1);
+
+	return ret;
+}
+
+static int mmi_dp_check_allowed_link_configs(struct dptx *dptx)
+{
+	u8 sink_max_rate;
+
+	dptx->link.lanes = min_t(u8, dptx->link.lanes, dptx->rx_caps.max_lane_count);
+
+	sink_max_rate = mmi_dp_bw_to_phy_rate(dptx->rx_caps.max_link_rate);
+	dptx->link.rate = min_t(u8, dptx->link.rate, sink_max_rate);
+
+	return 0;
+}
+
+int mmi_dp_full_link_training(struct dptx *dptx)
+{
+	int ret, retval;
+
+	/* Guarantee lanes and rates are supported by Source and Sink */
+	mmi_dp_check_allowed_link_configs(dptx);
+
+	do {
+		/* DPCD Link Configuration */
+		mmi_dp_dpcd_link_configuration(dptx);
+
+		/* CR Done Sequence */
+		do {
+			/* Reset Vswing and Preemph to minimum value */
+			memset(dptx->link.preemp_level, 0, sizeof(u8) * 4);
+			memset(dptx->link.vswing_level, 0, sizeof(u8) * 4);
+			/* Clean Link Status Info */
+			memset(dptx->link.status, 0, DP_LINK_STATUS_SIZE);
+
+			mmi_dp_adjust_vswing_and_preemphasis(dptx);
+			ret = mmi_dp_cr_done_seq(dptx);
+			if (ret != CR_DONE) {
+				/* Reduce Link Rate */
+				ret = mmi_dp_reduce_link_rate(dptx);
+				/* Already RBR? Reduce Lanes */
+				if (ret == -ELOWESTRATE) {
+					ret = reduce_link_lanes(dptx);
+					dptx->link.rate = dptx->max_rate;
+					mmi_dp_check_allowed_link_configs(dptx);
+
+					/* Not achieved? Stop Link Training */
+					if (ret == -ELOWESTLANENR) {
+						ret = -LT_CR_FAIL;
+						break;
+					}
+				}
+				/* Force no Transmitted Pattern */
+				mmi_dp_phy_set_pattern(dptx, DPTX_PHYIF_CTRL_TPS_NONE);
+				mmi_dp_write_dpcd(dptx, DP_TRAINING_PATTERN_SET,
+						  DP_TRAINING_PATTERN_DISABLE);
+
+				/* DPCD Link Configuration - There is a lane/rate change */
+				mmi_dp_dpcd_link_configuration(dptx);
+			}
+		} while (ret != CR_DONE);
+
+		/* Clock Recovery Process Failed, stop LT */
+		if (ret == -LT_CR_FAIL)
+			break;
+
+		/* Channel EQ Done Sequence */
+		do {
+			ret = mmi_dp_ch_eq_done_seq(dptx);
+			if (ret != CH_EQ_DONE) {
+				if (mmi_dp_any_lane_cr_bit_done(dptx)) {
+					ret = reduce_link_lanes(dptx);
+					if (ret == 0) {
+						ret = LANE_REDUCTION;
+						break;
+					}
+				}
+
+				ret = mmi_dp_reduce_link_rate(dptx);
+				if (ret == -ELOWESTRATE) {
+					ret = -LT_CH_EQ_FAIL;
+					break;
+				}
+				dptx->link.lanes = dptx->max_lanes;
+				mmi_dp_check_allowed_link_configs(dptx);
+				break;
+			}
+		} while (ret != CH_EQ_DONE);
+
+		if (ret == -LT_CH_EQ_FAIL)
+			break;
+		else if (ret == CH_EQ_DONE)
+			ret = LT_DONE;
+
+	} while (ret != LT_DONE);
+
+	/* Clean Pattern and end LT */
+	mmi_dp_phy_set_pattern(dptx, DPTX_PHYIF_CTRL_TPS_NONE);
+	mmi_dp_write_dpcd(dptx, DP_TRAINING_PATTERN_SET, DP_TRAINING_PATTERN_DISABLE);
+
+	if (ret == LT_DONE) {
+		/* dptx_phy_enable_xmit(dptx, dptx->link.lanes, true); */
+		dptx->link.trained = true;
+		dptx_info(dptx, "Successful Link Training - Rate: %d Lanes: %d",
+			  dptx->link.rate, dptx->link.lanes);
+		dptx->multipixel = DPTX_MP_SINGLE_PIXEL;
+		dptx->max_rate = DPTX_MAX_LINK_RATE;
+		dptx->max_lanes = DPTX_MAX_LINK_LANES;
+		mmi_dp_set_video_dynamic_range(dptx, 1);
+		mmi_dp_set_video_colorimetry(dptx, 1); /* for 601 */
+
+		retval = mmi_dp_set_bpc(dptx, COLOR_DEPTH_8);
+		if (retval)
+			dptx_info(dptx, "mmi_dp_set_bpc failed");
+
+		retval = mmi_dp_set_video_format(dptx, VCEA);
+		if (retval)
+			dptx_info(dptx, "mmi_dp_set_video_format failed");
+
+		retval = mmi_dp_set_pixel_enc(dptx, RGB);
+		if (retval)
+			dptx_info(dptx, "mmi_dp_set_pixel_enc failed");
+
+		retval = mmi_dp_set_video_mode(dptx); /* 1920x1080@60 */
+		if (retval)
+			dptx_info(dptx, "mmi_dp_set_video_mode failed");
+	} else {
+		dptx_info(dptx, "Link Training Failed");
+	}
+
+	return ret;
+}
diff --git a/drivers/staging/xlnx-mmi-dptx/mmi_dp_reg.h b/drivers/staging/xlnx-mmi-dptx/mmi_dp_reg.h
new file mode 100644
index 000000000..ca03c2773
--- /dev/null
+++ b/drivers/staging/xlnx-mmi-dptx/mmi_dp_reg.h
@@ -0,0 +1,383 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Multimedia Integrated DisplayPort Tx driver
+ *
+ * Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
+ */
+
+#ifndef __MMI_DP_REG_H__
+#define __MMI_DP_REG_H__
+
+/* Constants */
+#define DPTX_MP_SINGLE_PIXEL		0
+#define DPTX_MP_DUAL_PIXEL		1
+#define DPTX_MP_QUAD_PIXEL		2
+
+#define DPTX_ID_DEVICE_ID		0x9001
+#define DPTX_ID_VENDOR_ID		0x16c3
+#define DPTX_VERSION			0x30333030
+
+/* MST */
+#define DPTX_MST_VCP_TABLE_REG_N(n)	(0x210 + (n) * 4)
+
+#define DPTX_HPDSTS	0xD08
+
+/* Video Registers. N=0-3 */
+#define DPTX_VSAMPLE_CTRL_N(n)		(0x300 + 0x10000 * (n))
+#define DPTX_VSAMPLE_STUFF_CTRL1_N(n)	(0x304 + 0x10000 * (n))
+#define DPTX_VSAMPLE_STUFF_CTRL2_N(n)	(0x308 + 0x10000 * (n))
+#define DPTX_VSAMPLE_POLARITY_CTRL_N(n)	(0x30C + 0x10000 * (n))
+#define DPTX_VIDEO_CONFIG1_N(n)		(0x310 + 0x10000 * (n))
+#define DPTX_VIDEO_CONFIG2_N(n)		(0x314 + 0x10000 * (n))
+#define DPTX_VIDEO_CONFIG3_N(n)		(0x318 + 0x10000 * (n))
+#define DPTX_VIDEO_CONFIG4_N(n)		(0x31c + 0x10000 * (n))
+#define DPTX_VIDEO_CONFIG5_N(n)		(0x320 + 0x10000 * (n))
+#define DPTX_VIDEO_MSA1_N(n)		(0x324 + 0x10000 * (n))
+#define DPTX_VIDEO_MSA2_N(n)		(0x328 + 0x10000 * (n))
+#define DPTX_VIDEO_MSA3_N(n)		(0x32C + 0x10000 * (n))
+#define DPTX_VIDEO_HBLANK_INTERVAL_N(n)	(0x330 + 0x10000 * (n))
+
+/* Video Control Register Field Masks */
+#define H_ACTIVE_MASK			GENMASK(31, 16)
+#define H_BLANK_MASK			GENMASK(15, 2)
+#define I_P_MASK			BIT(1)
+#define R_V_BLANK_IN_OSC_MASK		BIT(0)
+#define V_BLANK_MASK			GENMASK(31, 16)
+#define V_ACTIVE_MASK			GENMASK(15, 0)
+#define H_SYNC_WIDTH_MASK		GENMASK(30, 16)
+#define V_SYNC_WIDTH_MASK		GENMASK(30, 16)
+#define INIT_THRESHOLD_HI_MASK		GENMASK(22, 21)
+#define ENABLE_3D_FRAME_FIELD_SEQ_MASK	BIT(20)
+#define AVERAGE_BYTES_PER_TU_FRAC_MASK	GENMASK(19, 14)
+#define INIT_THRESHOLD_MASK		GENMASK(13, 7)
+#define AVERAGE_BYTES_PER_TU_MASK	GENMASK(6, 0)
+#define MSA_V_START_MASK		GENMASK(31, 16)
+#define MSA_H_START_MASK		GENMASK(15, 0)
+#define MSA_MISC0_MASK			GENMASK(31, 24)
+#define MSA_MISC0_SYNC_MODE_MASK	BIT(24)
+#define MSA_MISC0_COLOR_MAP_MASK	GENMASK(28, 25)
+#define MSA_MISC0_BPC_MAP_MASK		GENMASK(31, 29)
+#define MSA_MVID_MASK			GENMASK(23, 0)
+#define MSA_MISC1_MASK			GENMASK(31, 24)
+#define MSA_MISC1_PIX_ENC_MASK		GENMASK(31, 30)
+#define MSA_NVID_MASK			GENMASK(23, 0)
+#define H_BLANK_INTERVAL_MASK		GENMASK(15, 0)
+#define DE_IN_POLARITY_MASK		BIT(2)
+#define H_SYNC_IN_POLARITY_MASK		BIT(1)
+#define V_SYNC_IN_POLARITY_MASK		BIT(0)
+#define VIDEO_STREAM_ENABLE_MASK	BIT(5)
+#define VIDEO_MAPPING_IPI_EN_MASK	BIT(15)
+#define VIDEO_MAPPING_MASK		GENMASK(20, 16)
+#define PIXEL_MODE_SELECT_MASK		GENMASK(22, 21)
+
+/* Register Bitfields */
+#define DPTX_ID_DEVICE_ID_SHIFT		16
+#define DPTX_ID_DEVICE_ID_MASK		GENMASK(31, 16)
+#define DPTX_ID_VENDOR_ID_SHIFT		0
+#define DPTX_ID_VENDOR_ID_MASK		GENMASK(15, 0)
+
+#define DPTX_CONFIG1_MP_MODE_SINGLE	1
+#define DPTX_CONFIG1_MP_MODE_DUAL	2
+#define DPTX_CONFIG1_MP_MODE_QUAD	4
+
+#define DPTX_CCTL_ENH_FRAME_EN		BIT(1)
+#define DPTX_CCTL_ENABLE_MST_MODE	BIT(25)
+#define DPTX_CCTL_INITIATE_MST_ACT_SEQ	BIT(28)
+
+#define DPTX_SRST_CTRL_CONTROLLER	BIT(0)
+#define DPTX_SRST_CTRL_PHY		BIT(1)
+#define DPTX_SRST_CTRL_HDCP		BIT(2)
+#define DPTX_SRST_CTRL_AUDIO_SAMPLER	BIT(3)
+#define DPTX_SRST_CTRL_AUX		BIT(4)
+#define DPTX_SRST_VIDEO_RESET_N(n)	BIT(5 + (n))
+#define DPTX_SRST_VIDEO_RESET_ALL	GENMASK(8, 5)
+#define DPTX_SRST_CTRL_ALL		(DPTX_SRST_CTRL_CONTROLLER |	\
+					 DPTX_SRST_CTRL_HDCP |		\
+					 DPTX_SRST_CTRL_AUDIO_SAMPLER |	\
+					 DPTX_SRST_CTRL_AUX)
+
+#define DPTX_PHYIF_CTRL_TPS_NONE		0
+#define DPTX_PHYIF_CTRL_TPS_1			1
+#define DPTX_PHYIF_CTRL_TPS_2			2
+#define DPTX_PHYIF_CTRL_TPS_3			3
+#define DPTX_PHYIF_CTRL_TPS_4			4
+#define DPTX_PHYIF_CTRL_TPS_SYM_ERM		5
+#define DPTX_PHYIF_CTRL_TPS_PRBS7		6
+#define DPTX_PHYIF_CTRL_TPS_CUSTOM80		7
+#define DPTX_PHYIF_CTRL_TPS_CP2520_1		8
+#define DPTX_PHYIF_CTRL_TPS_CP2520_2		9
+#define DPTX_PHYIF_CTRL_RATE_RBR		0x0
+#define DPTX_PHYIF_CTRL_RATE_HBR		0x1
+#define DPTX_PHYIF_CTRL_RATE_HBR2		0x2
+#define DPTX_PHYIF_CTRL_RATE_HBR3		0x3
+#define DPTX_PHYIF_CTRL_XMIT_EN(lane)		BIT(8 + (lane))
+#define DPTX_PHYIF_CTRL_XMIT_EN_ALL		GENMASK(11, 8)
+#define DPTX_PHYIF_CTRL_BUSY(lane)		BIT(12 + (lane))
+#define DPTX_POWER_DOWN_CTRL_LANE_MASK(lane)	(0x0000000F << (4 * (lane)))
+#define DPTX_PHY_POWER_ON			0x0
+#define DPTX_PHY_INTER_P2_POWER			0x2
+#define DPTX_PHY_POWER_DOWN			0x3
+#define DPTX_PHY_P4_POWER_STATE			0xc
+
+#define DPTX_PHY_TX_EQ_PREEMP_SHIFT(lane)	(6 * (lane))
+#define DPTX_PHY_TX_EQ_PREEMP_MASK(lane)	GENMASK(6 * (lane) + 1, 6 * (lane))
+#define DPTX_PHY_TX_EQ_VSWING_SHIFT(lane)	(6 * (lane) + 2)
+#define DPTX_PHY_TX_EQ_VSWING_MASK(lane)	GENMASK(6 * (lane) + 3, 6 * (lane) + 2)
+#define DPTX_PHY_TX_EQ_VSWING_LVL_3		3
+
+#define DPTX_AUX_CMD_REQ_LEN_SHIFT	0
+#define DPTX_AUX_CMD_REQ_LEN_MASK	GENMASK(3, 0)
+#define DPTX_AUX_CMD_I2C_ADDR_ONLY	BIT(4)
+#define DPTX_AUX_CMD_ADDR_SHIFT		8
+#define DPTX_AUX_CMD_ADDR_MASK		GENMASK(27, 8)
+#define DPTX_AUX_CMD_TYPE_SHIFT		28
+#define DPTX_AUX_CMD_TYPE_MASK		GENMASK(31, 28)
+#define DPTX_AUX_CMD_TYPE_WRITE		0x0
+#define DPTX_AUX_CMD_TYPE_READ		0x1
+#define DPTX_AUX_CMD_TYPE_WSU		0x2
+#define DPTX_AUX_CMD_TYPE_MOT		0x4
+#define DPTX_AUX_CMD_TYPE_NATIVE	0x8
+
+#define DPTX_AUX_STS_STATUS_SHIFT	4
+#define DPTX_AUX_STS_STATUS_ACK		0x0
+#define DPTX_AUX_STS_STATUS_NACK	0x1
+#define DPTX_AUX_STS_STATUS_DEFER	0x2
+#define DPTX_AUX_STS_STATUS_I2C_NACK	0x4
+#define DPTX_AUX_STS_STATUS_I2C_DEFER	0x8
+
+#define DPTX_ISTS_HPD			BIT(0)
+#define DPTX_ISTS_AUX_REPLY		BIT(1)
+#define DPTX_ISTS_HDCP			BIT(2)
+#define DPTX_ISTS_AUX_CMD_INVALID	BIT(3)
+#define DPTX_ISTS_SDP			BIT(4)
+#define DPTX_ISTS_AUDIO_FIFO_OVERFLOW	BIT(5)
+#define DPTX_ISTS_VIDEO_FIFO_OVERFLOW	BIT(6)
+#define DPTX_ISTS_VIDEO_FIFO_UNDERFLOW	BIT(8)
+#define DPTX_ISTS_ALL_INTR	(DPTX_ISTS_HPD |			\
+				 DPTX_ISTS_AUX_REPLY |			\
+				 DPTX_ISTS_HDCP |			\
+				 DPTX_ISTS_AUX_CMD_INVALID |		\
+				 DPTX_ISTS_SDP |			\
+				 DPTX_ISTS_VIDEO_FIFO_UNDERFLOW |	\
+				 DPTX_ISTS_VIDEO_FIFO_OVERFLOW)
+#define DPTX_IEN_HPD			BIT(0)
+#define DPTX_IEN_AUX_REPLY		BIT(1)
+#define DPTX_IEN_HDCP			BIT(2)
+#define DPTX_IEN_AUX_CMD_INVALID	BIT(3)
+#define DPTX_IEN_SDP			BIT(4)
+#define DPTX_IEN_AUDIO_FIFO_OVERFLOW	BIT(5)
+#define DPTX_IEN_VIDEO_FIFO_OVERFLOW	BIT(6)
+#define DPTX_IEN_VIDEO_FIFO_UNDERFLOW	BIT(8)
+#define DPTX_IEN_ALL_INTR	(DPTX_IEN_HPD |			\
+				 DPTX_IEN_AUX_REPLY |		\
+				 DPTX_IEN_AUX_CMD_INVALID)
+
+#define DPTX_HPDSTS_IRQ			BIT(0)
+#define DPTX_HPDSTS_HOT_PLUG		BIT(1)
+#define DPTX_HPDSTS_HOT_UNPLUG		BIT(2)
+
+#define DPTX_HPD_IEN_IRQ_EN		DPTX_HPDSTS_IRQ
+#define DPTX_HPD_IEN_HOT_PLUG_EN	DPTX_HPDSTS_HOT_PLUG
+#define DPTX_HPD_IEN_HOT_UNPLUG_EN	DPTX_HPDSTS_HOT_UNPLUG
+
+#define DPTX_VSAMPLE_CTRL_STREAM_EN	BIT(5)
+
+#define DPTX_AUD_CONFIG1_DATA_EN_IN_SHIFT		1
+#define DPTX_AUD_CONFIG1_DATA_EN_IN_MASK		GENMASK(4, 1)
+
+#define DPTX_VSAMPLE_CTRL_VMAP_BPC_SHIFT		16
+#define DPTX_VSAMPLE_CTRL_VMAP_BPC_MASK			GENMASK(20, 16)
+#define DPTX_VSAMPLE_CTRL_MULTI_PIXEL_SHIFT		21
+#define DPTX_VSAMPLE_CTRL_MULTI_PIXEL_MASK		GENMASK(22, 21)
+#define DPTX_VIDEO_VMSA2_BPC_SHIFT			29
+#define DPTX_VIDEO_VMSA2_BPC_MASK			GENMASK(31, 29)
+#define DPTX_VIDEO_VMSA2_COL_SHIFT			25
+#define DPTX_VIDEO_VMSA2_COL_MASK			GENMASK(28, 25)
+#define DPTX_VIDEO_VMSA3_PIX_ENC			BIT(31)
+#define DPTX_VIDEO_VMSA3_PIX_ENC_YCBCR420		BIT(30) /* ignore MSA */
+#define DPTX_VIDEO_VMSA3_PIX_ENC_MASK			GENMASK(31, 30)
+#define DPTX_POL_CTRL_V_SYNC_POL_EN			BIT(0)
+#define DPTX_POL_CTRL_H_SYNC_POL_EN			BIT(1)
+#define DPTX_VIDEO_CONFIG1_IN_OSC_EN			BIT(0)
+#define DPTX_VIDEO_CONFIG1_O_IP_EN			BIT(1)
+#define DPTX_VIDEO_H_BLANK_SHIFT			2
+#define DPTX_VIDEO_H_ACTIVE_SHIFT			16
+#define DPTX_VIDEO_V_BLANK_SHIFT			16
+#define DPTX_VIDEO_V_ACTIVE_SHIFT			0
+#define DPTX_VIDEO_H_FRONT_PORCH			0
+#define DPTX_VIDEO_H_SYNC_WIDTH				16
+#define DPTX_VIDEO_V_FRONT_PORCH			0
+#define DPTX_VIDEO_V_SYNC_WIDTH				16
+#define DPTX_VIDEO_MSA1_H_START_SHIFT			0
+#define DPTX_VIDEO_MSA1_V_START_SHIFT			16
+#define DPTX_VIDEO_CONFIG5_TU_SHIFT			0
+#define DPTX_VIDEO_CONFIG5_TU_MASK			GENMASK(6, 0)
+#define DPTX_VIDEO_CONFIG5_TU_FRAC_SHIFT_MST		14
+#define DPTX_VIDEO_CONFIG5_TU_FRAC_MASK_MST		GENMASK(19, 14)
+#define DPTX_VIDEO_CONFIG5_TU_FRAC_SHIFT_SST		16
+#define DPTX_VIDEO_CONFIG5_TU_FRAC_MASK_SST		GENMASK(19, 16)
+#define DPTX_VIDEO_CONFIG5_INIT_THRESHOLD_SHIFT		7
+#define DPTX_VIDEO_CONFIG5_INIT_THRESHOLD_MASK		GENMASK(13, 7)
+
+#define DPTX_EN_AUDIO_CH_1				1
+#define DPTX_EN_AUDIO_CH_2				1
+#define DPTX_EN_AUDIO_CH_3				3
+#define DPTX_EN_AUDIO_CH_4				9
+#define DPTX_EN_AUDIO_CH_5				7
+#define DPTX_EN_AUDIO_CH_6				7
+#define DPTX_EN_AUDIO_CH_7				0xF
+#define DPTX_EN_AUDIO_CH_8				0xF
+
+#define DPTX_HDCP22GPIOSTS				0x3628
+#define DPTX_HDCP22GPIOOUTCHNGSTS			0x362c
+
+/* REGMAPS ARCHITECTURE */
+
+#define DPTX_VERSION_NUMBER			0x0000
+#define DPTX_VERSION_TYPE			0x0004
+#define DPTX_ID					0x0008
+#define DPTX_CONFIG_REG1			0x0100
+#define DPTX_HDCP_SELECT_MASK			BIT(0)
+#define DPTX_AUDIO_SELECT_MASK			GENMASK(2, 1)
+#define DPTX_SDP_REG_BANK_SZ_MASK		GENMASK(8, 4)
+#define DPTX_FPGA_EN_MASK			BIT(9)
+#define DPTX_SYNC_DEPTH_MASK			GENMASK(13, 12)
+#define DPTX_NUM_STREAMS_MASK			GENMASK(18, 16)
+#define DPTX_MP_MODE_MASK			GENMASK(21, 19)
+#define DPTX_DSC_EN_MASK			BIT(22)
+#define DPTX_EDP_EN_MASK			BIT(23)
+#define DPTX_FEC_EN_MASK			BIT(24)
+#define DPTX_GEN2_PHY_MASK			BIT(29)
+#define DPTX_CONFIG_REG3			0x0108
+#define DPTX_PHY_TYPE_MASK			GENMASK(8, 7)
+#define DPTX_ADSYNC_EN_MASK			BIT(16)
+#define DPTX_PSR_VER_MASK			GENMASK(21, 20)
+#define CCTL					0x0200
+#define CCTL_ENHANCE_FRAMING_EN			BIT(1)
+#define CCTL_DEFAULT_FAST_LINK_TRAIN_EN		BIT(2)
+#define CCTL_ENABLE_MST_MODE			BIT(25)
+#define CCTL_ENHANCE_FRAMING_WITH_FEC_EN	BIT(29)
+#define SOFT_RESET_CTRL				0x0204
+#define HDCP_MODULE_RESET			BIT(2)
+#define VSAMPLE_CTRL				0x0300
+#define VSAMPLE_STUFF_CTRL1			0x0304
+#define VSAMPLE_STUFF_CTRL2			0x0308
+#define VINPUT_POLARITY_CTRL			0x030c
+#define VIDEO_CONFIG1				0x0310
+#define VIDEO_CONFIG2				0x0314
+#define VIDEO_CONFIG3				0x0318
+#define VIDEO_CONFIG4				0x031c
+#define VIDEO_CONFIG5				0x0320
+#define VIDEO_MSA1				0x0324
+#define VIDEO_MSA2				0x0328
+#define VIDEO_MSA3				0x032c
+#define VIDEO_HBLANK_INTERVAL			0x0330
+#define MVID_CONFIG1				0x0338
+#define MVID_CONFIG2				0x033c
+#define PM_CONFIG1				0x0350
+#define PM_CONFIG2				0x0354
+#define PM_CTRL1				0x0360
+#define PM_STS1					0x0370
+#define AUD_CONFIG1				0x0400
+#define SDP_VERTICAL_CTRL			0x0500
+#define SDP_HORIZONTAL_CTRL			0x0504
+#define SDP_STATUS_REGISTER			0x0508
+#define SDP_MANUAL_CTRL				0x050c
+#define SDP_STATUS_EN				0x0510
+#define SDP_CONFIG1				0x0520
+#define SDP_CONFIG2				0x0524
+#define SDP_CONFIG3				0x0528
+#define SDP_REGISTER_BANK_0			0x0600
+#define SDP_REGISTER_BANK_1			0x0604
+#define SDP_REGISTER_BANK_2			0x0608
+#define SDP_REGISTER_BANK_3			0x060c
+#define PHYIF_CTRL				0x0a00
+#define PHYIF_TPS_SEL				GENMASK(3, 0)
+#define PHYIF_PHY_RATE				GENMASK(5, 4)
+#define PHYIF_PHY_LANES				GENMASK(7, 6)
+#define PHYIF_PHY_BUSY				GENMASK(15, 12)
+#define PHYIF_PHY_SSC_DIS			BIT(16)
+#define PHYIF_PHY_POWER_DOWN			GENMASK(20, 17)
+#define PHYIF_PHY_WIDTH				BIT(25)
+#define PHY_TX_EQ				0x0a04
+#define CUSTOMPAT0				0x0a08
+#define CUSTOMPAT1				0x0a0c
+#define CUSTOMPAT2				0x0a10
+#define HBR2_COMPLIANCE_SCRAMBLER_RESET		0x0a14
+#define PHYIF_PWRDOWN_CTRL			0x0a18
+#define AUX_CMD					0x0b00
+#define AUX_STATUS				0x0b04
+#define AUX_STATUS_MASK				GENMASK(7, 0)
+#define AUX_M_MASK				GENMASK(15, 8)
+#define AUX_REPLY_MASK				BIT(16)
+#define AUX_TIMEOUT_MASK			BIT(17)
+#define AUX_BYTES_READ				GENMASK(23, 19)
+#define AUX_DATA0				0x0b08
+#define AUX_DATA1				0x0b0c
+#define AUX_DATA2				0x0b10
+#define AUX_DATA3				0x0b14
+#define AUX_250US_CNT_LIMIT			0x0b40
+#define AUX_2000US_CNT_LIMIT			0x0b44
+#define AUX_100000US_CNT_LIMIT			0x0b48
+#define COMBO_PHY_CTRL1				0x0c0c
+#define COMBO_PHY_STATUS1			0x0c10
+#define COMBO_PHY_OVR				0x0c14
+#define COMBO_PHY_OVR_MPLL_CTRL0		0x0c18
+#define COMBO_PHY_OVR_MPLL_CTRL1		0x0c1c
+#define COMBO_PHY_OVR_MPLL_CTRL2		0x0c20
+#define COMBO_PHY_GEN2_OVR_MPLL_CTRL0		0x0c24
+#define COMBO_PHY_GEN2_OVR_MPLL_CTRL1		0x0c28
+#define COMBO_PHY_GEN2_OVR_MPLL_CTRL2		0x0c2c
+#define COMBO_PHY_GEN2_OVR_MPLL_CTRL3		0x0c30
+#define COMBO_PHY_GEN2_OVR_MPLL_CTRL4		0x0c34
+#define COMBO_PHY_GEN2_OVR_MPLL_CTRL5		0x0c38
+#define COMBO_PHY_OVR_TERM_CTRL			0x0c44
+#define COMBO_PHY_OVR_TX_EQ_G1_CTRL		0x0c48
+#define COMBO_PHY_OVR_TX_EQ_G2_CTRL		0x0c4c
+#define COMBO_PHY_OVR_TX_EQ_G3_CTRL		0x0c50
+#define COMBO_PHY_OVR_TX_EQ_G4_CTRL		0x0c54
+#define COMBO_PHY_OVR_TX_EQ_G5_CTRL		0x0c58
+#define COMBO_PHY_OVR_TX_EQ_G6_CTRL		0x0c5c
+#define COMBO_PHY_OVR_TX_EQ_G7_CTRL		0x0c60
+#define COMBO_PHY_OVR_TX_EQ_G8_CTRL		0x0c64
+#define COMBO_PHY_OVR_TX_LANE0_CTRL		0x0c68
+#define COMBO_PHY_OVR_TX_LANE1_CTRL		0x0c6c
+#define COMBO_PHY_OVR_TX_LANE2_CTRL		0x0c70
+#define COMBO_PHY_OVR_TX_LANE3_CTRL		0x0c74
+#define GENERAL_INTERRUPT			0x0d00
+#define GEN_INTR_HPD_EVENT			BIT(0)
+#define GEN_INTR_AUX_REPLY_EVENT		BIT(1)
+#define GEN_INTR_HDCP_EVENT			BIT(2)
+#define GEN_INTR_SDP_EVENT_STREAM0		BIT(4)
+#define GEN_INTR_AUDIO_FIFO_OVERFLOW_STREAM0	BIT(5)
+#define GEN_INTR_VIDEO_FIFO_OVERFLOW_STREAM0	BIT(6)
+#define GEN_INTR_VIDEO_FIFO_UNDERFLOW_STREAM0	BIT(8)
+#define GENERAL_INTERRUPT_ENABLE		0x0d04
+#define HPD_STATUS				0x0d08
+#define HPD_IRQ					BIT(0)
+#define HPD_HOT_PLUG				BIT(1)
+#define HPD_HOT_UNPLUG				BIT(2)
+#define HPD_STATUS_MASK				BIT(8)
+#define HPD_INTERRUPT_ENABLE			0x0d0c
+#define HDCPCFG					0x0e00
+#define HDCPCFG_CP_IRQ				BIT(6)
+#define HDCPOBS					0x0e04
+#define HDCPAPIINTCLR				0x0e08
+#define HDCPAPIINTSTAT				0x0e0c
+#define HDCPAPIINTMSK				0x0e10
+#define HDCPKSVMEMCTRL				0x0e18
+#define HDCPREG_BKSV0				0x3600
+#define HDCPREG_BKSV1				0x3604
+#define HDCPREG_ANCONF				0x3608
+#define HDCPREG_AN0				0x360c
+#define HDCPREG_AN1				0x3610
+#define HDCPREG_RMLCTL				0x3614
+#define HDCPREG_RMLSTS				0x3618
+#define HDCPREG_SEED				0x361c
+#define HDCPREG_DPK0				0x3620
+#define HDCPREG_DPK1				0x3624
+#define HDCP2GPIOSTS				0x3628
+#define HDCP2GPIOCHNGSTS			0x362c
+#define HDCPREG_DPK_CRC				0x3630
+
+#endif
diff --git a/drivers/staging/xlnx_hdcp1x/Kconfig b/drivers/staging/xlnx_hdcp1x/Kconfig
new file mode 100644
index 000000000..f1da9a7c2
--- /dev/null
+++ b/drivers/staging/xlnx_hdcp1x/Kconfig
@@ -0,0 +1,8 @@
+config XLNX_HDCP1X_CIPHER
+	bool
+	help
+	  This code is developed for hdcp1x cipher functionality of
+	  xilinx hdcp1x IP.
+
+	  This code will be compiled by selection from the interface
+	  drivers (DP/HDMI) of xilinx.
diff --git a/drivers/staging/xlnx_hdcp1x/MAINTAINERS b/drivers/staging/xlnx_hdcp1x/MAINTAINERS
new file mode 100644
index 000000000..e2ba0c4f9
--- /dev/null
+++ b/drivers/staging/xlnx_hdcp1x/MAINTAINERS
@@ -0,0 +1,4 @@
+XILINX HDCP1X CIPHER DRIVER
+M:	Jagadeesh Banisetti <jagadeesh.banisetti@amd.com>
+S:	Maintained
+F:	drivers/staging/xlnx_hdcp1x
diff --git a/drivers/staging/xlnx_hdcp1x/Makefile b/drivers/staging/xlnx_hdcp1x/Makefile
new file mode 100644
index 000000000..b500fb1a1
--- /dev/null
+++ b/drivers/staging/xlnx_hdcp1x/Makefile
@@ -0,0 +1 @@
+obj-y += xilinx-hdcp1x-cipher.o
diff --git a/drivers/staging/xlnx_hdcp1x/xilinx-hdcp1x-cipher.c b/drivers/staging/xlnx_hdcp1x/xilinx-hdcp1x-cipher.c
new file mode 100644
index 000000000..ee3a4fc93
--- /dev/null
+++ b/drivers/staging/xlnx_hdcp1x/xilinx-hdcp1x-cipher.c
@@ -0,0 +1,989 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx HDCP1X Cipher driver
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ *
+ * Author: Jagadeesh Banisetti <jagadeesh.banisetti@xilinx.com>
+ */
+
+#include <linux/bitfield.h>
+#include <linux/xlnx/xilinx-hdcp1x-cipher.h>
+
+/********************** Static function definations ***************************/
+static void xhdcp1x_cipher_write(struct xhdcp1x_cipher *cipher,
+				 int offset, u32 val)
+{
+	writel(val, cipher->interface_base + offset);
+}
+
+static u32 xhdcp1x_cipher_read(struct xhdcp1x_cipher *cipher, int offset)
+{
+	return readl(cipher->interface_base + offset);
+}
+
+static void xhdcp1x_cipher_set_mask(struct xhdcp1x_cipher *cipher, int offset,
+				    u32 set_mask)
+{
+	u32 value;
+
+	value = xhdcp1x_cipher_read(cipher, offset);
+	value |= set_mask;
+	xhdcp1x_cipher_write(cipher, offset, value);
+}
+
+static void xhdcp1x_cipher_clr_mask(struct xhdcp1x_cipher *cipher, int offset,
+				    u32 clr_mask)
+{
+	u32 value;
+
+	value = xhdcp1x_cipher_read(cipher, offset);
+	value &= ~clr_mask;
+	xhdcp1x_cipher_write(cipher, offset, value);
+}
+
+static int xhdcp1x_cipher_is_enabled(struct xhdcp1x_cipher *cipher)
+{
+	return (xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL) &
+			XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE);
+}
+
+static u8 xhdcp1x_cipher_is_localksv_ready(struct xhdcp1x_cipher *cipher)
+{
+	return (xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_STATUS) &
+				    XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KSV_READY);
+}
+
+static u8 xhdcp1x_cipher_is_km_ready(struct xhdcp1x_cipher *cipher)
+{
+	return (xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_STATUS) &
+				    XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KM_READY);
+}
+
+u64 xhdcp1x_cipher_get_localksv(struct xhdcp1x_cipher *cipher)
+{
+	u64 ksv;
+	u32 val;
+	u32 guard = XHDCP1X_CIPHER_KSV_RETRIES;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return 0;
+
+	/* Check if the local ksv is not available */
+	val = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_STATUS);
+	val &= XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KSV_READY;
+
+	if (val)
+		return 0;
+
+	/* Abort any running KM calculation just in case */
+	xhdcp1x_cipher_set_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_ABORT_KM);
+	xhdcp1x_cipher_clr_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_ABORT_KM);
+
+	/* Load the local ksv */
+	xhdcp1x_cipher_set_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_LOCAL_KSV);
+	xhdcp1x_cipher_clr_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_LOCAL_KSV);
+
+	while ((!xhdcp1x_cipher_is_localksv_ready(cipher)) && (--guard > 0))
+		;
+
+	if (!guard)
+		return 0;
+
+	ksv = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KSV_LOCAL_H);
+	ksv = (ksv & 0xFF) << 32;
+	ksv |= xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KSV_LOCAL_L);
+
+	return ksv;
+}
+
+static void xhdcp1x_cipher_config_lanes(struct xhdcp1x_cipher *cipher)
+{
+	u32 value;
+
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_CONTROL_NUM_LANES;
+	value |= FIELD_PREP(XHDCP1X_CIPHER_BITMASK_CONTROL_NUM_LANES,
+			   cipher->num_lanes);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+}
+
+int xhdcp1x_cipher_do_request(void *ref,
+			      enum xhdcp1x_cipher_request_type request)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (request < XHDCP1X_CIPHER_REQUEST_BLOCK ||
+	    request >= XHDCP1X_CIPHER_REQUEST_MAX)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	/* Determine if there is a request in progress */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_STATUS);
+	value &= XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_REQUEST_IN_PROG;
+	if (value)
+		return -EBUSY;
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	/* Set the appropriate request bit and ensure that KM is always used */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_REQUEST;
+	value |= (XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_BLOCK << request);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL, value);
+
+	/* Ensure that the request bit(s) get cleared for next time */
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_REQUEST);
+
+	return 0;
+}
+
+/********************** Public function definitions ***************************/
+/**
+ * xhdcp1x_cipher_init - Create and initialize the cipher driver instance
+ * @dev: Pointer to platform structure
+ * @hdcp1x_base: Pointer to interface iomem base
+ * This function instantiate the cipher driver and initializes it.
+ *
+ * Return: void reference to cipher driver instance on success, error otherwise
+ */
+void *xhdcp1x_cipher_init(struct device *dev, void __iomem *hdcp1x_base)
+{
+	struct xhdcp1x_cipher *cipher;
+	u32 reg;
+
+	if (!dev || !hdcp1x_base)
+		return ERR_PTR(-EINVAL);
+
+	cipher = devm_kzalloc(dev, sizeof(*cipher), GFP_KERNEL);
+	if (!cipher)
+		return ERR_PTR(-ENOMEM);
+
+	cipher->dev = dev;
+	cipher->interface_base = hdcp1x_base;
+	cipher->num_lanes = XHDCP1X_CIPHER_MAX_LANES;
+
+	reg = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_TYPE);
+	cipher->is_tx = reg & XHDCP1X_CIPHER_BITMASK_TYPE_DIRECTION;
+	cipher->is_hdmi = (reg & XHDCP1X_CIPHER_BITMASK_TYPE_PROTOCOL) &
+			   XHDCP1X_CIPHER_VALUE_TYPE_PROTOCOL_HDMI;
+
+	xhdcp1x_cipher_reset(cipher);
+
+	return (void *)cipher;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_init);
+
+/**
+ * xhdcp1x_cipher_reset - Reset cipher
+ * @ref: reference to cipher instance
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_reset(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_RESET);
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_RESET);
+
+	/* Ensure all interrupts are disabled and cleared */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+			     XHDCP1X_CIPHER_INTR_ALL);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+			     XHDCP1X_CIPHER_INTR_ALL);
+
+	if (!cipher->is_hdmi)
+		xhdcp1x_cipher_config_lanes(cipher);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_reset);
+
+/**
+ * xhdcp1x_cipher_enable - Enable cipher
+ * @ref: reference to cipher instance
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_enable(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (xhdcp1x_cipher_is_enabled(cipher))
+		return -EBUSY;
+
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	/* Ensure that all encryption is disabled for now */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H, 0);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L, 0);
+
+	/* Ensure that XOR is disabled on tx and enabled for rx to start */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL);
+	if (cipher->is_tx)
+		value &= ~XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE;
+	else
+		value |= XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL, value);
+
+	/* Enable it */
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_enable);
+
+/**
+ * xhdcp1x_cipher_disable - Disable cipher
+ * @ref: reference to cipher instance
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_disable(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	/* Ensure all interrupts are disabled */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+			     XHDCP1X_CIPHER_INTR_ALL);
+
+	/* Enable bypass operation */
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE);
+
+	/* Ensure that all encryption is disabled for now */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H, 0x00);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L, 0x00);
+
+	/* Ensure that XOR is disabled */
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_disable);
+
+/**
+ * xhdcp1x_cipher_set_num_lanes - Set number of active lanes in cipher
+ * @ref: reference to cipher instance
+ * @num_lanes: Number of active lanes
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_num_lanes(void *ref, u8 num_lanes)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (num_lanes != XHDCP1X_CIPHER_NUM_LANES_1 &&
+	    num_lanes != XHDCP1X_CIPHER_NUM_LANES_2 &&
+	    num_lanes != XHDCP1X_CIPHER_NUM_LANES_4)
+		return -EINVAL;
+
+	cipher->num_lanes = num_lanes;
+
+	xhdcp1x_cipher_config_lanes(cipher);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_num_lanes);
+
+/**
+ * xhdcp1x_cipher_set_keyselect - Selects the key vector to read from keymgmt block
+ * @ref: reference to cipher instance
+ * @keyselect: key vector number
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_keyselect(void *ref, u8 keyselect)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (keyselect > XHDCP1X_CIPHER_KEYSELECT_MAX_VALUE)
+		return -EINVAL;
+
+	/* Update the device */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_SET_SELECT;
+	value |= (keyselect << XHDCP1X_CIPHER_SHIFT_KEYMGMT_CONTROL_SET_SELECT);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL, value);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_keyselect);
+
+/**
+ * xhdcp1x_cipher_load_bksv - load local ksv from cipher to buf
+ * @ref: reference to cipher instance
+ * @buf: 5 byte buffer to store the local KSV
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_load_bksv(void *ref, u8 *buf)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u64 my_ksv;
+	u32 is_enabled;
+
+	if (!cipher || !buf)
+		return -EINVAL;
+
+	is_enabled = xhdcp1x_cipher_is_enabled(cipher);
+
+	xhdcp1x_cipher_enable(cipher);
+	my_ksv = xhdcp1x_cipher_get_localksv(cipher);
+	if (!is_enabled)
+		xhdcp1x_cipher_disable(cipher);
+	if (!my_ksv)
+		return -EAGAIN;
+	memcpy(buf, &my_ksv, XHDCP1X_CIPHER_SIZE_LOCAL_KSV);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_load_bksv);
+
+/**
+ * xhdcp1x_cipher_set_remoteksv - set remote device KSV into cipher
+ * @ref: reference to cipher instance
+ * @ksv: remote device KSV
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_remoteksv(void *ref, u64 ksv)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 guard = XHDCP1X_CIPHER_KSV_RETRIES;
+	u32 value;
+
+	if (!cipher || !ksv)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	/* Read local ksv to put things into a known state */
+	xhdcp1x_cipher_get_localksv(cipher);
+
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	value = (u32)ksv;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_KSV_REMOTE_L, value);
+	value = (ksv >> 32) & 0xFF;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_KSV_REMOTE_H, value);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	/* Trigger the calculation of theKM */
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_BEGIN_KM);
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_BEGIN_KM);
+
+	/* Wait until KM is available */
+	while ((!xhdcp1x_cipher_is_km_ready(cipher)) && (--guard > 0))
+		;
+
+	if (!guard)
+		return -EAGAIN;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_remoteksv);
+
+/**
+ * xhdcp1x_cipher_get_ro - Read Ro from cipher
+ * @ref: reference to cipher instance
+ * @ro: reference to ro data
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_get_ro(void *ref, u16 *ro)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher || !ro)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	*ro = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_RO);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_get_ro);
+
+/**
+ * xhdcp1x_cipher_set_b - Set B value into cipher
+ * @ref: reference to cipher instance
+ * @an: An value
+ * @is_repeater: repeater flag
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_b(void *ref, u64 an, bool is_repeater)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value, x, y, z;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	x = (u32)(an & XHDCP1X_CIPHER_BITMASK_CIPHER_BX);
+	an >>= XHDCP1X_CIPHER_SHIFT_CIPHER_B;
+	y = (u32)(an & XHDCP1X_CIPHER_BITMASK_CIPHER_BY);
+	an >>= XHDCP1X_CIPHER_SHIFT_CIPHER_B;
+	z = (u32)an;
+	if (is_repeater)
+		z |= XHDCP1X_CIPHER_BITMASK_CIPHER_BZ_REPEATER;
+	z &= XHDCP1X_CIPHER_BITMASK_CIPHER_BZ;
+
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	value = x & XHDCP1X_CIPHER_BITMASK_CIPHER_BX;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BX, value);
+	value = y & XHDCP1X_CIPHER_BITMASK_CIPHER_BY;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BY, value);
+	value = z & XHDCP1X_CIPHER_BITMASK_CIPHER_BZ;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BZ, value);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	xhdcp1x_cipher_do_request(cipher, XHDCP1X_CIPHER_REQUEST_BLOCK);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_b);
+
+/**
+ * xhdcp1x_cipher_is_request_complete - check requested operation is completed
+ * @ref: reference to cipher instance
+ *
+ * Return: 1 on request completion, 0 otherwise
+ */
+int xhdcp1x_cipher_is_request_complete(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	return !(xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_STATUS) &
+			XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_REQUEST_IN_PROG);
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_is_request_complete);
+
+/**
+ * xhdcp1x_cipher_set_ri - Enable/Disable Ri Update Check
+ * @ref: reference to cipher instance
+ * @enable: Flag (True / False)
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_ri(void *ref, bool enable)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (!cipher->is_hdmi)
+		return -EINVAL;
+
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+			     XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE);
+
+	if (enable)
+		xhdcp1x_cipher_clr_mask(cipher,
+					XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+					XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE);
+	else
+		xhdcp1x_cipher_set_mask(cipher,
+					XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+					XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_ri);
+
+/**
+ * xhdcp1x_cipher_set_link_state_check - Enable/Disable link status check
+ * @ref: reference to cipher instance
+ * @is_enabled: 1 for enable, 0 for disable
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_link_state_check(void *ref, bool is_enabled)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (cipher->is_hdmi || cipher->is_tx)
+		return -EINVAL;
+
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+			     XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL);
+
+	if (is_enabled)
+		xhdcp1x_cipher_clr_mask(cipher,
+					XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+					XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL);
+	else
+		xhdcp1x_cipher_set_mask(cipher,
+					XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+					XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_link_state_check);
+
+/**
+ * xhdcp1x_cipher_is_request_to_change_ri - Check if ri update is required
+ * @ref: reference to cipher instance
+ *
+ * Return: 0 if ri update change is needed, error value otherwise
+ */
+int xhdcp1x_cipher_is_request_to_change_ri(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher || !xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_STATUS);
+	if (!(value & XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE))
+		return -EINVAL;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_is_request_to_change_ri);
+
+/**
+ * xhdcp1x_cipher_get_interrupts - Read and clear the interrupts and return same
+ * @ref: reference to cipher instance
+ * @interrupts: reference to interrupts data
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_get_interrupts(void *ref, u32 *interrupts)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher || !interrupts)
+		return -EINVAL;
+
+	/* Read and clear the interrupts */
+	*interrupts = xhdcp1x_cipher_read(cipher,
+					  XHDCP1X_CIPHER_REG_INTERRUPT_STATUS);
+
+	if (*interrupts)
+		xhdcp1x_cipher_write(cipher,
+				     XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+				     *interrupts);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_get_interrupts);
+
+/**
+ * xhdcp1x_cipher_is_linkintegrity_failed - Check if link integrity failed
+ * @ref: reference to cipher instance
+ *
+ * Return: 1 if link integrity failed, 0/error value otherwise
+ */
+int xhdcp1x_cipher_is_linkintegrity_failed(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (xhdcp1x_cipher_is_enabled(cipher)) {
+		value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_STATUS);
+		if (value & XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL)
+			return 1;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_is_linkintegrity_failed);
+
+/**
+ * xhdcp1x_cipher_get_ri - Read Ri from cipher
+ * @ref: reference to cipher instance
+ * @ri: reference to ri data
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_get_ri(void *ref, u16 *ri)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher || !ri || !xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	*ri = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_RI);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_get_ri);
+
+/**
+ * xhdcp1x_cipher_load_aksv - load local ksv from cipher to buf
+ * @ref: reference to cipher instance
+ * @buf: 5 byte buffer to store the local KSV
+ *
+ * Return: 0 on success, error otherwise.
+ */
+int xhdcp1x_cipher_load_aksv(void *ref, u8 *buf)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u64 my_ksv;
+	u32 is_enabled;
+
+	if (!cipher || !buf)
+		return -EINVAL;
+
+	is_enabled = xhdcp1x_cipher_is_enabled(cipher);
+
+	xhdcp1x_cipher_enable(cipher);
+	my_ksv = xhdcp1x_cipher_get_localksv(cipher);
+	if (!is_enabled)
+		xhdcp1x_cipher_disable(cipher);
+	if (!my_ksv)
+		return -EAGAIN;
+	memcpy(buf, &my_ksv, XHDCP1X_CIPHER_SIZE_LOCAL_KSV);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_load_aksv);
+
+/**
+ * xhdcp1x_cipher_getencryption - This Function retrives the current encryption Stream Map.
+ * @ref: reference to cipher instance
+ *
+ * Return: streammap based on request, 0/error value otherwise.
+ */
+int xhdcp1x_cipher_getencryption(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value = 0;
+	u64 streammap = 0;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (!(xhdcp1x_cipher_is_enabled(cipher)))
+		return streammap;
+
+	streammap = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H);
+	streammap <<= XHDCP1X_CIPHER_VALUE_SHIFT;
+	streammap |= xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L);
+
+	/* Determine if there is a request in progress */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_STATUS);
+	value &= XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_XOR_IN_PROG;
+
+	if (!streammap && value)
+		streammap = XHDCP1X_CIPHER_DEFAULT_STREAMMAP;
+
+	return streammap;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_getencryption);
+
+/**
+ * xhdcp1x_cipher_disableencryption - This function disables encryption on a set of streams.
+ * @ref: reference to cipher instance
+ * @streammap: Streammap for display Audio/video content encyption
+ *
+ * Return: 0 for success, error value otherwise.
+ */
+int xhdcp1x_cipher_disableencryption(void *ref, u64 streammap)
+{
+	u32 value = 0, checkxor = 1;
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!(xhdcp1x_cipher_is_enabled(cipher)))
+		return -EINVAL;
+
+	if (!streammap)
+		return 0;
+
+	/* Clear the Register update bit */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+
+	/* Update the LS 32-bits */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L);
+	value &= ~((u32)(streammap & XHDCP1X_CIPHER_DWORD_VALUE));
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L, value);
+	if (value)
+		checkxor = 0;
+
+	/* Update the MS 32-bits */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H);
+	value &= ~((u32)((streammap >> XHDCP1X_CIPHER_VALUE_SHIFT) & XHDCP1X_CIPHER_DWORD_VALUE));
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H, value);
+	if (value)
+		checkxor = 0;
+
+	if (cipher->is_hdmi == XHDCP1X_CIPHER_VALUE_TYPE_PROTOCOL_HDMI)
+		checkxor = true;
+
+	if (checkxor) {
+		value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL);
+		value &= ~XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE;
+		xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL, value);
+	}
+	/* Set the register update bit */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value |= XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_disableencryption);
+
+/**
+ * xhdcp1x_cipher_setb - This function writes the contents of the B register in BM0.
+ * @ref: reference to cipher instance
+ * @bx: is the value to be written to bx.
+ * @by: is the value to be written to by.
+ * @bz: is the value to be written to bz.
+ *
+ * @return:
+ *		- SUCCESS if successful.
+ *		- NON_ZERO otherwise.
+ */
+int xhdcp1x_cipher_setb(void *ref, u32 bx, u32 by, u32 bz)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value = 0;
+
+	if (!(xhdcp1x_cipher_is_enabled(cipher)))
+		return -EINVAL;
+
+	/* Clear the register update bit */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+
+	/* Update the Bx */
+	value = (bx & XHDCP1X_CIPHER_SET_B);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BX, value);
+
+	/* Update the By */
+	value = (by & XHDCP1X_CIPHER_SET_B);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BY, value);
+
+	/* Update the Bz */
+	value = (bz & XHDCP1X_CIPHER_SET_B);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BZ, value);
+
+	/* Set the register update bit */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value |= XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_setb);
+
+/**
+ * xhdcp1x_cipher_enable_encryption - This function enables the encryption on a set of streams.
+ * @ref: reference to cipher instance
+ * @streammap: Streammap for display Audio/video content encyption
+ *
+ * Return: 0 for succes, error value otherwise.
+ */
+int xhdcp1x_cipher_enable_encryption(void *ref, u64 streammap)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (!(xhdcp1x_cipher_is_enabled(cipher)))
+		return -EINVAL;
+
+	/* Check for nothing to do */
+	if (!streammap)
+		return 0;
+
+	/* Clear the register update bit */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+
+	/* Update the LS 32-bits */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L);
+	value |= ((u32)(streammap & XHDCP1X_CIPHER_DWORD_VALUE));
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L, value);
+
+	/* Write the MS 32-bits */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H);
+	value |= ((u32)((streammap >> XHDCP1X_CIPHER_VALUE_SHIFT) &
+			 XHDCP1X_CIPHER_DWORD_VALUE));
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H, value);
+	/* Ensure that the XOR is enabled */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL);
+	value |= XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL, value);
+
+	/* Set the register update bit */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value |= XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+	/* Check if XORInProgress bit is set in the status register*/
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_STATUS);
+	if ((value & XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_XOR_IN_PROG)) {
+		/* Do nothing for now. We can depend on the Cipher
+		 * to set the XorInProgress in status register when
+		 * we receive protected content.
+		 */
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_enable_encryption);
+
+/**
+ * xhdcp1x_cipher_get_mi: This function reads the contents of the Mi/An register of BM0.
+ * @ref: is the device to query.
+ *
+ * @return: The contents of the register.
+ */
+u64 xhdcp1x_cipher_get_mi(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u64 mi = 0;
+
+	/* Check that it is not disabled */
+	if (!(xhdcp1x_cipher_is_enabled(cipher)))
+		return -EINVAL;
+
+	/* Update Mi */
+	mi = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_MI_H);
+	mi <<= XHDCP1X_CIPHER_VALUE_SHIFT;
+	mi |= xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_MI_L);
+
+	return mi;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_get_mi);
+
+/**
+ * xhdcp1x_cipher_get_mo: This function reads the contents of the Mo register of the device.
+ * @ref: reference to cipher instance.
+ *
+ * @return: The contents of the Mo register.
+ */
+u64 xhdcp1x_cipher_get_mo(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u64 mo = 0;
+
+	/* Check that it is not disabled */
+	if (!(xhdcp1x_cipher_is_enabled(cipher)))
+		return -EINVAL;
+
+	/* Determine Mo */
+	mo = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_MO_H);
+	mo <<= XHDCP1X_CIPHER_VALUE_SHIFT;
+	mo |= xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_MO_L);
+
+	return mo;
+}
+
+/**
+ * xhdcp1x_cipher_set_ri_update: This function enables the interrupt for RI updates
+ * @ref: reference to cipher instance.
+ * @is_enabled: flag to enable/disable RI updation
+ *
+ * @return: 0 for success, error value otherwise.
+ */
+int xhdcp1x_cipher_set_ri_update(void *ref, int is_enabled)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	int status = 0, value = 0;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	xhdcp1x_cipher_write(cipher,
+			     XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+			     XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE);
+
+	value = xhdcp1x_cipher_read(cipher,
+				    XHDCP1X_CIPHER_REG_INTERRUPT_MASK);
+
+	if (is_enabled)
+		value &= ~XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE;
+	else
+		value |= XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE;
+
+	xhdcp1x_cipher_write(cipher,
+			     XHDCP1X_CIPHER_REG_INTERRUPT_MASK, value);
+
+	return status;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_ri_update);
diff --git a/drivers/staging/xlnxsync/Kconfig b/drivers/staging/xlnxsync/Kconfig
new file mode 100644
index 000000000..08e73384d
--- /dev/null
+++ b/drivers/staging/xlnxsync/Kconfig
@@ -0,0 +1,11 @@
+config XLNX_SYNC
+	tristate "Xilinx Synchronizer"
+	depends on ARCH_ZYNQMP
+	help
+	  This driver is developed for Xilinx Synchronizer IP. It is used to
+	  monitor the AXI addresses of the producer and initiate the
+	  consumer to start earlier, thereby reducing the latency to process
+	  the data.
+
+	  To compile this driver as a module, choose M here.
+	  If unsure, choose N
diff --git a/drivers/staging/xlnxsync/MAINTAINERS b/drivers/staging/xlnxsync/MAINTAINERS
new file mode 100644
index 000000000..e2d720419
--- /dev/null
+++ b/drivers/staging/xlnxsync/MAINTAINERS
@@ -0,0 +1,4 @@
+XILINX SYNCHRONIZER DRIVER
+M:	Vishal Sagar <vishal.sagar@xilinx.com>
+S:	Maintained
+F:	drivers/staging/xlnxsync
diff --git a/drivers/staging/xlnxsync/Makefile b/drivers/staging/xlnxsync/Makefile
new file mode 100644
index 000000000..b126a36da
--- /dev/null
+++ b/drivers/staging/xlnxsync/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_XLNX_SYNC) += xlnxsync.o
diff --git a/drivers/staging/xlnxsync/dt-binding.txt b/drivers/staging/xlnxsync/dt-binding.txt
new file mode 100644
index 000000000..f1ed9d724
--- /dev/null
+++ b/drivers/staging/xlnxsync/dt-binding.txt
@@ -0,0 +1,34 @@
+Xilinx Synchronizer
+-------------------
+
+The Xilinx Synchronizer is used for buffer synchronization between
+producer and consumer blocks. It manages to do so by tapping onto the bus
+where the producer block is writing frame data to memory and consumer block is
+reading the frame data from memory.
+
+It can work on the encode path with max 4 channels or on decode path with
+max 2 channels.
+
+Required properties:
+- compatible : Must contain "xlnx,sync-ip-1.0"
+- reg: Physical base address and length of the registers set for the device.
+- interrupts: Contains the interrupt line number.
+- interrupt-parent: phandle to interrupt controller.
+- clock-names: The input clock names for axilite, producer and consumer clock.
+- clocks: Reference to the clock that drives the axi interface, producer and consumer.
+- xlnx,num-chan: Range from 1 to 2 for decode.
+		 Range from 1 to 4 for encode.
+
+Optional properties:
+- xlnx,encode: Present if IP configured for encoding path, else absent.
+
+v_sync_vcu: subframe_sync_vcu@a00e0000 {
+	compatible = "xlnx,sync-ip-1.0";
+	reg = <0x0 0xa00e0000 0x0 0x10000>;
+	interrupt-parent = <&gic>;
+	interrupts = <0 96 4>;
+	clock-names = "s_axi_ctrl_aclk", "s_axi_mm_p_aclk", "s_axi_mm_aclk";
+	clocks = <&vid_s_axi_clk>, <&vid_stream_clk>, <&vid_stream_clk>;
+	xlnx,num-chan = <4>;
+	xlnx,encode;
+};
diff --git a/drivers/staging/xlnxsync/xlnxsync.c b/drivers/staging/xlnxsync/xlnxsync.c
new file mode 100644
index 000000000..f1bcb51c3
--- /dev/null
+++ b/drivers/staging/xlnxsync/xlnxsync.c
@@ -0,0 +1,1493 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx Synchronizer IP driver
+ *
+ * Copyright (C) 2019 Xilinx, Inc.
+ *
+ * Author: Vishal Sagar <vishal.sagar@xilinx.com>
+ *
+ * This driver is used to control the Xilinx Synchronizer IP
+ * to achieve sub frame latency for encode and decode with VCU.
+ * This is done by monitoring the address lines for specific values.
+ */
+
+#include <linux/cdev.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/ioctl.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/uaccess.h>
+#include <linux/xlnxsync.h>
+
+/* Register offsets and bit masks */
+#define XLNXSYNC_CTRL_REG		0x00
+#define XLNXSYNC_ISR_REG		0x04
+/* Producer Luma/Chroma Start/End Address */
+#define XLNXSYNC_PL_START_LO_REG	0x08
+#define XLNXSYNC_PL_START_HI_REG	0x0C
+#define XLNXSYNC_PC_START_LO_REG	0x20
+#define XLNXSYNC_PC_START_HI_REG	0x24
+#define XLNXSYNC_PL_END_LO_REG		0x38
+#define XLNXSYNC_PL_END_HI_REG		0x3C
+#define XLNXSYNC_PC_END_LO_REG		0x50
+#define XLNXSYNC_PC_END_HI_REG		0x54
+#define XLNXSYNC_L_MARGIN_REG		0x68
+#define XLNXSYNC_C_MARGIN_REG		0x74
+#define XLNXSYNC_IMR_REG		0x80
+#define XLNXSYNC_DBG_REG		0x84
+/* Consumer Luma/Chroma Start/End Address */
+#define XLNXSYNC_CL_START_LO_REG	0x88
+#define XLNXSYNC_CL_START_HI_REG	0x8C
+#define XLNXSYNC_CC_START_LO_REG	0xA0
+#define XLNXSYNC_CC_START_HI_REG	0xA4
+#define XLNXSYNC_CL_END_LO_REG		0xB8
+#define XLNXSYNC_CL_END_HI_REG		0xBC
+#define XLNXSYNC_CC_END_LO_REG		0xD0
+#define XLNXSYNC_CC_END_HI_REG		0xD4
+
+/* Luma/Chroma Core offset registers */
+#define XLNXSYNC_LCOREOFF_REG		0x400
+#define XLNXSYNC_CCOREOFF_REG		0x410
+#define XLNXSYNC_COREOFF_NEXT		0x4
+
+#define XLNXSYNC_CTRL_ENCDEC_MASK	BIT(0)
+#define XLNXSYNC_CTRL_ENABLE_MASK	BIT(1)
+#define XLNXSYNC_CTRL_INTR_EN_MASK	BIT(2)
+#define XLNXSYNC_CTRL_SOFTRESET		BIT(3)
+
+#define XLNXSYNC_ISR_PROD_SYNC_FAIL_MASK BIT(0)
+#define XLNXSYNC_ISR_PROD_WDG_ERR_MASK	BIT(1)
+/* Producer related */
+#define XLNXSYNC_ISR_PLDONE_SHIFT	(2)
+#define XLNXSYNC_ISR_PLDONE_MASK	GENMASK(3, 2)
+#define XLNXSYNC_ISR_PLSKIP_MASK	BIT(4)
+#define XLNXSYNC_ISR_PLVALID_MASK	BIT(5)
+#define XLNXSYNC_ISR_PCDONE_SHIFT	(6)
+#define XLNXSYNC_ISR_PCDONE_MASK	GENMASK(7, 6)
+#define XLNXSYNC_ISR_PCSKIP_MASK	BIT(8)
+#define XLNXSYNC_ISR_PCVALID_MASK	BIT(9)
+/* Consumer related */
+#define XLNXSYNC_ISR_CLDONE_SHIFT	(10)
+#define XLNXSYNC_ISR_CLDONE_MASK	GENMASK(11, 10)
+#define XLNXSYNC_ISR_CLSKIP_MASK	BIT(12)
+#define XLNXSYNC_ISR_CLVALID_MASK	BIT(13)
+#define XLNXSYNC_ISR_CCDONE_SHIFT	(14)
+#define XLNXSYNC_ISR_CCDONE_MASK	GENMASK(15, 14)
+#define XLNXSYNC_ISR_CCSKIP_MASK	BIT(16)
+#define XLNXSYNC_ISR_CCVALID_MASK	BIT(17)
+
+#define XLNXSYNC_ISR_LDIFF		BIT(18)
+#define XLNXSYNC_ISR_CDIFF		BIT(19)
+#define XLNXSYNC_ISR_CONS_SYNC_FAIL_MASK BIT(20)
+#define XLNXSYNC_ISR_CONS_WDG_ERR_MASK	BIT(21)
+
+/* bit 44 of start address */
+#define XLNXSYNC_FB_VALID_MASK		BIT(12)
+#define XLNXSYNC_FB_HI_ADDR_MASK	GENMASK(11, 0)
+
+#define XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK BIT(0)
+#define XLNXSYNC_IMR_PROD_WDG_ERR_MASK	BIT(1)
+/* Producer */
+#define XLNXSYNC_IMR_PLVALID_MASK	BIT(5)
+#define XLNXSYNC_IMR_PCVALID_MASK	BIT(9)
+/* Consumer */
+#define XLNXSYNC_IMR_CLVALID_MASK	BIT(13)
+#define XLNXSYNC_IMR_CCVALID_MASK	BIT(17)
+/* Diff */
+#define XLNXSYNC_IMR_LDIFF		BIT(18)
+#define XLNXSYNC_IMR_CDIFF		BIT(19)
+#define XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK BIT(20)
+#define XLNXSYNC_IMR_CONS_WDG_ERR_MASK	BIT(21)
+
+#define XLNXSYNC_IMR_ALL_MASK		(XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK |\
+					 XLNXSYNC_IMR_PROD_WDG_ERR_MASK |\
+					 XLNXSYNC_IMR_PLVALID_MASK |\
+					 XLNXSYNC_IMR_PCVALID_MASK |\
+					 XLNXSYNC_IMR_CLVALID_MASK |\
+					 XLNXSYNC_IMR_CCVALID_MASK |\
+					 XLNXSYNC_IMR_LDIFF |\
+					 XLNXSYNC_IMR_CDIFF |\
+					 XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK |\
+					 XLNXSYNC_IMR_CONS_WDG_ERR_MASK)
+
+/* Other macros */
+#define XLNXSYNC_CHAN_OFFSET		0x100
+#define XLNXSYNC_BUF_ADDR_SHIFT	0x3
+#define XLNXSYNC_DEVNAME_LEN		(32)
+
+#define XLNXSYNC_DRIVER_NAME		"xlnxsync"
+#define XLNXSYNC_DRIVER_VERSION		"0.1"
+
+#define XLNXSYNC_DEV_MAX		256
+
+/* Module Parameters */
+static struct class *xlnxsync_class;
+static dev_t xlnxsync_devt;
+/* Used to keep track of sync devices */
+static DEFINE_IDA(xs_ida);
+
+/**
+ * struct xlnxsync_device - Xilinx Synchronizer struct
+ * @chdev: Character device driver struct
+ * @dev: Pointer to device
+ * @iomem: Pointer to the register space
+ * @sync_mutex: Serialize general device specific ioctl calls
+ * @axi_clk: Pointer to clock structure for axilite clock
+ * @p_clk: Pointer to clock structure for producer clock
+ * @c_clk: Pointer to clock structure for consumer clock
+ * @user_count: Usage count
+ * @irq: IRQ number
+ * @irq_lock: Spinlock used to protect access to sync and watchdog error
+ * @minor: Device id count
+ * @config: IP config struct
+ * @channels: List head for syncip channel linked list
+ * @chan_count : Active channel number count
+ * @reserved : Bitmap to track reserved channels
+ *
+ * This structure contains the device driver related parameters
+ */
+struct xlnxsync_device {
+	struct cdev chdev;
+	struct device *dev;
+	void __iomem *iomem;
+	/* sync_mutex is used to serialize general device ioctl calls */
+	struct mutex sync_mutex;
+	struct clk *axi_clk;
+	struct clk *p_clk;
+	struct clk *c_clk;
+	atomic_t user_count;
+	unsigned int irq;
+	/* irq_lock is used to protect access to sync_err and wdg_err */
+	spinlock_t irq_lock;
+	u32 minor;
+	struct xlnxsync_config config;
+	struct list_head channels;
+	u8 chan_count;
+	unsigned long reserved;
+};
+
+/**
+ * struct xlnxsync_channel - Synchronizer context struct
+ * @dev: Xilinx synchronizer device struct
+ * @mutex: Serialize channel specific ioctl calls
+ * @id: Channel id
+ * @last_buf_used: Save last buffer id used
+ * @channel: list entry into syncip channel lists
+ * @wq_fbdone: Wait queue for frame buffer done events
+ * @wq_error: Wait queue for error events
+ * @l_done: Luma done result array
+ * @c_done: Chroma done result array
+ * @prod_sync_err: Capture synchronization error per channel
+ * @prod_wdg_err: Capture watchdog error per channel
+ * @cons_sync_err: Consumer synchronization error per channel
+ * @cons_wdg_err: Consumer watchdog error per channel
+ * @ldiff_err: Luma buffer diff > 1
+ * @cdiff_err: Chroma buffer diff > 1
+ * @err_event: Error event per channel
+ * @framedone_event: Framebuffer done event per channel
+ *
+ * This structure contains the syncip channel specific parameters
+ */
+struct xlnxsync_channel {
+	struct xlnxsync_device *dev;
+	/* Serialize channel specific ioctl calls */
+	struct mutex mutex;
+	u32 id;
+	int last_buf_used;
+	struct list_head channel;
+	wait_queue_head_t wq_fbdone;
+	wait_queue_head_t wq_error;
+	u8 l_done[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+	u8 c_done[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+	u8 prod_sync_err : 1;
+	u8 prod_wdg_err : 1;
+	u8 cons_sync_err : 1;
+	u8 cons_wdg_err : 1;
+	u8 ldiff_err : 1;
+	u8 cdiff_err : 1;
+	u8 err_event : 1;
+	u8 framedone_event : 1;
+};
+
+static inline u32 xlnxsync_read(struct xlnxsync_device *dev, u32 chan, u32 reg)
+{
+	return ioread32(dev->iomem + (chan * XLNXSYNC_CHAN_OFFSET) + reg);
+}
+
+static inline void xlnxsync_write(struct xlnxsync_device *dev, u32 chan,
+				  u32 reg, u32 val)
+{
+	iowrite32(val, dev->iomem + (chan * XLNXSYNC_CHAN_OFFSET) + reg);
+}
+
+static inline void xlnxsync_clr(struct xlnxsync_device *dev, u32 chan, u32 reg,
+				u32 clr)
+{
+	xlnxsync_write(dev, chan, reg, xlnxsync_read(dev, chan, reg) & ~clr);
+}
+
+static inline void xlnxsync_set(struct xlnxsync_device *dev, u32 chan, u32 reg,
+				u32 set)
+{
+	xlnxsync_write(dev, chan, reg, xlnxsync_read(dev, chan, reg) | set);
+}
+
+static bool xlnxsync_is_buf_done(struct xlnxsync_device *dev,
+				 u32 channel, u32 buf, u32 io,
+				 u8 force_clr_valid)
+{
+	u32 luma_valid, chroma_valid, reg_laddr, reg_caddr, buf_offset;
+
+	switch (io) {
+	case XLNXSYNC_PROD:
+		reg_laddr = XLNXSYNC_PL_START_HI_REG;
+		reg_caddr = XLNXSYNC_PC_START_HI_REG;
+		break;
+	case XLNXSYNC_CONS:
+		reg_laddr = XLNXSYNC_CL_START_HI_REG;
+		reg_caddr = XLNXSYNC_CC_START_HI_REG;
+		break;
+	default:
+		return false;
+	}
+
+	buf_offset = buf << XLNXSYNC_BUF_ADDR_SHIFT;
+	if (force_clr_valid) {
+		xlnxsync_clr(dev, channel, reg_laddr + buf_offset,
+			     XLNXSYNC_FB_VALID_MASK);
+		xlnxsync_clr(dev, channel, reg_caddr + buf_offset,
+			     XLNXSYNC_FB_VALID_MASK);
+	}
+
+	luma_valid = xlnxsync_read(dev, channel, reg_laddr + buf_offset) &
+				   XLNXSYNC_FB_VALID_MASK;
+	chroma_valid = xlnxsync_read(dev, channel, reg_caddr + buf_offset) &
+				     XLNXSYNC_FB_VALID_MASK;
+	if (!luma_valid && !chroma_valid)
+		return true;
+
+	return false;
+}
+
+static void xlnxsync_reset_chan(struct xlnxsync_device *dev, u32 chan)
+{
+	u8 num_retries = 50;
+
+	xlnxsync_set(dev, chan, XLNXSYNC_CTRL_REG, XLNXSYNC_CTRL_SOFTRESET);
+	/* Wait for a maximum of ~100ms to flush pending transactions */
+	while (num_retries--) {
+		if (!(xlnxsync_read(dev, chan, XLNXSYNC_CTRL_REG) &
+				XLNXSYNC_CTRL_SOFTRESET))
+			break;
+		usleep_range(2000, 2100);
+	}
+}
+
+static void xlnxsync_reset(struct xlnxsync_device *dev)
+{
+	u32 i;
+
+	for (i = 0; i < dev->config.max_channels; i++)
+		xlnxsync_reset_chan(dev, i);
+}
+
+static dma_addr_t xlnxsync_get_phy_addr(struct xlnxsync_device *dev,
+					u32 fd)
+{
+	struct dma_buf *dbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	dma_addr_t phy_addr = 0;
+
+	dbuf = dma_buf_get(fd);
+	if (IS_ERR(dbuf)) {
+		dev_err(dev->dev, "%s : Failed to get dma buf\n", __func__);
+		goto get_phy_addr_err;
+	}
+
+	attach = dma_buf_attach(dbuf, dev->dev);
+	if (IS_ERR(attach)) {
+		dev_err(dev->dev, "%s : Failed to attach buf\n", __func__);
+		goto fail_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
+	if (IS_ERR(sgt)) {
+		dev_err(dev->dev, "%s : Failed to attach map\n", __func__);
+		goto fail_map;
+	}
+
+	phy_addr = sg_dma_address(sgt->sgl);
+	dma_buf_unmap_attachment(attach, sgt, DMA_BIDIRECTIONAL);
+
+fail_map:
+	dma_buf_detach(dbuf, attach);
+fail_attach:
+	dma_buf_put(dbuf);
+get_phy_addr_err:
+	return phy_addr;
+}
+
+static int xlnxsync_chan_config(struct xlnxsync_channel *channel,
+				void __user *arg)
+{
+	struct xlnxsync_chan_config cfg;
+	int ret, i = 0, j;
+	dma_addr_t phy_start_address;
+	u64 luma_start_address[XLNXSYNC_IO];
+	u64 chroma_start_address[XLNXSYNC_IO];
+	u64 luma_end_address[XLNXSYNC_IO];
+	u64 chroma_end_address[XLNXSYNC_IO];
+	struct xlnxsync_device *dev = channel->dev;
+
+	ret = copy_from_user(&cfg, arg, sizeof(cfg));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	if (cfg.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			cfg.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	/* Calculate luma/chroma physical addresses */
+	phy_start_address = xlnxsync_get_phy_addr(dev, cfg.dma_fd);
+	if (!phy_start_address) {
+		dev_err(dev->dev, "%s : Failed to obtain physical address\n",
+			__func__);
+		return -EINVAL;
+	}
+
+	luma_start_address[XLNXSYNC_PROD] =
+		cfg.luma_start_offset[XLNXSYNC_PROD] + phy_start_address;
+	luma_start_address[XLNXSYNC_CONS] =
+		cfg.luma_start_offset[XLNXSYNC_CONS] + phy_start_address;
+	chroma_start_address[XLNXSYNC_PROD] =
+		cfg.chroma_start_offset[XLNXSYNC_PROD] + phy_start_address;
+	chroma_start_address[XLNXSYNC_CONS] =
+		cfg. chroma_start_offset[XLNXSYNC_CONS] + phy_start_address;
+	luma_end_address[XLNXSYNC_PROD] =
+		cfg.luma_end_offset[XLNXSYNC_PROD] + phy_start_address;
+	luma_end_address[XLNXSYNC_CONS] =
+		cfg.luma_end_offset[XLNXSYNC_CONS] + phy_start_address;
+	chroma_end_address[XLNXSYNC_PROD] =
+		cfg.chroma_end_offset[XLNXSYNC_PROD] + phy_start_address;
+	chroma_end_address[XLNXSYNC_CONS] =
+		cfg.chroma_end_offset[XLNXSYNC_CONS] + phy_start_address;
+
+	dev_dbg(dev->dev, "Channel id = %d", channel->id);
+	dev_dbg(dev->dev, "Producer address\n");
+	dev_dbg(dev->dev, "Luma Start Addr = 0x%llx End Addr = 0x%llx Margin = 0x%08x\n",
+		luma_start_address[XLNXSYNC_PROD],
+		luma_end_address[XLNXSYNC_PROD], cfg.luma_margin);
+	dev_dbg(dev->dev, "Chroma Start Addr = 0x%llx End Addr = 0x%llx Margin = 0x%08x\n",
+		chroma_start_address[XLNXSYNC_PROD],
+		chroma_end_address[XLNXSYNC_PROD], cfg.chroma_margin);
+	dev_dbg(dev->dev, "FB id = %d IsMono = %d\n",
+		cfg.fb_id[XLNXSYNC_PROD], cfg.ismono[XLNXSYNC_PROD]);
+	dev_dbg(dev->dev, "Consumer address\n");
+	dev_dbg(dev->dev, "Luma Start Addr = 0x%llx End Addr = 0x%llx\n",
+		luma_start_address[XLNXSYNC_CONS],
+		luma_end_address[XLNXSYNC_CONS]);
+	dev_dbg(dev->dev, "Chroma Start Addr = 0x%llx End Addr = 0x%llx\n",
+		chroma_start_address[XLNXSYNC_CONS],
+		chroma_end_address[XLNXSYNC_CONS]);
+	dev_dbg(dev->dev, "FB id = %d IsMono = %d\n",
+		cfg.fb_id[XLNXSYNC_CONS], cfg.ismono[XLNXSYNC_CONS]);
+
+	for (j = 0; j < XLNXSYNC_IO; j++) {
+		u32 l_start_reg, l_end_reg, c_start_reg, c_end_reg;
+
+		if (cfg.fb_id[j] == XLNXSYNC_AUTO_SEARCH) {
+			/*
+			 * When fb_id is 0xFF auto search for free fb
+			 * in a channel
+			 */
+			dev_dbg(dev->dev, "%s : auto search free fb\n",
+				__func__);
+			switch (channel->last_buf_used) {
+			case 0:
+				i = 1;
+				break;
+			case 1:
+				i = 2;
+				break;
+			case 2:
+			case -1:
+			default:
+				i = 0;
+				break;
+			}
+
+			if (xlnxsync_is_buf_done(dev, channel->id, i, j, false)) {
+				dev_dbg(dev->dev,
+					"Channel %d %s FB %d is assigned\n",
+					channel->id, j ? "cons" : "prod", i);
+				if (j)
+					channel->last_buf_used = i;
+			} else {
+				dev_dbg(dev->dev,
+					"Channel %d %s FB %d is busy\n",
+					channel->id, j ? "cons" : "prod", i);
+				if (j) {
+					/* Force clear producer buf valid */
+					if (!xlnxsync_is_buf_done(dev,
+								  channel->id,
+								  i, 0, true)) {
+						dev_err(dev->dev,
+							"prod:buffer valid:\t"
+							"resetting failed\n");
+					}
+				}
+				return -EBUSY;
+			}
+		} else if (cfg.fb_id[j] >= 0 &&
+			   cfg.fb_id[j] < XLNXSYNC_BUF_PER_CHAN) {
+			/* If fb_id is specified, check its availability */
+			if (!(xlnxsync_is_buf_done(dev, channel->id,
+						   cfg.fb_id[j], j, false))) {
+				dev_dbg(dev->dev,
+					"%s : %s FB %d in channel %d is busy!\n",
+					__func__, j ? "prod" : "cons",
+					i, channel->id);
+				return -EBUSY;
+			}
+			dev_dbg(dev->dev, "%s : Configure fb %d\n",
+				__func__, i);
+		} else {
+			/* Invalid fb_id passed */
+			dev_err(dev->dev, "Invalid FB id %d for configuration!\n",
+				cfg.fb_id[j]);
+			return -EINVAL;
+		}
+
+		if (j == XLNXSYNC_PROD) {
+			l_start_reg = XLNXSYNC_PL_START_LO_REG;
+			l_end_reg = XLNXSYNC_PL_END_LO_REG;
+			c_start_reg = XLNXSYNC_PC_START_LO_REG;
+			c_end_reg = XLNXSYNC_PC_END_LO_REG;
+		} else {
+			l_start_reg = XLNXSYNC_CL_START_LO_REG;
+			l_end_reg = XLNXSYNC_CL_END_LO_REG;
+			c_start_reg = XLNXSYNC_CC_START_LO_REG;
+			c_end_reg = XLNXSYNC_CC_END_LO_REG;
+		}
+
+		/* Start Address */
+		xlnxsync_write(dev, channel->id, l_start_reg + (i << 3),
+			       lower_32_bits(luma_start_address[j]));
+
+		xlnxsync_write(dev, channel->id,
+			       (l_start_reg + 4) + (i << 3),
+			       upper_32_bits(luma_start_address[j]) &
+			       XLNXSYNC_FB_HI_ADDR_MASK);
+
+		/* End Address */
+		xlnxsync_write(dev, channel->id, l_end_reg + (i << 3),
+			       lower_32_bits(luma_end_address[j]));
+		xlnxsync_write(dev, channel->id, l_end_reg + 4 + (i << 3),
+			       upper_32_bits(luma_end_address[j]));
+
+		/* Set margin */
+		xlnxsync_write(dev, channel->id,
+			       XLNXSYNC_L_MARGIN_REG + (i << 2),
+			       cfg.luma_margin);
+
+		if (!cfg.ismono[j]) {
+			dev_dbg(dev->dev, "%s : Not monochrome. Program Chroma\n",
+				__func__);
+
+			/* Chroma Start Address */
+			xlnxsync_write(dev, channel->id,
+				       c_start_reg + (i << 3),
+				       lower_32_bits(chroma_start_address[j]));
+
+			xlnxsync_write(dev, channel->id,
+				       c_start_reg + 4 + (i << 3),
+				       upper_32_bits(chroma_start_address[j]) &
+				       XLNXSYNC_FB_HI_ADDR_MASK);
+
+			/* Chroma End Address */
+			xlnxsync_write(dev, channel->id,
+				       c_end_reg + (i << 3),
+				       lower_32_bits(chroma_end_address[j]));
+
+			xlnxsync_write(dev, channel->id,
+				       c_end_reg + 4 + (i << 3),
+				       upper_32_bits(chroma_end_address[j]));
+
+			/* Chroma Margin */
+			xlnxsync_write(dev, channel->id,
+				       XLNXSYNC_C_MARGIN_REG + (i << 2),
+				       cfg.chroma_margin);
+
+			/* Set the Valid bit */
+			xlnxsync_set(dev, channel->id,
+				     c_start_reg + 4 + (i << 3),
+				     XLNXSYNC_FB_VALID_MASK);
+		}
+
+		/* Set the Valid bit */
+		xlnxsync_set(dev, channel->id, l_start_reg + 4 + (i << 3),
+			     XLNXSYNC_FB_VALID_MASK);
+	}
+
+	for (i = 0; i < XLNXSYNC_MAX_CORES; i++) {
+		iowrite32(cfg.luma_core_offset[i],
+			  dev->iomem + XLNXSYNC_LCOREOFF_REG +
+			  (i * XLNXSYNC_COREOFF_NEXT));
+
+		iowrite32(cfg.chroma_core_offset[i],
+			  dev->iomem + XLNXSYNC_CCOREOFF_REG +
+			  (i * XLNXSYNC_COREOFF_NEXT));
+	}
+
+	return 0;
+}
+
+static int xlnxsync_chan_get_status(struct xlnxsync_channel *channel,
+				    void __user *arg)
+{
+	int ret;
+	u32 i, j;
+	unsigned long flags;
+	struct xlnxsync_stat status;
+	struct xlnxsync_device *dev = channel->dev;
+
+	/* Update Buffers status */
+	for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+		for (j = 0; j < XLNXSYNC_IO; j++) {
+			if (xlnxsync_is_buf_done(dev, channel->id, i, j, false))
+				status.fbdone[i][j] = true;
+			else
+				status.fbdone[i][j] = false;
+		}
+	}
+
+	/* Update channel enable status */
+	if (xlnxsync_read(dev, channel->id, XLNXSYNC_CTRL_REG) &
+	    XLNXSYNC_CTRL_ENABLE_MASK)
+		status.enable = true;
+
+	/* Update channel error status */
+	spin_lock_irqsave(&dev->irq_lock, flags);
+	status.err.prod_sync = channel->prod_sync_err;
+	status.err.prod_wdg = channel->prod_wdg_err;
+	status.err.cons_sync = channel->cons_sync_err;
+	status.err.cons_wdg = channel->cons_wdg_err;
+	status.err.ldiff = channel->ldiff_err;
+	status.err.cdiff = channel->cdiff_err;
+	spin_unlock_irqrestore(&dev->irq_lock, flags);
+
+	status.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+
+	ret = copy_to_user(arg, &status, sizeof(status));
+	if (ret) {
+		dev_err(dev->dev, "%s: failed to copy result data to user\n",
+			__func__);
+	} else {
+		channel->prod_sync_err = 0;
+		channel->prod_wdg_err = 0;
+		channel->cons_sync_err = 0;
+		channel->cons_wdg_err = 0;
+		channel->ldiff_err = 0;
+		channel->cdiff_err = 0;
+	}
+
+	return ret;
+}
+
+static int xlnxsync_reset_slot(struct xlnxsync_channel *channel)
+{
+	struct xlnxsync_device *dev = channel->dev;
+	int slot;
+
+	if (dev->config.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "ioctl not supported!\n");
+		return -EINVAL;
+	}
+
+	/* check channel v/s max from dt */
+	if (channel->id >= dev->config.max_channels) {
+		dev_err(dev->dev, "Invalid channel %d. Max channels = %d!\n",
+			channel->id, dev->config.max_channels);
+		return -EINVAL;
+	}
+
+	slot = channel->last_buf_used + 1;
+	if (slot == XLNXSYNC_BUF_PER_CHAN)
+		slot = 0;
+
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_CL_START_LO_REG + 4 + (slot << 3), 0);
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_CC_START_LO_REG + 4 + (slot << 3), 0);
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_PL_START_LO_REG + 4 + (slot << 3), 0);
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_PC_START_LO_REG + 4 + (slot << 3), 0);
+
+	return 0;
+}
+
+static int xlnxsync_chan_enable(struct xlnxsync_channel *channel, bool enable)
+{
+	struct xlnxsync_device *dev = channel->dev;
+	unsigned int i, j;
+
+	if (dev->config.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "ioctl not supported!\n");
+		return -EINVAL;
+	}
+
+	/* check channel v/s max from dt */
+	if (channel->id >= dev->config.max_channels) {
+		dev_err(dev->dev, "Invalid channel %d. Max channels = %d!\n",
+			channel->id, dev->config.max_channels);
+		return -EINVAL;
+	}
+
+	if (enable) {
+		dev_dbg(dev->dev, "Enabling %d channel\n", channel->id);
+		xlnxsync_set(dev, channel->id, XLNXSYNC_CTRL_REG,
+			     XLNXSYNC_CTRL_ENABLE_MASK |
+			     XLNXSYNC_CTRL_INTR_EN_MASK);
+	} else {
+		dev_dbg(dev->dev, "Disabling %d channel\n", channel->id);
+		xlnxsync_reset_chan(dev, channel->id);
+		xlnxsync_clr(dev, channel->id, XLNXSYNC_CTRL_REG,
+			     XLNXSYNC_CTRL_ENABLE_MASK |
+			     XLNXSYNC_CTRL_INTR_EN_MASK);
+		channel->prod_sync_err = false;
+		channel->prod_wdg_err = false;
+		channel->cons_sync_err = false;
+		channel->cons_wdg_err = false;
+		channel->ldiff_err = false;
+		channel->cdiff_err = false;
+
+		for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+			for (j = 0; j < XLNXSYNC_IO; j++) {
+				channel->l_done[i][j] = false;
+				channel->c_done[i][j] = false;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int xlnxsync_get_config(struct xlnxsync_channel *channel,
+			       void __user *arg)
+{
+	struct xlnxsync_config cfg;
+	int ret;
+	struct xlnxsync_device *dev = channel->dev;
+
+	cfg.encode = dev->config.encode;
+	cfg.max_channels = dev->config.max_channels;
+	cfg.active_channels = dev->chan_count;
+	cfg.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+	cfg.reserved_id = channel->id;
+	dev_dbg(dev->dev, "IP Config : encode = %d max_channels = %d\n",
+		cfg.encode, cfg.max_channels);
+	dev_dbg(dev->dev, "IP Config : active channels = %d reserved id = %d\n",
+		cfg.active_channels, cfg.reserved_id);
+	dev_dbg(dev->dev, "ioctl version = 0x%llx\n", cfg.hdr_ver);
+	ret = copy_to_user(arg, &cfg, sizeof(cfg));
+	if (ret) {
+		dev_err(dev->dev, "%s: failed to copy result data to user\n",
+			__func__);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int xlnxsync_chan_clr_err(struct xlnxsync_channel *channel,
+				 void __user *arg)
+{
+	struct xlnxsync_clr_err errcfg;
+	u32 intr_unmask_val = 0;
+	int ret;
+	unsigned long flags;
+	struct xlnxsync_device *dev = channel->dev;
+
+	ret = copy_from_user(&errcfg, arg, sizeof(errcfg));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	if (errcfg.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			errcfg.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	dev_dbg(dev->dev, "%s : Clearing %d channel errors\n",
+		__func__, channel->id);
+	/* Clear channel error status */
+	spin_lock_irqsave(&dev->irq_lock, flags);
+	if (errcfg.err.prod_sync) {
+		dev_dbg(dev->dev, "Unmasking producer sync err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK;
+	}
+
+	if (errcfg.err.prod_wdg) {
+		dev_dbg(dev->dev, "Unmasking producer wdg err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_PROD_WDG_ERR_MASK;
+	}
+
+	if (errcfg.err.cons_sync) {
+		dev_dbg(dev->dev, "Unmasking consumer sync err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK;
+	}
+
+	if (errcfg.err.cons_wdg) {
+		dev_dbg(dev->dev, "Unmasking consumer wdg err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_CONS_WDG_ERR_MASK;
+	}
+
+	if (errcfg.err.ldiff) {
+		dev_dbg(dev->dev, "Unmasking ldiff_err err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_LDIFF;
+	}
+
+	if (errcfg.err.cdiff) {
+		dev_dbg(dev->dev, "Unmasking cdiff_err err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_CDIFF;
+	}
+
+	xlnxsync_clr(dev, channel->id, XLNXSYNC_IMR_REG, intr_unmask_val);
+
+	dev_dbg(dev->dev, "Channel num:%d IMR: %x\n", channel->id,
+		xlnxsync_read(dev, channel->id, XLNXSYNC_IMR_REG));
+
+	spin_unlock_irqrestore(&dev->irq_lock, flags);
+
+	return 0;
+}
+
+static int xlnxsync_chan_get_fbdone_status(struct xlnxsync_channel *channel,
+					   void __user *arg)
+{
+	struct xlnxsync_fbdone fbdone_stat;
+	int ret, i, j;
+	struct xlnxsync_device *dev = channel->dev;
+
+	fbdone_stat.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+
+	for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++)
+		for (j = 0; j < XLNXSYNC_IO; j++)
+			if (channel->l_done[i][j] &&
+			    channel->c_done[i][j])
+				fbdone_stat.status[i][j] = true;
+
+	ret = copy_to_user(arg, &fbdone_stat, sizeof(fbdone_stat));
+	if (ret)
+		dev_err(dev->dev, "%s: failed to copy result data to user\n",
+			__func__);
+
+	return ret;
+}
+
+static int xlnxsync_chan_clr_fbdone_status(struct xlnxsync_channel *channel,
+					   void __user *arg)
+{
+	struct xlnxsync_fbdone fbd;
+	int ret, i, j;
+	unsigned long flags;
+	struct xlnxsync_device *dev = channel->dev;
+
+	ret = copy_from_user(&fbd, arg, sizeof(fbd));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	if (fbd.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			fbd.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	/* Clear channel error status */
+	spin_lock_irqsave(&dev->irq_lock, flags);
+	for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+		for (j = 0; j < XLNXSYNC_IO; j++) {
+			fbd.status[i][j] = false;
+			channel->l_done[i][j] = false;
+			channel->c_done[i][j] = false;
+		}
+	}
+	spin_unlock_irqrestore(&dev->irq_lock, flags);
+
+	return 0;
+}
+
+static int xlnxsync_chan_set_int_mask(struct xlnxsync_channel *channel,
+				      void __user *arg)
+{
+	struct xlnxsync_device *dev = channel->dev;
+	struct xlnxsync_intr intr_mask;
+	u32 intr_mask_val = 0;
+	int ret;
+
+	ret = copy_from_user(&intr_mask, arg, sizeof(intr_mask));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	/* check driver header version */
+	if (intr_mask.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			intr_mask.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	if (intr_mask.err.prod_sync)
+		intr_mask_val |= XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK;
+	if (intr_mask.err.prod_wdg)
+		intr_mask_val |= XLNXSYNC_IMR_PROD_WDG_ERR_MASK;
+	if (intr_mask.err.cons_sync)
+		intr_mask_val |= XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK;
+	if (intr_mask.err.cons_wdg)
+		intr_mask_val |= XLNXSYNC_IMR_CONS_WDG_ERR_MASK;
+	if (intr_mask.err.ldiff)
+		intr_mask_val |= XLNXSYNC_IMR_LDIFF;
+	if (intr_mask.err.cdiff)
+		intr_mask_val |= XLNXSYNC_IMR_CDIFF;
+	if (intr_mask.prod_lfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_PLVALID_MASK;
+	if (intr_mask.prod_cfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_PCVALID_MASK;
+	if (intr_mask.cons_lfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_CLVALID_MASK;
+	if (intr_mask.cons_cfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_CCVALID_MASK;
+
+	dev_dbg(dev->dev, "Set interrupt mask: 0x%x for channel: %d\n",
+		intr_mask_val, channel->id);
+
+	xlnxsync_write(dev, channel->id, XLNXSYNC_IMR_REG, intr_mask_val);
+
+	return ret;
+}
+
+static long xlnxsync_ioctl(struct file *fptr, unsigned int cmd,
+			   unsigned long data)
+{
+	int ret = -EINVAL;
+	void __user *arg = (void __user *)data;
+	struct xlnxsync_channel *channel = fptr->private_data;
+	struct xlnxsync_device *xlnxsync_dev;
+
+	xlnxsync_dev = channel->dev;
+	if (!xlnxsync_dev) {
+		pr_err("%s: File op error\n", __func__);
+		return -EIO;
+	}
+
+	dev_dbg(xlnxsync_dev->dev, "ioctl = 0x%08x\n", cmd);
+
+	switch (cmd) {
+	case XLNXSYNC_GET_CFG:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_get_config(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_GET_STATUS:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_get_status(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_SET_CONFIG:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_config(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_ENABLE:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_enable(channel, true);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_DISABLE:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_enable(channel, false);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_CLR_ERR:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_clr_err(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_GET_FBDONE_STAT:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_get_fbdone_status(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_CLR_FBDONE_STAT:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_clr_fbdone_status(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_SET_INTR_MASK:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_set_int_mask(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_RESET_SLOT:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_reset_slot(channel);
+		mutex_unlock(&channel->mutex);
+		break;
+	}
+
+	return ret;
+}
+
+static __poll_t xlnxsync_poll(struct file *fptr, poll_table *wait)
+{
+	__poll_t ret = 0, req_events = poll_requested_events(wait);
+	struct xlnxsync_channel *channel = fptr->private_data;
+	struct xlnxsync_device *dev;
+	unsigned long flags;
+
+	dev = channel->dev;
+	if (!dev) {
+		pr_err("%s: File op error\n", __func__);
+		return -EIO;
+	}
+
+	dev_dbg_ratelimited(dev->dev, "%s : entered req_events = 0x%x!\n",
+			    __func__, req_events);
+
+	if (!(req_events & (POLLPRI | POLLIN)))
+		return 0;
+
+	if (req_events & EPOLLPRI) {
+		poll_wait(fptr, &channel->wq_error, wait);
+		spin_lock_irqsave(&dev->irq_lock, flags);
+		if (channel->err_event) {
+			dev_dbg_ratelimited(dev->dev,
+					    "%s : error event in chan = %d!\n",
+					     __func__, channel->id);
+			ret |= POLLPRI;
+			channel->err_event = false;
+		}
+		spin_unlock_irqrestore(&dev->irq_lock, flags);
+	}
+
+	if (req_events & EPOLLIN) {
+		poll_wait(fptr, &channel->wq_fbdone, wait);
+		spin_lock_irqsave(&dev->irq_lock, flags);
+		if (channel->framedone_event) {
+			dev_dbg_ratelimited(dev->dev,
+					    "%s : fbdone event in chan = %d!\n",
+					    __func__, channel->id);
+			ret |= POLLIN;
+			channel->framedone_event = false;
+		}
+		spin_unlock_irqrestore(&dev->irq_lock, flags);
+	}
+
+	return ret;
+}
+
+static int xlnxsync_open(struct inode *iptr, struct file *fptr)
+{
+	struct xlnxsync_device *dev;
+	struct xlnxsync_channel *chan;
+	unsigned int i;
+
+	dev = container_of(iptr->i_cdev, struct xlnxsync_device, chdev);
+	if (!dev) {
+		pr_err("%s: failed to get xlnxsync driver handle\n", __func__);
+		return -EAGAIN;
+	}
+
+	chan = devm_kzalloc(dev->dev, sizeof(*chan), GFP_KERNEL);
+	if (!chan)
+		return -ENOMEM;
+
+	if (mutex_lock_interruptible(&dev->sync_mutex))
+		return -ERESTARTSYS;
+	i = find_first_zero_bit_le(&dev->reserved, dev->config.max_channels);
+	if (i >= dev->config.max_channels) {
+		dev_err(dev->dev, "No free channel available\n");
+		mutex_unlock(&dev->sync_mutex);
+		return -ENOSPC;
+	}
+	dev_dbg(dev->dev, "Reserving channel %d\n", i);
+	set_bit(i, &dev->reserved);
+	chan->id = i;
+	chan->last_buf_used = -1;
+	list_add_tail(&chan->channel, &dev->channels);
+	chan->dev = dev;
+	fptr->private_data = chan;
+	mutex_init(&chan->mutex);
+	init_waitqueue_head(&chan->wq_fbdone);
+	init_waitqueue_head(&chan->wq_error);
+	dev->chan_count++;
+	atomic_inc(&dev->user_count);
+	dev_dbg(dev->dev, "%s: tid=%d Opened with user count = %d\n",
+		__func__, current->pid, atomic_read(&dev->user_count));
+	mutex_unlock(&dev->sync_mutex);
+
+	return 0;
+}
+
+static int xlnxsync_release(struct inode *iptr, struct file *fptr)
+{
+	struct xlnxsync_device *dev;
+	struct xlnxsync_channel *channel = fptr->private_data;
+
+	dev = container_of(iptr->i_cdev, struct xlnxsync_device, chdev);
+	if (!dev) {
+		pr_err("%s: failed to get xlnxsync driver handle", __func__);
+		return -EAGAIN;
+	}
+
+	dev_dbg(dev->dev, "%s: tid=%d user count = %d id = %d\n",
+		__func__, current->pid, atomic_read(&dev->user_count),
+		channel->id);
+
+	if (xlnxsync_read(dev, channel->id, XLNXSYNC_CTRL_REG) &
+			XLNXSYNC_CTRL_ENABLE_MASK) {
+		dev_dbg(dev->dev, "Disabling %d channel\n", channel->id);
+		xlnxsync_reset_chan(dev, channel->id);
+		xlnxsync_clr(dev, channel->id, XLNXSYNC_CTRL_REG,
+			     XLNXSYNC_CTRL_ENABLE_MASK |
+			     XLNXSYNC_CTRL_INTR_EN_MASK);
+	}
+
+	if (mutex_lock_interruptible(&dev->sync_mutex))
+		return -ERESTARTSYS;
+	clear_bit(channel->id, &dev->reserved);
+	dev->chan_count--;
+	list_del(&channel->channel);
+	mutex_unlock(&dev->sync_mutex);
+	devm_kfree(dev->dev, channel);
+
+	if (atomic_dec_and_test(&dev->user_count)) {
+		xlnxsync_reset(dev);
+		dev_dbg(dev->dev,
+			"%s: tid=%d Stopping and clearing device",
+			__func__, current->pid);
+	}
+
+	return 0;
+}
+
+static const struct file_operations xlnxsync_fops = {
+	.open = xlnxsync_open,
+	.release = xlnxsync_release,
+	.unlocked_ioctl = xlnxsync_ioctl,
+	.poll = xlnxsync_poll,
+};
+
+static irqreturn_t xlnxsync_irq_handler(int irq, void *data)
+{
+	struct xlnxsync_device *xlnxsync = (struct xlnxsync_device *)data;
+	u32 val;
+	u32 intr_mask_val = 0;
+	struct xlnxsync_channel *chan;
+
+	/*
+	 * Use simple spin_lock (instead of spin_lock_irqsave) as interrupt
+	 * is registered with irqf_oneshot and !irqf_shared
+	 */
+	spin_lock(&xlnxsync->irq_lock);
+	list_for_each_entry(chan, &xlnxsync->channels, channel) {
+		u32 i, j;
+
+		val = xlnxsync_read(xlnxsync, chan->id, XLNXSYNC_ISR_REG);
+
+		if (val & XLNXSYNC_ISR_PROD_SYNC_FAIL_MASK) {
+			chan->prod_sync_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK;
+		}
+		if (val & XLNXSYNC_ISR_PROD_WDG_ERR_MASK) {
+			chan->prod_wdg_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_PROD_WDG_ERR_MASK;
+		}
+		if (val & XLNXSYNC_ISR_LDIFF) {
+			chan->ldiff_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_LDIFF;
+		}
+		if (val & XLNXSYNC_ISR_CDIFF) {
+			chan->cdiff_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_CDIFF;
+		}
+		if (val & XLNXSYNC_ISR_CONS_SYNC_FAIL_MASK) {
+			chan->cons_sync_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK;
+		}
+		if (val & XLNXSYNC_ISR_CONS_WDG_ERR_MASK) {
+			chan->cons_wdg_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_CONS_WDG_ERR_MASK;
+		}
+		if (chan->prod_sync_err || chan->prod_wdg_err ||
+		    chan->ldiff_err || chan->cdiff_err ||
+		    chan->cons_sync_err || chan->cons_wdg_err)
+			chan->err_event = true;
+
+		if (val & XLNXSYNC_ISR_PLVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_PLDONE_MASK) >>
+				XLNXSYNC_ISR_PLDONE_SHIFT;
+
+			chan->l_done[i][XLNXSYNC_PROD] = true;
+		}
+
+		if (val & XLNXSYNC_ISR_PCVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_PCDONE_MASK) >>
+				XLNXSYNC_ISR_PCDONE_SHIFT;
+
+			chan->c_done[i][XLNXSYNC_PROD] = true;
+		}
+
+		if (val & XLNXSYNC_ISR_CLVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_CLDONE_MASK) >>
+				XLNXSYNC_ISR_CLDONE_SHIFT;
+
+			chan->l_done[i][XLNXSYNC_CONS] = true;
+		}
+
+		if (val & XLNXSYNC_ISR_CCVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_CCDONE_MASK) >>
+				XLNXSYNC_ISR_CCDONE_SHIFT;
+
+			chan->c_done[i][XLNXSYNC_CONS] = true;
+		}
+
+		for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+			for (j = 0; j < XLNXSYNC_IO; j++) {
+				if (chan->l_done[i][j] &&
+				    chan->c_done[i][j])
+					chan->framedone_event = true;
+			}
+		}
+
+		/* Mask corresponding interrupts */
+		if (intr_mask_val)
+			xlnxsync_set(xlnxsync, chan->id, XLNXSYNC_IMR_REG,
+				     intr_mask_val);
+
+		if (chan->err_event) {
+			dev_dbg(xlnxsync->dev, "%s : error occurred at channel->id = %d\n",
+				__func__, chan->id);
+			wake_up_interruptible(&chan->wq_error);
+		}
+
+		if (chan->framedone_event) {
+			dev_dbg_ratelimited(xlnxsync->dev, "%s : framedone occurred\n",
+					    __func__);
+			wake_up_interruptible(&chan->wq_fbdone);
+		}
+
+	}
+
+	spin_unlock(&xlnxsync->irq_lock);
+
+	return IRQ_HANDLED;
+}
+
+static int xlnxsync_parse_dt_prop(struct xlnxsync_device *xlnxsync)
+{
+	struct device_node *node = xlnxsync->dev->of_node;
+	int ret;
+
+	xlnxsync->config.encode = of_property_read_bool(node, "xlnx,encode");
+	dev_dbg(xlnxsync->dev, "synchronizer type = %s\n",
+		xlnxsync->config.encode ? "encode" : "decode");
+
+	ret = of_property_read_u32(node, "xlnx,num-chan",
+				   (u32 *)&xlnxsync->config.max_channels);
+	if (ret)
+		return ret;
+
+	dev_dbg(xlnxsync->dev, "max channels = %d\n",
+		xlnxsync->config.max_channels);
+
+	if (xlnxsync->config.max_channels == 0 ||
+	    xlnxsync->config.max_channels > XLNXSYNC_MAX_ENC_CHAN) {
+		dev_err(xlnxsync->dev, "Number of channels should be 1 to 4.\n");
+		dev_err(xlnxsync->dev, "Invalid number of channels : %d\n",
+			xlnxsync->config.max_channels);
+		return -EINVAL;
+	}
+
+	if (!xlnxsync->config.encode &&
+	    xlnxsync->config.max_channels > XLNXSYNC_MAX_DEC_CHAN) {
+		dev_err(xlnxsync->dev, "Decode can't have more than 2 channels.\n");
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+static int xlnxsync_clk_setup(struct xlnxsync_device *xlnxsync)
+{
+	int ret;
+
+	xlnxsync->axi_clk = devm_clk_get(xlnxsync->dev, "s_axi_ctrl_aclk");
+	if (IS_ERR(xlnxsync->axi_clk)) {
+		ret = PTR_ERR(xlnxsync->axi_clk);
+		dev_err(xlnxsync->dev, "failed to get axi_aclk (%d)\n", ret);
+		return ret;
+	}
+
+	xlnxsync->p_clk = devm_clk_get(xlnxsync->dev, "s_axi_mm_p_aclk");
+	if (IS_ERR(xlnxsync->p_clk)) {
+		ret = PTR_ERR(xlnxsync->p_clk);
+		dev_err(xlnxsync->dev, "failed to get p_aclk (%d)\n", ret);
+		return ret;
+	}
+
+	xlnxsync->c_clk = devm_clk_get(xlnxsync->dev, "s_axi_mm_aclk");
+	if (IS_ERR(xlnxsync->c_clk)) {
+		ret = PTR_ERR(xlnxsync->c_clk);
+		dev_err(xlnxsync->dev, "failed to get axi_mm (%d)\n", ret);
+		return ret;
+	}
+
+	ret = clk_prepare_enable(xlnxsync->axi_clk);
+	if (ret) {
+		dev_err(xlnxsync->dev, "failed to enable axi_clk (%d)\n", ret);
+		return ret;
+	}
+
+	ret = clk_prepare_enable(xlnxsync->p_clk);
+	if (ret) {
+		dev_err(xlnxsync->dev, "failed to enable p_clk (%d)\n", ret);
+		goto err_pclk;
+	}
+
+	ret = clk_prepare_enable(xlnxsync->c_clk);
+	if (ret) {
+		dev_err(xlnxsync->dev, "failed to enable axi_mm (%d)\n", ret);
+		goto err_cclk;
+	}
+
+	return ret;
+
+err_cclk:
+	clk_disable_unprepare(xlnxsync->p_clk);
+err_pclk:
+	clk_disable_unprepare(xlnxsync->axi_clk);
+
+	return ret;
+}
+
+static int xlnxsync_probe(struct platform_device *pdev)
+{
+	struct xlnxsync_device *xlnxsync;
+	struct device *dc;
+	struct resource *res;
+	int ret;
+
+	xlnxsync = devm_kzalloc(&pdev->dev, sizeof(*xlnxsync), GFP_KERNEL);
+	if (!xlnxsync)
+		return -ENOMEM;
+
+	xlnxsync->minor = ida_simple_get(&xs_ida, 0, XLNXSYNC_DEV_MAX,
+					 GFP_KERNEL);
+	if (xlnxsync->minor < 0)
+		return xlnxsync->minor;
+
+	xlnxsync->dev = &pdev->dev;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "Failed to get resource.\n");
+		return -ENODEV;
+	}
+
+	xlnxsync->iomem = devm_ioremap(xlnxsync->dev, res->start,
+				       resource_size(res));
+	if (!xlnxsync->iomem) {
+		dev_err(&pdev->dev, "ip register mapping failed.\n");
+		return -ENOMEM;
+	}
+
+	ret = xlnxsync_parse_dt_prop(xlnxsync);
+	if (ret < 0)
+		return ret;
+
+	xlnxsync->config.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+	dev_dbg(xlnxsync->dev, "ioctl header version = 0x%llx\n",
+		xlnxsync->config.hdr_ver);
+
+	xlnxsync->irq = irq_of_parse_and_map(xlnxsync->dev->of_node, 0);
+	if (!xlnxsync->irq) {
+		dev_err(xlnxsync->dev, "Unable to parse and get irq.\n");
+		return -EINVAL;
+	}
+	ret = devm_request_threaded_irq(xlnxsync->dev, xlnxsync->irq, NULL,
+					xlnxsync_irq_handler,
+					IRQF_ONESHOT | IRQF_TRIGGER_RISING,
+					dev_name(xlnxsync->dev), xlnxsync);
+
+	if (ret) {
+		dev_err(xlnxsync->dev, "Err = %d Interrupt handler reg failed!\n",
+			ret);
+		return ret;
+	}
+
+	ret = xlnxsync_clk_setup(xlnxsync);
+	if (ret) {
+		dev_err(xlnxsync->dev, "clock setup failed!\n");
+		return ret;
+	}
+
+	INIT_LIST_HEAD(&xlnxsync->channels);
+	spin_lock_init(&xlnxsync->irq_lock);
+
+	mutex_init(&xlnxsync->sync_mutex);
+
+	cdev_init(&xlnxsync->chdev, &xlnxsync_fops);
+	xlnxsync->chdev.owner = THIS_MODULE;
+	ret = cdev_add(&xlnxsync->chdev,
+		       MKDEV(MAJOR(xlnxsync_devt), xlnxsync->minor), 1);
+	if (ret < 0) {
+		dev_err(xlnxsync->dev, "cdev_add failed");
+		goto clk_err;
+	}
+
+	if (!xlnxsync_class) {
+		dev_err(xlnxsync->dev, "xvfsync device class not created");
+		goto cdev_err;
+	}
+	dc = device_create(xlnxsync_class, xlnxsync->dev,
+			   MKDEV(MAJOR(xlnxsync_devt), xlnxsync->minor),
+			   xlnxsync, "xlnxsync%d", xlnxsync->minor);
+	if (IS_ERR(dc)) {
+		ret = PTR_ERR(dc);
+		dev_err(xlnxsync->dev, "Unable to create device");
+		goto cdev_err;
+	}
+
+	ret = dma_set_mask(&pdev->dev, DMA_BIT_MASK(44));
+	if (ret) {
+		dev_err(&pdev->dev, "dma_set_mask: %d\n", ret);
+		return ret;
+	}
+
+	platform_set_drvdata(pdev, xlnxsync);
+	dev_info(xlnxsync->dev, "Xilinx Synchronizer probe successful!\n");
+
+	return 0;
+
+cdev_err:
+	cdev_del(&xlnxsync->chdev);
+clk_err:
+	clk_disable_unprepare(xlnxsync->c_clk);
+	clk_disable_unprepare(xlnxsync->p_clk);
+	clk_disable_unprepare(xlnxsync->axi_clk);
+	ida_simple_remove(&xs_ida, xlnxsync->minor);
+
+	return ret;
+}
+
+static void xlnxsync_remove(struct platform_device *pdev)
+{
+	struct xlnxsync_device *xlnxsync = platform_get_drvdata(pdev);
+
+	if (!xlnxsync || !xlnxsync_class)
+		return;
+
+	cdev_del(&xlnxsync->chdev);
+	clk_disable_unprepare(xlnxsync->c_clk);
+	clk_disable_unprepare(xlnxsync->p_clk);
+	clk_disable_unprepare(xlnxsync->axi_clk);
+	ida_simple_remove(&xs_ida, xlnxsync->minor);
+}
+
+static const struct of_device_id xlnxsync_of_match[] = {
+	{ .compatible = "xlnx,sync-ip-1.0", },
+	{ /* end of table*/ }
+};
+MODULE_DEVICE_TABLE(of, xlnxsync_of_match);
+
+static struct platform_driver xlnxsync_driver = {
+	.driver = {
+		.name = XLNXSYNC_DRIVER_NAME,
+		.of_match_table = xlnxsync_of_match,
+	},
+	.probe = xlnxsync_probe,
+	.remove = xlnxsync_remove,
+};
+
+static int __init xlnxsync_init_mod(void)
+{
+	int err;
+
+	xlnxsync_class = class_create(XLNXSYNC_DRIVER_NAME);
+	if (IS_ERR(xlnxsync_class)) {
+		pr_err("%s : Unable to create xlnxsync class", __func__);
+		return PTR_ERR(xlnxsync_class);
+	}
+	err = alloc_chrdev_region(&xlnxsync_devt, 0,
+				  XLNXSYNC_DEV_MAX, XLNXSYNC_DRIVER_NAME);
+	if (err < 0) {
+		pr_err("%s: Unable to get major number for xlnxsync", __func__);
+		goto err_class;
+	}
+	err = platform_driver_register(&xlnxsync_driver);
+	if (err < 0) {
+		pr_err("%s: Unable to register %s driver",
+		       __func__, XLNXSYNC_DRIVER_NAME);
+		goto err_pdrv;
+	}
+	return 0;
+err_pdrv:
+	unregister_chrdev_region(xlnxsync_devt, XLNXSYNC_DEV_MAX);
+err_class:
+	class_destroy(xlnxsync_class);
+	return err;
+}
+
+static void __exit xlnxsync_cleanup_mod(void)
+{
+	platform_driver_unregister(&xlnxsync_driver);
+	unregister_chrdev_region(xlnxsync_devt, XLNXSYNC_DEV_MAX);
+	class_destroy(xlnxsync_class);
+	xlnxsync_class = NULL;
+}
+module_init(xlnxsync_init_mod);
+module_exit(xlnxsync_cleanup_mod);
+
+MODULE_AUTHOR("Vishal Sagar");
+MODULE_DESCRIPTION("Xilinx Synchronizer IP Driver");
+MODULE_LICENSE("GPL");
+MODULE_VERSION(XLNXSYNC_DRIVER_VERSION);
+MODULE_IMPORT_NS(DMA_BUF);
diff --git a/drivers/staging/xroeframer/Kconfig b/drivers/staging/xroeframer/Kconfig
new file mode 100644
index 000000000..0aaf47478
--- /dev/null
+++ b/drivers/staging/xroeframer/Kconfig
@@ -0,0 +1,18 @@
+#
+# Xilinx Radio over Ethernet Framer driver
+#
+
+config XROE_FRAMER
+	tristate "Xilinx Radio over Ethernet Framer driver"
+	help
+	  The "Radio Over Ethernet Framer" IP (roe_framer) ingests/generates
+	  Ethernet packet data, (de-)multiplexes packets based on protocol
+	  into/from various Radio Antenna data streams.
+
+	  It has 2 main, independent, data paths:
+
+	  - Downlink, from the BaseBand to the Phone, Ethernet to Antenna,
+	  we call this the De-Framer path, or defm on all related IP signals.
+
+	  - Uplink, from the Phone to the BaseBand, Antenna to Ethernet,
+	  we call this the Framer path, or fram on all related IP signals.
diff --git a/drivers/staging/xroeframer/Makefile b/drivers/staging/xroeframer/Makefile
new file mode 100644
index 000000000..f7bf07e98
--- /dev/null
+++ b/drivers/staging/xroeframer/Makefile
@@ -0,0 +1,12 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for Radio over Ethernet Framer driver
+#
+obj-$(CONFIG_XROE_FRAMER)	:= framer.o
+
+framer-objs := 	xroe_framer.o \
+		sysfs_xroe.o \
+		sysfs_xroe_framer_ipv4.o \
+		sysfs_xroe_framer_ipv6.o \
+		sysfs_xroe_framer_udp.o \
+		sysfs_xroe_framer_stats.o
diff --git a/drivers/staging/xroeframer/README b/drivers/staging/xroeframer/README
new file mode 100644
index 000000000..505a46c2c
--- /dev/null
+++ b/drivers/staging/xroeframer/README
@@ -0,0 +1,47 @@
+Xilinx Radio over Ethernet Framer driver
+=========================================
+
+About the RoE Framer
+
+The "Radio Over Ethernet Framer" IP (roe_framer) ingests/generates Ethernet
+packet data, (de-)multiplexes packets based on protocol into/from various
+Radio Antenna data streams.
+
+It has 2 main, independent, data paths
+
+- Downlink, from the BaseBand to the Phone, Ethernet to Antenna,
+we call this the De-Framer path, or defm on all related IP signals.
+
+- Uplink, from the Phone to the BaseBand, Antenna to Ethernet,
+we call this the Framer path, or fram on all related IP signals.
+
+Key points:
+
+- Apart from the AXI4-Lite configuration port and a handful of strobe/control
+signals all data interfaces are AXI Stream(AXIS).
+- The IP does not contain an Ethernet MAC IP, rather it routes, or creates
+packets based on the direction through the roe_framer.
+- Currently designed to work with
+	- 1, 2 or 4 10G Ethernet AXIS stream ports to/from 1, 2, 4, 8, 16,
+	or 32 antenna ports
+		Note: each Ethernet port is 64 bit data @ 156.25MHz
+	- 1 or 2 25G Ethernet AXIS stream ports to/from 1, 2, 4, 8, 16,
+	or 32 antenna ports
+		Note: each Ethernet port is 64 bit data @ 390.25MHz
+- Contains a filter so that all non-protocol packets, or non-hardware-IP
+processed packets can be forwarded to another block for processing. In general
+this in a Microprocessor, specifically the Zynq ARM in our case. This filter
+function can move into the optional switch when TSN is used.
+
+About the Linux Driver
+
+The RoE Framer Linux Driver provides sysfs access to the framer controls. The
+loading of the driver to the hardware is possible using Device Tree binding
+(see "dt-binding.txt" for more information). When the driver is loaded, the
+general controls (such as framing mode, enable, restart etc) are exposed
+under /sys/kernel/xroe. Furthermore, specific controls can be found under
+/sys/kernel/xroe/framer. These include protocol-specific settings, for
+IPv4, IPv6 & UDP.
+
+There is also the option of accessing the framer's register map using
+ioctl calls for both reading and writing (where permitted) directly.
diff --git a/drivers/staging/xroeframer/dt-binding.txt b/drivers/staging/xroeframer/dt-binding.txt
new file mode 100644
index 000000000..8dabef16d
--- /dev/null
+++ b/drivers/staging/xroeframer/dt-binding.txt
@@ -0,0 +1,17 @@
+* Xilinx Radio over Ethernet Framer driver
+
+Required properties:
+- compatible: must be "xlnx,roe-framer-1.0"
+- reg: physical base address of the framer and length of memory mapped region
+- clock-names: list of clock names
+- clocks: list of clock sources corresponding to the clock names
+
+Example:
+	roe_framer@a0000000 {
+		compatible = "xlnx,roe-framer-1.0";
+		reg = <0x0 0xa0000000 0x0 0x10000>;
+		clock-names = "s_axi_aclk", "m_axis_defm_aclk",
+			      "s_axis_fram_aclk", "tx0_eth_port_clk",
+			      "internal_bus_clk";
+		clocks = <0x43 0x44 0x44 0x45 0x45>;
+	};
diff --git a/drivers/staging/xroeframer/roe_framer_ctrl.h b/drivers/staging/xroeframer/roe_framer_ctrl.h
new file mode 100644
index 000000000..162c49a9b
--- /dev/null
+++ b/drivers/staging/xroeframer/roe_framer_ctrl.h
@@ -0,0 +1,1088 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+/*-----------------------------------------------------------------------------
+ * C Header bank BASE definitions
+ *-----------------------------------------------------------------------------
+ */
+#define ROE_FRAMER_V1_0_CFG_BASE_ADDR 0x0 /* 0 */
+#define ROE_FRAMER_V1_0_FRAM_BASE_ADDR 0x2000 /* 8192 */
+#define ROE_FRAMER_V1_0_FRAM_DRP_BASE_ADDR 0x4000 /* 16384 */
+#define ROE_FRAMER_V1_0_DEFM_BASE_ADDR 0x6000 /* 24576 */
+#define ROE_FRAMER_V1_0_DEFM_DRP_BASE_ADDR 0x8000 /* 32768 */
+#define ROE_FRAMER_V1_0_ETH_BASE_ADDR 0xa000 /* 40960 */
+#define ROE_FRAMER_V1_0_STATS_BASE_ADDR 0xc000 /* 49152 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_cfg
+ * with prefix cfg_ @ address 0x0
+ *-----------------------------------------------------------------------------
+ */
+/* Type = roInt */
+#define CFG_MAJOR_REVISION_ADDR 0x0 /* 0 */
+#define CFG_MAJOR_REVISION_MASK 0xff000000 /* 4278190080 */
+#define CFG_MAJOR_REVISION_OFFSET 0x18 /* 24 */
+#define CFG_MAJOR_REVISION_WIDTH 0x8 /* 8 */
+#define CFG_MAJOR_REVISION_DEFAULT 0x1 /* 1 */
+
+/* Type = roInt */
+#define CFG_MINOR_REVISION_ADDR 0x0 /* 0 */
+#define CFG_MINOR_REVISION_MASK 0xff0000 /* 16711680 */
+#define CFG_MINOR_REVISION_OFFSET 0x10 /* 16 */
+#define CFG_MINOR_REVISION_WIDTH 0x8 /* 8 */
+#define CFG_MINOR_REVISION_DEFAULT 0x0 /* 0 */
+
+/* Type = roInt */
+#define CFG_VERSION_REVISION_ADDR 0x0 /* 0 */
+#define CFG_VERSION_REVISION_MASK 0xff00 /* 65280 */
+#define CFG_VERSION_REVISION_OFFSET 0x8 /* 8 */
+#define CFG_VERSION_REVISION_WIDTH 0x8 /* 8 */
+#define CFG_VERSION_REVISION_DEFAULT 0x0 /* 0 */
+
+/* Type = roInt */
+#define CFG_INTERNAL_REVISION_ADDR 0x4 /* 4 */
+#define CFG_INTERNAL_REVISION_MASK 0xffffffff /* 4294967295 */
+#define CFG_INTERNAL_REVISION_OFFSET 0x0 /* 0 */
+#define CFG_INTERNAL_REVISION_WIDTH 0x20 /* 32 */
+#define CFG_INTERNAL_REVISION_DEFAULT 0x12345678 /* 305419896 */
+
+/* Type = rw */
+#define CFG_TIMEOUT_VALUE_ADDR 0x8 /* 8 */
+#define CFG_TIMEOUT_VALUE_MASK 0xfff /* 4095 */
+#define CFG_TIMEOUT_VALUE_OFFSET 0x0 /* 0 */
+#define CFG_TIMEOUT_VALUE_WIDTH 0xc /* 12 */
+#define CFG_TIMEOUT_VALUE_DEFAULT 0x80 /* 128 */
+
+/* Type = rw */
+#define CFG_USER_RW_OUT_ADDR 0xc /* 12 */
+#define CFG_USER_RW_OUT_MASK 0xff /* 255 */
+#define CFG_USER_RW_OUT_OFFSET 0x0 /* 0 */
+#define CFG_USER_RW_OUT_WIDTH 0x8 /* 8 */
+#define CFG_USER_RW_OUT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_USER_RO_IN_ADDR 0xc /* 12 */
+#define CFG_USER_RO_IN_MASK 0xff0000 /* 16711680 */
+#define CFG_USER_RO_IN_OFFSET 0x10 /* 16 */
+#define CFG_USER_RO_IN_WIDTH 0x8 /* 8 */
+#define CFG_USER_RO_IN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_MASTER_INT_ENABLE_ADDR 0x10 /* 16 */
+#define CFG_MASTER_INT_ENABLE_MASK 0x1 /* 1 */
+#define CFG_MASTER_INT_ENABLE_OFFSET 0x0 /* 0 */
+#define CFG_MASTER_INT_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_MASTER_INT_ENABLE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_FRAM_FIFO_OF_ENABLE_ADDR 0x14 /* 20 */
+#define CFG_FRAM_FIFO_OF_ENABLE_MASK 0x1 /* 1 */
+#define CFG_FRAM_FIFO_OF_ENABLE_OFFSET 0x0 /* 0 */
+#define CFG_FRAM_FIFO_OF_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_FIFO_OF_ENABLE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_FRAM_FIFO_UF_ENABLE_ADDR 0x14 /* 20 */
+#define CFG_FRAM_FIFO_UF_ENABLE_MASK 0x2 /* 2 */
+#define CFG_FRAM_FIFO_UF_ENABLE_OFFSET 0x1 /* 1 */
+#define CFG_FRAM_FIFO_UF_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_FIFO_UF_ENABLE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_AXI_TIMEOUT_ENABLE_ADDR 0x14 /* 20 */
+#define CFG_AXI_TIMEOUT_ENABLE_MASK 0x80000000 /* 2147483648 */
+#define CFG_AXI_TIMEOUT_ENABLE_OFFSET 0x1f /* 31 */
+#define CFG_AXI_TIMEOUT_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_AXI_TIMEOUT_ENABLE_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define CFG_INTERRUPT_STATUS_SAMPLE_ADDR 0x1c /* 28 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_MASK 0x1 /* 1 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_OFFSET 0x0 /* 0 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_WIDTH 0x1 /* 1 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_DEFAULT 0x1 /* 1 */
+
+/* Type = roSig */
+#define CFG_FRAM_RESET_STATUS_ADDR 0x18 /* 24 */
+#define CFG_FRAM_RESET_STATUS_MASK 0x1 /* 1 */
+#define CFG_FRAM_RESET_STATUS_OFFSET 0x0 /* 0 */
+#define CFG_FRAM_RESET_STATUS_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_RESET_STATUS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_DEFM_RESET_STATUS_ADDR 0x18 /* 24 */
+#define CFG_DEFM_RESET_STATUS_MASK 0x2 /* 2 */
+#define CFG_DEFM_RESET_STATUS_OFFSET 0x1 /* 1 */
+#define CFG_DEFM_RESET_STATUS_WIDTH 0x1 /* 1 */
+#define CFG_DEFM_RESET_STATUS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ANT_OF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_MASK 0x100 /* 256 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_OFFSET 0x8 /* 8 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ETH_OF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_MASK 0x200 /* 512 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_OFFSET 0x9 /* 9 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ANT_UF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_MASK 0x400 /* 1024 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_OFFSET 0xa /* 10 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ETH_UF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_MASK 0x800 /* 2048 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_OFFSET 0xb /* 11 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_AXI_TIMEOUT_STATUS_ADDR 0x18 /* 24 */
+#define CFG_AXI_TIMEOUT_STATUS_MASK 0x80000000 /* 2147483648 */
+#define CFG_AXI_TIMEOUT_STATUS_OFFSET 0x1f /* 31 */
+#define CFG_AXI_TIMEOUT_STATUS_WIDTH 0x1 /* 1 */
+#define CFG_AXI_TIMEOUT_STATUS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_ADDR 0x20 /* 32 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_MASK 0xffff /* 65535 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_OFFSET 0x0 /* 0 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_WIDTH 0x10 /* 16 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_ADDR 0x20 /* 32 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_MASK 0xffff0000 /* 4294901760 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_OFFSET 0x10 /* 16 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_WIDTH 0x10 /* 16 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_ADDR 0x24 /* 36 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_MASK 0x3ff /* 1023 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_OFFSET 0x0 /* 0 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_WIDTH 0xa /* 10 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_ETH_SPEED_ADDR 0x24 /* 36 */
+#define CFG_CONFIG_ETH_SPEED_MASK 0x3ff0000 /* 67043328 */
+#define CFG_CONFIG_ETH_SPEED_OFFSET 0x10 /* 16 */
+#define CFG_CONFIG_ETH_SPEED_WIDTH 0xa /* 10 */
+#define CFG_CONFIG_ETH_SPEED_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_fram
+ * with prefix fram_ @ address 0x2000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rwpdef */
+#define FRAM_DISABLE_ADDR 0x2000 /* 8192 */
+#define FRAM_DISABLE_MASK 0x1 /* 1 */
+#define FRAM_DISABLE_OFFSET 0x0 /* 0 */
+#define FRAM_DISABLE_WIDTH 0x1 /* 1 */
+#define FRAM_DISABLE_DEFAULT 0x1 /* 1 */
+
+/* Type = roSig */
+#define FRAM_READY_ADDR 0x2000 /* 8192 */
+#define FRAM_READY_MASK 0x2 /* 2 */
+#define FRAM_READY_OFFSET 0x1 /* 1 */
+#define FRAM_READY_WIDTH 0x1 /* 1 */
+#define FRAM_READY_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define FRAM_FIFO_FULL_INDICATOR_ADDR 0x2004 /* 8196 */
+#define FRAM_FIFO_FULL_INDICATOR_MASK 0xffffffff /* 4294967295 */
+#define FRAM_FIFO_FULL_INDICATOR_OFFSET 0x0 /* 0 */
+#define FRAM_FIFO_FULL_INDICATOR_WIDTH 0x20 /* 32 */
+#define FRAM_FIFO_FULL_INDICATOR_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_MIN_ADDR 0x2020 /* 8224 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_MAX_ADDR 0x2024 /* 8228 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_ADDR 0x2028 /* 8232 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_DEFAULT 0x75 /* 117 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_ADDR 0x202c /* 8236 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_ADDR 0x2030 /* 8240 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_ADDR 0x2034 /* 8244 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_ADDR 0x2038 /* 8248 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_ADDR 0x203c /* 8252 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_ADDR 0x2050 /* 8272 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_ADDR 0x2054 /* 8276 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_ADDR 0x2058 /* 8280 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_DEFAULT 0x75 /* 117 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_ADDR 0x205c /* 8284 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_ADDR 0x2060 /* 8288 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_ADDR 0x2064 /* 8292 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_ADDR 0x2068 /* 8296 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_ADDR 0x206c /* 8300 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_PROTOCOL_DEFINITION_ADDR 0x2200 /* 8704 */
+#define FRAM_PROTOCOL_DEFINITION_MASK 0xf /* 15 */
+#define FRAM_PROTOCOL_DEFINITION_OFFSET 0x0 /* 0 */
+#define FRAM_PROTOCOL_DEFINITION_WIDTH 0x4 /* 4 */
+#define FRAM_PROTOCOL_DEFINITION_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_GEN_VLAN_TAG_ADDR 0x2200 /* 8704 */
+#define FRAM_GEN_VLAN_TAG_MASK 0x10 /* 16 */
+#define FRAM_GEN_VLAN_TAG_OFFSET 0x4 /* 4 */
+#define FRAM_GEN_VLAN_TAG_WIDTH 0x1 /* 1 */
+#define FRAM_GEN_VLAN_TAG_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_ADDR 0x2200 /* 8704 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_MASK 0x60 /* 96 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_OFFSET 0x5 /* 5 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_WIDTH 0x2 /* 2 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_fram_drp
+ * with prefix fram_drp @ address 0x4000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rw */
+#define FRAM_DRPFRAM_DATA_PC_ID_ADDR 0x4000 /* 16384 */
+#define FRAM_DRPFRAM_DATA_PC_ID_MASK 0xffff /* 65535 */
+#define FRAM_DRPFRAM_DATA_PC_ID_OFFSET 0x0 /* 0 */
+#define FRAM_DRPFRAM_DATA_PC_ID_WIDTH 0x10 /* 16 */
+#define FRAM_DRPFRAM_DATA_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_ADDR 0x4000 /* 16384 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_MASK 0xff0000 /* 16711680 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_OFFSET 0x10 /* 16 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_ADDR 0x4000 /* 16384 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_MASK 0xff000000 /* 4278190080 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_OFFSET 0x18 /* 24 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_CTRL_PC_ID_ADDR 0x4400 /* 17408 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_MASK 0xffff /* 65535 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_OFFSET 0x0 /* 0 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_WIDTH 0x10 /* 16 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_ADDR 0x4400 /* 17408 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_MASK 0xff0000 /* 16711680 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_OFFSET 0x10 /* 16 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_ADDR 0x4400 /* 17408 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_MASK 0xff000000 /* 4278190080 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_OFFSET 0x18 /* 24 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_defm
+ * with prefix defm_ @ address 0x6000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rw */
+#define DEFM_RESTART_ADDR 0x6000 /* 24576 */
+#define DEFM_RESTART_MASK 0x1 /* 1 */
+#define DEFM_RESTART_OFFSET 0x0 /* 0 */
+#define DEFM_RESTART_WIDTH 0x1 /* 1 */
+#define DEFM_RESTART_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_READY_ADDR 0x6000 /* 24576 */
+#define DEFM_READY_MASK 0x2 /* 2 */
+#define DEFM_READY_OFFSET 0x1 /* 1 */
+#define DEFM_READY_WIDTH 0x1 /* 1 */
+#define DEFM_READY_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_ERR_PACKET_FILTER_ADDR 0x6004 /* 24580 */
+#define DEFM_ERR_PACKET_FILTER_MASK 0x3 /* 3 */
+#define DEFM_ERR_PACKET_FILTER_OFFSET 0x0 /* 0 */
+#define DEFM_ERR_PACKET_FILTER_WIDTH 0x2 /* 2 */
+#define DEFM_ERR_PACKET_FILTER_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_ADDR 0x6008 /* 24584 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_MASK 0xff /* 255 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_OFFSET 0x0 /* 0 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_ADDR 0x600c /* 24588 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_MASK 0xff /* 255 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_OFFSET 0x0 /* 0 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_LOW_CNT_MIN_ADDR 0x6020 /* 24608 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_LOW_CNT_MAX_ADDR 0x6024 /* 24612 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_ADDR 0x602c /* 24620 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_ADDR 0x6030 /* 24624 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_ADDR 0x6034 /* 24628 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_ADDR 0x603c /* 24636 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_ADDR 0x6050 /* 24656 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_ADDR 0x6054 /* 24660 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_ADDR 0x605c /* 24668 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_ADDR 0x6060 /* 24672 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_ADDR 0x6064 /* 24676 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_ADDR 0x606c /* 24684 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_31_0_ADDR 0x6100 /* 24832 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_63_32_ADDR 0x6104 /* 24836 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_95_64_ADDR 0x6108 /* 24840 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_127_96_ADDR 0x610c /* 24844 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_DEFAULT 0xfffffeae /* 4294966958 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_MASK_ADDR 0x6110 /* 24848 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_DEFAULT 0xcfff /* 53247 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_31_0_ADDR 0x6120 /* 24864 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_63_32_ADDR 0x6124 /* 24868 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_95_64_ADDR 0x6128 /* 24872 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_127_96_ADDR 0x612c /* 24876 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_MASK_ADDR 0x6130 /* 24880 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_DEFAULT 0xffff /* 65535 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_31_0_ADDR 0x6140 /* 24896 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_63_32_ADDR 0x6144 /* 24900 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_95_64_ADDR 0x6148 /* 24904 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_127_96_ADDR 0x614c /* 24908 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_MASK_ADDR 0x6150 /* 24912 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_DEFAULT 0xffff /* 65535 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_31_0_ADDR 0x6160 /* 24928 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_63_32_ADDR 0x6164 /* 24932 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_95_64_ADDR 0x6168 /* 24936 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_127_96_ADDR 0x616c /* 24940 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_MASK_ADDR 0x6170 /* 24944 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_DEFAULT 0xffff /* 65535 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_defm_drp
+ * with prefix defm_drp @ address 0x8000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rw */
+#define DEFM_DRPDEFM_DATA_PC_ID_ADDR 0x8000 /* 32768 */
+#define DEFM_DRPDEFM_DATA_PC_ID_MASK 0xffff /* 65535 */
+#define DEFM_DRPDEFM_DATA_PC_ID_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_DATA_PC_ID_WIDTH 0x10 /* 16 */
+#define DEFM_DRPDEFM_DATA_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_DRPDEFM_CTRL_PC_ID_ADDR 0x8400 /* 33792 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_MASK 0xffff /* 65535 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_WIDTH 0x10 /* 16 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_MASK 0xffffff /* 16777215 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_WIDTH 0x18 /* 24 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_MASK 0x1000000 /* 16777216 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_OFFSET 0x18 /* 24 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_MASK 0x2000000 /* 33554432 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_OFFSET 0x19 /* 25 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_MASK 0x4000000 /* 67108864 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_OFFSET 0x1a /* 26 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_MASK 0x8000000 /* 134217728 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_OFFSET 0x1b /* 27 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_MASK 0xf0000000 /* 4026531840 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_OFFSET 0x1c /* 28 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_WIDTH 0x4 /* 4 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_MASK 0xffffff /* 16777215 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_WIDTH 0x18 /* 24 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_MASK 0x1000000 /* 16777216 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_OFFSET 0x18 /* 24 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_MASK 0x2000000 /* 33554432 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_OFFSET 0x19 /* 25 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_MASK 0x4000000 /* 67108864 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_OFFSET 0x1a /* 26 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_MASK 0x8000000 /* 134217728 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_OFFSET 0x1b /* 27 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_MASK 0xf0000000 /* 4026531840 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_OFFSET 0x1c /* 28 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_WIDTH 0x4 /* 4 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_eth
+ * with prefix eth_ @ address 0xa000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rwpdef */
+#define ETH_DEST_ADDR_31_0_ADDR 0xa000 /* 40960 */
+#define ETH_DEST_ADDR_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_DEST_ADDR_31_0_OFFSET 0x0 /* 0 */
+#define ETH_DEST_ADDR_31_0_WIDTH 0x20 /* 32 */
+#define ETH_DEST_ADDR_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_DEST_ADDR_47_32_ADDR 0xa004 /* 40964 */
+#define ETH_DEST_ADDR_47_32_MASK 0xffff /* 65535 */
+#define ETH_DEST_ADDR_47_32_OFFSET 0x0 /* 0 */
+#define ETH_DEST_ADDR_47_32_WIDTH 0x10 /* 16 */
+#define ETH_DEST_ADDR_47_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_SRC_ADDR_31_0_ADDR 0xa008 /* 40968 */
+#define ETH_SRC_ADDR_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_SRC_ADDR_31_0_OFFSET 0x0 /* 0 */
+#define ETH_SRC_ADDR_31_0_WIDTH 0x20 /* 32 */
+#define ETH_SRC_ADDR_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_SRC_ADDR_47_32_ADDR 0xa00c /* 40972 */
+#define ETH_SRC_ADDR_47_32_MASK 0xffff /* 65535 */
+#define ETH_SRC_ADDR_47_32_OFFSET 0x0 /* 0 */
+#define ETH_SRC_ADDR_47_32_WIDTH 0x10 /* 16 */
+#define ETH_SRC_ADDR_47_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_VLAN_ID_ADDR 0xa010 /* 40976 */
+#define ETH_VLAN_ID_MASK 0xfff /* 4095 */
+#define ETH_VLAN_ID_OFFSET 0x0 /* 0 */
+#define ETH_VLAN_ID_WIDTH 0xc /* 12 */
+#define ETH_VLAN_ID_DEFAULT 0x1 /* 1 */
+
+/* Type = rwpdef */
+#define ETH_VLAN_DEI_ADDR 0xa010 /* 40976 */
+#define ETH_VLAN_DEI_MASK 0x1000 /* 4096 */
+#define ETH_VLAN_DEI_OFFSET 0xc /* 12 */
+#define ETH_VLAN_DEI_WIDTH 0x1 /* 1 */
+#define ETH_VLAN_DEI_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_VLAN_PCP_ADDR 0xa010 /* 40976 */
+#define ETH_VLAN_PCP_MASK 0xe000 /* 57344 */
+#define ETH_VLAN_PCP_OFFSET 0xd /* 13 */
+#define ETH_VLAN_PCP_WIDTH 0x3 /* 3 */
+#define ETH_VLAN_PCP_DEFAULT 0x7 /* 7 */
+
+/* Type = rw */
+#define ETH_IPV4_VERSION_ADDR 0xa030 /* 41008 */
+#define ETH_IPV4_VERSION_MASK 0xf /* 15 */
+#define ETH_IPV4_VERSION_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_VERSION_WIDTH 0x4 /* 4 */
+#define ETH_IPV4_VERSION_DEFAULT 0x4 /* 4 */
+
+/* Type = rw */
+#define ETH_IPV4_IHL_ADDR 0xa030 /* 41008 */
+#define ETH_IPV4_IHL_MASK 0xf0 /* 240 */
+#define ETH_IPV4_IHL_OFFSET 0x4 /* 4 */
+#define ETH_IPV4_IHL_WIDTH 0x4 /* 4 */
+#define ETH_IPV4_IHL_DEFAULT 0x5 /* 5 */
+
+/* Type = rw */
+#define ETH_IPV4_DSCP_ADDR 0xa034 /* 41012 */
+#define ETH_IPV4_DSCP_MASK 0x3f /* 63 */
+#define ETH_IPV4_DSCP_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_DSCP_WIDTH 0x6 /* 6 */
+#define ETH_IPV4_DSCP_DEFAULT 0x2e /* 46 */
+
+/* Type = rw */
+#define ETH_IPV4_ECN_ADDR 0xa034 /* 41012 */
+#define ETH_IPV4_ECN_MASK 0xc0 /* 192 */
+#define ETH_IPV4_ECN_OFFSET 0x6 /* 6 */
+#define ETH_IPV4_ECN_WIDTH 0x2 /* 2 */
+#define ETH_IPV4_ECN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV4_ID_ADDR 0xa038 /* 41016 */
+#define ETH_IPV4_ID_MASK 0xffff /* 65535 */
+#define ETH_IPV4_ID_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_ID_WIDTH 0x10 /* 16 */
+#define ETH_IPV4_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV4_FLAGS_ADDR 0xa03c /* 41020 */
+#define ETH_IPV4_FLAGS_MASK 0x7 /* 7 */
+#define ETH_IPV4_FLAGS_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_FLAGS_WIDTH 0x3 /* 3 */
+#define ETH_IPV4_FLAGS_DEFAULT 0x2 /* 2 */
+
+/* Type = rw */
+#define ETH_IPV4_FRAGMENT_OFFSET_ADDR 0xa03c /* 41020 */
+#define ETH_IPV4_FRAGMENT_OFFSET_MASK 0x1fff8 /* 131064 */
+#define ETH_IPV4_FRAGMENT_OFFSET_OFFSET 0x3 /* 3 */
+#define ETH_IPV4_FRAGMENT_OFFSET_WIDTH 0xe /* 14 */
+#define ETH_IPV4_FRAGMENT_OFFSET_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV4_TIME_TO_LIVE_ADDR 0xa040 /* 41024 */
+#define ETH_IPV4_TIME_TO_LIVE_MASK 0xff /* 255 */
+#define ETH_IPV4_TIME_TO_LIVE_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_TIME_TO_LIVE_WIDTH 0x8 /* 8 */
+#define ETH_IPV4_TIME_TO_LIVE_DEFAULT 0x40 /* 64 */
+
+/* Type = rw */
+#define ETH_IPV4_PROTOCOL_ADDR 0xa044 /* 41028 */
+#define ETH_IPV4_PROTOCOL_MASK 0xff /* 255 */
+#define ETH_IPV4_PROTOCOL_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_PROTOCOL_WIDTH 0x8 /* 8 */
+#define ETH_IPV4_PROTOCOL_DEFAULT 0x11 /* 17 */
+
+/* Type = rwpdef */
+#define ETH_IPV4_SOURCE_ADD_ADDR 0xa048 /* 41032 */
+#define ETH_IPV4_SOURCE_ADD_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV4_SOURCE_ADD_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_SOURCE_ADD_WIDTH 0x20 /* 32 */
+#define ETH_IPV4_SOURCE_ADD_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV4_DESTINATION_ADD_ADDR 0xa04c /* 41036 */
+#define ETH_IPV4_DESTINATION_ADD_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV4_DESTINATION_ADD_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_DESTINATION_ADD_WIDTH 0x20 /* 32 */
+#define ETH_IPV4_DESTINATION_ADD_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_UDP_SOURCE_PORT_ADDR 0xa050 /* 41040 */
+#define ETH_UDP_SOURCE_PORT_MASK 0xffff /* 65535 */
+#define ETH_UDP_SOURCE_PORT_OFFSET 0x0 /* 0 */
+#define ETH_UDP_SOURCE_PORT_WIDTH 0x10 /* 16 */
+#define ETH_UDP_SOURCE_PORT_DEFAULT 0x8000 /* 32768 */
+
+/* Type = rw */
+#define ETH_UDP_DESTINATION_PORT_ADDR 0xa050 /* 41040 */
+#define ETH_UDP_DESTINATION_PORT_MASK 0xffff0000 /* 4294901760 */
+#define ETH_UDP_DESTINATION_PORT_OFFSET 0x10 /* 16 */
+#define ETH_UDP_DESTINATION_PORT_WIDTH 0x10 /* 16 */
+#define ETH_UDP_DESTINATION_PORT_DEFAULT 0xc000 /* 49152 */
+
+/* Type = rw */
+#define ETH_IPV6_V_ADDR 0xa080 /* 41088 */
+#define ETH_IPV6_V_MASK 0xf /* 15 */
+#define ETH_IPV6_V_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_V_WIDTH 0x4 /* 4 */
+#define ETH_IPV6_V_DEFAULT 0x6 /* 6 */
+
+/* Type = rw */
+#define ETH_IPV6_TRAFFIC_CLASS_ADDR 0xa084 /* 41092 */
+#define ETH_IPV6_TRAFFIC_CLASS_MASK 0xff /* 255 */
+#define ETH_IPV6_TRAFFIC_CLASS_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_TRAFFIC_CLASS_WIDTH 0x8 /* 8 */
+#define ETH_IPV6_TRAFFIC_CLASS_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV6_FLOW_LABEL_ADDR 0xa088 /* 41096 */
+#define ETH_IPV6_FLOW_LABEL_MASK 0xfffff /* 1048575 */
+#define ETH_IPV6_FLOW_LABEL_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_FLOW_LABEL_WIDTH 0x14 /* 20 */
+#define ETH_IPV6_FLOW_LABEL_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV6_NEXT_HEADER_ADDR 0xa08c /* 41100 */
+#define ETH_IPV6_NEXT_HEADER_MASK 0xff /* 255 */
+#define ETH_IPV6_NEXT_HEADER_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_NEXT_HEADER_WIDTH 0x8 /* 8 */
+#define ETH_IPV6_NEXT_HEADER_DEFAULT 0x11 /* 17 */
+
+/* Type = rw */
+#define ETH_IPV6_HOP_LIMIT_ADDR 0xa090 /* 41104 */
+#define ETH_IPV6_HOP_LIMIT_MASK 0xff /* 255 */
+#define ETH_IPV6_HOP_LIMIT_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_HOP_LIMIT_WIDTH 0x8 /* 8 */
+#define ETH_IPV6_HOP_LIMIT_DEFAULT 0x40 /* 64 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_31_0_ADDR 0xa094 /* 41108 */
+#define ETH_IPV6_SOURCE_ADD_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_31_0_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_31_0_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_63_32_ADDR 0xa098 /* 41112 */
+#define ETH_IPV6_SOURCE_ADD_63_32_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_63_32_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_63_32_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_63_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_95_64_ADDR 0xa09c /* 41116 */
+#define ETH_IPV6_SOURCE_ADD_95_64_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_95_64_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_95_64_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_95_64_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_127_96_ADDR 0xa0a0 /* 41120 */
+#define ETH_IPV6_SOURCE_ADD_127_96_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_127_96_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_127_96_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_127_96_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_31_0_ADDR 0xa0a4 /* 41124 */
+#define ETH_IPV6_DEST_ADD_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_31_0_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_31_0_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_63_32_ADDR 0xa0a8 /* 41128 */
+#define ETH_IPV6_DEST_ADD_63_32_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_63_32_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_63_32_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_63_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_95_64_ADDR 0xa0ac /* 41132 */
+#define ETH_IPV6_DEST_ADD_95_64_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_95_64_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_95_64_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_95_64_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_127_96_ADDR 0xa0b0 /* 41136 */
+#define ETH_IPV6_DEST_ADD_127_96_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_127_96_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_127_96_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_127_96_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_stats
+ * with prefix stats_ @ address 0xc000
+ *------------------------------------------------------------------------------
+ */
+/* Type = roSig */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_ADDR 0xc000 /* 49152 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_ADDR 0xc004 /* 49156 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_ADDR 0xc008 /* 49160 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_PACKETS_CNT_ADDR 0xc00c /* 49164 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_ADDR 0xc010 /* 49168 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_ADDR 0xc014 /* 49172 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_ADDR 0xc018 /* 49176 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_ADDR 0xc01c /* 49180 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_ADDR 0xc020 /* 49184 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_ADDR 0xc024 /* 49188 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_ADDR 0xc028 /* 49192 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_PKTS_RATE_ADDR 0xc02c /* 49196 */
+#define STATS_USER_DATA_RX_PKTS_RATE_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_PKTS_RATE_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_PKTS_RATE_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_PKTS_RATE_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_PKTS_RATE_ADDR 0xc030 /* 49200 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_DEFAULT 0x0 /* 0 */
diff --git a/drivers/staging/xroeframer/sysfs_xroe.c b/drivers/staging/xroeframer/sysfs_xroe.c
new file mode 100644
index 000000000..837283abd
--- /dev/null
+++ b/drivers/staging/xroeframer/sysfs_xroe.c
@@ -0,0 +1,562 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 15 };
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+/**
+ * version_show - Returns the block's revision number
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the revision string
+ *
+ * Returns the block's major, minor & version revision numbers
+ * in a %d.%d.%d format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t version_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buff)
+{
+	u32 major_rev;
+	u32 minor_rev;
+	u32 version_rev;
+
+	major_rev = utils_sysfs_show_wrapper(CFG_MAJOR_REVISION_ADDR,
+					     CFG_MAJOR_REVISION_OFFSET,
+					     CFG_MAJOR_REVISION_MASK, kobj);
+	minor_rev = utils_sysfs_show_wrapper(CFG_MINOR_REVISION_ADDR,
+					     CFG_MINOR_REVISION_OFFSET,
+					     CFG_MINOR_REVISION_MASK, kobj);
+	version_rev = utils_sysfs_show_wrapper(CFG_VERSION_REVISION_ADDR,
+					       CFG_VERSION_REVISION_OFFSET,
+					       CFG_VERSION_REVISION_MASK, kobj);
+	sprintf(buff, "%d.%d.%d\n", major_rev, minor_rev, version_rev);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * version_store - Writes to the framer version sysfs entry (not permitted)
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the revision string
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the framer version sysfs entry (not permitted)
+ *
+ * Return: 0
+ */
+static ssize_t version_store(struct  kobject *kobj, struct kobj_attribute *attr,
+			     const char *buff, size_t count)
+{
+	return 0;
+}
+
+/**
+ * enable_show - Returns the framer's enable status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the enable status
+ *
+ * Reads and writes the framer's enable status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t enable_show(struct kobject *kobj, struct kobj_attribute *attr,
+			   char *buff)
+{
+	u32 enable;
+
+	enable = utils_sysfs_show_wrapper(CFG_MASTER_INT_ENABLE_ADDR,
+					  CFG_MASTER_INT_ENABLE_OFFSET,
+					  CFG_MASTER_INT_ENABLE_MASK, kobj);
+	if (enable)
+		sprintf(buff, "true\n");
+	else
+		sprintf(buff, "false\n");
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * enable_store - Writes to the framer's enable status register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the enable status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the framer's enable status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t enable_store(struct kobject *kobj, struct kobj_attribute *attr,
+			    const char *buff, size_t count)
+{
+	u32 enable = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0)
+		enable = 1;
+	else if (strncmp(xroe_tmp, "false", xroe_size) == 0)
+		enable = 0;
+	utils_sysfs_store_wrapper(CFG_MASTER_INT_ENABLE_ADDR,
+				  CFG_MASTER_INT_ENABLE_OFFSET,
+				  CFG_MASTER_INT_ENABLE_MASK, enable, kobj);
+	return xroe_size;
+}
+
+/**
+ * framer_restart_show - Returns the framer's restart status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ *
+ * Reads and writes the framer's restart status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t framer_restart_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	u32 restart;
+
+	restart = utils_sysfs_show_wrapper(FRAM_DISABLE_ADDR,
+					   FRAM_DISABLE_OFFSET,
+					   FRAM_DISABLE_MASK, kobj);
+	if (restart)
+		sprintf(buff, "true\n");
+
+	else
+		sprintf(buff, "false\n");
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * framer_restart_store - Writes to the framer's restart status register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the framer's restart status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t framer_restart_store(struct  kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buff, size_t count)
+{
+	u32 restart = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0)
+		restart = 0x01;
+	else if (strncmp(xroe_tmp, "false", xroe_size) == 0)
+		restart = 0x00;
+	utils_sysfs_store_wrapper(FRAM_DISABLE_ADDR, FRAM_DISABLE_OFFSET,
+				  FRAM_DISABLE_MASK, restart, kobj);
+	return xroe_size;
+}
+
+/**
+ * deframer_restart_show - Returns the deframer's restart status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ *
+ * Reads and writes the deframer's restart status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t deframer_restart_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	u32 offset = DEFM_RESTART_OFFSET;
+	u32 mask = DEFM_RESTART_MASK;
+	u32 buffer = 0;
+	u32 restart = 0;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr
+	+ DEFM_RESTART_ADDR);
+
+	buffer = ioread32(working_address);
+	restart = (buffer & mask) >> offset;
+
+	if (restart)
+		sprintf(buff, "true\n");
+
+	else
+		sprintf(buff, "false\n");
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * deframer_restart_store - Writes to the deframer's restart status register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the deframer's restart status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t deframer_restart_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buff, size_t count)
+{
+	u32 offset = DEFM_RESTART_OFFSET;
+	u32 mask = DEFM_RESTART_MASK;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr
+	+ DEFM_RESTART_ADDR);
+	u32 restart = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0) {
+		restart = 0x01;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	} else if (strncmp(xroe_tmp, "false", xroe_size) == 0) {
+		restart = 0x00;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	}
+
+	return xroe_size;
+}
+
+/**
+ * xxv_reset_show - Returns the XXV's reset status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ *
+ * Reads and writes the XXV's reset status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t xxv_reset_show(struct kobject *kobj, struct kobj_attribute *attr,
+			      char *buff)
+{
+	u32 offset = CFG_USER_RW_OUT_OFFSET;
+	u32 mask = CFG_USER_RW_OUT_MASK;
+	u32 buffer = 0;
+	u32 restart = 0;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr +
+	CFG_USER_RW_OUT_ADDR);
+
+	buffer = ioread32(working_address);
+	restart = (buffer & mask) >> offset;
+	if (restart)
+		sprintf(buff, "true\n");
+	else
+		sprintf(buff, "false\n");
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * xxv_reset_store - Writes to the XXV's reset register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the XXV's reset status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t xxv_reset_store(struct  kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buff, size_t count)
+{
+	u32 offset = CFG_USER_RW_OUT_OFFSET;
+	u32 mask = CFG_USER_RW_OUT_MASK;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr +
+	CFG_USER_RW_OUT_ADDR);
+	u32 restart = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0) {
+		restart = 0x01;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	} else if (strncmp(xroe_tmp, "false", xroe_size) == 0) {
+		restart = 0x00;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	}
+	return xroe_size;
+}
+
+/**
+ * framing_show - Returns the current framing
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ *
+ * Reads and writes the current framing type to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t framing_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buff)
+{
+	u32 offset = (DEFM_DATA_PKT_MESSAGE_TYPE_ADDR +
+	DEFM_DATA_PKT_MESSAGE_TYPE_OFFSET);
+	u8 buffer = 0;
+	u8 framing = 0xff;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr + offset);
+
+	buffer = ioread8(working_address);
+	framing = buffer;
+	if (framing == 0)
+		sprintf(buff, "eCPRI\n");
+	else if (framing == 1)
+		sprintf(buff, "1914.3\n");
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * framing_store - Writes to the current framing register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the current framing
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t framing_store(struct kobject *kobj, struct kobj_attribute *attr,
+			     const char *buff, size_t count)
+{
+	u32 offset = (DEFM_DATA_PKT_MESSAGE_TYPE_ADDR +
+	DEFM_DATA_PKT_MESSAGE_TYPE_OFFSET);
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr + offset);
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "eCPRI", xroe_size) == 0)
+		iowrite8(0, working_address);
+	else if (strncmp(xroe_tmp, "1914.3", xroe_size) == 0)
+		iowrite8(1, working_address);
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute version_attribute =
+	__ATTR(version, 0444, version_show, version_store);
+
+static struct kobj_attribute enable_attribute =
+	__ATTR(enable, 0660, enable_show, enable_store);
+
+static struct kobj_attribute framer_restart =
+	__ATTR(framer_restart, 0660, framer_restart_show, framer_restart_store);
+
+static struct kobj_attribute deframer_restart =
+	__ATTR(deframer_restart, 0660, deframer_restart_show,
+	       deframer_restart_store);
+
+static struct kobj_attribute xxv_reset =
+	__ATTR(xxv_reset, 0660, xxv_reset_show, xxv_reset_store);
+
+static struct kobj_attribute framing_attribute =
+	__ATTR(framing, 0660, framing_show, framing_store);
+
+static struct attribute *attrs[] = {
+	&version_attribute.attr,
+	&enable_attribute.attr,
+	&framer_restart.attr,
+	&deframer_restart.attr,
+	&xxv_reset.attr,
+	&framing_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+struct kobject *root_xroe_kobj;
+
+/**
+ * xroe_sysfs_init - Creates the xroe sysfs directory and entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs directory and entries, as well as the
+ * subdirectories for IPv4, IPv6 & UDP
+ */
+int xroe_sysfs_init(void)
+{
+	int ret;
+
+	root_xroe_kobj = kobject_create_and_add("xroe", kernel_kobj);
+	if (!root_xroe_kobj)
+		return -ENOMEM;
+	ret = sysfs_create_group(root_xroe_kobj, &attr_group);
+	if (ret)
+		kobject_put(root_xroe_kobj);
+	ret = xroe_sysfs_ipv4_init();
+	if (ret)
+		return ret;
+	ret = xroe_sysfs_ipv6_init();
+	if (ret)
+		return ret;
+	ret = xroe_sysfs_udp_init();
+	if (ret)
+		return ret;
+	ret = xroe_sysfs_stats_init();
+	return ret;
+}
+
+/**
+ * xroe_sysfs_exit - Deletes the xroe sysfs directory and entries
+ *
+ * Deletes the xroe sysfs directory and entries, as well as the
+ * subdirectories for IPv4, IPv6 & UDP
+ *
+ */
+void xroe_sysfs_exit(void)
+{
+	int i;
+
+	xroe_sysfs_ipv4_exit();
+	xroe_sysfs_ipv6_exit();
+	xroe_sysfs_udp_exit();
+	xroe_sysfs_stats_exit();
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_eth_ports[i]);
+	kobject_put(kobj_framer);
+	kobject_put(root_xroe_kobj);
+}
+
+/**
+ * utils_write32withmask - Writes a masked 32-bit value
+ * @working_address:	The starting address to write
+ * @value:			The value to be written
+ * @mask:			The mask to be used
+ * @offset:			The offset from the provided starting address
+ *
+ * Writes a 32-bit value to the provided address with the input mask
+ *
+ * Return: 0 on success
+ */
+int utils_write32withmask(void __iomem *working_address, u32 value,
+			  u32 mask, u32 offset)
+{
+	u32 read_register_value = 0;
+	u32 register_value_to_write = 0;
+	u32 delta = 0, buffer = 0;
+
+	read_register_value = ioread32(working_address);
+	buffer = (value << offset);
+	register_value_to_write = read_register_value & ~mask;
+	delta = buffer & mask;
+	register_value_to_write |= delta;
+	iowrite32(register_value_to_write, working_address);
+	return 0;
+}
+
+/**
+ * utils_sysfs_path_to_eth_port_num - Get the current ethernet port
+ * @kobj:	The kobject of the entry calling the function
+ *
+ * Extracts the number of the current ethernet port instance
+ *
+ * Return: The number of the ethernet port instance (0 - MAX_NUM_ETH_PORTS) on
+ * success, -1 otherwise
+ */
+static int utils_sysfs_path_to_eth_port_num(struct kobject *kobj)
+{
+	char *current_path = NULL;
+	int port;
+	int ret;
+
+	current_path = kobject_get_path(kobj, GFP_KERNEL);
+	ret = sscanf(current_path, "/kernel/xroe/framer/eth_port_%d/", &port);
+	/* if sscanf() returns 0, no fields were assigned, therefore no
+	 * adjustments will be made for port number
+	 */
+	if (ret == 0)
+		port = 0;
+//	printk(KERN_ALERT "current_path: %s port: %d\n", current_path, port);
+	kfree(current_path);
+	return port;
+}
+
+/**
+ * utils_sysfs_store_wrapper - Wraps the storing function for sysfs entries
+ * @address:	The address of the register to be written
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be written
+ * @value:	The value to be written to the register
+ * @kobj:	The kobject of the entry calling the function
+ *
+ * Wraps the core functionality of all "store" functions of sysfs entries.
+ * After calculating the ethernet port number (in N/A cases, it's 0), the value
+ * is written to the designated register
+ *
+ */
+void utils_sysfs_store_wrapper(u32 address, u32 offset, u32 mask, u32 value,
+			       struct kobject *kobj)
+{
+	int port;
+	void __iomem *working_address;
+
+	port = utils_sysfs_path_to_eth_port_num(kobj);
+	working_address = (void __iomem *)(xroe_lp->base_addr +
+			  (address + (0x100 * port)));
+	utils_write32withmask(working_address, value, mask, offset);
+}
+
+/**
+ * utils_sysfs_show_wrapper - Wraps the storing function for sysfs entries
+ * @address:	The address of the register to be read
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be read
+ * @kobj:	The kobject of the entry calling the function
+ *
+ * Wraps the core functionality of all "show" functions of sysfs entries.
+ * After calculating the ethernet port number (in N/A cases, it's 0), the value
+ * is read from the designated register and returned.
+ *
+ * Return: The value designated by the address, offset and mask
+ */
+u32 utils_sysfs_show_wrapper(u32 address, u32 offset, u32 mask,
+			     struct kobject *kobj)
+{
+	int port;
+	void __iomem *working_address;
+	u32 buffer;
+
+	port = utils_sysfs_path_to_eth_port_num(kobj);
+	working_address = (void __iomem *)(xroe_lp->base_addr +
+			  (address + (0x100 * port)));
+	buffer = ioread32(working_address);
+	return (buffer & mask) >> offset;
+}
diff --git a/drivers/staging/xroeframer/sysfs_xroe_framer_ipv4.c b/drivers/staging/xroeframer/sysfs_xroe_framer_ipv4.c
new file mode 100644
index 000000000..aaaefb10c
--- /dev/null
+++ b/drivers/staging/xroeframer/sysfs_xroe_framer_ipv4.c
@@ -0,0 +1,718 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 15 };
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+static void utils_ipv4addr_hextochar(u32 ip, unsigned char *bytes);
+static int utils_ipv4addr_chartohex(char *ip_addr, uint32_t *p_ip_addr);
+
+/**
+ * ipv4_version_show - Returns the IPv4 version number
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 version number
+ *
+ * Returns the IPv4 version number
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_version_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	u32 version;
+
+	version = utils_sysfs_show_wrapper(ETH_IPV4_VERSION_ADDR,
+					   ETH_IPV4_VERSION_OFFSET,
+					   ETH_IPV4_VERSION_MASK, kobj);
+	sprintf(buff, "%d\n", version);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_version_store - Writes to the IPv4 version number sysfs entry
+ * (not permitted)
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 version
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 version number sysfs entry (not permitted)
+ *
+ * Return: 0
+ */
+static ssize_t ipv4_version_store(struct kobject *kobj,
+				  struct kobj_attribute *attr, const char *buff,
+				  size_t count)
+{
+	return 0;
+}
+
+/**
+ * ipv4_ihl_show - Returns the IPv4 IHL
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 IHL
+ *
+ * Returns the IPv4 IHL
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_ihl_show(struct kobject *kobj,
+			     struct kobj_attribute *attr, char *buff)
+{
+	u32 ihl;
+
+	ihl = utils_sysfs_show_wrapper(ETH_IPV4_IHL_ADDR, ETH_IPV4_IHL_OFFSET,
+				       ETH_IPV4_IHL_MASK, kobj);
+	sprintf(buff, "%d\n", ihl);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_ihl_store - Writes to the IPv4 IHL sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 IHL
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 IHL sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_ihl_store(struct kobject *kobj,
+			      struct kobj_attribute *attr, const char *buff,
+			      size_t count)
+{
+	int ret;
+	u32 ihl;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &ihl);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_IHL_ADDR, ETH_IPV4_IHL_OFFSET,
+				  ETH_IPV4_IHL_MASK, ihl, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_dscp_show - Returns the IPv4 DSCP
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 DSCP
+ *
+ * Returns the IPv4 DSCP
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_dscp_show(struct kobject *kobj,
+			      struct kobj_attribute *attr, char *buff)
+{
+	u32 dscp;
+
+	dscp = utils_sysfs_show_wrapper(ETH_IPV4_DSCP_ADDR,
+					ETH_IPV4_DSCP_OFFSET,
+					ETH_IPV4_DSCP_MASK, kobj);
+	sprintf(buff, "%d\n", dscp);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_dscp_store - Writes to the IPv4 DSCP sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 DSCP
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 DSCP sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_dscp_store(struct kobject *kobj,
+			       struct kobj_attribute *attr, const char *buff,
+			       size_t count)
+{
+	int ret;
+	u32 dscp;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &dscp);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_DSCP_ADDR, ETH_IPV4_DSCP_OFFSET,
+				  ETH_IPV4_DSCP_MASK, dscp, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_ecn_show - Returns the IPv4 ECN
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ECN
+ *
+ * Returns the IPv4 ECN
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_ecn_show(struct kobject *kobj,
+			     struct kobj_attribute *attr, char *buff)
+{
+	u32 ecn;
+
+	ecn = utils_sysfs_show_wrapper(ETH_IPV4_ECN_ADDR, ETH_IPV4_ECN_OFFSET,
+				       ETH_IPV4_ECN_MASK, kobj);
+	sprintf(buff, "%d\n", ecn);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_ecn_store - Writes to the IPv4 ECN sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ECN
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 ECN sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_ecn_store(struct kobject *kobj,
+			      struct kobj_attribute *attr, const char *buff,
+			      size_t count)
+{
+	int ret;
+	u32 ecn;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &ecn);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_ECN_ADDR, ETH_IPV4_ECN_OFFSET,
+				  ETH_IPV4_ECN_MASK, ecn, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_id_show - Returns the IPv4 ID
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ID
+ *
+ * Returns the IPv4 ID
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_id_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buff)
+{
+	u32 id;
+
+	id = utils_sysfs_show_wrapper(ETH_IPV4_ID_ADDR, ETH_IPV4_ID_OFFSET,
+				      ETH_IPV4_ID_MASK, kobj);
+	sprintf(buff, "%d\n", id);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_id_store - Writes to the IPv4 ID sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ID
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 ID sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_id_store(struct kobject *kobj,
+			     struct kobj_attribute *attr, const char *buff,
+			     size_t count)
+{
+	int ret;
+	u32 id;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &id);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_ID_ADDR, ETH_IPV4_ID_OFFSET,
+				  ETH_IPV4_ID_MASK, id, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_flags_show - Returns the IPv4 flags
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 flags
+ *
+ * Returns the IPv4 flags
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_flags_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buff)
+{
+	u32 flags;
+
+	flags = utils_sysfs_show_wrapper(ETH_IPV4_FLAGS_ADDR,
+					 ETH_IPV4_FLAGS_OFFSET,
+					 ETH_IPV4_FLAGS_MASK, kobj);
+	sprintf(buff, "%d\n", flags);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_flags_store - Writes to the IPv4 flags sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 flags
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 flags sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_flags_store(struct kobject *kobj,
+				struct kobj_attribute *attr, const char *buff,
+				size_t count)
+{
+	int ret;
+	u32 flags;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &flags);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_FLAGS_ADDR, ETH_IPV4_FLAGS_OFFSET,
+				  ETH_IPV4_FLAGS_MASK, flags, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_fragment_offset_show - Returns the IPv4 fragment offset
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 fragment offset
+ *
+ * Returns the IPv4 fragment offset
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_fragment_offset_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 fragment;
+
+	fragment = utils_sysfs_show_wrapper(ETH_IPV4_FRAGMENT_OFFSET_ADDR,
+					    ETH_IPV4_FRAGMENT_OFFSET_OFFSET,
+					    ETH_IPV4_FRAGMENT_OFFSET_MASK,
+					    kobj);
+	sprintf(buff, "%d\n", fragment);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_fragment_offset_store - Writes to the IPv4 fragment offset sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 fragment offset
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 fragment offset sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_fragment_offset_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	int ret;
+	u32 fragment;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &fragment);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_FRAGMENT_OFFSET_ADDR,
+				  ETH_IPV4_FRAGMENT_OFFSET_OFFSET,
+				  ETH_IPV4_FRAGMENT_OFFSET_MASK, fragment,
+				  kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_ttl_show - Returns the IPv4 TTL
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 TTL
+ *
+ * Returns the IPv4 TTL
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_ttl_show(struct kobject *kobj, struct kobj_attribute *attr,
+			     char *buff)
+{
+	u32 ttl;
+
+	ttl = utils_sysfs_show_wrapper(ETH_IPV4_TIME_TO_LIVE_ADDR,
+				       ETH_IPV4_TIME_TO_LIVE_OFFSET,
+				       ETH_IPV4_TIME_TO_LIVE_MASK, kobj);
+	sprintf(buff, "%d\n", ttl);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_ttl_store - Writes to the IPv4 TTL sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 TTL
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 TTL sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_ttl_store(struct kobject *kobj,
+			      struct kobj_attribute *attr, const char *buff,
+			      size_t count)
+{
+	int ret;
+	u32 ttl;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &ttl);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_TIME_TO_LIVE_ADDR,
+				  ETH_IPV4_TIME_TO_LIVE_OFFSET,
+				  ETH_IPV4_TIME_TO_LIVE_MASK, ttl, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_protocol_show - Returns the IPv4 protocol
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 protocol
+ *
+ * Returns the IPv4 protocol
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_protocol_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buff)
+{
+	u32 protocol;
+
+	protocol = utils_sysfs_show_wrapper(ETH_IPV4_PROTOCOL_ADDR,
+					    ETH_IPV4_PROTOCOL_OFFSET,
+					    ETH_IPV4_PROTOCOL_MASK, kobj);
+	sprintf(buff, "%d\n", protocol);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_protocol_store - Writes to the IPv4 protocol sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 protocol
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 protocol sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_protocol_store(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   const char *buff, size_t count)
+{
+	int ret;
+	u32 protocol;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &protocol);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_PROTOCOL_ADDR,
+				  ETH_IPV4_PROTOCOL_OFFSET,
+				  ETH_IPV4_PROTOCOL_MASK, protocol, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_source_address_show - Returns the IPv4 source address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ *
+ * Returns the IPv4 source address in x.x.x.x format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_source_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 source_add = 0;
+	unsigned char ip_addr_char[4];
+
+	source_add = utils_sysfs_show_wrapper(ETH_IPV4_SOURCE_ADD_ADDR,
+					      ETH_IPV4_SOURCE_ADD_OFFSET,
+					      ETH_IPV4_SOURCE_ADD_MASK, kobj);
+	utils_ipv4addr_hextochar(source_add, ip_addr_char);
+	sprintf(buff, "%d.%d.%d.%d\n", ip_addr_char[3], ip_addr_char[2],
+		ip_addr_char[1], ip_addr_char[0]);
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_source_address_store - Writes to the IPv4 source address sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 source address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_source_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 source_add = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv4addr_chartohex(xroe_tmp, &source_add) == 4)
+		utils_sysfs_store_wrapper(ETH_IPV4_SOURCE_ADD_ADDR,
+					  ETH_IPV4_SOURCE_ADD_OFFSET,
+					  ETH_IPV4_SOURCE_ADD_MASK, source_add,
+					  kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_destination_address_show - Returns the IPv4 destination address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ *
+ * Returns the IPv4 destination address in x.x.x.x format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_destination_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 dest_add = 0;
+	unsigned char ip_addr_char[4];
+
+	dest_add = utils_sysfs_show_wrapper(ETH_IPV4_DESTINATION_ADD_ADDR,
+					    ETH_IPV4_DESTINATION_ADD_OFFSET,
+					    ETH_IPV4_DESTINATION_ADD_MASK,
+					    kobj);
+	utils_ipv4addr_hextochar(dest_add, ip_addr_char);
+	sprintf(buff, "%d.%d.%d.%d\n", ip_addr_char[3], ip_addr_char[2],
+		ip_addr_char[1], ip_addr_char[0]);
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_destination_address_store - Writes to the IPv4 destination address
+ * sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 destination address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_destination_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 dest_add = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv4addr_chartohex(xroe_tmp, &dest_add) == 4)
+		utils_sysfs_store_wrapper(ETH_IPV4_DESTINATION_ADD_ADDR,
+					  ETH_IPV4_DESTINATION_ADD_OFFSET,
+					  ETH_IPV4_DESTINATION_ADD_MASK,
+					  dest_add, kobj);
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute version_attribute =
+	__ATTR(version, 0444, ipv4_version_show, ipv4_version_store);
+static struct kobj_attribute ihl_attribute =
+	__ATTR(ihl, 0660, ipv4_ihl_show, ipv4_ihl_store);
+static struct kobj_attribute dscp_attribute =
+	__ATTR(dscp, 0660, ipv4_dscp_show, ipv4_dscp_store);
+static struct kobj_attribute ecn_attribute =
+	__ATTR(ecn, 0660, ipv4_ecn_show, ipv4_ecn_store);
+static struct kobj_attribute id_attribute =
+	__ATTR(id, 0660, ipv4_id_show, ipv4_id_store);
+static struct kobj_attribute flags_attribute =
+	__ATTR(flags, 0660, ipv4_flags_show, ipv4_flags_store);
+static struct kobj_attribute fragment_offset_attribute =
+	__ATTR(fragment_offset, 0660, ipv4_fragment_offset_show,
+	       ipv4_fragment_offset_store);
+static struct kobj_attribute ttl_attribute =
+	__ATTR(ttl, 0660, ipv4_ttl_show, ipv4_ttl_store);
+static struct kobj_attribute protocol_attribute =
+	__ATTR(protocol, 0660, ipv4_protocol_show, ipv4_protocol_store);
+static struct kobj_attribute source_add_attribute =
+	__ATTR(source_add, 0660, ipv4_source_address_show,
+	       ipv4_source_address_store);
+static struct kobj_attribute destination_add_attribute =
+	__ATTR(dest_add, 0660, ipv4_destination_address_show,
+	       ipv4_destination_address_store);
+
+static struct attribute *attrs[] = {
+	&version_attribute.attr,
+	&ihl_attribute.attr,
+	&dscp_attribute.attr,
+	&ecn_attribute.attr,
+	&id_attribute.attr,
+	&flags_attribute.attr,
+	&fragment_offset_attribute.attr,
+	&ttl_attribute.attr,
+	&protocol_attribute.attr,
+	&source_add_attribute.attr,
+	&destination_add_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+struct kobject *kobj_framer;
+static struct kobject *kobj_ipv4[MAX_NUM_ETH_PORTS];
+struct kobject *kobj_eth_ports[MAX_NUM_ETH_PORTS];
+
+/**
+ * xroe_sysfs_ipv4_init - Creates the xroe sysfs "ipv4" subdirectory & entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "ipv4" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_ipv4_init(void)
+{
+	int ret;
+	int i;
+	char eth_port_dir_name[11];
+
+	kobj_framer = kobject_create_and_add("framer", root_xroe_kobj);
+	if (!kobj_framer)
+		return -ENOMEM;
+	for (i = 0; i < 4; i++) {
+		snprintf(eth_port_dir_name, sizeof(eth_port_dir_name),
+			 "eth_port_%d", i);
+		kobj_eth_ports[i] = kobject_create_and_add(eth_port_dir_name,
+							   kobj_framer);
+		if (!kobj_eth_ports[i])
+			return -ENOMEM;
+		kobj_ipv4[i] = kobject_create_and_add("ipv4",
+						      kobj_eth_ports[i]);
+		if (!kobj_ipv4[i])
+			return -ENOMEM;
+		ret = sysfs_create_group(kobj_ipv4[i], &attr_group);
+		if (ret)
+			kobject_put(kobj_ipv4[i]);
+	}
+	return ret;
+}
+
+/**
+ * xroe_sysfs_ipv4_exit - Deletes the xroe sysfs "ipv4" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "ipv4" subdirectory and entries,
+ * under the "xroe" entry
+ */
+void xroe_sysfs_ipv4_exit(void)
+{
+	int i;
+
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_ipv4[i]);
+}
+
+/**
+ * utils_ipv4addr_hextochar - Integer to char array for IPv4 addresses
+ * @ip:		The IP address in integer format
+ * @bytes:	The IP address in a 4-byte array
+ *
+ * Coverts an IPv4 address given in unsigned integer format to a character array
+ */
+static void utils_ipv4addr_hextochar(u32 ip, unsigned char *bytes)
+{
+	bytes[0] = ip & 0xFF;
+	bytes[1] = (ip >> 8) & 0xFF;
+	bytes[2] = (ip >> 16) & 0xFF;
+	bytes[3] = (ip >> 24) & 0xFF;
+}
+
+/**
+ * utils_ipv4addr_chartohex - Character to char array for IPv4 addresses
+ * @ip_addr:	The character array containing the IP address
+ * @p_ip_addr:	The converted IPv4 address
+ *
+ * Coverts an IPv4 address given as a character array to integer format
+ *
+ * Return: 4 (the length of the resulting character array) on success,
+ * -1 in case of wrong input
+ */
+static int utils_ipv4addr_chartohex(char *ip_addr, uint32_t *p_ip_addr)
+{
+	int count = 0, ret = -1;
+	char *string;
+	unsigned char *found;
+	u32 byte_array[4];
+	u32 byte = 0;
+
+	string = ip_addr;
+	while ((found = (unsigned char *)strsep(&string, ".")) != NULL) {
+		if (count <= 4) {
+			ret = kstrtouint(found, 10, &byte);
+			if (ret)
+				return ret;
+			byte_array[count] = byte;
+		} else {
+			break;
+		}
+		count++;
+	}
+
+	if (count == 4) {
+		ret = count;
+		*p_ip_addr = byte_array[3] | (byte_array[2] << 8)
+		| (byte_array[1] << 16) | (byte_array[0] << 24);
+	}
+	return ret;
+}
diff --git a/drivers/staging/xroeframer/sysfs_xroe_framer_ipv6.c b/drivers/staging/xroeframer/sysfs_xroe_framer_ipv6.c
new file mode 100644
index 000000000..c58e53279
--- /dev/null
+++ b/drivers/staging/xroeframer/sysfs_xroe_framer_ipv6.c
@@ -0,0 +1,571 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 60 };
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+static void utils_ipv6addr_32to16(u32 *ip32, uint16_t *ip16);
+static int utils_ipv6addr_chartohex(char *ip_addr, uint32_t *p_ip_addr);
+
+/**
+ * ipv6_version_show - Returns the IPv6 version number
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 version number
+ *
+ * Returns the IPv6 version number
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_version_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	u32 version;
+
+	version = utils_sysfs_show_wrapper(ETH_IPV6_V_ADDR, ETH_IPV6_V_OFFSET,
+					   ETH_IPV6_V_MASK, kobj);
+	sprintf(buff, "%d\n", version);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_version_store - Writes to the IPv6 version number sysfs entry
+ * (not permitted)
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 version
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 version number sysfs entry (not permitted)
+ *
+ * Return: 0
+ */
+static ssize_t ipv6_version_store(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  const char *buff, size_t count)
+{
+	return 0;
+}
+
+/**
+ * ipv6_traffic_class_show - Returns the IPv6 traffic class
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 traffic class
+ *
+ * Returns the IPv6 traffic class
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_traffic_class_show(struct kobject *kobj,
+				       struct kobj_attribute *attr, char *buff)
+{
+	u32 traffic_class;
+
+	traffic_class = utils_sysfs_show_wrapper(ETH_IPV6_TRAFFIC_CLASS_ADDR,
+						 ETH_IPV6_TRAFFIC_CLASS_OFFSET,
+						 ETH_IPV6_TRAFFIC_CLASS_MASK,
+						 kobj);
+	sprintf(buff, "%d\n", traffic_class);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_traffic_class_store - Writes to the IPv6 traffic class
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 traffic class
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 traffic class sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_traffic_class_store(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					const char *buff, size_t count)
+{
+	int ret;
+	u32 traffic_class;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &traffic_class);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_TRAFFIC_CLASS_ADDR,
+				  ETH_IPV6_TRAFFIC_CLASS_OFFSET,
+				  ETH_IPV6_TRAFFIC_CLASS_MASK, traffic_class,
+				  kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_flow_label_show - Returns the IPv6 flow label
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 flow label
+ *
+ * Returns the IPv6 flow label
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_flow_label_show(struct kobject *kobj,
+				    struct kobj_attribute *attr, char *buff)
+{
+	u32 flow_label;
+
+	flow_label = utils_sysfs_show_wrapper(ETH_IPV6_FLOW_LABEL_ADDR,
+					      ETH_IPV6_FLOW_LABEL_OFFSET,
+					      ETH_IPV6_FLOW_LABEL_MASK, kobj);
+	sprintf(buff, "%d\n", flow_label);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_flow_label_store - Writes to the IPv6 flow label
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 flow label
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 flow label sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_flow_label_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	int ret;
+	u32 flow_label;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &flow_label);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_FLOW_LABEL_ADDR,
+				  ETH_IPV6_FLOW_LABEL_OFFSET,
+				  ETH_IPV6_FLOW_LABEL_MASK, flow_label, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_next_header_show - Returns the IPv6 next header
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 next header
+ *
+ * Returns the IPv6 next header
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_next_header_show(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     char *buff)
+{
+	u32 next_header;
+
+	next_header = utils_sysfs_show_wrapper(ETH_IPV6_NEXT_HEADER_ADDR,
+					       ETH_IPV6_NEXT_HEADER_OFFSET,
+					       ETH_IPV6_NEXT_HEADER_MASK, kobj);
+	sprintf(buff, "%d\n", next_header);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_next_header_store - Writes to the IPv6 next header
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 next header
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 next header sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_next_header_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buff, size_t count)
+{
+	int ret;
+	u32 next_header;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &next_header);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_NEXT_HEADER_ADDR,
+				  ETH_IPV6_NEXT_HEADER_OFFSET,
+				  ETH_IPV6_NEXT_HEADER_MASK, next_header, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_hop_limit_show - Returns the IPv6 hop limit
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 hop limit
+ *
+ * Returns the IPv6 hop limit
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_hop_limit_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	u32 hop_limit;
+
+	hop_limit = utils_sysfs_show_wrapper(ETH_IPV6_HOP_LIMIT_ADDR,
+					     ETH_IPV6_HOP_LIMIT_OFFSET,
+					     ETH_IPV6_HOP_LIMIT_MASK, kobj);
+	sprintf(buff, "%d\n", hop_limit);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_hop_limit_store - Writes to the IPv6 hop limit
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 hop limit
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 hop limit sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_hop_limit_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	int ret;
+	u32 hop_limit;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &hop_limit);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_HOP_LIMIT_ADDR,
+				  ETH_IPV6_HOP_LIMIT_OFFSET,
+				  ETH_IPV6_HOP_LIMIT_MASK, hop_limit, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_source_address_show - Returns the IPv6 source address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ *
+ * Returns the IPv6 source address in xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx
+ * format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_source_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 source[4];
+	u16 source_add16[8];
+
+	source[0] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_31_0_ADDR,
+					     ETH_IPV6_SOURCE_ADD_31_0_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_31_0_MASK,
+					     kobj);
+	source[1] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_63_32_ADDR,
+					     ETH_IPV6_SOURCE_ADD_63_32_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_63_32_MASK,
+					     kobj);
+	source[2] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_95_64_ADDR,
+					     ETH_IPV6_SOURCE_ADD_95_64_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_95_64_MASK,
+					     kobj);
+	source[3] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_127_96_ADDR,
+					     ETH_IPV6_SOURCE_ADD_127_96_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_127_96_MASK,
+					     kobj);
+
+	utils_ipv6addr_32to16(source, source_add16);
+	sprintf(buff, "%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x\n",
+		source_add16[0], source_add16[1], source_add16[2],
+		source_add16[3],
+		source_add16[4], source_add16[5], source_add16[6],
+		source_add16[7]);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_source_address_store - Writes to the IPv6 source address sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 source address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_source_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 source_add[4];
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv6addr_chartohex(xroe_tmp, source_add) == 8) {
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_31_0_ADDR,
+					  ETH_IPV6_SOURCE_ADD_31_0_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_31_0_MASK,
+					  source_add[0], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_63_32_ADDR,
+					  ETH_IPV6_SOURCE_ADD_63_32_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_63_32_MASK,
+					  source_add[1], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_95_64_ADDR,
+					  ETH_IPV6_SOURCE_ADD_95_64_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_95_64_MASK,
+					  source_add[2], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_127_96_ADDR,
+					  ETH_IPV6_SOURCE_ADD_127_96_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_127_96_MASK,
+					  source_add[3], kobj);
+	}
+	return xroe_size;
+}
+
+/**
+ * ipv6_destination_address_show - Returns the IPv6 destination address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ *
+ * Returns the IPv6 destination address in
+ * xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_destination_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 dest[4];
+	u16 dest_add16[8];
+
+	dest[0] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_31_0_ADDR,
+					   ETH_IPV6_DEST_ADD_31_0_OFFSET,
+					   ETH_IPV6_DEST_ADD_31_0_MASK,
+					   kobj);
+	dest[1] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_63_32_ADDR,
+					   ETH_IPV6_DEST_ADD_63_32_OFFSET,
+					   ETH_IPV6_DEST_ADD_63_32_MASK,
+					   kobj);
+	dest[2] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_95_64_ADDR,
+					   ETH_IPV6_DEST_ADD_95_64_OFFSET,
+					   ETH_IPV6_DEST_ADD_95_64_MASK,
+					   kobj);
+	dest[3] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_127_96_ADDR,
+					   ETH_IPV6_DEST_ADD_127_96_OFFSET,
+					   ETH_IPV6_DEST_ADD_127_96_MASK,
+					   kobj);
+
+	utils_ipv6addr_32to16(dest, dest_add16);
+	sprintf(buff, "%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x\n",
+		dest_add16[0], dest_add16[1], dest_add16[2],
+		dest_add16[3],
+		dest_add16[4], dest_add16[5], dest_add16[6],
+		dest_add16[7]);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_destination_address_store - Writes to the IPv6 destination address
+ * sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 destination address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_destination_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 dest_add[4];
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv6addr_chartohex(xroe_tmp, dest_add) == 8) {
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_31_0_ADDR,
+					  ETH_IPV6_DEST_ADD_31_0_OFFSET,
+					  ETH_IPV6_DEST_ADD_31_0_MASK,
+					  dest_add[0], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_63_32_ADDR,
+					  ETH_IPV6_DEST_ADD_63_32_OFFSET,
+					  ETH_IPV6_DEST_ADD_63_32_MASK,
+					  dest_add[1], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_95_64_ADDR,
+					  ETH_IPV6_DEST_ADD_95_64_OFFSET,
+					  ETH_IPV6_DEST_ADD_95_64_MASK,
+					  dest_add[2], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_127_96_ADDR,
+					  ETH_IPV6_DEST_ADD_127_96_OFFSET,
+					  ETH_IPV6_DEST_ADD_127_96_MASK,
+					  dest_add[3], kobj);
+	}
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute version_attribute =
+	__ATTR(version, 0444, ipv6_version_show, ipv6_version_store);
+static struct kobj_attribute traffic_class =
+	__ATTR(traffic_class, 0660, ipv6_traffic_class_show,
+	       ipv6_traffic_class_store);
+static struct kobj_attribute flow_label =
+	__ATTR(flow_label, 0660, ipv6_flow_label_show, ipv6_flow_label_store);
+static struct kobj_attribute next_header =
+	__ATTR(next_header, 0660, ipv6_next_header_show,
+	       ipv6_next_header_store);
+static struct kobj_attribute hop_limit =
+	__ATTR(hop_limit, 0660, ipv6_hop_limit_show, ipv6_hop_limit_store);
+static struct kobj_attribute source_add_attribute =
+	__ATTR(source_add, 0660, ipv6_source_address_show,
+	       ipv6_source_address_store);
+static struct kobj_attribute dest_add_attribute =
+	__ATTR(dest_add, 0660, ipv6_destination_address_show,
+	       ipv6_destination_address_store);
+
+static struct attribute *attrs[] = {
+	&version_attribute.attr,
+	&traffic_class.attr,
+	&flow_label.attr,
+	&next_header.attr,
+	&hop_limit.attr,
+	&source_add_attribute.attr,
+	&dest_add_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+static struct kobject *kobj_ipv6[MAX_NUM_ETH_PORTS];
+
+/**
+ * xroe_sysfs_ipv6_init - Creates the xroe sysfs "ipv6" subdirectory & entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "ipv6" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_ipv6_init(void)
+{
+	int ret;
+	int i;
+
+	for (i = 0; i < 4; i++) {
+		kobj_ipv6[i] = kobject_create_and_add("ipv6",
+						      kobj_eth_ports[i]);
+		if (!kobj_ipv6[i])
+			return -ENOMEM;
+		ret = sysfs_create_group(kobj_ipv6[i], &attr_group);
+		if (ret)
+			kobject_put(kobj_ipv6[i]);
+	}
+	return ret;
+}
+
+/**
+ * xroe_sysfs_ipv6_exit - Deletes the xroe sysfs "ipv6" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "ipv6" subdirectory and entries,
+ * under the "xroe" entry
+ *
+ */
+void xroe_sysfs_ipv6_exit(void)
+{
+	int i;
+
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_ipv6[i]);
+}
+
+/**
+ * utils_ipv6addr_32to16 - uint32_t to uint16_t for IPv6 addresses
+ * @ip32:	The IPv6 address in uint32_t format
+ * @ip16:	The IPv6 address in uint16_t format
+ *
+ * Coverts an IPv6 address given in uint32_t format to uint16_t
+ */
+static void utils_ipv6addr_32to16(u32 *ip32, uint16_t *ip16)
+{
+	ip16[0] = ip32[0] >> 16;
+	ip16[1] = ip32[0] & 0x0000FFFF;
+	ip16[2] = ip32[1] >> 16;
+	ip16[3] = ip32[1] & 0x0000FFFF;
+	ip16[4] = ip32[2] >> 16;
+	ip16[5] = ip32[2] & 0x0000FFFF;
+	ip16[6] = ip32[3] >> 16;
+	ip16[7] = ip32[3] & 0x0000FFFF;
+}
+
+/**
+ * utils_ipv6addr_chartohex - Character to char array for IPv6 addresses
+ * @ip_addr:	The character array containing the IP address
+ * @p_ip_addr:	The converted IPv4 address
+ *
+ * Coverts an IPv6 address given as a character array to integer format
+ *
+ * Return: 8 (the length of the resulting character array) on success,
+ * -1 in case of wrong input
+ */
+static int utils_ipv6addr_chartohex(char *ip_addr, uint32_t *p_ip_addr)
+{
+	int ret;
+	int count;
+	char *string;
+	unsigned char *found;
+	u16 ip_array_16[8];
+	u32 field;
+
+	ret = -1;
+	count = 0;
+	string = ip_addr;
+	while ((found = (unsigned char *)strsep(&string, ":")) != NULL) {
+		if (count <= 8) {
+			ret = kstrtouint(found, 16, &field);
+			if (ret)
+				return ret;
+			ip_array_16[count] = (uint16_t)field;
+		} else {
+			break;
+		}
+		count++;
+	}
+	if (count == 8) {
+		p_ip_addr[0] = ip_array_16[1] | (ip_array_16[0] << 16);
+		p_ip_addr[1] = ip_array_16[3] | (ip_array_16[2] << 16);
+		p_ip_addr[2] = ip_array_16[5] | (ip_array_16[4] << 16);
+		p_ip_addr[3] = ip_array_16[7] | (ip_array_16[6] << 16);
+		ret = count;
+	}
+	return ret;
+}
diff --git a/drivers/staging/xroeframer/sysfs_xroe_framer_stats.c b/drivers/staging/xroeframer/sysfs_xroe_framer_stats.c
new file mode 100644
index 000000000..063664bb9
--- /dev/null
+++ b/drivers/staging/xroeframer/sysfs_xroe_framer_stats.c
@@ -0,0 +1,401 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2019 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+/**
+ * total_rx_good_pkt_show - Returns the total good rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total good rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_good_pkt_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_TOTAL_RX_GOOD_PKT_CNT_ADDR,
+					 STATS_TOTAL_RX_GOOD_PKT_CNT_OFFSET,
+					 STATS_TOTAL_RX_GOOD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_pkt_show - Returns the total bad rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_pkt_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_TOTAL_RX_BAD_PKT_CNT_ADDR,
+					 STATS_TOTAL_RX_BAD_PKT_CNT_OFFSET,
+					 STATS_TOTAL_RX_BAD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_fcs_show - Returns the total bad fcs count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad frame check sequences count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_fcs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_TOTAL_RX_BAD_FCS_CNT_ADDR,
+					 STATS_TOTAL_RX_BAD_FCS_CNT_OFFSET,
+					 STATS_TOTAL_RX_BAD_FCS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_user_pkt_show - Returns the total user rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_user_pkt_show(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_PACKETS_CNT_ADDR,
+					 STATS_USER_DATA_RX_PACKETS_CNT_OFFSET,
+					 STATS_USER_DATA_RX_PACKETS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_good_user_pkt_show - Returns the total good user rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total good user rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_good_user_pkt_show(struct kobject *kobj,
+					   struct kobj_attribute *attr,
+					   char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_GOOD_PKT_CNT_ADDR,
+					 STATS_USER_DATA_RX_GOOD_PKT_CNT_OFFSET,
+					 STATS_USER_DATA_RX_GOOD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_pkt_show - Returns the total bad user rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_pkt_show(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_BAD_PKT_CNT_ADDR,
+					 STATS_USER_DATA_RX_BAD_PKT_CNT_OFFSET,
+					 STATS_USER_DATA_RX_BAD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_fcs_show - Returns the total bad user rx fcs count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user frame check sequences count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_fcs_show(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_BAD_FCS_CNT_ADDR,
+					 STATS_USER_DATA_RX_BAD_FCS_CNT_OFFSET,
+					 STATS_USER_DATA_RX_BAD_FCS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_user_ctrl_pkt_show - Returns the total user rx control packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx control packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_user_ctrl_pkt_show(struct kobject *kobj,
+					   struct kobj_attribute *attr,
+					   char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_PACKETS_CNT_ADDR,
+					 STATS_USER_CTRL_RX_PACKETS_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_PACKETS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_good_user_ctrl_pkt_show - Returns the total good user rx
+ * control packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total good user rx control packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_good_user_ctrl_pkt_show(struct kobject *kobj,
+						struct kobj_attribute *attr,
+						char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_GOOD_PKT_CNT_ADDR,
+					 STATS_USER_CTRL_RX_GOOD_PKT_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_GOOD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_ctrl_pkt_show - Returns the total bad user rx
+ * control packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user rx control packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_ctrl_pkt_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_BAD_PKT_CNT_ADDR,
+					 STATS_USER_CTRL_RX_BAD_PKT_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_BAD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_ctrl_fcs_show - Returns the total bad user rx
+ * control fcs count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user control frame check sequences count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_ctrl_fcs_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_BAD_FCS_CNT_ADDR,
+					 STATS_USER_CTRL_RX_BAD_FCS_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_BAD_FCS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * rx_user_pkt_rate_show - Returns the rate of user packets
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx packet count
+ *
+ * Return: Returns the rate of user packets
+ */
+static ssize_t rx_user_pkt_rate_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	u32 rate;
+
+	rate = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_PKTS_RATE_ADDR,
+					STATS_USER_DATA_RX_PKTS_RATE_OFFSET,
+					STATS_USER_DATA_RX_PKTS_RATE_MASK,
+					kobj);
+	return sprintf(buff, "%d\n", rate);
+}
+
+/**
+ * rx_user_ctrl_pkt_rate_show - Returns the rate of user control packets
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx packet count
+ *
+ * Return: Returns the rate of user control packets
+ */
+static ssize_t rx_user_ctrl_pkt_rate_show(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  char *buff)
+{
+	u32 rate;
+
+	rate = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_PKTS_RATE_ADDR,
+					STATS_USER_CTRL_RX_PKTS_RATE_OFFSET,
+					STATS_USER_CTRL_RX_PKTS_RATE_MASK,
+					kobj);
+	return sprintf(buff, "%d\n", rate);
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+static struct kobj_attribute total_rx_good_pkt_attribute =
+	__ATTR(total_rx_good_pkt, 0444, total_rx_good_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_pkt_attribute =
+	__ATTR(total_rx_bad_pkt, 0444, total_rx_bad_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_fcs_attribute =
+	__ATTR(total_rx_bad_fcs, 0444, total_rx_bad_fcs_show, NULL);
+static struct kobj_attribute total_rx_user_pkt_attribute =
+	__ATTR(total_rx_user_pkt, 0444, total_rx_user_pkt_show, NULL);
+static struct kobj_attribute total_rx_good_user_pkt_attribute =
+	__ATTR(total_rx_good_user_pkt, 0444, total_rx_good_user_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_pkt_attribute =
+	__ATTR(total_rx_bad_user_pkt, 0444, total_rx_bad_user_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_fcs_attribute =
+	__ATTR(total_rx_bad_user_fcs, 0444, total_rx_bad_user_fcs_show, NULL);
+static struct kobj_attribute total_rx_user_ctrl_pkt_attribute =
+	__ATTR(total_rx_user_ctrl_pkt, 0444, total_rx_user_ctrl_pkt_show, NULL);
+static struct kobj_attribute total_rx_good_user_ctrl_pkt_attribute =
+	__ATTR(total_rx_good_user_ctrl_pkt, 0444,
+	       total_rx_good_user_ctrl_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_ctrl_pkt_attribute =
+	__ATTR(total_rx_bad_user_ctrl_pkt, 0444,
+	       total_rx_bad_user_ctrl_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_ctrl_fcs_attribute =
+	__ATTR(total_rx_bad_user_ctrl_fcs, 0444,
+	       total_rx_bad_user_ctrl_fcs_show, NULL);
+static struct kobj_attribute rx_user_pkt_rate_attribute =
+	__ATTR(rx_user_pkt_rate, 0444, rx_user_pkt_rate_show, NULL);
+static struct kobj_attribute rx_user_ctrl_pkt_rate_attribute =
+	__ATTR(rx_user_ctrl_pkt_rate, 0444, rx_user_ctrl_pkt_rate_show, NULL);
+
+static struct attribute *attrs[] = {
+	&total_rx_good_pkt_attribute.attr,
+	&total_rx_bad_pkt_attribute.attr,
+	&total_rx_bad_fcs_attribute.attr,
+	&total_rx_user_pkt_attribute.attr,
+	&total_rx_good_user_pkt_attribute.attr,
+	&total_rx_bad_user_pkt_attribute.attr,
+	&total_rx_bad_user_fcs_attribute.attr,
+	&total_rx_user_ctrl_pkt_attribute.attr,
+	&total_rx_good_user_ctrl_pkt_attribute.attr,
+	&total_rx_bad_user_ctrl_pkt_attribute.attr,
+	&total_rx_bad_user_ctrl_fcs_attribute.attr,
+	&rx_user_pkt_rate_attribute.attr,
+	&rx_user_ctrl_pkt_rate_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+struct kobject *kobj_stats;
+
+/**
+ * xroe_sysfs_stats_init - Creates the xroe sysfs "stats" subdirectory & entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "stats" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_stats_init(void)
+{
+	int ret;
+
+	kobj_stats = kobject_create_and_add("stats", root_xroe_kobj);
+	if (!kobj_stats)
+		return -ENOMEM;
+
+	ret = sysfs_create_group(kobj_stats, &attr_group);
+	if (ret)
+		kobject_put(kobj_stats);
+
+	return ret;
+}
+
+/**
+ * xroe_sysfs_stats_exit - Deletes the xroe sysfs "ipv4" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "stats" subdirectory and entries,
+ * under the "xroe" entry
+ */
+void xroe_sysfs_stats_exit(void)
+{
+	kobject_put(kobj_stats);
+}
diff --git a/drivers/staging/xroeframer/sysfs_xroe_framer_udp.c b/drivers/staging/xroeframer/sysfs_xroe_framer_udp.c
new file mode 100644
index 000000000..73f17ffdc
--- /dev/null
+++ b/drivers/staging/xroeframer/sysfs_xroe_framer_udp.c
@@ -0,0 +1,181 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 15 };
+static int xroe_size;
+
+/**
+ * udp_source_port_show - Returns the UDP source port
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP source port
+ *
+ * Returns the UDP source port
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t udp_source_port_show(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    char *buff)
+{
+	u32 source_port;
+
+	source_port = utils_sysfs_show_wrapper(ETH_UDP_SOURCE_PORT_ADDR,
+					       ETH_UDP_SOURCE_PORT_OFFSET,
+					       ETH_UDP_SOURCE_PORT_MASK, kobj);
+	sprintf(buff, "%d\n", source_port);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * udp_source_port_store - Writes to the UDP source port sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP source port
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the UDP source port sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t udp_source_port_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	int ret;
+	u32 source_port;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &source_port);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_UDP_SOURCE_PORT_ADDR,
+				  ETH_UDP_SOURCE_PORT_OFFSET,
+				  ETH_UDP_SOURCE_PORT_MASK, source_port, kobj);
+	return xroe_size;
+}
+
+/**
+ * udp_destination_port_show - Returns the UDP destination port
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP destination port
+ *
+ * Returns the UDP destination port
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t udp_destination_port_show(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 char *buff)
+{
+	u32 dest_port;
+
+	dest_port = utils_sysfs_show_wrapper(ETH_UDP_DESTINATION_PORT_ADDR,
+					     ETH_UDP_DESTINATION_PORT_OFFSET,
+					     ETH_UDP_DESTINATION_PORT_MASK,
+					     kobj);
+	sprintf(buff, "%d\n", dest_port);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * udp_destination_port_store - Writes to the UDP destination port sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP destination port
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the UDP destination port sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t udp_destination_port_store(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  const char *buff, size_t count)
+{
+	int ret;
+	u32 dest_port;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &dest_port);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_UDP_DESTINATION_PORT_ADDR,
+				  ETH_UDP_DESTINATION_PORT_OFFSET,
+				  ETH_UDP_DESTINATION_PORT_MASK, dest_port,
+				  kobj);
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute source_port =
+	__ATTR(source_port, 0660, udp_source_port_show,
+	       udp_source_port_store);
+static struct kobj_attribute dest_port =
+	__ATTR(dest_port, 0660, udp_destination_port_show,
+	       udp_destination_port_store);
+
+static struct attribute *attrs[] = {
+	&source_port.attr,
+	&dest_port.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+static struct kobject *kobj_udp[MAX_NUM_ETH_PORTS];
+
+/**
+ * xroe_sysfs_udp_init - Creates the xroe sysfs "udp" subdirectory and entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "udp" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_udp_init(void)
+{
+	int ret;
+	int i;
+
+	for (i = 0; i < 4; i++) {
+		kobj_udp[i] = kobject_create_and_add("udp",  kobj_eth_ports[i]);
+		if (!kobj_udp[i])
+			return -ENOMEM;
+		ret = sysfs_create_group(kobj_udp[i], &attr_group);
+		if (ret)
+			kobject_put(kobj_udp[i]);
+	}
+	return ret;
+}
+
+/**
+ * xroe_sysfs_udp_exit - Deletes the xroe sysfs "udp" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "udp" subdirectory and entries,
+ * under the "xroe" entry
+ *
+ */
+void xroe_sysfs_udp_exit(void)
+{
+	int i;
+
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_udp[i]);
+}
diff --git a/drivers/staging/xroeframer/xroe_framer.c b/drivers/staging/xroeframer/xroe_framer.c
new file mode 100644
index 000000000..e57d3449c
--- /dev/null
+++ b/drivers/staging/xroeframer/xroe_framer.c
@@ -0,0 +1,156 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include "xroe_framer.h"
+
+#define DRIVER_NAME "framer"
+
+/*
+ * TODO: to be made static as well, so that multiple instances can be used. As
+ * of now, the "xroe_lp" structure is shared among the multiple source files
+ */
+struct framer_local *xroe_lp;
+static struct platform_driver framer_driver;
+/*
+ * TODO: placeholder for the IRQ once it's been implemented
+ * in the framer block
+ */
+static irqreturn_t framer_irq(int irq, void *xroe_lp)
+{
+	return IRQ_HANDLED;
+}
+
+/**
+ * framer_probe - Probes the device tree to locate the framer block
+ * @pdev:	The structure containing the device's details
+ *
+ * Probes the device tree to locate the framer block and maps it to
+ * the kernel virtual memory space
+ *
+ * Return: 0 on success or a negative errno on error.
+ */
+static int framer_probe(struct platform_device *pdev)
+{
+	struct resource *r_mem; /* IO mem resources */
+	struct resource *r_irq;
+	struct device *dev = &pdev->dev;
+	int rc = 0;
+
+	dev_dbg(dev, "Device Tree Probing\n");
+	xroe_lp = devm_kzalloc(&pdev->dev, sizeof(*xroe_lp), GFP_KERNEL);
+	if (!xroe_lp)
+		return -ENOMEM;
+
+	/* Get iospace for the device */
+	r_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	xroe_lp->base_addr = devm_ioremap_resource(&pdev->dev, r_mem);
+	if (IS_ERR(xroe_lp->base_addr))
+		return PTR_ERR(xroe_lp->base_addr);
+
+	dev_set_drvdata(dev, xroe_lp);
+	xroe_sysfs_init();
+	/* Get IRQ for the device */
+	/*
+	 * TODO: No IRQ *yet* in the DT from the framer block, as it's still
+	 * under development. To be added once it's in the block, and also
+	 * replace with platform_get_irq_byname()
+	 */
+	r_irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
+	if (IS_ERR(r_irq)) {
+		dev_info(dev, "no IRQ found\n");
+		/*
+		 * TODO: Return non-zero (error) code on no IRQ found.
+		 * To be implemented once the IRQ is in the block
+		 */
+		return 0;
+	}
+	rc = devm_request_irq(dev, xroe_lp->irq, &framer_irq, 0, DRIVER_NAME, xroe_lp);
+	if (rc) {
+		dev_err(dev, "testmodule: Could not allocate interrupt %d.\n",
+			xroe_lp->irq);
+		/*
+		 * TODO: Return non-zero (error) code on no IRQ found.
+		 * To be implemented once the IRQ is in the block
+		 */
+		return 0;
+	}
+
+	return rc;
+}
+
+/**
+ * framer_init - Registers the driver
+ *
+ * Return: 0 on success, -1 on allocation error
+ *
+ * Registers the framer driver and creates character device drivers
+ * for the whole block, as well as separate ones for stats and
+ * radio control.
+ */
+static int __init framer_init(void)
+{
+	int ret;
+
+	pr_debug("XROE framer driver init\n");
+
+	ret = platform_driver_register(&framer_driver);
+
+	return ret;
+}
+
+/**
+ * framer_exit - Destroys the driver
+ *
+ * Unregisters the framer driver and destroys the character
+ * device driver for the whole block, as well as the separate ones
+ * for stats and radio control. Returns 0 upon successful execution
+ */
+static void __exit framer_exit(void)
+{
+	xroe_sysfs_exit();
+	platform_driver_unregister(&framer_driver);
+	pr_info("XROE Framer exit\n");
+}
+
+module_init(framer_init);
+module_exit(framer_exit);
+
+static const struct of_device_id framer_of_match[] = {
+	{ .compatible = "xlnx,roe-framer-1.0", },
+	{ /* end of list */ },
+};
+MODULE_DEVICE_TABLE(of, framer_of_match);
+
+static struct platform_driver framer_driver = {
+	.driver = {
+		/*
+		 * TODO: .name shouldn't be necessary, though removing
+		 * it results in kernel panic. To investigate further
+		 */
+		.name = DRIVER_NAME,
+		.of_match_table = framer_of_match,
+	},
+	.probe = framer_probe,
+};
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Xilinx Inc.");
+MODULE_DESCRIPTION("framer - Xilinx Radio over Ethernet Framer driver");
diff --git a/drivers/staging/xroeframer/xroe_framer.h b/drivers/staging/xroeframer/xroe_framer.h
new file mode 100644
index 000000000..67b833310
--- /dev/null
+++ b/drivers/staging/xroeframer/xroe_framer.h
@@ -0,0 +1,63 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+#include "roe_framer_ctrl.h"
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/ioctl.h>
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include <linux/uaccess.h>
+#include <uapi/linux/stat.h> /* S_IRUSR, S_IWUSR */
+
+/* TODO: Remove hardcoded value of number of Ethernet ports and read the value
+ * from the device tree.
+ */
+#define MAX_NUM_ETH_PORTS 0x4
+/* TODO: to be made static as well, so that multiple instances can be used. As
+ * of now, the following 3 structures are shared among the multiple
+ * source files
+ */
+extern struct framer_local *xroe_lp;
+extern struct kobject *root_xroe_kobj;
+extern struct kobject *kobj_framer;
+extern struct kobject *kobj_eth_ports[MAX_NUM_ETH_PORTS];
+struct framer_local {
+	int irq;
+	unsigned long mem_start;
+	unsigned long mem_end;
+	void __iomem *base_addr;
+};
+
+int xroe_sysfs_init(void);
+int xroe_sysfs_ipv4_init(void);
+int xroe_sysfs_ipv6_init(void);
+int xroe_sysfs_udp_init(void);
+int xroe_sysfs_stats_init(void);
+void xroe_sysfs_exit(void);
+void xroe_sysfs_ipv4_exit(void);
+void xroe_sysfs_ipv6_exit(void);
+void xroe_sysfs_udp_exit(void);
+void xroe_sysfs_stats_exit(void);
+int utils_write32withmask(void __iomem *working_address, u32 value,
+			  u32 mask, u32 offset);
+int utils_check_address_offset(u32 offset, size_t device_size);
+void utils_sysfs_store_wrapper(u32 address, u32 offset, u32 mask, u32 value,
+			       struct kobject *kobj);
+u32 utils_sysfs_show_wrapper(u32 address, u32 offset, u32 mask,
+			     struct kobject *kobj);
diff --git a/drivers/staging/xroetrafficgen/Kconfig b/drivers/staging/xroetrafficgen/Kconfig
new file mode 100644
index 000000000..d2ead1483
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/Kconfig
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0
+
+#
+# Xilinx Radio over Ethernet Traffic Generator driver
+#
+
+config XROE_TRAFFIC_GEN
+	tristate "Xilinx Radio over Ethernet Traffic Generator driver"
+	help
+	  The Traffic Generator is used for in testing of other RoE IP Blocks
+	  (currenty the XRoE Framer) and simulates an radio antenna interface.
+	  It generates rolling rampdata for eCPRI antenna paths.
+	  Each path is tagged with the antenna number. The sink locks to this
+	  ramp data, then checks the next value is as expected.
diff --git a/drivers/staging/xroetrafficgen/Makefile b/drivers/staging/xroetrafficgen/Makefile
new file mode 100644
index 000000000..e180a9bbc
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/Makefile
@@ -0,0 +1,8 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for Radio over Ethernet Framer driver
+#
+obj-$(XROE_TRAFFIC_GEN)	:= xroe_traffic_gen.o
+
+framer-objs := 	xroe-traffic-gen.o \
+		xroe-traffic-gen-sysfs.o \
diff --git a/drivers/staging/xroetrafficgen/README b/drivers/staging/xroetrafficgen/README
new file mode 100644
index 000000000..1828426af
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/README
@@ -0,0 +1,19 @@
+Xilinx Radio over Ethernet Traffic Generator driver
+===================================================
+
+About the RoE Framer Traffic Generator
+
+The Traffic Generator is used for in testing of other RoE IP Blocks (currenty
+the XRoE Framer) and simulates an radio antenna interface. It generates rolling
+rampdata for eCPRI antenna paths. Each path is tagged with the antenna number.
+The sink locks to this ramp data, then checks the next value is as expected.
+
+
+About the Linux Driver
+
+The RoE Traffic Generator Linux Driver provides sysfs access to control a
+simulated radio antenna interface.
+The loading of the driver to the hardware is possible using Device Tree binding
+(see "dt-binding.txt" for more information). When the driver is loaded, the
+general controls (such as sink lock, enable, loopback etc) are exposed
+under /sys/kernel/xroetrafficgen.
diff --git a/drivers/staging/xroetrafficgen/dt-binding.txt b/drivers/staging/xroetrafficgen/dt-binding.txt
new file mode 100644
index 000000000..3516d3ff8
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/dt-binding.txt
@@ -0,0 +1,15 @@
+* Xilinx Radio over Ethernet Traffic Generator driver
+
+Required properties:
+- compatible: must be "xlnx,roe-framer-1.0"
+- reg: physical base address of the framer and length of memory mapped region
+- clock-names: list of clock names
+- clocks: list of clock sources corresponding to the clock names
+
+Example:
+	roe_radio_ctrl@a0060000 {
+		compatible = "xlnx,roe-traffic-gen-1.0";
+		reg = <0x0 0xa0060000 0x0 0x10000>;
+		clock-names = "s_axis_fram_aclk", "s_axi_aclk";
+		clocks = <0x44 0x43>;
+	};
diff --git a/drivers/staging/xroetrafficgen/roe_radio_ctrl.h b/drivers/staging/xroetrafficgen/roe_radio_ctrl.h
new file mode 100644
index 000000000..e093386f3
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/roe_radio_ctrl.h
@@ -0,0 +1,183 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+/*-----------------------------------------------------------------------------
+ * C Header bank BASE definitions
+ *-----------------------------------------------------------------------------
+ */
+#define ROE_RADIO_CFG_BASE_ADDR 0x0
+#define ROE_RADIO_SOURCE_BASE_ADDR 0x1000
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_radio_cfg
+ * with prefix radio_ @ address 0x0
+ *-----------------------------------------------------------------------------
+ */
+/* Type = roInt */
+#define RADIO_ID_ADDR 0x0
+#define RADIO_ID_MASK 0xffffffff
+#define RADIO_ID_OFFSET 0x0
+#define RADIO_ID_WIDTH 0x20
+#define RADIO_ID_DEFAULT 0x120001
+
+/* Type = rw */
+#define RADIO_TIMEOUT_ENABLE_ADDR 0x4
+#define RADIO_TIMEOUT_ENABLE_MASK 0x1
+#define RADIO_TIMEOUT_ENABLE_OFFSET 0x0
+#define RADIO_TIMEOUT_ENABLE_WIDTH 0x1
+#define RADIO_TIMEOUT_ENABLE_DEFAULT 0x0
+
+/* Type = ro */
+#define RADIO_TIMEOUT_STATUS_ADDR 0x8
+#define RADIO_TIMEOUT_STATUS_MASK 0x1
+#define RADIO_TIMEOUT_STATUS_OFFSET 0x0
+#define RADIO_TIMEOUT_STATUS_WIDTH 0x1
+#define RADIO_TIMEOUT_STATUS_DEFAULT 0x1
+
+/* Type = rw */
+#define RADIO_TIMEOUT_VALUE_ADDR 0xc
+#define RADIO_TIMEOUT_VALUE_MASK 0xfff
+#define RADIO_TIMEOUT_VALUE_OFFSET 0x0
+#define RADIO_TIMEOUT_VALUE_WIDTH 0xc
+#define RADIO_TIMEOUT_VALUE_DEFAULT 0x80
+
+/* Type = rw */
+#define RADIO_GPIO_CDC_LEDMODE2_ADDR 0x10
+#define RADIO_GPIO_CDC_LEDMODE2_MASK 0x1
+#define RADIO_GPIO_CDC_LEDMODE2_OFFSET 0x0
+#define RADIO_GPIO_CDC_LEDMODE2_WIDTH 0x1
+#define RADIO_GPIO_CDC_LEDMODE2_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_GPIO_CDC_LEDGPIO_ADDR 0x10
+#define RADIO_GPIO_CDC_LEDGPIO_MASK 0x30
+#define RADIO_GPIO_CDC_LEDGPIO_OFFSET 0x4
+#define RADIO_GPIO_CDC_LEDGPIO_WIDTH 0x2
+#define RADIO_GPIO_CDC_LEDGPIO_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_GPIO_CDC_DIPSTATUS_ADDR 0x14
+#define RADIO_GPIO_CDC_DIPSTATUS_MASK 0xff
+#define RADIO_GPIO_CDC_DIPSTATUS_OFFSET 0x0
+#define RADIO_GPIO_CDC_DIPSTATUS_WIDTH 0x8
+#define RADIO_GPIO_CDC_DIPSTATUS_DEFAULT 0x0
+
+/* Type = wPlsH */
+#define RADIO_SW_TRIGGER_ADDR 0x20
+#define RADIO_SW_TRIGGER_MASK 0x1
+#define RADIO_SW_TRIGGER_OFFSET 0x0
+#define RADIO_SW_TRIGGER_WIDTH 0x1
+#define RADIO_SW_TRIGGER_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_CDC_ENABLE_ADDR 0x24
+#define RADIO_CDC_ENABLE_MASK 0x1
+#define RADIO_CDC_ENABLE_OFFSET 0x0
+#define RADIO_CDC_ENABLE_WIDTH 0x1
+#define RADIO_CDC_ENABLE_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_ADDR 0x24
+#define RADIO_CDC_ERROR_MASK 0x2
+#define RADIO_CDC_ERROR_OFFSET 0x1
+#define RADIO_CDC_ERROR_WIDTH 0x1
+#define RADIO_CDC_ERROR_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_ADDR 0x24
+#define RADIO_CDC_STATUS_MASK 0x4
+#define RADIO_CDC_STATUS_OFFSET 0x2
+#define RADIO_CDC_STATUS_WIDTH 0x1
+#define RADIO_CDC_STATUS_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_CDC_LOOPBACK_ADDR 0x28
+#define RADIO_CDC_LOOPBACK_MASK 0x1
+#define RADIO_CDC_LOOPBACK_OFFSET 0x0
+#define RADIO_CDC_LOOPBACK_WIDTH 0x1
+#define RADIO_CDC_LOOPBACK_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_SINK_ENABLE_ADDR 0x2c
+#define RADIO_SINK_ENABLE_MASK 0x1
+#define RADIO_SINK_ENABLE_OFFSET 0x0
+#define RADIO_SINK_ENABLE_WIDTH 0x1
+#define RADIO_SINK_ENABLE_DEFAULT 0x1
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_31_0_ADDR 0x30
+#define RADIO_CDC_ERROR_31_0_MASK 0xffffffff
+#define RADIO_CDC_ERROR_31_0_OFFSET 0x0
+#define RADIO_CDC_ERROR_31_0_WIDTH 0x20
+#define RADIO_CDC_ERROR_31_0_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_63_32_ADDR 0x34
+#define RADIO_CDC_ERROR_63_32_MASK 0xffffffff
+#define RADIO_CDC_ERROR_63_32_OFFSET 0x0
+#define RADIO_CDC_ERROR_63_32_WIDTH 0x20
+#define RADIO_CDC_ERROR_63_32_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_95_64_ADDR 0x38
+#define RADIO_CDC_ERROR_95_64_MASK 0xffffffff
+#define RADIO_CDC_ERROR_95_64_OFFSET 0x0
+#define RADIO_CDC_ERROR_95_64_WIDTH 0x20
+#define RADIO_CDC_ERROR_95_64_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_127_96_ADDR 0x3c
+#define RADIO_CDC_ERROR_127_96_MASK 0xffffffff
+#define RADIO_CDC_ERROR_127_96_OFFSET 0x0
+#define RADIO_CDC_ERROR_127_96_WIDTH 0x20
+#define RADIO_CDC_ERROR_127_96_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_31_0_ADDR 0x40
+#define RADIO_CDC_STATUS_31_0_MASK 0xffffffff
+#define RADIO_CDC_STATUS_31_0_OFFSET 0x0
+#define RADIO_CDC_STATUS_31_0_WIDTH 0x20
+#define RADIO_CDC_STATUS_31_0_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_63_32_ADDR 0x44
+#define RADIO_CDC_STATUS_63_32_MASK 0xffffffff
+#define RADIO_CDC_STATUS_63_32_OFFSET 0x0
+#define RADIO_CDC_STATUS_63_32_WIDTH 0x20
+#define RADIO_CDC_STATUS_63_32_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_95_64_ADDR 0x48
+#define RADIO_CDC_STATUS_95_64_MASK 0xffffffff
+#define RADIO_CDC_STATUS_95_64_OFFSET 0x0
+#define RADIO_CDC_STATUS_95_64_WIDTH 0x20
+#define RADIO_CDC_STATUS_95_64_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_127_96_ADDR 0x4c
+#define RADIO_CDC_STATUS_127_96_MASK 0xffffffff
+#define RADIO_CDC_STATUS_127_96_OFFSET 0x0
+#define RADIO_CDC_STATUS_127_96_WIDTH 0x20
+#define RADIO_CDC_STATUS_127_96_DEFAULT 0x0
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_radio_source
+ * with prefix fram_ @ address 0x1000
+ *-----------------------------------------------------------------------------
+ */
+/* Type = rwpdef */
+#define FRAM_PACKET_DATA_SIZE_ADDR 0x1000
+#define FRAM_PACKET_DATA_SIZE_MASK 0x7f
+#define FRAM_PACKET_DATA_SIZE_OFFSET 0x0
+#define FRAM_PACKET_DATA_SIZE_WIDTH 0x7
+#define FRAM_PACKET_DATA_SIZE_DEFAULT 0x0
+
+/* Type = rwpdef */
+#define FRAM_PAUSE_DATA_SIZE_ADDR 0x1004
+#define FRAM_PAUSE_DATA_SIZE_MASK 0x7f
+#define FRAM_PAUSE_DATA_SIZE_OFFSET 0x0
+#define FRAM_PAUSE_DATA_SIZE_WIDTH 0x7
+#define FRAM_PAUSE_DATA_SIZE_DEFAULT 0x0
diff --git a/drivers/staging/xroetrafficgen/xroe-traffic-gen-sysfs.c b/drivers/staging/xroetrafficgen/xroe-traffic-gen-sysfs.c
new file mode 100644
index 000000000..c9b05866f
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/xroe-traffic-gen-sysfs.c
@@ -0,0 +1,824 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/device.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "roe_radio_ctrl.h"
+#include "xroe-traffic-gen.h"
+
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+/**
+ * utils_sysfs_store_wrapper - Wraps the storing function for sysfs entries
+ * @dev:	The structure containing the device's information
+ * @address:	The address of the register to be written
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be written
+ * @value:	The value to be written to the register
+ *
+ * Wraps the core functionality of all "store" functions of sysfs entries.
+ */
+static void utils_sysfs_store_wrapper(struct device *dev, u32 address,
+				      u32 offset, u32 mask, u32 value)
+{
+	void __iomem *working_address;
+	u32 read_register_value = 0;
+	u32 register_value_to_write = 0;
+	u32 delta = 0;
+	u32 buffer = 0;
+	struct xroe_traffic_gen_local *lp = dev_get_drvdata(dev);
+
+	working_address = (void __iomem *)(lp->base_addr + address);
+	read_register_value = ioread32(working_address);
+	buffer = (value << offset);
+	register_value_to_write = read_register_value & ~mask;
+	delta = buffer & mask;
+	register_value_to_write |= delta;
+	iowrite32(register_value_to_write, working_address);
+}
+
+/**
+ * utils_sysfs_show_wrapper - Wraps the "show" function for sysfs entries
+ * @dev:	The structure containing the device's information
+ * @address:	The address of the register to be read
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be read
+ *
+ * Wraps the core functionality of all "show" functions of sysfs entries.
+ *
+ * Return: The value designated by the address, offset and mask
+ */
+static u32 utils_sysfs_show_wrapper(struct device *dev, u32 address, u32 offset,
+				    u32 mask)
+{
+	void __iomem *working_address;
+	u32 buffer;
+	struct xroe_traffic_gen_local *lp = dev_get_drvdata(dev);
+
+	working_address = (void __iomem *)(lp->base_addr + address);
+	buffer = ioread32(working_address);
+	return (buffer & mask) >> offset;
+}
+
+/**
+ * radio_id_show - Returns the block's ID number
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's ID (0x1179649 by default)
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_id_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
+{
+	u32 radio_id;
+
+	radio_id = utils_sysfs_show_wrapper(dev, RADIO_ID_ADDR,
+					    RADIO_ID_OFFSET,
+					    RADIO_ID_MASK);
+	return sprintf(buf, "%d\n", radio_id);
+}
+static DEVICE_ATTR_RO(radio_id);
+
+/**
+ * timeout_enable_show - Returns the traffic gen's timeout enable status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's timeout enable status to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t timeout_enable_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 timeout_enable;
+
+	timeout_enable = utils_sysfs_show_wrapper(dev,
+						  RADIO_TIMEOUT_ENABLE_ADDR,
+						  RADIO_TIMEOUT_ENABLE_OFFSET,
+						  RADIO_TIMEOUT_ENABLE_MASK);
+	return sprintf(buf, "%d\n", timeout_enable);
+}
+
+/**
+ * timeout_enable_store - Writes to the traffic gens's timeout enable
+ * status register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's timeout enable
+ * status to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t timeout_enable_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	u32 enable = 0;
+
+	strncpy(xroe_tmp, buf, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0)
+		enable = 1;
+	else if (strncmp(xroe_tmp, "false", xroe_size) == 0)
+		enable = 0;
+	utils_sysfs_store_wrapper(dev, RADIO_TIMEOUT_ENABLE_ADDR,
+				  RADIO_TIMEOUT_ENABLE_OFFSET,
+				  RADIO_TIMEOUT_ENABLE_MASK, enable);
+	return count;
+}
+static DEVICE_ATTR_RW(timeout_enable);
+
+/**
+ * timeout_status_show - Returns the timeout status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's timeout status (0x1 by default)
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t timeout_status_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 timeout;
+
+	timeout = utils_sysfs_show_wrapper(dev, RADIO_TIMEOUT_STATUS_ADDR,
+					   RADIO_TIMEOUT_STATUS_OFFSET,
+					   RADIO_TIMEOUT_STATUS_MASK);
+	return sprintf(buf, "%d\n", timeout);
+}
+static DEVICE_ATTR_RO(timeout_status);
+
+/**
+ * timeout_enable_show - Returns the traffic gen's timeout value
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's timeout value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t timeout_value_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	u32 timeout_value;
+
+	timeout_value = utils_sysfs_show_wrapper(dev, RADIO_TIMEOUT_VALUE_ADDR,
+						 RADIO_TIMEOUT_VALUE_OFFSET,
+						 RADIO_TIMEOUT_VALUE_MASK);
+	return sprintf(buf, "%d\n", timeout_value);
+}
+
+/**
+ * timeout_enable_store - Writes to the traffic gens's timeout value
+ * status register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's timeout value
+ * to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t timeout_value_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count)
+{
+	u32 timeout_value;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &timeout_value);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_TIMEOUT_VALUE_ADDR,
+				  RADIO_TIMEOUT_VALUE_OFFSET,
+				  RADIO_TIMEOUT_VALUE_MASK, timeout_value);
+	return count;
+}
+static DEVICE_ATTR_RW(timeout_value);
+
+/**
+ * ledmode_show - Returns the current LED mode
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's LED mode value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t ledmode_show(struct device *dev, struct device_attribute *attr,
+			    char *buf)
+{
+	u32 ledmode;
+
+	ledmode = utils_sysfs_show_wrapper(dev, RADIO_GPIO_CDC_LEDMODE2_ADDR,
+					   RADIO_GPIO_CDC_LEDMODE2_OFFSET,
+					   RADIO_GPIO_CDC_LEDMODE2_MASK);
+	return sprintf(buf, "%d\n", ledmode);
+}
+
+/**
+ * ledmode_store - Writes to the current LED mode register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's LED mode value
+ * to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t ledmode_store(struct device *dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t count)
+{
+	u32 ledmode;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &ledmode);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_GPIO_CDC_LEDMODE2_ADDR,
+				  RADIO_GPIO_CDC_LEDMODE2_OFFSET,
+				  RADIO_GPIO_CDC_LEDMODE2_MASK, ledmode);
+	return count;
+}
+static DEVICE_ATTR_RW(ledmode);
+
+/**
+ * ledgpio_show - Returns the current LED gpio
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's LED gpio value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t ledgpio_show(struct device *dev, struct device_attribute *attr,
+			    char *buf)
+{
+	u32 ledgpio;
+
+	ledgpio = utils_sysfs_show_wrapper(dev, RADIO_GPIO_CDC_LEDGPIO_ADDR,
+					   RADIO_GPIO_CDC_LEDGPIO_OFFSET,
+					   RADIO_GPIO_CDC_LEDGPIO_MASK);
+	return sprintf(buf, "%d\n", ledgpio);
+}
+
+/**
+ * ledgpio_store - Writes to the current LED gpio register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's LED gpio value
+ * to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t ledgpio_store(struct device *dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t count)
+{
+	u32 ledgpio;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &ledgpio);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_GPIO_CDC_LEDGPIO_ADDR,
+				  RADIO_GPIO_CDC_LEDGPIO_OFFSET,
+				  RADIO_GPIO_CDC_LEDGPIO_MASK, ledgpio);
+	return count;
+}
+static DEVICE_ATTR_RW(ledgpio);
+
+/**
+ * dip_status_show - Returns the current DIP switch value
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the GPIO DIP switch value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t dip_status_show(struct device *dev,
+			       struct device_attribute *attr, char *buf)
+{
+	u32 dip_status;
+
+	dip_status = utils_sysfs_show_wrapper(dev, RADIO_GPIO_CDC_LEDGPIO_ADDR,
+					      RADIO_GPIO_CDC_LEDGPIO_OFFSET,
+					      RADIO_GPIO_CDC_LEDGPIO_MASK);
+	return sprintf(buf, "0x%08x\n", dip_status);
+}
+static DEVICE_ATTR_RO(dip_status);
+
+/**
+ * sw_trigger_show - Returns the current SW trigger status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's SW trigger status value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t sw_trigger_show(struct device *dev,
+			       struct device_attribute *attr,
+			       char *buf)
+{
+	u32 sw_trigger;
+
+	sw_trigger = utils_sysfs_show_wrapper(dev, RADIO_SW_TRIGGER_ADDR,
+					      RADIO_SW_TRIGGER_OFFSET,
+					      RADIO_SW_TRIGGER_MASK);
+	return sprintf(buf, "%d\n", sw_trigger);
+}
+
+/**
+ * sw_trigger_store - Writes to the SW trigger status register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's SW trigger
+ * value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t sw_trigger_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+	u32 sw_trigger;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &sw_trigger);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_SW_TRIGGER_ADDR,
+				  RADIO_SW_TRIGGER_OFFSET,
+				  RADIO_SW_TRIGGER_MASK, sw_trigger);
+	return count;
+}
+static DEVICE_ATTR_RW(sw_trigger);
+
+/**
+ * radio_enable_show - Returns the current radio enable status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's radio enable value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_enable_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	u32 radio_enable;
+
+	radio_enable = utils_sysfs_show_wrapper(dev, RADIO_CDC_ENABLE_ADDR,
+						RADIO_CDC_ENABLE_OFFSET,
+						RADIO_CDC_ENABLE_MASK);
+	return sprintf(buf, "%d\n", radio_enable);
+}
+
+/**
+ * radio_enable_store - Writes to the radio enable register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's radio enable
+ * value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t radio_enable_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t count)
+{
+	u32 radio_enable;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &radio_enable);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_CDC_ENABLE_ADDR,
+				  RADIO_CDC_ENABLE_OFFSET,
+				  RADIO_CDC_ENABLE_MASK,
+				  radio_enable);
+	return count;
+}
+static DEVICE_ATTR_RW(radio_enable);
+
+/**
+ * radio_error_show - Returns the current radio error status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the error status
+ *
+ * Reads and writes the traffic gen's radio error value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_error_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	u32 radio_error;
+
+	radio_error = utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_ADDR,
+					       RADIO_CDC_STATUS_OFFSET,
+					       RADIO_CDC_STATUS_MASK);
+	return sprintf(buf, "%d\n", radio_error);
+}
+static DEVICE_ATTR_RO(radio_error);
+
+/**
+ * radio_status_show - Returns the current radio status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the status
+ *
+ * Reads and writes the traffic gen's radio status value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_status_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	u32 radio_status;
+
+	radio_status = utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_ADDR,
+						RADIO_CDC_STATUS_OFFSET,
+						RADIO_CDC_STATUS_MASK);
+	return sprintf(buf, "%d\n", radio_status);
+}
+static DEVICE_ATTR_RO(radio_status);
+
+/**
+ * radio_loopback_show - Returns the current radio loopback status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's radio loopback value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_loopback_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 radio_loopback;
+
+	radio_loopback = utils_sysfs_show_wrapper(dev,
+						  RADIO_CDC_LOOPBACK_ADDR,
+						  RADIO_CDC_LOOPBACK_OFFSET,
+						  RADIO_CDC_LOOPBACK_MASK);
+	return sprintf(buf, "%d\n", radio_loopback);
+}
+
+/**
+ * radio_loopback_store - Writes to the radio loopback register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's radio loopback
+ * value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t radio_loopback_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	u32 radio_loopback;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &radio_loopback);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_CDC_LOOPBACK_ADDR,
+				  RADIO_CDC_LOOPBACK_OFFSET,
+				  RADIO_CDC_LOOPBACK_MASK, radio_loopback);
+	return count;
+}
+static DEVICE_ATTR_RW(radio_loopback);
+
+/**
+ * radio_sink_enable_show - Returns the current radio sink enable status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's radio sink enable value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_sink_enable_show(struct device *dev,
+				      struct device_attribute *attr, char *buf)
+{
+	u32 sink_enable;
+
+	sink_enable = utils_sysfs_show_wrapper(dev, RADIO_SINK_ENABLE_ADDR,
+					       RADIO_SINK_ENABLE_OFFSET,
+					       RADIO_SINK_ENABLE_MASK);
+	return sprintf(buf, "%d\n", sink_enable);
+}
+
+/**
+ * radio_sink_enable_store - Writes to the radio sink enable register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's radio sink
+ * enable value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t radio_sink_enable_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	u32 sink_enable;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &sink_enable);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_SINK_ENABLE_ADDR,
+				  RADIO_SINK_ENABLE_OFFSET,
+				  RADIO_SINK_ENABLE_MASK, sink_enable);
+	return count;
+}
+static DEVICE_ATTR_RW(radio_sink_enable);
+
+/**
+ * antenna_status_show - Returns the status for all antennas
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's status for all antennas
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t antenna_status_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 status_0_31;
+	u32 status_63_32;
+	u32 status_95_64;
+	u32 status_127_96;
+
+	status_0_31 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_31_0_ADDR,
+				 RADIO_CDC_STATUS_31_0_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_31_0_MASK));
+	status_63_32 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_63_32_ADDR,
+				 RADIO_CDC_STATUS_63_32_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_63_32_MASK));
+	status_95_64 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_95_64_ADDR,
+				 RADIO_CDC_STATUS_95_64_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_95_64_MASK));
+	status_127_96 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_127_96_ADDR,
+				 RADIO_CDC_STATUS_127_96_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_127_96_MASK));
+
+	return sprintf(buf, "0x%08x 0x%08x 0x%08x 0x%08x\n",
+		       status_0_31, status_63_32, status_95_64, status_127_96);
+}
+static DEVICE_ATTR_RO(antenna_status);
+
+/**
+ * antenna_error_show - Returns the error for all antennas
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's error for all antennas
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t antenna_error_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	u32 error_0_31;
+	u32 error_63_32;
+	u32 error_95_64;
+	u32 error_127_96;
+
+	error_0_31 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_31_0_ADDR,
+				 RADIO_CDC_ERROR_31_0_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_31_0_MASK));
+	error_63_32 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_63_32_ADDR,
+				 RADIO_CDC_ERROR_63_32_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_63_32_MASK));
+	error_95_64 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_95_64_ADDR,
+				 RADIO_CDC_ERROR_95_64_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_95_64_MASK));
+	error_127_96 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_127_96_ADDR,
+				 RADIO_CDC_ERROR_127_96_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_127_96_MASK));
+
+	return sprintf(buf, "0x%08x 0x%08x 0x%08x 0x%08x\n",
+		       error_0_31, error_63_32, error_95_64, error_127_96);
+}
+static DEVICE_ATTR_RO(antenna_error);
+
+/**
+ * framer_packet_size_show - Returns the size of the framer's packet
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's framer packet size value
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t framer_packet_size_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	u32 packet_size;
+
+	packet_size = utils_sysfs_show_wrapper(dev, FRAM_PACKET_DATA_SIZE_ADDR,
+					       FRAM_PACKET_DATA_SIZE_OFFSET,
+					       FRAM_PACKET_DATA_SIZE_MASK);
+	return sprintf(buf, "%d\n", packet_size);
+}
+
+/**
+ * framer_packet_size_store - Writes to the framer's packet size register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's framer packet
+ * size value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t framer_packet_size_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t count)
+{
+	u32 packet_size;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &packet_size);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, FRAM_PACKET_DATA_SIZE_ADDR,
+				  FRAM_PACKET_DATA_SIZE_OFFSET,
+				  FRAM_PACKET_DATA_SIZE_MASK, packet_size);
+	return count;
+}
+static DEVICE_ATTR_RW(framer_packet_size);
+
+/**
+ * framer_pause_size_show - Returns the size of the framer's pause
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's framer pause size value
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t framer_pause_size_show(struct device *dev,
+				      struct device_attribute *attr, char *buf)
+{
+	u32 pause_size;
+
+	pause_size = utils_sysfs_show_wrapper(dev, FRAM_PAUSE_DATA_SIZE_ADDR,
+					      FRAM_PAUSE_DATA_SIZE_OFFSET,
+					      FRAM_PAUSE_DATA_SIZE_MASK);
+	return sprintf(buf, "%d\n", pause_size);
+}
+
+/**
+ * framer_pause_size_store - Writes to the framer's pause size register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's framer pause
+ * size value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t framer_pause_size_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	u32 pause_size;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &pause_size);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, FRAM_PAUSE_DATA_SIZE_ADDR,
+				  FRAM_PAUSE_DATA_SIZE_OFFSET,
+				  FRAM_PAUSE_DATA_SIZE_MASK, pause_size);
+	return count;
+}
+static DEVICE_ATTR_RW(framer_pause_size);
+
+static struct attribute *xroe_traffic_gen_attrs[] = {
+	&dev_attr_radio_id.attr,
+	&dev_attr_timeout_enable.attr,
+	&dev_attr_timeout_status.attr,
+	&dev_attr_timeout_value.attr,
+	&dev_attr_ledmode.attr,
+	&dev_attr_ledgpio.attr,
+	&dev_attr_dip_status.attr,
+	&dev_attr_sw_trigger.attr,
+	&dev_attr_radio_enable.attr,
+	&dev_attr_radio_error.attr,
+	&dev_attr_radio_status.attr,
+	&dev_attr_radio_loopback.attr,
+	&dev_attr_radio_sink_enable.attr,
+	&dev_attr_antenna_status.attr,
+	&dev_attr_antenna_error.attr,
+	&dev_attr_framer_packet_size.attr,
+	&dev_attr_framer_pause_size.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(xroe_traffic_gen);
+
+/**
+ * xroe_traffic_gen_sysfs_init - Creates the xroe sysfs directory and entries
+ * @dev:	The device's structure
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroetrafficgen sysfs directory and entries
+ */
+int xroe_traffic_gen_sysfs_init(struct device *dev)
+{
+	int ret;
+
+	dev->groups = xroe_traffic_gen_groups;
+	ret = sysfs_create_group(&dev->kobj, *xroe_traffic_gen_groups);
+	if (ret)
+		dev_err(dev, "sysfs creation failed\n");
+
+	return ret;
+}
+
+/**
+ * xroe_traffic_gen_sysfs_exit - Deletes the xroe sysfs directory and entries
+ * @dev:	The device's structure
+ *
+ * Deletes the xroetrafficgen sysfs directory and entries
+ */
+void xroe_traffic_gen_sysfs_exit(struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, *xroe_traffic_gen_groups);
+}
diff --git a/drivers/staging/xroetrafficgen/xroe-traffic-gen.c b/drivers/staging/xroetrafficgen/xroe-traffic-gen.c
new file mode 100644
index 000000000..a3409974f
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/xroe-traffic-gen.c
@@ -0,0 +1,124 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/stat.h>
+#include <linux/sysfs.h>
+#include "xroe-traffic-gen.h"
+
+#define DRIVER_NAME "xroe_traffic_gen"
+
+static struct platform_driver xroe_traffic_gen_driver;
+
+/**
+ * xroe_traffic_gen_probe - Probes the device tree to locate the traffic gen
+ * block
+ * @pdev:	The structure containing the device's details
+ *
+ * Probes the device tree to locate the traffic gen block and maps it to
+ * the kernel virtual memory space
+ *
+ * Return: 0 on success or a negative errno on error.
+ */
+static int xroe_traffic_gen_probe(struct platform_device *pdev)
+{
+	struct xroe_traffic_gen_local *lp;
+	struct resource *r_mem; /* IO mem resources */
+	struct device *dev = &pdev->dev;
+
+	lp = devm_kzalloc(&pdev->dev, sizeof(*lp), GFP_KERNEL);
+	if (!lp)
+		return -ENOMEM;
+
+	/* Get iospace for the device */
+	/*
+	 * TODO: Use platform_get_resource_byname() instead when the DT entry
+	 * of the traffic gen block has been finalised (when it gets out of
+	 * the development stage).
+	 */
+	r_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp->base_addr = devm_ioremap_resource(&pdev->dev, r_mem);
+	if (IS_ERR(lp->base_addr))
+		return PTR_ERR(lp->base_addr);
+
+	dev_set_drvdata(dev, lp);
+	xroe_traffic_gen_sysfs_init(dev);
+	return 0;
+}
+
+/**
+ * xroe_traffic_gen_remove - Removes the sysfs entries created by the driver
+ * @pdev:	The structure containing the device's details
+ *
+ * Removes the sysfs entries created by the driver
+ *
+ * Return: 0
+ */
+static int xroe_traffic_gen_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	xroe_traffic_gen_sysfs_exit(dev);
+	return 0;
+}
+
+/**
+ * xroe_traffic_gen_init - Registers the driver
+ *
+ * Return: 0 on success, -1 on allocation error
+ *
+ * Registers the traffic gen driver and creates the sysfs entries related
+ * to it
+ */
+static int __init xroe_traffic_gen_init(void)
+{
+	int ret;
+
+	pr_info("XROE traffic generator driver init\n");
+	ret = platform_driver_register(&xroe_traffic_gen_driver);
+	return ret;
+}
+
+/**
+ * xroe_traffic_gen_exit - Destroys the driver
+ *
+ * Unregisters the traffic gen driver
+ */
+static void __exit xroe_traffic_gen_exit(void)
+{
+	platform_driver_unregister(&xroe_traffic_gen_driver);
+	pr_debug("XROE traffic generator driver exit\n");
+}
+
+static const struct of_device_id xroe_traffic_gen_of_match[] = {
+	{ .compatible = "xlnx,roe-traffic-gen-1.0", },
+	{ /* end of list */ },
+};
+MODULE_DEVICE_TABLE(of, xroe_traffic_gen_of_match);
+
+static struct platform_driver xroe_traffic_gen_driver = {
+	.driver = {
+		.name = DRIVER_NAME,
+		.of_match_table	= xroe_traffic_gen_of_match,
+	},
+	.probe = xroe_traffic_gen_probe,
+	.remove = xroe_traffic_gen_remove,
+};
+
+module_init(xroe_traffic_gen_init);
+module_exit(xroe_traffic_gen_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Xilinx Inc.");
+MODULE_DESCRIPTION("Xilinx Radio over Ethernet Traffic Generator driver");
diff --git a/drivers/staging/xroetrafficgen/xroe-traffic-gen.h b/drivers/staging/xroetrafficgen/xroe-traffic-gen.h
new file mode 100644
index 000000000..55d968d89
--- /dev/null
+++ b/drivers/staging/xroetrafficgen/xroe-traffic-gen.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+struct xroe_traffic_gen_local {
+	void __iomem *base_addr;
+};
+
+enum { XROE_SIZE_MAX = 15 };
+
+int xroe_traffic_gen_sysfs_init(struct device *dev);
+void xroe_traffic_gen_sysfs_exit(struct device *dev);
diff --git a/include/linux/xlnx/xilinx-hdcp1x-cipher.h b/include/linux/xlnx/xilinx-hdcp1x-cipher.h
new file mode 100644
index 000000000..0757d1713
--- /dev/null
+++ b/include/linux/xlnx/xilinx-hdcp1x-cipher.h
@@ -0,0 +1,294 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Xilinx HDCP1X Cipher driver
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ *
+ * Author: Jagadeesh Banisetti <jagadeesh.banisetti@xilinx.com>
+ */
+
+#ifndef __XILINX_HDCP1X_CIPHER_H__
+#define __XILINX_HDCP1X_CIPHER_H__
+
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/types.h>
+
+/* HDCP Cipher register offsets */
+#define XHDCP1X_CIPHER_REG_VERSION		0x00
+#define XHDCP1X_CIPHER_REG_TYPE			0x04
+#define XHDCP1X_CIPHER_REG_SCRATCH		0x08
+#define XHDCP1X_CIPHER_REG_CONTROL		0x0C
+#define XHDCP1X_CIPHER_REG_STATUS		0x10
+#define XHDCP1X_CIPHER_REG_INTERRUPT_MASK	0x14
+#define XHDCP1X_CIPHER_REG_INTERRUPT_STATUS	0x18
+#define XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H	0x20
+#define XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L	0x24
+#define XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL	0x2C
+#define XHDCP1X_CIPHER_REG_KEYMGMT_STATUS	0x30
+#define XHDCP1X_CIPHER_REG_KSV_LOCAL_H		0x38
+#define XHDCP1X_CIPHER_REG_KSV_LOCAL_L		0x3C
+#define XHDCP1X_CIPHER_REG_KSV_REMOTE_H		0x40
+#define XHDCP1X_CIPHER_REG_KSV_REMOTE_L		0x44
+#define XHDCP1X_CIPHER_REG_KM_H			0x48
+#define XHDCP1X_CIPHER_REG_KM_L			0x4C
+#define XHDCP1X_CIPHER_REG_CIPHER_CONTROL	0x50
+#define XHDCP1X_CIPHER_REG_CIPHER_STATUS	0x54
+#define XHDCP1X_CIPHER_REG_CIPHER_BX		0x58
+#define XHDCP1X_CIPHER_REG_CIPHER_BY		0x5C
+#define XHDCP1X_CIPHER_REG_CIPHER_BZ		0x60
+#define XHDCP1X_CIPHER_REG_CIPHER_KX		0x64
+#define XHDCP1X_CIPHER_REG_CIPHER_KY		0x68
+#define XHDCP1X_CIPHER_REG_CIPHER_KZ		0x6C
+#define XHDCP1X_CIPHER_REG_CIPHER_MI_H		0x70
+#define XHDCP1X_CIPHER_REG_CIPHER_MI_L		0x74
+#define XHDCP1X_CIPHER_REG_CIPHER_RI		0x78
+#define XHDCP1X_CIPHER_REG_CIPHER_RO		0x7C
+#define XHDCP1X_CIPHER_REG_CIPHER_MO_H		0x80
+#define XHDCP1X_CIPHER_REG_CIPHER_MO_L		0x84
+#define XHDCP1X_CIPHER_REG_BLANK_VALUE		0xBC
+#define XHDCP1X_CIPHER_REG_BLANK_SEL		0xC0
+
+/* HDCP Cipher register bit mask definitions */
+#define XHDCP1X_CIPHER_BITMASK_TYPE_PROTOCOL			GENMASK(1, 0)
+#define XHDCP1X_CIPHER_BITMASK_TYPE_DIRECTION			BIT(2)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE			BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE			BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_NUM_LANES		GENMASK(6, 4)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_RESET			BIT(31)
+#define XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL		BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE		BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_LOCAL_KSV	BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_BEGIN_KM		BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_ABORT_KM		BIT(2)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_SET_SELECT	GENMASK(18, 16)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KSV_READY		BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KM_READY		BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE	BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_REQUEST		GENMASK(10, 8)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_XOR_IN_PROG	BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_REQUEST_IN_PROG	GENMASK(10, 8)
+#define XHDCP1X_CIPHER_BITMASK_BLANK_VALUE			GENMASK(31, 0)
+#define XHDCP1X_CIPHER_BITMASK_BLANK_SEL			BIT(0)
+
+/* HDCP Cipher register bit value definitions */
+#define XHDCP1X_CIPHER_VALUE_TYPE_PROTOCOL_DP			0
+#define XHDCP1X_CIPHER_VALUE_TYPE_PROTOCOL_HDMI			1
+#define XHDCP1X_CIPHER_VALUE_TYPE_DIRECTION_MASK		BIT(2)
+#define XHDCP1X_CIPHER_VALUE_TYPE_DIRECTION_RX			0
+#define XHDCP1X_CIPHER_VALUE_TYPE_DIRECTION_TX			1
+#define XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_BLOCK	BIT(8)
+#define XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_REKEY	BIT(9)
+#define XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_RNG		BIT(10)
+
+#define XHDCP1X_CIPHER_SIZE_LOCAL_KSV				5
+#define XHDCP1X_CIPHER_KSV_RETRIES				1024
+#define XHDCP1X_CIPHER_SHIFT_NUM_LANES				4
+#define XHDCP1X_CIPHER_MAX_LANES				4
+#define XHDCP1X_CIPHER_INTR_ALL					GENMASK(31, 0)
+#define XHDCP1X_CIPHER_KEYSELECT_MAX_VALUE			8
+#define XHDCP1X_CIPHER_SHIFT_KEYMGMT_CONTROL_SET_SELECT		16
+#define XHDCP1X_CIPHER_NUM_LANES_1				1
+#define XHDCP1X_CIPHER_NUM_LANES_2				2
+#define XHDCP1X_CIPHER_NUM_LANES_4				4
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BX			GENMASK(27, 0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BY			GENMASK(27, 0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BZ_REPEATER		BIT(8)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BZ			GENMASK(7, 0)
+#define XHDCP1X_CIPHER_SHIFT_CIPHER_B				28
+#define XHDCP1X_CIPHER_VALUE_SHIFT				32
+#define XHDCP1X_CIPHER_DWORD_VALUE				0xFFFFFFFFul
+#define XHDCP1X_CIPHER_SET_B					0x0FFFFFFFul
+#define XHDCP1X_CIPHER_DEFAULT_STREAMMAP			0x01ul
+#define XHDCP1X_CIPHER_KSV_VAL					0xFF
+
+enum xhdcp1x_cipher_request_type {
+	XHDCP1X_CIPHER_REQUEST_BLOCK = 0,
+	XHDCP1X_CIPHER_REQUEST_REKEY = 1,
+	XHDCP1X_CIPHER_REQUEST_RNG = 2,
+	XHDCP1X_CIPHER_REQUEST_MAX = 3,
+};
+
+/**
+ * struct xhdcp1x_cipher - hdcp1x cipher driver structure
+ * @interface_base: iommu base of interface driver
+ * @dev: Platform data
+ * @is_tx: flag for tx, 1 for tx and 0 for rx
+ * @is_hdmi: flag for HDMI, 1 for HDMI and 0 for DP
+ * @num_lanes: number of active lanes in interface driver, possible lanes are 1, 2 and 4
+ */
+struct xhdcp1x_cipher {
+	void __iomem *interface_base;
+	struct device *dev;
+	u8 is_tx;
+	u8 is_hdmi;
+	u8 num_lanes;
+};
+
+#if IS_ENABLED(CONFIG_XLNX_HDCP1X_CIPHER)
+void *xhdcp1x_cipher_init(struct device *dev, void __iomem *hdcp1x_base);
+int xhdcp1x_cipher_reset(void *ref);
+int xhdcp1x_cipher_enable(void *ref);
+int xhdcp1x_cipher_disable(void *ref);
+int xhdcp1x_cipher_set_num_lanes(void *ref, u8 num_lanes);
+int xhdcp1x_cipher_set_keyselect(void *ref, u8 keyselect);
+int xhdcp1x_cipher_load_bksv(void *ref, u8 *buf);
+int xhdcp1x_cipher_set_remoteksv(void *ref, u64 ksv);
+int xhdcp1x_cipher_get_ro(void *ref, u16 *ro);
+int xhdcp1x_cipher_set_b(void *ref, u64 an, bool is_repeater);
+int xhdcp1x_cipher_is_request_complete(void *ref);
+int xhdcp1x_cipher_set_link_state_check(void *ref, bool is_enabled);
+int xhdcp1x_cipher_get_interrupts(void *ref, u32 *interrupts);
+int xhdcp1x_cipher_is_linkintegrity_failed(void *ref);
+int xhdcp1x_cipher_set_ri(void *ref, bool enable);
+int xhdcp1x_cipher_is_request_to_change_ri(void *ref);
+int xhdcp1x_cipher_get_ri(void *ref, u16 *ri);
+int xhdcp1x_cipher_load_aksv(void *ref, u8 *buf);
+int xhdcp1x_cipher_do_request(void *ref, enum xhdcp1x_cipher_request_type request);
+u64 xhdcp1x_cipher_get_localksv(struct xhdcp1x_cipher *cipher);
+int xhdcp1x_cipher_getencryption(void *ref);
+int xhdcp1x_cipher_disableencryption(void *ref, u64 streammap);
+int xhdcp1x_cipher_setb(void *ref, u32 bx, u32 by, u32 bz);
+int xhdcp1x_cipher_enable_encryption(void *ref, u64 streammap);
+int xhdcp1x_cipher_set_ri_update(void *ref, int is_enabled);
+u64 xhdcp1x_cipher_get_mi(void *ref);
+u64 xhdcp1x_cipher_get_mo(void *ref);
+#else
+static inline void *xhdcp1x_cipher_init(struct device *dev,
+					void __iomem *hdcp1x_base)
+{
+	return NULL;
+}
+
+static inline int xhdcp1x_cipher_reset(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_enable(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_disable(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_num_lanes(void *ref, u8 num_lanes)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_keyselect(void *ref, u8 keyselect)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_load_bksv(void *ref, u8 *buf)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_remoteksv(void *ref, u64 ksv)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_get_ro(void *ref, u16 *ro)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_b(void *ref, u64 value, bool is_repeater)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_is_request_complete(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_link_state_check(void *ref, u8 is_enabled)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_get_interrupts(void *ref, u32 *interrupts)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_is_linkintegrity_failed(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_get_ri(void *ref, u16 *ri)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_is_request_to_change_ri(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_ri(void *ref, bool enable)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_load_aksv(void *ref, u8 *buf)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_do_request(void *ref, enum xhdcp1x_cipher_request_type request)
+{
+	return -EINVAL;
+}
+
+static inline u64 xhdcp1x_cipher_get_localksv(struct xhdcp1x_cipher *cipher)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_getencryption(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_disableencryption(void *ref, u64 streammap)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_setb(void *ref, u32 bx, u32 by, u32 bz)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_enable_encryption(void *ref, u64 streammap)
+{
+	return -EINVAL;
+}
+
+static inline u64 xhdcp1x_cipher_get_mi(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline u64 xhdcp1x_cipher_get_mo(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_ri_update(void *ref, int is_enabled)
+{
+	return -EINVAL;
+}
+#endif /* CONFIG_XLNX_HDCP1X_CIPHER */
+
+#endif /* __XILINX_HDCP1X_CIPHER_H__ */
diff --git a/include/linux/xlnx/xlnx_hdcp2x_cipher.h b/include/linux/xlnx/xlnx_hdcp2x_cipher.h
new file mode 100644
index 000000000..d125c2bf1
--- /dev/null
+++ b/include/linux/xlnx/xlnx_hdcp2x_cipher.h
@@ -0,0 +1,103 @@
+/* SPDX-License-Identifier: GPL-2.0*/
+/*
+ * Xilinx HDCP2X Cipher driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_HDCP2X_CIPHER_H_
+#define _XLNX_HDCP2X_CIPHER_H_
+
+#include <linux/types.h>
+
+#define XHDCP2X_CIPHER_VER_BASE			(0 * 64)
+#define XHDCP2X_CIPHER_VER_ID_OFFSET		((XHDCP2X_CIPHER_VER_BASE) + (0 * 4))
+#define XHDCP2X_CIPHER_VER_VERSION_OFFSET	((XHDCP2X_CIPHER_VER_BASE) + (1 * 4))
+
+#define XHDCP2X_CIPHER_REG_BASE			(1 * 64)
+#define XHDCP2X_CIPHER_REG_CTRL_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (0 * 4))
+#define XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (1 * 4))
+#define XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (2 * 4))
+#define XHDCP2X_CIPHER_REG_STA_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (3 * 4))
+#define XHDCP2X_CIPHER_REG_KS_1_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (4 * 4))
+#define XHDCP2X_CIPHER_REG_KS_2_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (5 * 4))
+#define XHDCP2X_CIPHER_REG_KS_3_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (6 * 4))
+#define XHDCP2X_CIPHER_REG_KS_4_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (7 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_1_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (8 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_2_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (9 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_3_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (10 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_4_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (11 * 4))
+#define XHDCP2X_CIPHER_REG_RIV_1_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (12 * 4))
+#define XHDCP2X_CIPHER_REG_RIV_2_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (13 * 4))
+#define XHDCP2X_CIPHER_REG_INPUTCTR_1_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (14 * 4))
+#define XHDCP2X_CIPHER_REG_INPUTCTR_2_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (15 * 4))
+
+#define XHDCP2X_CIPHER_REG_CTRL_RUN_MASK	BIT(0)
+#define XHDCP2X_CIPHER_REG_CTRL_IE_MASK		BIT(1)
+#define XHDCP2X_CIPHER_REG_CTRL_ENCRYPT_MASK	BIT(3)
+#define XHDCP2X_CIPHER_REG_CTRL_BLANK_MASK	BIT(4)
+#define XHDCP2X_CIPHER_REG_CTRL_NOISE_MASK	BIT(5)
+#define XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_MASK	GENMASK(9, 6)
+#define XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_BIT_POS	6
+
+#define XHDCP2X_CIPHER_REG_STA_IRQ_MASK		BIT(0)
+#define XHDCP2X_CIPHER_REG_STA_EVT_MASK		BIT(1)
+#define XHDCP2X_CIPHER_REG_STA_ENCRYPTED_MASK	BIT(2)
+#define XHDCP2X_CIPHER_REG_CTRL_MODE_MASK	BIT(2)
+
+#define XHDCP2X_CIPHER_KEY_LENGTH		16
+#define XHDCP2X_CIPHER_SHIFT_16			16
+#define XHDCP2X_CIPHER_MASK_16			GENMASK(31, 16)
+#define XHDCP2X_CIPHER_VER_ID			0x2200
+
+/**
+ * struct xlnx_hdcp2x_cipher_hw - HDCP2X internal cipher engine hardware structure
+ * @cipher_coreaddress: HDCP2X cipher core address
+ */
+struct xlnx_hdcp2x_cipher_hw {
+	void __iomem *cipher_coreaddress;
+};
+
+#define xlnx_hdcp2x_cipher_write(coreaddress, reg_offset, data) \
+	writel(data, (coreaddress) + (reg_offset))
+
+#define xlnx_hdcp2x_cipher_read(coreaddress, reg_offset) \
+	readl((coreaddress) + (reg_offset))
+
+#define xlnx_hdcp2x_cipher_get_status(cipher_address) \
+	xlnx_hdcp2x_cipher_read(cipher_address, XHDCP2X_CIPHER_REG_STA_OFFSET)
+
+#define xlnx_hdcp2x_cipher_is_encrypted(cipher_address) \
+	((xlnx_hdcp2x_cipher_get_status(cipher_address) \
+	& XHDCP2X_CIPHER_REG_STA_ENCRYPTED_MASK) == \
+	XHDCP2X_CIPHER_REG_STA_ENCRYPTED_MASK)
+
+#define xlnx_hdcp2x_cipher_enable(cipher_address) \
+	xlnx_hdcp2x_cipher_write(cipher_address, \
+	(XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET), (XHDCP2X_CIPHER_REG_CTRL_RUN_MASK))
+
+#define xlnx_hdcp2x_cipher_disable(cipher_address) \
+		xlnx_hdcp2x_cipher_write(cipher_address, \
+		(XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET), (XHDCP2X_CIPHER_REG_CTRL_RUN_MASK))
+
+#define xlnx_hdcp2x_cipher_set_txmode(cipher_address) \
+	xlnx_hdcp2x_cipher_write(cipher_address, \
+	(XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET), (XHDCP2X_CIPHER_REG_CTRL_MODE_MASK))
+
+#define xlnx_hdcp2x_cipher_set_rxmode(cipher_address) \
+		xlnx_hdcp2x_cipher_write(cipher_address, \
+		(XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET), (XHDCP2X_CIPHER_REG_CTRL_MODE_MASK))
+
+int xlnx_hdcp2x_cipher_cfg_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg);
+void xlnx_hdcp2x_cipher_set_keys(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				 const u8 *buf, u32 offset, u16 len);
+void xlnx_hdcp2x_cipher_set_lanecount(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				      u8 lanecount);
+void xlnx_hdcp2x_cipher_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg);
+void xlnx_hdcp2x_rx_cipher_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg);
+void  xlnx_hdcp2x_tx_cipher_update_encryption(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+					      u8 enable);
+
+#endif
diff --git a/include/linux/xlnx/xlnx_hdcp2x_mmult.h b/include/linux/xlnx/xlnx_hdcp2x_mmult.h
new file mode 100644
index 000000000..976223a6c
--- /dev/null
+++ b/include/linux/xlnx/xlnx_hdcp2x_mmult.h
@@ -0,0 +1,59 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Xilinx HDCP 2X Montgomery Modular Multiplier Driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Kunal Rane <kunal.rane@amd.com>
+ */
+
+#ifndef __XLNX_HDCP2X_MMULT_H__
+#define __XLNX_HDCP2X_MMULT_H__
+
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/platform_device.h>
+#include <linux/types.h>
+
+#define XHDCP2X_MMULT_ADDR_AP		0x000
+#define XHDCP2X_MMULT_ADDR_U_BASE	0x040
+#define XHDCP2X_MMULT_ADDR_U_HIGH	0x07f
+#define XHDCP2X_MMULT_A_BASE		0x080
+#define XHDCP2X_MMULT_A_HIGH		0x0bf
+#define XHDCP2X_MMULT_B_BASE		0x0c0
+#define XHDCP2X_MMULT_B_HIGH		0x0ff
+#define XHDCP2X_MMULT_N_BASE		0x100
+#define XHDCP2X_MMULT_N_HIGH		0x13f
+#define XHDCP2X_MMULT_NPRIME_BASE	0x140
+#define XHDCP2X_MMULT_NPRIME_HIGH	0x17f
+#define XHDCP2X_MMULT_OFFSET_MULT	0x4
+#define XHDCP2X_MMULT_ADDR_AP_RD	0x80
+#define XHDCP2X_MMULT_ADDR_AP_WR	0x01
+#define XHDCP2X_MMULT_CHECK_CONST	0x1
+#define XHDCP2X_MMULT_MAX_TYPES		4
+#define XHDCP2X_MMULT_ADDR		2
+
+#define XHDCP2X_MMULT_DONE		BIT(1)
+#define XHDCP2X_MMULT_READY		BIT(0)
+
+struct xlnx_hdcp2x_mmult_hw {
+	void __iomem *mmult_coreaddress;
+};
+
+enum xhdcp2x_mmult_type {
+	XHDCP2X_MMULT_A = 0,
+	XHDCP2X_MMULT_B = 1,
+	XHDCP2X_MMULT_N = 2,
+	XHDCP2X_MMULT_NPRIME = 3
+};
+
+u32 xlnx_hdcp2x_mmult_is_done(struct xlnx_hdcp2x_mmult_hw *mmult_cfg);
+u32 xlnx_hdcp2x_mmult_is_ready(struct xlnx_hdcp2x_mmult_hw *mmult_cfg);
+u32 xlnx_hdcp2x_mmult_read_u_words(struct xlnx_hdcp2x_mmult_hw *mmult_cfg,
+				   int offset, int *data, int length);
+int xlnx_hdcp2x_mmult_write_type(struct xlnx_hdcp2x_mmult_hw *mmult_cfg,
+				 int offset, int *data, int length, int type);
+int xlnx_hdcp2x_mmult_cfginit(struct xlnx_hdcp2x_mmult_hw *mmult_cfg);
+void xlnx_hdcp2x_mmult_enable(struct xlnx_hdcp2x_mmult_hw *mmult_cfg);
+
+#endif  /* __XLNX_HDCP2X_MMULT_H__ */
diff --git a/include/linux/xlnx/xlnx_hdcp_common.h b/include/linux/xlnx/xlnx_hdcp_common.h
new file mode 100644
index 000000000..93c25cb1b
--- /dev/null
+++ b/include/linux/xlnx/xlnx_hdcp_common.h
@@ -0,0 +1,34 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Xilinx HDCP Common driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_HDCP_COMMON_H_
+#define _XLNX_HDCP_COMMON_H_
+
+#include <linux/types.h>
+
+int mp_mod_exp(unsigned int y[], const unsigned int x[], const unsigned int n[],
+	       unsigned int d[], size_t ndigits);
+size_t mp_conv_to_octets(const unsigned int a[], size_t ndigits, unsigned char *c,
+			 size_t nbytes);
+size_t mp_conv_from_octets(unsigned int a[], size_t ndigits, const unsigned char *c,
+			   size_t nbytes);
+u32 mp_subtract(u32 w[], const u32 u[], const u32 v[], size_t ndigits);
+unsigned int mp_add(unsigned int w[], const unsigned int u[], const unsigned int v[],
+		    size_t ndigits);
+unsigned int mp_shift_left(unsigned int a[], const unsigned int *b, size_t shift, size_t ndigits);
+int mp_multiply(unsigned int w[], const unsigned int u[], const unsigned int v[], size_t ndigits);
+int mp_mod_mult(u32 a[], const u32 x[], const u32 y[], u32 m[], size_t ndigits);
+int mp_modulo(u32 r[], const u32 u[], size_t udigits, u32 v[], size_t vdigits);
+int mp_mod_inv(u32 inv[], const u32 u[], const u32 v[], size_t ndigits);
+int mp_equal(const u32 a[], const u32 b[], size_t ndigits);
+int mp_get_bit(u32 a[], size_t ndigits, size_t ibit);
+int mp_short_cmp(const u32 a[], u32 d, size_t ndigits);
+int mp_divide(unsigned int q[], unsigned int r[], const unsigned int u[],
+	      size_t udigits, unsigned int v[], size_t vdigits);
+#endif
diff --git a/include/linux/xlnx/xlnx_hdcp_rng.h b/include/linux/xlnx/xlnx_hdcp_rng.h
new file mode 100644
index 000000000..cb3e14ff5
--- /dev/null
+++ b/include/linux/xlnx/xlnx_hdcp_rng.h
@@ -0,0 +1,43 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Xilinx HDCP 2X Random Number Generator Driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_HDCP_RNG_H_
+#define _XLNX_HDCP_RNG_H_
+
+#include <linux/types.h>
+
+/**
+ * struct xlnx_hdcp2x_rng_hw - HDCP 2X random number generator configuration structure
+ * @rng_coreaddress: HDCP 2X random number generator core address
+ */
+struct xlnx_hdcp2x_rng_hw {
+	void __iomem *rng_coreaddress;
+};
+
+#define XHDCP2X_RNG_VER_BASE			(0 * 64)
+#define XHDCP2X_RNG_VER_ID_OFFSET		((XHDCP2X_RNG_VER_BASE) + (0 * 4))
+#define XHDCP2X_RNG_VER_VERSION_OFFSET		((XHDCP2X_RNG_VER_BASE) + (1 * 4))
+#define XHDCP2X_RNG_REG_BASE			(1 * 64)
+#define XHDCP2X_RNG_REG_CTRL_OFFSET		((XHDCP2X_RNG_REG_BASE) + (0 * 4))
+#define XHDCP2X_RNG_REG_CTRL_SET_OFFSET		((XHDCP2X_RNG_REG_BASE) + (1 * 4))
+#define XHDCP2X_RNG_REG_CTRL_CLR_OFFSET		((XHDCP2X_RNG_REG_BASE) + (2 * 4))
+#define XHDCP2X_RNG_REG_STA_OFFSET		((XHDCP2X_RNG_REG_BASE) + (3 * 4))
+#define XHDCP2X_RNG_REG_RN_1_OFFSET		((XHDCP2X_RNG_REG_BASE) + (4 * 4))
+#define XHDCP2X_RNG_SHIFT_16			16
+#define XHDCP2X_RNG_MASK_16			GENMASK(31, 16)
+#define XHDCP2X_RNG_VER_ID			0x2200
+#define XHDCP2X_RNG_REG_CTRL_RUN_MASK		BIT(0)
+
+int xlnx_hdcp2x_rng_cfg_init(struct xlnx_hdcp2x_rng_hw *rng_cfg);
+void xlnx_hdcp2x_rng_get_random_number(struct xlnx_hdcp2x_rng_hw *rng_cfg,
+				       u8 *writeptr, u16 length, u16 randomlength);
+void xlnx_hdcp2x_rng_enable(struct xlnx_hdcp2x_rng_hw *rng_cfg);
+void xlnx_hdcp2x_rng_disable(struct xlnx_hdcp2x_rng_hw *rng_cfg);
+
+#endif
diff --git a/include/linux/xlnx/xlnx_timer.h b/include/linux/xlnx/xlnx_timer.h
new file mode 100644
index 000000000..2ebdb0507
--- /dev/null
+++ b/include/linux/xlnx/xlnx_timer.h
@@ -0,0 +1,108 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * The Xilinx timer/counter component. This component supports the Xilinx
+ * timer/counter which supports the following features:
+ *  - Polled mode.
+ *  - Interrupt driven mode
+ *  - enabling and disabling specific timers
+ *  - PWM operation
+ *  - Cascade Operation
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_TIMER_H_
+#define _XLNX_TIMER_H_
+
+#include <linux/types.h>
+
+/**
+ * struct xlnx_hdcp_timer_hw - This structure contains hardware subcore configuration
+ * information about AXI Timer.
+ * @coreaddress: AXI Timer core address
+ * @sys_clock_freq: System Clock Frequency
+ */
+struct xlnx_hdcp_timer_hw {
+	void __iomem *coreaddress;
+	u32 sys_clock_freq;
+};
+
+/*
+ * Detailed register descriptions available in
+ * Programming Guide PG079.
+ * https://docs.xilinx.com/v/u/en-US/pg079-axi-timer
+ */
+#define XTC_DEVICE_TIMER_COUNT		2
+#define XTC_TIMER_COUNTER_OFFSET	16
+#define XTC_CASCADE_MODE_OPTION		BIT(7)
+#define XTC_ENABLE_ALL_OPTION		BIT(6)
+#define XTC_DOWN_COUNT_OPTION		BIT(5)
+#define XTC_CAPTURE_MODE_OPTION		BIT(4)
+#define XTC_INT_MODE_OPTION		BIT(3)
+#define XTC_AUTO_RELOAD_OPTION		BIT(2)
+#define XTC_EXT_COMPARE_OPTION		BIT(1)
+#define XTC_TIMER_0			0
+#define XTC_TIMER_1			1
+
+#define XTC_TCSR_OFFSET			0
+#define XTC_TLR_OFFSET			4
+#define XTC_TCR_OFFSET			8
+#define XTC_CSR_CASC_MASK		BIT(11)
+#define XTC_CSR_ENABLE_ALL_MASK		BIT(10)
+#define XTC_CSR_ENABLE_PWM_MASK		BIT(9)
+#define XTC_CSR_INT_OCCURED_MASK	BIT(8)
+#define XTC_CSR_ENABLE_TMR_MASK		BIT(7)
+#define XTC_CSR_ENABLE_INT_MASK		BIT(6)
+#define XTC_CSR_LOAD_MASK		BIT(5)
+#define XTC_CSR_AUTO_RELOAD_MASK	BIT(4)
+#define XTC_CSR_EXT_CAPTURE_MASK	BIT(3)
+#define XTC_CSR_EXT_GENERATE_MASK	BIT(2)
+#define XTC_CSR_DOWN_COUNT_MASK		BIT(1)
+#define XTC_CSR_CAPTURE_MODE_MASK	BIT(0)
+#define XTC_MAX_LOAD_VALUE		GENMASK(31, 0)
+#define XTC_COMPONENT_IS_READY		BIT(0)
+#define XTC_COMPONENT_IS_STARTED	BIT(1)
+
+typedef void (*xlnx_timer_cntr_handler) (void *callbackref, u8 tmr_cntr_number);
+
+/**
+ * struct xlnx_hdcp_timer_config - The user is required to allocate a
+ * variable of this type for every timer/counter device in the system.
+ * @hw_config: Configuration of timer hardware core
+ * @handler: Timer callback handler
+ * @callbackref: Timer callback reference
+ * @is_tmrcntr0_started: Timercnt0 is initialized and started
+ * @is_tmrcntr1_started: Timercnt1 is initialized and started
+ */
+struct xlnx_hdcp_timer_config {
+	struct xlnx_hdcp_timer_hw hw_config;
+	xlnx_timer_cntr_handler handler;
+	void *callbackref;
+	u32 is_tmrcntr0_started;
+	u32 is_tmrcntr1_started;
+};
+
+u32 xlnx_hdcp_tmrcntr_get_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				u8 tmr_cntr_number);
+int xlnx_hdcp_tmrcntr_init(struct xlnx_hdcp_timer_config *xtimercntr);
+void xlnx_hdcp_tmrcntr_stop(struct xlnx_hdcp_timer_config *xtimercntr,
+			    u8 tmr_cntr_number);
+void xlnx_hdcp_tmrcntr_start(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number);
+void xlnx_hdcp_tmrcntr_cfg_init(struct xlnx_hdcp_timer_config *xtimercntr);
+void xlnx_hdcp_tmrcntr_start(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number);
+void xlnx_hdcp_tmrcntr_set_reset_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				       u8 tmr_cntr_number, u32 reset_value);
+void xlnx_hdcp_tmrcntr_set_options(struct xlnx_hdcp_timer_config *xtimercntr,
+				   u8 tmr_cntr_number, u32 options);
+void xlnx_hdcp_tmrcntr_set_handler(struct xlnx_hdcp_timer_config *xtimercntr,
+				   xlnx_timer_cntr_handler funcptr,
+				   void *callbackref);
+void xlnx_hdcp_tmrcntr_interrupt_handler(struct xlnx_hdcp_timer_config *xtimercntr);
+void xlnx_hdcp_tmrcntr_reset(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number);
+
+#endif
diff --git a/include/uapi/linux/xlnxsync.h b/include/uapi/linux/xlnxsync.h
new file mode 100644
index 000000000..69fa1138f
--- /dev/null
+++ b/include/uapi/linux/xlnxsync.h
@@ -0,0 +1,179 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+
+#ifndef __XLNXSYNC_H__
+#define __XLNXSYNC_H__
+
+#include <linux/types.h>
+
+#define XLNXSYNC_IOCTL_HDR_VER		0x10004
+
+/*
+ * This is set in the fb_id of struct xlnxsync_chan_config when
+ * configuring the channel. This makes the driver auto search for
+ * a free framebuffer slot.
+ */
+#define XLNXSYNC_AUTO_SEARCH		0xFF
+
+#define XLNXSYNC_MAX_ENC_CHAN		4
+#define XLNXSYNC_MAX_DEC_CHAN		2
+#define XLNXSYNC_BUF_PER_CHAN		3
+
+#define XLNXSYNC_PROD			0
+#define XLNXSYNC_CONS			1
+#define XLNXSYNC_IO			2
+
+#define XLNXSYNC_MAX_CORES		4
+
+/**
+ * struct xlnxsync_err_intr - Channel error interrupt types
+ * @prod_sync: Producer synchronization error interrupt
+ * @prod_wdg: Producer watchdog interrupt
+ * @cons_sync: Consumer synchronization error interrupt
+ * @cons_wdg: Consumer watchdog interrupt
+ * @ldiff: Luma buffer difference interrupt
+ * @cdiff: Chroma buffer difference interrupt
+ */
+struct xlnxsync_err_intr {
+	__u8 prod_sync : 1;
+	__u8 prod_wdg : 1;
+	__u8 cons_sync : 1;
+	__u8 cons_wdg : 1;
+	__u8 ldiff : 1;
+	__u8 cdiff : 1;
+};
+
+/**
+ * struct xlnxsync_intr - Channel Interrupt types
+ * @hdr_ver: IOCTL header version
+ * @err: Structure for error interrupts
+ * @prod_lfbdone: Producer luma frame buffer done interrupt
+ * @prod_cfbdone: Producer chroma frame buffer done interrupt
+ * @cons_lfbdone: Consumer luma frame buffer done interrupt
+ * @cons_cfbdone: Consumer chroma frame buffer done interrupt
+ */
+struct xlnxsync_intr {
+	__u64 hdr_ver;
+	struct xlnxsync_err_intr err;
+	__u8 prod_lfbdone : 1;
+	__u8 prod_cfbdone : 1;
+	__u8 cons_lfbdone : 1;
+	__u8 cons_cfbdone : 1;
+};
+
+/**
+ * struct xlnxsync_chan_config - Synchronizer channel configuration struct
+ * @hdr_ver: IOCTL header version
+ * @luma_start_offset: Start offset of Luma buffer
+ * @chroma_start_offset: Start offset of Chroma buffer
+ * @luma_end_offset: End offset of Luma buffer
+ * @chroma_end_offset: End offset of Chroma buffer
+ * @luma_margin: Margin for Luma buffer
+ * @chroma_margin: Margin for Chroma buffer
+ * @luma_core_offset: Array of 4 offsets for luma
+ * @chroma_core_offset: Array of 4 offsets for chroma
+ * @dma_fd: File descriptor of dma
+ * @fb_id: Framebuffer index. Valid values 0/1/2/XLNXSYNC_AUTO_SEARCH
+ * @ismono: Flag to indicate if buffer is Luma only.
+ * Valid 0..3 & XLNXSYNC_AUTO_SEARCH
+ *
+ * This structure contains the configuration for monitoring a particular
+ * framebuffer on a particular channel.
+ */
+struct xlnxsync_chan_config {
+	__u64 hdr_ver;
+	__u64 luma_start_offset[XLNXSYNC_IO];
+	__u64 chroma_start_offset[XLNXSYNC_IO];
+	__u64 luma_end_offset[XLNXSYNC_IO];
+	__u64 chroma_end_offset[XLNXSYNC_IO];
+	__u32 luma_margin;
+	__u32 chroma_margin;
+	__u32 luma_core_offset[XLNXSYNC_MAX_CORES];
+	__u32 chroma_core_offset[XLNXSYNC_MAX_CORES];
+	__u32 dma_fd;
+	__u8 fb_id[XLNXSYNC_IO];
+	__u8 ismono[XLNXSYNC_IO];
+};
+
+/**
+ * struct xlnxsync_clr_err - Clear channel error
+ * @hdr_ver: IOCTL header version
+ * @err: Structure for error interrupts
+ */
+struct xlnxsync_clr_err {
+	__u64 hdr_ver;
+	struct xlnxsync_err_intr err;
+};
+
+/**
+ * struct xlnxsync_fbdone - Framebuffer Done
+ * @hdr_ver: IOCTL header version
+ * @status: Framebuffer Done status
+ */
+struct xlnxsync_fbdone {
+	__u64 hdr_ver;
+	__u8 status[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+};
+
+/**
+ * struct xlnxsync_config - Synchronizer IP configuration
+ * @hdr_ver: IOCTL header version
+ * @encode: true if encoder type, false for decoder type
+ * @max_channels: Maximum channels this IP supports
+ * @active_channels: Number of active IP channels
+ * @reserved_id: Reserved channel ID for instance
+ */
+struct xlnxsync_config {
+	__u64	hdr_ver;
+	__u8	encode;
+	__u8	max_channels;
+	__u8	active_channels;
+	__u8	reserved_id;
+	__u32	reserved[10];
+};
+
+/**
+ * struct xlnxsync_stat - Sync IP channel status
+ * @hdr_ver: IOCTL header version
+ * @fbdone: for every pair of luma/chroma buffer for every producer/consumer
+ * @enable: channel enable
+ * @err: Structure for error interrupts
+ */
+struct xlnxsync_stat {
+	__u64 hdr_ver;
+	__u8 fbdone[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+	__u8 enable;
+	struct xlnxsync_err_intr err;
+};
+
+#define XLNXSYNC_MAGIC			'X'
+
+/*
+ * This ioctl is used to get the IP config (i.e. encode / decode)
+ * and max number of channels
+ */
+#define XLNXSYNC_GET_CFG		_IOR(XLNXSYNC_MAGIC, 1,\
+					     struct xlnxsync_config *)
+/* This ioctl is used to get the channel status */
+#define XLNXSYNC_CHAN_GET_STATUS	_IOR(XLNXSYNC_MAGIC, 2, __u32 *)
+/* This is used to set the framebuffer address for a channel */
+#define XLNXSYNC_CHAN_SET_CONFIG	_IOW(XLNXSYNC_MAGIC, 3,\
+					     struct xlnxsync_chan_config *)
+/* Enable a channel. */
+#define XLNXSYNC_CHAN_ENABLE		_IO(XLNXSYNC_MAGIC, 4)
+/* Disable a channel. */
+#define XLNXSYNC_CHAN_DISABLE		_IO(XLNXSYNC_MAGIC, 5)
+/* This is used to clear the Sync and Watchdog errors  for a channel */
+#define XLNXSYNC_CHAN_CLR_ERR		_IOW(XLNXSYNC_MAGIC, 6,\
+					     struct xlnxsync_clr_err *)
+/* This is used to get the framebuffer done status for a channel */
+#define XLNXSYNC_CHAN_GET_FBDONE_STAT	_IOR(XLNXSYNC_MAGIC, 7,\
+					     struct xlnxsync_fbdone *)
+/* This is used to clear the framebuffer done status for a channel */
+#define XLNXSYNC_CHAN_CLR_FBDONE_STAT	_IOW(XLNXSYNC_MAGIC, 8,\
+					     struct xlnxsync_fbdone *)
+/* This is used to set interrupt mask */
+#define XLNXSYNC_CHAN_SET_INTR_MASK	_IOW(XLNXSYNC_MAGIC, 9,\
+					     struct xlnxsync_intr *)
+/* This is used to reset the last programmed slot */
+#define XLNXSYNC_RESET_SLOT		_IO(XLNXSYNC_MAGIC, 10)
+#endif
